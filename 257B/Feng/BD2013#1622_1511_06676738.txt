Tape Cloud: Scalable and Cost Efficient Big Data
 Infrastructure for Cloud Computing
 Varun S. Prakash+, Yuanfeng Wen?, Weidong Shi†
 Department of Computer Science, University of Houston, Houston, TX 77004, U.S.A
 Email: vsprakash@uh.edu+, wyf@cs.uh.edu?, larryshi@cs.uh.edu†
 Abstract—Magnetic tapes have been a primary medium of
 backup storage for a long time in many organizations. In this
 paper, the possibility of establishing an inter-network accessible,
 centralized, tape based data backup facility is evaluated. Our
 motive is to develop a cloud storage service that organizations
 can use for long term storage of big data which is typically
 Write-Once-Read-Many. This Infrastructure-as-a-Service (IaaS)
 cloud can provide the much needed cost effectiveness in storing
 huge amounts of data exempting client organizations from high
 infrastructure investments. We make an attempt to understand
 some of the limitations induced by the usage of tapes by studying
 the latency of tape libraries in scenarios most likely faced in the
 backing up process in comparison to its hard disk counterpart.
 The result of this study is an outline of methods to overcome
 these limitations by adopting novel tape storage architectures,
 filesystem, schedulers to manage data transaction requests from
 various clients and develop faster ways to retrieve requested data
 to extend the applications beyond backup. We use commercially
 available tapes and a tape library to perform latency tests and
 understand the basic operations of tape. With the optimistic
 backing of statistics that suggests the extensive usage of tapes
 to this day and in future, we propose an architecture to provide
 data backup to a large and diverse client base.
 I. INTRODUCTION
 The last decade has seen an explosion of data generated
 by individuals and organizations. For instance, the amount of
 video data captured by a single HD surveillance camera at
 30fs in 14 days requires 1TB storage space [1]. The number
 of CCTV cameras in UK alone is estimated to be 1.85 million
 [2]. There are more than one factors that organizations con-
 sider before investing in a certain type of storage infrastructure
 [3] [4]:(1)Longevity of the data or the intended period that
 the data needs to be stored or backed up; (2)Durability of the
 storage media which should have low susceptibility to physical
 damage and be tolerant of a wide range of environmental
 conditions without data loss; (3)Obsolescence of the storage
 technology or inversely, the technology’s ability to be easily
 updated based on the availability of newer solutions; (4)Cost
 or the over all expense of ownership including cost of pur-
 chasing and maintaining the necessary hardware, software and
 the media require; and (5)Capacity or the overall amount of
 data that need to be stored or backed up for the organization’s
 future use, (6)Data Criticalness or the importance of data that
 needs to be stored[5].
 Based on these factors, organizations requiring data storage
 solutions can either have in house data back up facility or
 rely on a service provider to carry out the task efficiently
 and economically [6]. However, there are many intermediate
 considerations that both players need to consider. Choice of
 storage media can be meagerly categorized on the basis of the
 capacity of data that need to be stored because although it is
 a nearly irrefutable factor, it has a close relationship with the
 overall costs. Storage service providers and organization will
 be forced to decide, in most cases, on a tradeoff between a
 smaller, high speed, expensive storage media and larger, low
 speed, inexpensive storage media [7]. A high initial investment
 for the operating hardware and software may force smaller
 organizations to discard the option although the cost of the
 media itself is very economical. Similarly, high media costs
 can create scalability issues for organizations that needs stor-
 age expansion on a regular basis even in a nearly automated
 environment.
 Magnetic tapes, which started off as a primary storage media
 decades ago, have been preferred for backup storage of data
 generated by organizations for a long time now. There has been
 a continuous development of quality, form factor, capacity and
 robustness of the storage cartridges[8]. It also continues to be
 a very economic type of storage media. The data stored on
 tapes has a property that justifies this choice; in that, this data
 may or may not be accessed in the near future. For instance,
 studies of enterprise file servers show that in a three month
 period, more than 90 percent of the data on the servers was
 never accessed [9]. Tapes serve its purpose well in situations
 where information was lost due to natural calamity, human
 error or system failures. For this reason, trained personnel are
 hired in order to maintain and service tape drives and tape
 libraries [10].
 Despite the advantages of tapes, there has not been an
 increase in its usage due to high initial investment of the
 operating hardware and software which force smaller or-
 ganizations to discard the option although the cost of the
 media itself is very economical [6]. This defeats the very
 purpose of affordable backup and limits the storage bandwidth
 significantly. Another important reason for the flat rate of
 increase in tape usage is its inability to promise high data
 rate transactions. The fact that tapes are linear data access
 media causes many processes waiting for data input/output
 from tapes to stay idle for longer periods accounts against
 tapes to be used for storing data which needs a more volatile
 storage environment as compared to backup or archival data.
 The main contributions of our work are as follows,
 • We propose and evaluate for the first time in literature,
 an exciting new alternative in tape based big data stor-
 2013 IEEE Sixth International Conference on Cloud Computing
 978-0-7695-5028-2/13 $26.00 © 2013 IEEE
 DOI 10.1109/CLOUD.2013.129
 541
 	

 

 

 

 
 

 
 

  
!"
 Fig. 1. Tape Cloud is a cloud storage service that uses magnetic tapes as
 the main storage media to store unstructured and big data unlike most of the
 commercial cloud storage solution available today.
 age model that allows organizations to benefit from a
 large storage bandwidth without having to invest in the
 infrastructure itself. The crux of this approach lies in the
 optimization of the usage of the most affordable media.
 This allows storage service providers to focus on the
 technicalities such as using affordable media, required
 infrastructure and man power to handle them and permit
 client organizations to concentrate monetary expenditures
 away from investing on the fundamentals of data backup
 and storage.
 • The design of a cloud based storage framework provided
 as a service which implements the necessary middleware
 for seamlessly integration of large scale multi-user tape
 libraries with the cloud model is presented. The tape
 cloud is designed to work autonomously or in conjunction
 with the current cloud infrastructure.
 • An attempt to optimize performance of the tape cloud
 with a multi-level hybrid storage model where hard disks
 are employed for providing cache spaces for data to
 be stored or retrieved from tape libraries(one LTO 5
 tape library can provide the storage capacity from 72TB
 to more than 1PB) is made and the observations are
 reported.
 • Solutions are devised to support efficient multi-user ac-
 cesses to tape cloud using optimized hardware config-
 urations and combinations, and high level scheduling
 approaches which are evaluated for their performance.
 • The proposed tape cloud framework points to a new
 direction for creating service oriented, cost effective,
 massive scale infrastructure to meet the growing storage
 challenge in the coming era of big data enabled industries
 and research.
 The paper is organized as follows. A study of economic
 impacts of tapes is done in section II. The system design for
 the tape cloud along with operational details are provided in
 section III. We propose models for testing hypotheses on I/O
 performance on tapes, details of which is discussed in section
 IV and results of which have been posted in section V. A note
 on works related to our research is given in section VI and
 the conclusion and our future plan is given in section VII.
 II. BACKGROUND
 Magnetic tapes have been in active usage over the last 60
 years, a time period that is comparatively high for storage
 technology in the growing IT age. This has been possible
 due to the continuous improvement in the quality, capacity,
 durability and areas of application of tapes. The dominant
 motivation to use tapes, however, arises from the fact that
 tapes are highly economical and ideal to store large amount
 of data which may or may not be useful to the organization in
 the near future. The Linear Tape-Open is a set of standards that
 directs development and manages licensing and certification of
 media and mechanism manufacturers.The standard form-factor
 of LTO technology goes by the name Ultrium, the original
 version of which was released in 2000 and could hold 100
 GB of data in a cartridge.
 LTO version 6 released in 2012 can hold 2.5 TB in a
 cartridge of the same size as its predecessors. Another very
 important storage media that has been around for a long time
 is the Hard Disk Drive. The similarity between hard disk and
 tapes is only the principle of using magnetic material to store
 data, hard disks rose to the scene as a better option to store
 data that needed faster retrieval. The cost of operating hard
 disks has been an attractive tradeoff to make to the cost of the
 media itself. This also has proved to be the “tape killer” as the
 hardware required to operate tapes and personnel required to
 maintain them costs unreasonably high. A comparative study
 performed let us see that the monetary expenses in the form
 of initial investment involved create an obstruction for many
 small organizations to have backup solutions on tapes despite
 its competitive cost advantages, results of which have been
 shown in table I.
 TABLE I
 COMPARISON OF COSTS OF STORING UNIT DATA ON DIFFERENT
 STORAGE MEDIA
 Solid State Hard Disk Tapes
 $/TB/yr % total $/TB/yr % total $/TB/yr % total
 Media 19456 96.0 220 25.1 23 5.4
 Capital 197 0.009 163 18.6 155 36.4
 Maintenance 152 0.007 176 20.1 88 20.7
 Facilities 221 0.010 118 13.5 56 13.1
 Personnel 234 0.01 197 22.5 103 22.4
 Total 20260 874 425
 
 
 
 
 
    
 	
 

 	
 
 
 
 
 	
 
 
 	
 
 
 
 
 	
 
 
		
  !"
 	#	
 $%&$ "!'()
 *+&,(-.!(
 		-!
 ,/ !-
 		. 0
 Fig. 2. Operating Temperatures of
 different media
 --
 "-
 -
 . 
 1
 ."
 
 .
 -
  
 -
 "
 
 
 
 
 	 
  !"
 $%&
 $ "!'()
 *+
 &,(-.!(
 		
 -!
 
 ,/ !-
 		
 . 0
 (
 	
 
 2
 3
 
 	
 
 4
 
 
 5	
 
 
 	&	
 $5	
 62
 Fig. 3. Unused State Energy Con-
 sumption of different media
 Another contributor to costs is the overall energy consump-
 tion of storage media and the associated infrastructure. Power
 inefficiencies in storage systems can arise at two stages, when
 the system uses a lot of energy even in the idle state (Figure 3)
 and when the operating temperatures (Figure 2) of the system
 results in the need for external conditioning systems. A study
 performed with a tape drive and six hard disk drives of
 542
#$%&'&(
 #%%)*
 #$(*++++
 #$$+$(+
 #+
 #,+++++
 #%+++++
 #*+++++
 #-+++++
 #$++++++
 #$,+++++
 #$%+++++
 #$*+++++
 ./  )0" ./  )0"
 Fig. 4. Tape and Disk: Acquisition and Energy Cost from INSIC Study [11].
 The five year period energy cost of disks is comparable with the acquisition
 cost of the tapes with equal storage capacities.
 different specification and manufacturer reveals that tape is
 much more power efficient than hard disk when used on a large
 scale such as in data centers. The fact that hard disks need to be
 electrically powered for operation is a mammoth disadvantage
 over tapes which is a non powered static storage and has very
 low power consumption per unit data [6]. Figure 4 shows
 the relative expenditure of acquiring the two different kinds
 of media as compared to the operational costs in terms of
 power required for their usage. From the perspective of the
 storage infrastructure service provider, scalability becomes
 very expensive [12] [13] [14].
 The infrastructure for using tapes which, in most cases,
 is the library, introduces a new kind of expenses in the
 form of delays which need to be incurred due to the use
 of robotics inside the device. The delay caused can create a
 slowdown of I/O processes also affecting business processes of
 the organization [8]. We study some of these device specific
 delays. We conducted experiments over commercial LTO-5
 tape libraries (e.g., Tandberg tape library[15]).
 Figure 5 shows the diagrammatic representation of the
 simple tape library (the number of tape slots is a parameter that
 varies among different tape library products). The numbered
 slots are tape cartridge holders. The robotic cart runs on the
 rail in front of the tape driver and helps load the tapes into
 the driver. To complete our analysis, we have made a multi
 trial recording of delay of the various operations performed
 within the library. We can understand the basic principles
 of operation and create a time profile for these operations
 which helps us in creating faster and more efficient hardware.
 Table II in section V shows the delay incurred in moving tape
 cartridges in the numbered slots to the drive and back to the
 slots after performing the operation. The average time taken
 for the transport of cartridges in both cases is more than a
 minute. Once the tape is in place, it takes nearly 30 seconds for
 it to load and be ready to read or write. The tape transportation
 cart has an upward path time of 2.6 seconds and a total end
 to end path time of 7.4 seconds. The robot usually performs
 both together during the load or unload operation from a slot
 at the very end of the library so time can be saved. Based on
 the numbers, we can get a clear time profile about the tape
 library operations. One of our design objectives is to reduce
 
 
 
 
 
 
 
 	
 
 

 
 
 
 
 
 
 
  !" 
"#$!%!&'()
 "'"
 '*+,
 Fig. 5. Representation of the Tandberg T24 Tape Library. At the bottom
 is the Tape library showing parts with numbers mapped to the representation
 above it. Bottom right is a single LTO5 tape cartridge.
 time spent on the tasks such as movement of tapes from the
 slots to the drive.
 III. SYSTEM DESIGN
 Figure 6 shows a bird’s eye view of the tape cloud architec-
 ture. Our effort in creating a cloud storage with a media that is
 known to be at the bottom of the ladder of performance, calls
 for alteration of hardware assembly and design. In order to
 get best results, we create a design for the hardware that cases
 the tapes along with special instructions for multiple reader
 rewinder sets aimed to keep the setup cost effective. Although
 the unit operations such as writing data onto tapes remains
 similar, we consider some augmentative enhancements that are
 specific to our case. The software of the tape cloud can be
 effectively termed as a middleware which operates between
 faster yet smaller hard disk buffers and comparatively slower
 yet larger tape backend storage. This middleware needs to
 function as an agent arbitrating various components in order to
 reduce the overhead caused by using a slower backend media
 [16] as shown in Figure 9. It also performs various other tasks
 such as data set segmentation, scheduling, encryption, load
 balancing and management of database containing the meta
 data and block IDs of data stored in tapes. Figure 8 provides
 an understanding of the middle ware that operates between the
 distributed file system and tape storage. This serves as an agent
 aiding the distributed file system in overcoming the latencies
 of using a slower media and also serves as an abstraction in
 clouds using hybrid storage infrastructure.
 A. An Abstraction for Tape Library Hardware
 In order to make the workflow seamless and application
 independent, special considerations about the hardware setup
 and configuration need to be performed. The massive scale of
 our focus encompasses multiple drives, high speed robotics
 that seek, grab and load tapes to drivers at high speeds and
 hazard resistant tape storage space. To make data retrieval and
 deposition more efficient, we consider a conventional driver
 to be split into two parts. One part rewinds the subsequent
 scheduled tapes to the correct position and the other part is
 involved in reading and writing into tapes (the read and write
 head also has a rewinding functionality to perform small and
 quick seeks so it could be the addition of a rewinder in the
 543
"1	  21	
 

 34
 35 !
 
!
 	
 !
 
 Fig. 6. Implementation Architecture of Tape Cloud. The arrows represent
 the direction of flow of data.
 system). This way we can pipeline the rewinding process and
 isolate the rewind latency from the overall read operation.
 B. Multi Tier File System
 FUSE [17] is a framework to help develop customized file
 system. FUSE module has been officially merged into the
 Linux kernel tree since kernel version 2.6.14. FUSE provides
 35 interfaces to fully comply with POSIX file operations.
 We design a file system using FUSE used at different tiers
 in the architecture. The file system is monolithic but logi-
 cally distributed and staged based on functionality as shown
 Figure 7. Figure 8 provides a block representation of the
 filesystem which is an important part of the middleware.
 The collection servers (the PUT-Collection servers and GET-
 Collection servers) implement modules which acquire client
 data to be written to or retrieved from tapes. Based on client
 specific policies, the data to be stored on tapes is encrypted and
 segmented. Each of the segments are identified and accounted
 in local databases. Similarly, to retrieve data from tapes, the
 filesystem queries the local databases and requests particular
 blocks from tapes and converts it to the pristine data. The
 Tape Interface Machines (TIM) are networked to the collection
 servers and blocks of data are sent and received via high speed
 connections. The load balancing server manages a workload
 based scheduling system in order to efficiently distribute data
 to various TIMs to avoid IO bottle necks. The filesystem is
 highly customizable in the sense, data from clients can be
 blocked and stored based on preference chosen by the clients.
 For example, video surveillance data, should the client be able
 to obtain data by the hour, must be handled differently as
 compared to be able to obtain data by days. So the collection
 servers are responsible to block these data in a manner easy
 for retrieval.
 C. Load Balancing
 The duties of the Load balancing servers are by far the
 most critical in the system. It is involved in many services to
 all other entities and ensures that there is no data clogging at
 a certain point in the network. First of all, it acts as a contact
 point to all the clients who wish to store or retrieve data from
 the tapes. After the client is authorized, the load balancing
 
   
 !
 	

 6	
 
 !
 
 
 	
 
 	
 !

 "
  
 6
 
 78
 6
 
 3

 	,
 	$
 Fig. 7. Stages and functions of each stage of the filesystem for Tape Cloud.
 Although distributed by functionality, the filesystem is monolithic across the
 storage system.
 	
 		
 			
 	 	
 	
 !"	 	##
 $""	 	"
 	"	
 %
  	&'	
 		
  		
 (
  "
  	
 "
  	"
 	$!
 )"	*
  		
 
 +	"	
 %
 Fig. 9. The block representation of the middleware on the basis of roles and
 responsibilities within the system.
 server requests a memory allocation on the collection servers
 where the user data is going to be collected. It also creates and
 manages database entries for the blocks that are created and
 manages redundancy to avoid data loss and increase availi-
 bility. The load balancing servers connect and talk directly
 with the metaserver and the central database which contain
 information about the load on the Tape Interface Machines
 and currently serviced clients.
 D. Data Handling
 a) Data Acquisition: The system’s data collection from
 the user can be performed in two main ways. Data sets of
 smaller sizes which can be uploaded directly through the
 internet are stored in the Collection servers. The client side
 has an application that requests a data upload session from
 the Load balancing server which then allocates the required
 amount of space in the collection server and initiates the client
 to directly upload its data to the specified location. Another
 data acquisition protocol is designed for very large data sets
 where data is received in the form of storage media units itself.
 The collection servers, in this case, provide a docking interface
 functionality where the media can be mounted directly on the
 servers and the extracted data be treated in the same way as
 uploaded data.
 544
3

  
 92	":
 3
 	
 3
 3
 "5


!
 ;5
 	

 9	
 
 9
  
 9
  
 9
  
 9
  
 
 ;

 
 ;

 
 ;

 
 ;

 !
 
43
 "5


!
 9	
 
 9	
 3
 
 3

8!
 Fig. 8. Logical diagram of a tape cloud node to show the relationship between the various functional units within the FUSE enabled tape cloud infrastructure.
 $
 )
 &
 ,
 *
 $+
 (
 '
 $$
 %
 -
 $,
 $
 )
 &
 ,
 *
 $+
 (
 '
 $$
 %
 -
 $,
 ;".
 $<	"$
 $+!6:	
 ;".
 '<	",
 $+!6:	
 ;".
 &<	"$
 $+!6:	
 =;"
 (<	"$
 $+!6:	
 ;".
 )<	",
 $+!6:	
 $
 )
 &
 ,
 *
 $+
 (
 '
 $$
 %
 -
 $,
 $
 )
 &
 ,
 *
 $+
 (
 '
 $$
 %
 -
 $,
 ;".
 $<	"$
 $+!6:	
 ;".
 '<	",
 $+!6:	
 ;".
 &<	"$
 $+!6:	
 =;"
 (<	"$
 $+!6:	
 ;".
 )<	",
 $+!6:	
 	"<$ 	"<,
 	"<,	"<$
 !	

 .	

 ;
4=>

 ;
4=>

 Fig. 10. The Closest Process First Schedule: before scheduling(top) and after
 scheduling(bottom)
 b) Data distribution to Tape Interface Machines (TIM):
 Once the data has been segmented at collection servers, it is
 distributed to the TIM computers. Data is distributed to a TIM
 based on its current IO queue depth. Overloading a single TIM
 computer with more than required write requests would cause
 a long latency in the read process as they tend to pile up at
 a single TIM computer. A record or a map of these segments
 are made and stored in the meta server.
 c) Distributed Tape Write/Read: Like any other cloud
 service, the architecture is complex with a large number of tape
 drives. In order to increase the IO bandwidth, parallel reading
 and writing of data into multiple tapes can be envisioned. This
 consideration, however, needs to be analysed from more than
 one perspective. The larger the number of subdivisions, the
 larger the latency induced by seeking for the data. Yet, a huge
 single read or write to tape can cause an increase in the waiting
 time of other requests. Many task scheduling algorithms have
 been designed[18] to improve performance of IO in tapes and
 better utilize drive resources, but little has been done with the
 latest LTO5 tapes drives. Based on the latency test results of
 the tape device, we see that the seek time of the robotics takes
 a considerable longer time than reading or writing of data to
 the disk. In order to improve time efficiency, we schedule the
 I/O requests such that each request spends the least amount
 of time performing seek operations and longer periods of read
 or write operations.
 E. Closest Process First Scheduling
 From the latency analysis, we can see that the the largest
 time consumption is by the seek, grab and load into driver
 operations. In order to make the system time efficient, we
 need to let the system spend most of the time reading from
 or writing data onto the tapes. For this reason, we have to
 schedule read and write operations in a way that the next
 operation to be performed is on a tape that is either the same
 tape or one that is closest to the current tape. The scheduling
 algorithm is applied at the Tape Interface Machines after it
 obtains the data from the collection servers or after it receives
 a read read request from the load balancer. Figure 10 has a
 representation of how the scheduling is done at the TIMs. We
 evaluate our design and latency model for a scheduler called
 Closest Process First that does exactly this. The algorithm is
 shown below.
 Algorithm 1 Closest Process First Scheduler
 Input: m tapes: ?t = {tape1, tape2, · · ·, tapem}
 n requests: ?r = {request1, request2, · · ·, requestn}
 Output: Drive/Rewinder Seek Path
 1: New Request reqx
 2: Location value of reqx = Lx = Infinity
 3: Record current location of the drive/rewinder Lc
 4: while true do
 5: wait for a request
 6: if new request reqx arrives then
 7: Get location Lx of process to be performed
 8: for i ≤ n do
 9: if Lx is closer to Lc than Li then
 10: place reqx before i in the schedule queue
 11: else
 12: move to next request Li + 1
 13: end if
 14: end for
 15: end if
 16: end while
 IV. SYSTEM MODELS
 In this section, we describe several models to analyze
 system response time. The assumptions and notations for the
 models are as follows.
 • Tseek(i) is the time to move the tape reader to the correct
 tape for the ith request, and load the tape to the reader.
 545
• The data users request is loaded by blocks. Once a
 block is loaded, users can start processing the blocks, for
 example, applying the video analysis on the loaded block
 of videos, while the next block can be loaded simultane-
 ously. The size of each block is chose to minimize the
 average response time. The size of a block is denoted as
 BLK.
 • Twind(i,j) is the time to wind the tape fast forward to the
 first byte of the jth block to read for ith request.
 • ¯Twind(i) is the average time to wind the tape fast forward
 to the first byte read for ith request.
 • Ttransfer(i,j) is the time to either read the jth block
 frome the tape for the ith request.
 • ¯Ttransfer(i) is the average time to either read a block
 from the tape for the ith request.
 • Rtransfer is the data transfer rate of the tape;
 Ttransfer = SizetransferRtransfer ; Rtransfer varies from 30MB/s
 to 160MB/s [19].
 • Rconsume(i) is the data consumption rate of the ith
 request, which describes how fast users can finish pro-
 cessing the data that they request.
 A. Response Time Model
 The response time of a user request is defined as the length
 of the period starting from when the request is released and
 ending at the point all the data are fully consumed by the user.
 For example, if one user requests 200 GB data given Tseek =
 10s, Twind = 4s, Ttransfer = 1, 400s, Tconsume = 20, 000s,
 then the response time is 21,414s. If multiple requests are
 queued and processed in the order of their arrival only after the
 only after the completion of the previous request, the average
 response time would be very long. As discussed before, the
 data consumption rate is usually much smaller than the data
 transfer rate, therefore, it is beneficial to read data partially in
 order to reduce the average response time for each request.
 B. System I/O Model
 The I/O time based on the experiments conducted on the
 tape library has been analyzed to be the sum of three major
 components: TI/O = Tseek + Twind + Ttransfer.
 Tseek is proportional to the distance between the tape drive
 and the location of the tape in the library that the robot has
 to cover. Seeking process is very time consuming. Thus, we
 have to minimize the seek time by reducing both the number
 of the seeks and the time for moving the tape reader. For the
 first purpose, the system chooses a large block size. For the
 second purpose, the system has to use an optimal schedule for
 processing the requests.
 Block Size: The data stored on the tape are logically
 organized as blocks. A block is considered the minimum unit
 when reading from or writing to the tapes. The block size
 can not be either too large or too small. If the block size
 is too large, it will block the following requests or make
 them to wait much longer. On the other hand, if it is too
 small, frequent seeking to perform the next IO will degrade
 performance. Generally, the block size is set based on the
 fact that the transfer time of one block should be larger (or
 much larger) than the average seek time. Here is an example
 to demonstrate how the block size is determined. Assume
 that there are two requests in the queue, each of them asks
 to load D GB data. Since the system uses a block size of
 BLK, after the jth block is loaded from the tape to the
 hard drive, it will take BLK/Rconsume units of time to
 process. During this period, the reader can switch to process
 the other request to load the kth block, which will take
 Tseek(2)+Twind(2, k)+Ttransfer(2, k) to load the kth block
 for that request. Then the reader switches back, and loads
 the (j + 1)th block for the first request, which will take
 Tseek(1) + Twind(1, j + 1) + Ttransfer(1, j + 1). Ideally, if
 BLK/Rconsume ≥ Tseek(2)+Twind(2, k)+Ttransfer(2, k)+
 Tseek(1)+Twind(1, j+1)+Ttransfer(1, j+1), the first request
 will not even notice that its data loading process has been
 interrupted. Generally, if there are w requests waiting in the
 queue, as long as
 BLK
 Rconsume(i) ≥
 ∑w
 p=1(Tseek(p) + ¯Twind(p) + ¯Ttransfer(p)),
 the ith request will be processed smoothly without noticing
 that the data loading process has been interrupted.
 C. Multiple Reader Model
 If there are more than one readers available, the scheduling
 problem is known to be NP-Hard. Since it is impossible to
 find the optimal solution, we propose a partitioned solution for
 multiple reader scheduling as shown in Alg.2. In this design,
 each reader is assigned to be responsible for a specific set of
 tapes. Each reader only stops at the tapes assigned to it and
 skips the others. For example, if there are two readers and 100
 tapes, the first reader may take care of the first 50 tapes while
 the second reader are in charge of the rest. The tapes in one
 set should be physically close to each other.
 However, this design may result in “hot spot”, that is, one
 reader may be busy all the time while others are idle. To solve
 this, the system allows the idle readers help to process the
 requests, but have to be back to their own duties if requests
 to their assigned tapes arrive.
 Algorithm 2 Partitioned Task Scheduling Algorithm for Mul-
 tiple Tape Readers
 Input: m tapes: ?t = {tape1, tape2, ·, tapem}
 n tape readers: ?tr = {reader1, reader2, · · · , readern}
 Output: Online Schedule for each coming requests
 1: Assign the m tapes to n readers. Each reader will take care at
 most m/n tapes which are close to each other.
 2: Store the assignments to the global schedule manager.
 3: while TRUE do
 4: Wait for the next request, req.
 5: Get the meta-information of the req from the database and
 find the tapeid for req.
 6: Forward req to the reader, which is in charge of the requests
 for tapeid.
 7: Schedule req at the reader locally using elevator schedule
 algorithm.
 8: end while
 546
TABLE II
 TANDGERG T24 LOAD AND UNLOAD DELAYS
 Type From To Motion(sec) Load(sec) Type From To Motion(sec) Load(sec)
 LOAD 1 Drive 62.4 33.3 UNLOAD Drive 1 61.6 30.1
 LOAD 2 Drive 62.9 31.9 UNLOAD Drive 2 62.3 30.6
 LOAD 3 Drive 64.06 32.6 UNLOAD Drive 3 62.26 30.3
 LOAD 4 Drive 65.2 34.6 UNLOAD Drive 4 64.0 30.3
 LOAD 5 Drive 62.42 34.0 UNLOAD Drive 5 61.3 30.9
 LOAD 6 Drive 63.3 33.6 UNLOAD Drive 6 61.76 31.01
 LOAD 7 Drive 64.2 31.3 UNLOAD Drive 7 62.22 30.1
 LOAD 8 Drive 65.45 33.9 UNLOAD Drive 8 63.8 29.62
 LOAD 9 Drive 61.8 34.0 UNLOAD Drive 9 60.7 30.3
 LOAD 10 Drive 62.3 31.6 UNLOAD Drive 10 61.4 30.34
 LOAD 11 Drive 63.7 32.23 UNLOAD Drive 11 61.97 33.9
 LOAD 12 Drive 64.02 33.8 UNLOAD Drive 12 63.6 32.59
 Average ? ? 63.64 33.1 Average ? ? 62.37 31.21
 
 
 .
 
 -
 
  -   1 " . 
 !#
 	
 
 	
 
 )	
 
 
 
 
 	
 
 
 

 	
 

 
 
 %578	/%
 Fig. 11. Average Response Time under Different Block Sizes
 V. EVALUATION
 We simulate a tape cloud node having 100 tapes and up to
 10 tape readers used simultaneously to evaluate our design.
 The parameters used in the simulation are measured from an
 actual Tandberg LTO-5 tape library [15], [19]. The physical
 capacity per cartridge is 1.5 TB; the data transfer rate is 140
 MB/s; the rewind speed is 10 meters/sec; the tape length is 846
 meters; the cartridge memory is 8 KB; the load and unload
 delays are summarized in Table II.
 In the first set of experiments, the average response time
 under different block sizes are studied. Each user request
 requires 1TB tape data in total. In this experiment, the number
 of tape reader is one. User requests are randomly generated.
 Data is read and processed by blocks. The consumption rate is
 20MB/s. As discussed in section IV-B, once one block of data
 is loaded, the tape reader switch to serve the next request.
 As shown in Figure 11, when the block size is between
 50GB and 100GB, the average response time is minimized.
 
 
 
 
 
 
 
 
   . " - 
 !#
 	
 
 	
 
 )	
 
 
 
 
 	
 
 
 

 	
 
 

 
 
 9:	;	<
 **
 ,*
 Fig. 12. Average Response Time under Different Scheduling Algorithms
 
 
 
 
 
 
    . . " " -
 !#
 	
 
 
 	
 
 )	
 
 
 
 
 	
 
 
 

 	
 

 
 
 9:	;	<
 0/565,
 0/565,
 Fig. 13. Average Response Time Using Globally Partitioned Workspace
 
 
 
 
   . " - 
 !#
 	
 
 	
 
 )	
 
 
 
 
 	
 
 
 

 	
 
 

 
 
 9:	#	
 ,	
 	5
 3	
 	5
 Fig. 14. Average Response Time Using Partitioned and Unpartitioned
 Scheduling Algorithms
 In addition, if the block size is too large, the average response
 time will increase significantly. The average response time
 with the block size of 500GB is almost 1.5 times of that with
 the block size of 50GB.
 Second, the average response time results under two differ-
 ent scheduling algorithms, i.e., First Come First Serve (FCFS)
 and Closet Process First (CPF), are shown in Figure 12.
 Up to 80,000 requests are generated and their response times
 are measured. According to the results, the CPF scheduling
 algorithm saves around 80% of the response time.
 When handling multiple tape drives, we either have the
 option of creating an any-tape to any-drive environment where
 a drive has access to any tape in a limited system or we
 can partition the system such that a drive services only a
 specific set of tapes, referred to as Global partitioning. Global
 partitioning also avoids uncontrolled tape seek time Tseek
 which, from our study, can be seen to be most expensive.
 Employing global partitioning decreasing the average response
 time for requests as shown in Figure 13.
 We also found that the scheduling algorithms get affected
 when we use global partitioning. The average response time
 results when using multiple readers are evaluated in Figure 14.
 One or more (up to 10) tape reader are enabled to work simul-
 taneously. Two different scheduling algorithms are compared,
 the partitioned algorithm shown in Alg.2 and the unpartitioned
 algorithm, which allows each reader to serve any tapes in
 the library. The results are presented in Figure 14. As the
 number of working tape readers increase, the average response
 time is reduced accordingly. However, when the number of
 tape readers goes beyond 5, both scheduling approaches show
 547
little changes in response time. This is because under the
 current workload, 5 readers are able to serve the requests
 efficiently. Even if more readers are added, some of them will
 be idle for most of the time. Figure 14 indicates that the
 partitioned scheduling algorithm performs much better than
 the unpartitioned one.
 VI. RELATED WORK
 In spite of the potential to be the most scalable and cost
 efficient solution for meeting the growing storage demand,
 cloud based services leveraging tapes have received little or
 almost no attention from the academic research community.
 To our best knowledge, our paper is the first and the only
 one that provides a complete tape based service model and
 integrates large scale tape infrastructure with the cloud. In
 industry, there has been a long history of improving the
 existing and designing new tape libraries for enterprise cus-
 tomers (e.g., Hewlett-Packard StorageWorks ESL/EML, IBM
 TS3400/TS3500, Quantum Scalar, Oracle StorageTek). These
 tape libraries are designed as end-user products for the enter-
 prise users to own their own on-premise tape infrastructures.
 Our efforts represent an opposite off-premise cloud based
 direction with the objective to provide low cost tape based
 storage to the users without the requirement to own a private
 tape infrastructure.
 VII. CONCLUSION AND FUTURE WORK
 We present and evaluate a design to provide a cloud storage
 system with tape media as backend which is a first in its kind
 which implements centralized data storage facility using tape
 media. We recognize some of the barriers of using tape tech-
 nology both economically and operationally. Some solutions
 and ideas are evaluated in order to reduce the consequences
 faced by these barriers and provide a smooth performance in
 data storage and retrieval process. From the results obtained in
 design evaluation, we can see that there is an increase in I/O
 throughput using the scheduling and parallel I/O models that
 are discussed in the paper, thus paving way for the possibility
 of having large scale operations using these techniques.
 The speed of data retrieval is critical for a cloud service.
 Tape, by nature, has been used for backing up data and creating
 archives which means that request for data stored on tapes
 arrives with a probability that varies well below 1 as compared
 to other day-to-day cloud storage. In this case, the tape cloud
 data centers provide sufficient security and safety against theft,
 fire and natural calamity as compared to on site backing up.
 One of the most exciting aspects of our research is the
 opportunities it presents for future work. Understanding the
 economics of revisiting a legacy system to solve the data
 explosion problems of today requires an overhaul of nearly
 every piece of technology associated with the storage system.
 An interesting study to perform is the relation between the
 number of media units and the number of drivers used in large
 scale deployments. Also we would like to implement an easy
 and direct interfacing system which allows the tape cloud to
 be connected directly to major cloud storage providers such
 as Dropbox or Google Cloud and see the effects of using
 tape media in conjunction with the storage media used by
 other storage providers. Other plans of extension include the
 evaluation of tape cloud in applications which require much
 higher I/O throughput.
 VIII. ACKNOWLEDGEMENT
 We would like to thank the reviewers for their comments
 which significantly improved the paper. This research is par-
 tially supported by the National Science Foundation under
 Award Number CNS 1205708. The conclusions contained in
 this document are those of the authors and should not be
 interpreted as representing the opinions or policies of NSF.
 REFERENCES
 [1] Seagate, “Video surveillance storage: How much is enough?”
 [2] “County of cameras: Cheshire constabulary aims to count every private
 camera in the county,” CCTV Image Online.
 [3] D. Ally, “Choosing the appropriate storage media to collect video-based
 evidentiary data,” Digital Ally Inc., Tech. Rep., 2009.
 [4] W. Purvis, “The hot new storage technology for 2011 is tape?” Research
 Note, Data Mobility Group, March 1 2011.
 [5] J.-H. Lee, T. Feng, W. Shi, A. Bedagkar-Gala, S. K. Shah, and
 H. Yoshida, “Towards quality aware collaborative video analytic cloud,”
 in IEEE CLOUD, 2012, pp. 147–154.
 [6] D. S. Rosenthal, D. Rosenthal, E. L. Miller, I. Adams, M. W. Storer,
 and E. Zadok, “The economics of long-term digital storage,” in The
 Memory of the World in the Digital Age: Digitization and Preservation,
 Sep. 2012.
 [7] I. Foster, “Globus online: Accelerating and democratizing science
 through cloud-based services,” Internet Computing, IEEE, vol. 15, no. 3,
 pp. 70 –73, May-June 2011.
 [8] A. J. Argumedo, D. Berman, R. G. Biskeborn, G. Cherubini, R. D.
 Cideciyan, E. Eleftheriou, W. Ha¨berle, D. J. Hellman, R. Hutchins,
 W. Imaino, J. Jelitto, K. Judd, P.-O. Jubert, M. A. Lantz, G. M.
 McClelland, T. Mittelholzer, C. Narayan, S. ¨Olc¸er, and P. J. Seger,
 “Scaling tape-recording areal densities to 100 gb/in 2,” IBM J. Res.
 Dev., vol. 52, no. 4, pp. 513–527, Jul. 2008.
 [9] J. Jackson, “Most network data sits untouched,” Government Computer
 News, July 2008. [Online]. Available: http://gcn.com/Articles/2008/07/
 01/Most-network-data-sits-untouched.aspx
 [10] F. Moore, “Tape: New game. new rules - tape re-architects for 21st
 century data explosion.” [Online]. Available: http://www.horison.com/
 Tape21stCentury.pdf
 [11] D. Reine, “In search of the long-term archiving solution tape
 delivers significant tco advantage over disk,” The Clipper Group,
 December 23 2010. [Online]. Available: http://www.clipper.com/
 research/TCG2010054.pdf
 [12] IDC, “Worldwide storage in the cloud 2010-2014 forecast: Growth in
 public cloud storage services continues as firms decapitalize it.”
 [13] “International magnetic tape storage roadmap,” INFORMATION STOR-
 AGE INDUSTRY CONSORTIUM, nov 2011.
 [14] “Two thirds of disk-only users look to add tape into storage infras-
 tructure,” Storage Newsletter, March 2008. [Online]. Available: http:
 //www.storagenewsletter.com/news/tapes/lto-programsurvey-tape-disk
 [15] “Tandberg storagelibrary t24.” [Online]. Available:
 http://www.tandbergdata.com/us/index.cfm/products/tape-automation/
 storagelibrary/storagelibrary-t24/
 [16] I. Drago, M. Mellia, M. M. Munafo, A. Sperotto, R. Sadre, and A. Pras,
 “Inside dropbox: understanding personal cloud storage services,” in
 Proceedings of the 2012 ACM conference on Internet measurement
 conference, ser. IMC ’12, 2012, pp. 481–494.
 [17] “Fuse filesystem project.” [Online]. Available: http://fuse.sourceforge.
 net/
 [18] O. Sandsta and R. Midtstraum, “Improving the access time performance
 of serpentine tape drives,” in Data Engineering, 1999. Proceedings., 15th
 International Conference on, 1999, pp. 542–551.
 [19] “Linear tape-open.” [Online]. Available: http://en.wikipedia.org/wiki/
 Linear Tape-Open
 548

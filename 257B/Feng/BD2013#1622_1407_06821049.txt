Mining User Similarity Based on Users Trajectories 
Wei Li
 liw@buaa.edu.cn
 Jiangtao Jiang
 jiang212999@126.com
 GuoJun Li
 aluenkinglee@gmail.com
 School of Computer Science and Technology
 Key Lab. of Beijing Network Technology
 Beihang University
 Beijing, P.R.China
 Abstract-In this paper, we first extract latent topics
 of users’ checkin by topic model. In order to learn the 
geographic features of topics, we analyze the 
distribution and correlation of the venues of each topic.
 Based on the assumption that each latent topic, to some 
extent, reflects some users’ habits or behavior patterns 
in certain area, we use the latent topics to mining user 
similarity. When use the latent topics to mining user 
similarity, we first calculate the entropy of each topic, 
and then use it to analyze which topics will be salient 
when calculating user similarity. And we define two sets 
of topic. One is common topic set. The other is 
variability topic set. Subsequently, we propose two 
models to calculate user similarity. One is single-layer 
model. The other is two-layer model. Two-layer model is 
improved to single model. Finally, we use the proposed 
models to calculate user similarity in all topic set, 
common topic set and variability topic set. And then we 
verify our models by experiment.
 Keywords-Location-based social networking; user similarity; 
topic modeling; check-in; geographic feature. 
1. INTRODUCTION
 Currently, with the rapid development of the fourth generation 
mobile communication networks, as well as map services and 
embedded GPS module interface supports for smart phone,
 making it easy to identify their location for mobile users which 
prompts more and more users use location-based social 
networks(LBSN)[6] and share their location information. By 
discovering or creating venues, user can checkin and leave tips or 
comments at where he visits. Many LBSN, such as Foursquare, 
Facebook Places and Sina WeiBo, etc, have taken different 
mechanism to attract users and encourage them to checkin and 
share.
 Recently, some researchers have begun to focus more on the 
large amount of geo-spatially tagged data being produced from 
users’ check-ins, for instance, approaching questions of societal-
 level interest in an entirely data-driven manner[6][7][8][9], finding 
patterns of human mobility[10][11][12], predicting friendship on 
social networking sites based on location data[13][14], and better 
understanding different aspects of cities[15][16]. 
In [7] and [17], authors use topic-modeling to get geographic 
topics by using users’ check-ins. They have adopted the topic 
model Latent Dirichlet Allocation (LDA) for data analysis. In [17], 
authors extract latent topics by LDA and analyze their geographic 
features. Experiment results show that some topics’ venues are 
geographic close, and some are not. Meanwhile, authors also 
analyze the relationship of venues in each topic. Experiment 
results show that some topics’ venues belong to the same type, 
such as school, but some topics’ venues are not, such as mall, 
restaurants, etc. In [7], authors focus on getting the factors that 
drive user to checkin. By analysis the results, authors thought the 
factors that drive user to checkin mainly include interest factor, 
community factor and user type factor.
 Based on the following two reasons, we use LDA topic-model 
to extract the latent topic from users’ checkin records, and use the 
extracted topics to calculate user similarity. One reason is that
 when using LDA topic-model to extract topics, (1) no pre-defined 
topics; (2) extracted topics represent a meaningful probability 
distribution of words and document. The other is that extracted 
topics can be interpreted as users’ habits or mobility patterns [7].
 In this paper, we first analyze the extracted topics geographic 
features, as well as the relationships of venues under the topic. We 
believe that not every topic is suitable for calculating user 
similarity. Therefore, we adopted the information entropy of topic 
in each user to select appropriate topic set to calculate user 
similarity. We obtain two topic sets by topic entropy value. One is 
the top 10 topics of high entropy which we call it as variability
 topic set. The other is the top 10 topics of low entropy which we 
call it as common topic set. And then we use variability topic set, 
common topic set and all topic set to calculate user similarity. 
Experiment results validate our conjecture:” not every topic is 
suitable for calculating user similarity”. Finally, we use 
nDCG[4](normalized discount cumulative gain) to evaluate the 
effectiveness of calculating user similarity method.
 2. RELATED WORK
 In the past decade, calculating user similarity has become an 
important research direction of information retrieval and 
recommendation system. By calculating user similarity, we can 
recommend friend and location, and predict activity, etc. Recently, 
several studies calculate user similarity by trajectory similarity [19]
 [20]
 . In [19], authors explore a geometric approach to trajectory 
similarity by exploiting three types of distance, that is: vertical 
distance, parallel distance and angular distance. While it limit to 
the geospatial realm, overlooking the types of activities and social 
information related to the activity location. In [22], authors 
focused on the spatial components of user trajectories. Their 
method employs hierarchical trajectory sequence matching to 
determine similar user.
 When measuring user similarity based on geospatial aspects of 
user trajectories, we believe that an understanding of the 
2013 International Conference on Cloud Computing and Big Data
 978-1-4799-2829-3/13 $26.00 © 2013 IEEE
 DOI 10.1109/CLOUDCOM-ASIA.2013.74
 557978-1-4799-2830-9/14 $31.00 © 2014 IEE
semantics of activity space is essential. In [23], authors investigate 
the concept of semantic annotations for venue categorization. In 
developing a semantic signature for a categorized place based on 
check-in behavior, similar, uncategorized places could be 
discovered. This concept of semantic signatures may also be 
applied to assessing user similarity through semantic trajectories.
 In [18], authors measure semantic similarity between user 
trajectories in order to develop a friend recommendation system. 
This work focuses on the type of activities completed by each user 
and the sequence in which these activities take place. Similarly, in 
[22], authors focus on stay cells and obtaining a semantic 
understanding of the types of activities conducted within the cells.
 And semantic trajectory has become a measure of user similarity.
 Recently, in [24], authors present a method for determining 
user similarity based on LBSN data. While authors also made use 
of checkin information, they concentrated on the hierarchy 
location categories supplied by Foursquare in conjunction with 
the frequency of check-ins to determine a measure of similarity.
 By comparison, our approach is novel in that besides considering 
checkin information and frequency, we also extract the latent 
topics of users’ checkin records and then calculate user similarity 
based on the extracted topics. 
3. METHODOLOGY
 In this section, we describe the data collection, topic extraction, 
and methodology used for developing our user similarity 
measures.
 3.1 DATA SET
 We first obtain 902 users’ id in the surrounding of Beihang 
University by Sina Weibo API. And then we crawled all these 
users’ checkin list until May 2013. Of these, 675 users have 
checkin records, that is, 72.828% of users have checkin records. 
In our data set, there are 13769 check-ins and 4189 venues. We 
carried out the following processing of our data set: (1) remove 
those users whose check-ins is less than 5; (2) remove those 
venues that have less than 5 users to checkin; (3) also remove 
some interpret information of title. After the above processing,
 there are 415 users left, and 9639 check-ins and 3917 venues. 
Sina Weibo defines a venue system hierarchy, involving 20 major 
categories including Sights, Auto Repair, Street Furniture,
 Science&Education&Culture, Finance&Insurance, Living 
Services and Food Services, etc.
 From Figure 1, we can see that the check-ins in 
Science&Education&Culture and Business&residential categories 
are obviously higher than other categories. Figure 2 shows the 
distribution of users checkin, horizontal axis represents the 
number of users check-ins. From Figure 2, we can see that 90% 
users’ check-ins is less than 50. The average user’ check-ins is 
probably about 30.
 Sig
 hts
 Au
 to 
Re
 pai
 r
 Str
 ee
 t Fu
 rn
 itur
 e
 Sci
 enc
 e&
 Edu
 ca
 tion
 &C
 ultu
 re
 Fin
 an
 ce
 &In
 su
 ra
 nc
 e
 Liv
 ing
  Se
 rv
 ice
 s
 Pub
 lic 
Fac
 ilite
 s
 Au
 to 
Ser
 vic
 es
 Ac
 co
 m
 m
 oda
 tion
  
Ser
 vic
 es
 Car
  
Sal
 es
 Spo
 rts
  
Ser
 vic
 es
 Bu
 sin
 es
 s&
 Re
 sid
 en
 tial
 Foo
 d S
 erv
 ice
 s
 Me
 dic
 al S
 erv
 ice
 s
 Ad
 dre
 ss 
Ser
 vic
 es
 Go
 ve
 rm
 en
 t&C
 om
 m
 un
 ity
 Co
 m
 pan
 y
 Tra
 ffic
  
Ser
 vic
 es
 Ma
 ll S
 erv
 ice
 s
 Mo
 tor
 cyc
 le S
 erv
 ice
 s
 0
 500
 1000
 1500
 2000
 2500
 nu
 m
 category
 num
 Figure 1. Check-ins distribution in 20 venues categories
 100 101 102 103
 10-3
 10-2
 10-1
 100
 Distribution of Users Checkin Number
 Users Checkin Number
 P(X
 >x
 )
 Figure 2. Distribution of users check-ins
 3.2 LDA MODEL
 Based on the following two reasons, we use LDA topic-model 
to extract the latent topic from users’ checkin records, and use the 
extracted topics to calculate user similarity. One reason is that 
when using LDA topic-model to extract topics, (1) no pre-defined 
topics; (2) extracted topics represent a meaningful probability 
distribution of words and document. The other is that extracted 
topics can be interpreted as users’ habits or mobility patterns [7]. 
While numerous topic models are discussed in the literature, 
LDA is a state-of-the art generative probabilistic topic model that 
can be used to infer the latent topics in a large textual corpus in an 
unsupervised manner [2]. A topic is a multinomial distribution 
over terms, where the distribution describes the probabilities that 
a topic will generate a specific word. LDA models each document 
as a mixture of these topics based on a Dirichlet distribution.
 Several mature implementations of LDA with improvements exist,
 we employ the JGibbsLDA version.
 558
Figure 3. Graphic model represents of LDA[1]
 The graphic model of LDA is shown in Figure 3. 
 
  and 
 are parameters of the Dirichlet prior on the per-document topic, 
respectively. 
m
 
 is the topic distribution for document m . k
 
 is 
the word distribution for topic k .
 ,m nw is the 
thn word in 
document m and 
,m nz is the topic for ,m nw . And K is the 
number of topics, M is the number of documents and 
mN is the 
number of words of thm document.
 We employ the LDA to identify topics in our data set. The basic 
unit is word in the LDA and, in this paper, the venue in a single 
checkin record represents a word. A user’ trajectory consisting of 
all the venues of his check-ins represents a document, which is a 
set of words. Every user’s trajectory can be described as a mixture 
of the geographic topics that are essentially distributions over the 
venues. Therefore, the LDA can be applied to the mobility data in 
this way. In this paper, we set the number of 
topics 30K  , 50 / K  , 0.2  . It is worth to note that why 
we set 30K  . From Figure 3, we can see that most of users’
 checkin times is in the range 20-50, and the average probably is 
30.
 3.3 ANALYSIS OF TOPICS GEOGRAPHIC FEATURES
 In this section, we will analyze the overall topics’ geographic 
features. We will analyze the geographic features of each topic, i.e. 
whether the venues of each topic are close to each other or not? 
Whether there are any spatial relations among the venues in the 
same topic?
 116 118 120 122
 36
 38
 40
 lantitude
 lan
 titu
 de
 lontitude
 Figure 4. (a) The venues latitude and longitude distribution of topic 1
 111 114 117 120
 24
 32
 40
 lantitude
 lan
 titu
 de
 lontitude
 Figure 4. (b) the venues latitude and longitude distribution of topic 13 
After analysis, geographic features of topics mainly have the 
following two cases: (1) there is an obvious concentrated area, 
that is, a considerable portion or substantially all venues in 
geographic space is close, but a small part of venues scatter in 
distant locations; (2) there are two or more concentrated area. 
Figure 4(a) shows the venues latitude and longitude distribution 
of topic 1. From Figure 4(a), we can see that this topic belongs to 
the first case that is having an obvious concentrated area. And the 
Figure 4(b) shows the venues latitude and longitude distribution 
of topic 13, which belongs to the second case that is having two 
or more concentrated areas. Due to the space limit we only give 
two typical distribution diagrams of topic.
 Belonging to the first case also has topic 2,
 3,4,5,6,8,9,10,14,16,17,18,19,20,22,23,24,26,27,28,29. The rest 
topics are belonging to the second case. Generating the first 
distribution may be due to: many users concentrated in this region
 together to work or study, and occasionally, some users go to 
relatively distant places for sightseeing or working. While 
generating the second distribution may be due to: some users 
often travel or work from one place to another. By analysis the 
detail information of check-ins in the first case, it can be divided 
into the following three cases: (1) most venues in the topic are 
related to residential, such as, topic 1, 2, 6, etc. The reason for this 
situation might be the mutual residential relocation of people in 
these communities. (2) Most venues are related to school, 
residential areas, surrounding park, shopping mall, catering and 
hotels. This distribution is mainly reflected the activity patterns of 
students or residents in these communities. (3) Most venues are 
related to recreation places. In contrast, the second case is 
relatively easy to understand. Because the venues under this type
 are relative dispersion and have the obvious characteristics, that is, 
involving railway station, airport, bus stations, etc. We also can 
infer whether the user was for work or for travel according to the 
other venues’ information.
 Through the above analysis, we can get the following 
conclusions: (1) topics can be interpreted as users’ common 
characteristics or lifestyle or habits; (2) each topic in some certain 
extent reflects the similarity of users in habits or interests.
 3.4 ENTROPY OF TOPICS
 We believe that not every topic is suitable for calculating user 
similarity. Therefore, we adopted the information entropy of topic 
in each user to select appropriate topic set to calculate user 
similarity. For any topic t, the probability of this topic belonging 
to document d is defined as P (t|d). So the topic’s entropy over all 
documents is defined as Equation 1. 
( | )
 2( ) ( | ) log    P t d
 t T d D
 E t P t d
  
   	 (1) 
559
In [25], authors thought that when two objects are compared for 
similarity, the set of objects from which the two objects are 
selected has the effect of making some properties more or less 
salient in the similarity judgment. The properties that are more 
salient are termed to be more ‘diagnostic’. Authors argued that 
two factors contribute to the diagnosticity of a property. The first 
is variability, which finds that the properties that vary across the 
elements of the context set are used more to determine the 
similarity (or dissimilarity) of two objects. The second factor 
commonality is the opposite that properties that are shared by 
most elements of the context set are the important properties, 
because they help explain what is important in the domain of 
discourse.
 Based on the above ideas, we obtain two topic sets by topic 
entropy value. One is the top 10 topics of high entropy which we 
call it as variability topic set. The other is the top 10 topics of low 
entropy which we call it as common topic set. And then we use 
variability topic set, common topic set and all topic set to 
calculate user similarity. 
3.5 SIMILARITY CALCULATION MODEL
 In this section, we will describe the detail of the similarity 
calculation models. We specify two models. One is single-layer 
model. The other is two-layer model.
 3.5.1 Single-layer model
 Assuming T is the selected topic set. Then extract each user’
 weight vector Uf based on the selected topic set. And we will 
calculate the weight of user in a particular topic with Equation 2.
 1
 ( ) ( )Tk
 k
 Ti
 U
 U k Tn
 U
 i
 NUM
 f T U
 NUM
 
 
 
 	
  ( 2) 
U represents a user. kT represents a topic. TkUNUM represents 
the number of user U check-ins in the topic kT . ( )kTU
 represents the decay function of user U in the topic kT . The 
purpose of introducing decay function is to prevent that only 
consider the weight of topic but ignoring the quantitative of users’
 check-ins. After we get the users’ topic weight vector, the 
similarity of two users ( , )u vU U is the comparison of these 
vectors using the cosine similarity and Pearson correlation 
coefficient.
 cos ( , ) | || |
 u v
 u v
 U U
 ine u v
 U U
 f f
 SIM U U f f (3) 
2 2
 ( )( )
 ( , )
 ( ) ( )
 u u v vT Tk T k T
 k
 u u v vT Tk T k T
 k k
 U U U U
 T T
 pearson u v
 U U U U
 T T T T
 NUM NUM NUM NUM
 SIM U U
 NUM NUM NUM NUM
 
  
  
 
  
 	
 	 	
  (4) 
uT
 UNUM and vTUNUM represents the average check-ins of 
user 
uU and vU
 3.5.2 Two-layer model
 Two-layer model is the improvement of single-model, which 
calculates user similarity in fine-grained. Compared with single-
 layer, two-layer model first calculate the similarity of two users in 
each topic, and then sum to represents this two users’ similarity. 
During summation, different users will be different weighting 
each topic. This weight is based on the importance of topic to user. 
Assuming 1 2{ , ,..., }k nT venue venue venue is one topic. 
nvenue represents a venue. Then extract each user’ weight vector 
( )U kf T in topic kT . And we will calculate the weight of user in a 
venue with Equation 5.
 1
 ( ) ( )venuek
 k
 venuei
 U
 U k venuen
 U
 i
 NUMf venue U
 NUM
 
 
 
 	
 ( 5) 
U represents a user. kvenue represents a venue. ( )kvenueU
 represents a decay function. 
venuekU
 NUM represents that the 
checkin times of user U in kvenue . The purpose of introducing 
decay function is to prevent that only consider the weight of 
venue but ignoring the quantitative of users’ check-ins. After we 
get the users’ weight vector in a topic, the similarity in the topic 
of two users ( , )u vU U is the comparison of these vectors using 
the cosine similarity and Pearson correlation coefficient.
 cos
 ( ) ( )
 _ ( , | ) | || |
 u v
 k
 u v
 U k U k
 venue T
 ine u v k
 U U
 f venue f venue
 T SIM U U T f f
 
 
 	
 (6) 
2 2
 ( )( )
 _ ( , | )
 ( ) ( )
 u u v vvenue venuei T i Tk k
 i
 u u v vvenue venuei T i Tk k
 i i
 U U U U
 venue T
 pearson u v k
 U U U U
 venue T venue T
 NUM NUM NUM NUM
 T SIM U U T
 NUM NUM NUM NUM
 
  
  
 
  
 	
 	 	
 (7) 
Then the user similarity in the selected topic set is expressed 
Equation 8:
 _ ( , ) _ ( , | ) ( | ) ( | )
 k
 u v u v k k u k v
 T T
 T SIM U U T SIM U U T P T U P T U
 
  	 (8) 
In Equation 8, we can use Equation 6 or Equation 7 to 
replace _ ( , | )u v kT SIM U U T . ( | )k uP T U represents the weight 
of topic kT for uU , and ( | )k vP T U  represents the weight of 
topic kT for vU
 3.6 EVALUATION
 Our approach is evaluated as an information retrieval problem. 
According to Sina Weibo API, we can obtain users’ real 
relationship matrix in our data set. So we can use normalized 
discounted cumulative gain (nDCG) [4] to evaluate the search 
results.
 Since Sina Weibo API constraints, we obtain the relationship 
between the users’ settings shown in Table 1.
 560
Table 1. User relationship settings
 Relevance level Relationship
 2 Bidirectional
 1 Unidirectional
 0 No-Relation
 For instance, when iU queries the top p users that with 
similarity, the similarity calculation model will be returned to  iU
 a list of users which size is p. According to the real relationship 
matrix we can get a relevance vector V. Then we can use nDCG to 
evaluate the search results. nDCG is used to compute the relative-
 to-the-ideal performance of information retrieval techniques. The 
discounted cumulative gain of V is computed as follows: (In our 
experiments, b=2)
 
 
 
 
 
  
 
 
  
 
                              1
 1                        
1       
log ib
 V i if i
 DCG i DCG i V i if i b
 V i
 DCG i if i b
 
 
    
   
 (9) 
Given the ideal discounted cumulative gain IDCG, then 
nDCG@p can be computed as 
 
 @
 DCG p
 nDCG p
 IDCG p
  . For example, 
1,0, 2,1, 2V   , 
 
 
 5@5 0.7062
 5
 DCG
 nDCG
 IDCG
   . 
3.7 EXPRIMENTAL RESULTS
 In this section, we mainly through the experiments to validate 
our proposed similarity calculation model effectiveness, stability. 
First, we explain our experiments setup. And then we will show 
and discuss our results.
 We conduct three experiments for each similarity calculation 
model. In the first experiment, we use all extracted topic set to 
calculate user similarity. In the second and third experiments, we 
use the common topic set and variability topic set to calculate user 
similarity, respectively.
 Figure 5. The nDCG variation with the change of parameter p of 
single-layer model
 First, we introduce the experiments for single-layer model. 
Figure 5 shows the nDCG variation with the change of parameter 
p. The experiment has five groups, which uses p-value to group. 
When the p-value equal to, we can observe the comparing results 
of calculating user similarity by cosine and pearson. From Figure 
5, we can see that the nDCG will decrease with the increment of p.
 This explains, as more users to obtain, the model performance will 
decline. For each experiment, we can see that when using the
 common topic set and the variability topic set to calculate user 
similarity, the performance are better than using all extracted topic 
set. This may be that there is some noise for the extracted topic set 
by LDA model. In this paper, by selecting topics with topic 
entropy remove the noise topics. Experiments show that noise can 
be eliminated through the topic information entropy. In each 
experiment, we can find that the performance is basically the same 
by cosine and pearson. We also can see that when using cosine to 
calculate user similarity, the performance is slightly higher by 
using the common topic set than using the variability topic set, 
except p = 25. While using pearson to calculate user similarity, 
the performance is just opposite with using cosine. This explains 
that cosine is suitable to calculate similarity on the common topic 
set and pearson is suitable on variability topic set.
 Two-layer model is the improvement of single-model, which 
calculates user similarity in fine-grained. Compared with single-
 layer, two-layer model first calculate the similarity of two users in 
each topic, and then sum to represents this two users’ similarity. 
During summation, different users will be different weighting 
each topic. This weight is based on the importance of topic to user.
 Figure 6. The nDCG variation with the change of parameter p of 
two-layer model
 From Figure 6, we can visually see that each group’ nDCG 
value will decrease with the increment of p-value. However, 
compared with the single-model, the magnitude of reducing is 
smaller, which illustrates the two-layer model is more stable and 
faster convergence. For each group of experiments, When using 
cosine to calculate user similarity on the variability topic set, the 
performance is worst, and the other two topic sets are 
substantially uniform. When using pearson to calculate user 
similarity on the variability topic set, the performance is best. This 
explains when using two-layer model, different calculation 
methods may produce significant different results.
 In summary, when calculating similarity using common topic 
set and all extracted topic set, the performance of two-layer model
 is higher than single-layer model. And with the increment of p-
 value, the two-layer model is more stable and faster convergence. 
But when using the variability topic set the performance of two-
 layer model vary greatly. 
4. CONCLUSIONS 
In this paper, we propose a novel method to calculate user 
similarity. We first extract latent topic of users’ checkin by topic 
model. In order to learn the geographic features of topics, we 
analyze the distribution and correlation of the venues of each 
561
topic. Based on the assumption that each latent topic, to some 
extent, reflects some users’ habits or behavior patterns in certain 
area, we use the latent topics to calculate user similarity. By 
analysis the geographic features of extracted topics, we verify the 
assumption that each latent topic, to some extent, reflects some 
users’ habits or behavior patterns in certain area. And when using 
the extracted topic from LDA model to calculate user similarity, 
these topics are noisy which should be removed. In this paper, we 
use the information entropy of topic to remove noisy. And this 
approach is proved to be effective by experiments. Our approach 
is evaluated as an information retrieval problem. We find that 
when calculating similarity using common topic set and all 
extracted topic set, the performance of two-layer model is higher 
than single-layer model, and with the increment of p-value, the 
two-layer model is more stable and faster convergence. While 
using variability topic set, pearson outperforms.
 In the future work, we will first expand our data set for further 
experiments. Meanwhile, we will be to consider the time factor. In 
this paper, we did not consider the time factor. At the same time, 
our real relationship is too sample, in the further work, we will 
enrich it. Also we will consider other ways to remove noisy of the 
extracted topics.
 REFERENCES
 [1] [1]Blei, D.M., Ng, A.Y., and Jordan, M.I. Latent Dirichlet allocation. 
Journal of Machine Learning Research, 3:393-1022, 2003.
 [2] [2]Ferrari, L. and Mamei, M. Discovering daily routines from google 
latitude with topic models. In CoMoRea’ll, 2011.
 [3] [3]Sarwar, B., Karypis, G., KonStan, J., and Riedl, J. Application of 
Dimensionality Reduction Recommender System—A Case Study, In 
ACM WebKDD WorkShop, 2000.
 [4] [4]D. Manning, P. Raghavan and H. Schutze. Introduction to 
Information Retrieval. Cambridge University Press, 2008.
 [5] [5] Ying, J.J.c., Lu, E.H.c., Lee, W.c., Tseng, V.S.: Mining User 
Similarity from Semantic Trajectories. Cell pp. 19-26 (2010).
 [6] [6]Zheng, Y and Zhou, X., Eds. Location-based social networks: 
Users. Computing with Spatial Trajectories. Springer, 2011.
 [7] [7]Kenneth Joseph, Chun How Tan, Kathleen M.Carley. Beyond 
“Local”, “Category” and “Friends”: Clustering foursquare Users 
with Latent “Topics”. UbiComp’12, 2012.
 [8] [8] J. Cranshaw, R. Schwartz, J. I. Hong, and N. Sadeh. The 
livehoods project: Utilizing social media to understand the dynamics 
of a city. In Proc. ICWSM ’12. AAAI, 2012.
 [9] [9] Y. Zheng, L. Zhang, Z. Ma, X. Xie, and W. Ma. Recommending 
friends and locations based on
 [10] individual location history. ACM Trans. Web, 5(1):5:1–5:44, Feb. 
2011.
 [11] [10] Z. Cheng, J. Caverlee, K. Lee, and D. Z. Sui. Exploring 
millions of footprints in location sharing services. In Proc. 
ICWSM ’11, pages 81–88. AAAI, 2011.
 [12] [11] E. Cho, S. A. Myers, and J. Leskovec. Friendship and mobility: 
user movement in location-based social networks. In Proc. 
SIGKDD ’11, pages 1082–1090. ACM, 2011.
 [13] [12] A. Noulas, S. Scellato, C. Mascolo, and M. Pontil. An empirical 
study of geographic user activity patterns in foursquare. In Proc. 
ICWSM ’11, pages 570–573. AAAI, 2011.
 [14] [13] A. Sadilek, H. Kautz, and J. P. Bigham. Finding your friends 
and following them to where you are. In Proc. WSDM ’12, pages 
723–732. ACM, 2012.
 [15] [14] S. Scellato, A. Noulas, and C. Mascolo. Exploiting place
 features in link prediction on location-based social networks. In Proc. 
SIGKDD ’11, pages 1046–1054. ACM, 2011.
 [16] [15] J. Cranshaw, R. Schwartz, J. I. Hong, and N. Sadeh. The 
livehoods project: Utilizing social media to understand the dynamics 
of a city. In Proc. ICWSM ’12. AAAI, 2012.
 [17] [16] A. Noulas, S. Scellato, R. Lambiotte, M. Pontil, and C. 
Mascolo. A tale of many cities: universal patterns in human urban 
mobility. ArXiv e-prints, Aug. 2011.
 [18] [17]XueLian Long, Lei Jin, Jams Joshi. Exploring Trajectory-Driven 
Local Geographic Topics in Foursquare. UbiComp’12, 2012.
 [19] [18]Josh Jia-Ching Ying, Eric Hsueh-Chan Lu, Wang-Chien Lee. 
Mining User Similarity from Semantic Trajectory.  In ACM 
LBSN’10, 2010.
 [20] [19] J.-G. Lee, J. Han and K.-Y. Whang. Trajectory Clustering: A 
Partition-and-Group Framework. In roceedings of International 
Conference on Management of Data (ACM SIGMOD), pp. 593-604, 
Jun. 2007.
 [21] [20] Y. Zheng, L. Zhang, and X. Xie. Recommending friends and 
locations based on individual location history. ACM Transaction on 
the Web, 2010.
 [22] [21] E. H.-C. Lu and V. S. Tseng. Mining Cluster-Based Mobile 
Sequential Patterns in Location-Based Service Environments. In 
Proceedings of IEEE International Conference on Mobile Data 
Management (MDM), May. 2009.
 [23] [22] Quannan Li, Yu Zheng, Xing Xie, Yukun Chen, Wenyu Liu, 
and Wei-Ying Ma. Mining user similarity based on location history. 
Proceedings of the 16th ACM SIGSPATIAL international 
conference on Advances in  geographic information sys-tems - GIS 
'08, page 34, 2008.
 [24] [23] Mau Ye, Dong Shou, Wang chien Lee, Peifeng Yin, and 
Krzysztof Janowicz. On the semantic annotation of places in 
location-based social networks. In Proceedings of the 17th ACM 
SIGKDD International Conference on Knowledge discovery and 
data mining, pages 520-528, 2011.
 [25] [24] M Lee and C Chung. A user similarity calculation based on the 
location for social network services. DASFAA, pages 38-52, 2011.
 [26] [25] A. Tversky. Features of similarity. Psychological Review, 
84(4):327-352, 1977.
 562

H-PFSP: Efficient Hybrid Parallel PFSP Protected Scheduling for MapReduce
 System
 Yin Li?, Chuang Lin?, Fengyuan Ren?, Yifeng Geng †
 Tsinghua National Laboratory for Information Science and Technology (TNList)
 Tsinghua University
 Beijing, China
 Email: ?{yinli, chlin, renfy}@csnet1.cs.tsinghua.edu.cn
 †gengyifeng@gmail.com
 Abstract—MapReduce provides a data-parallel computing
 framework, and has emerged as a popular processing model
 due to the simplicity of operations for big data application
 developers. Data processing applications from many different
 domains such as search and data mining are usually developed
 using open-source Hadoop implementation of MapReduce or
 self-developed MapReduce-like implementations like Dryad [1]
 and Ciel [2]. In cloud environments, products like Amazon’s
 Elastic Compute Cloud (EC2) [3] provide MapReduce services
 as third-party multi-tenant service. Even within a company,
 a number of products may share the MapReduce cluster.
 Therefore, a fair and efficient scheduler is crucial to improve
 performance of submitted jobs and guarantee multi-user fair-
 ness. However, in practice, it is hard to guarantee both fairness
 and per-job performance, especially when jobs are scheduled
 without accurate estimation. We show that processor sharing
 (PS) type of schedulers like Fair Scheduling degrade the per-job
 performance in a multi-user environment. We present a new
 scheduling policy, Hybrid Parallel pessimistic Fair Schedule
 Protocol (H-PFSP), that can finish every job no later than Fair
 scheduler does. Unlike Fair scheduler, however, it can improve
 the per-job performance of MapReduce systems with relatively
 accurate job progress estimation.
 Keywords-MapReduce; H-PFSP; fair scheduling; perfor-
 mance;
 I. INTRODUCTION
 The advent of big data has increased the prevalence
 of parallel computing models. Large amount of data is
 accumulated every day, especially by Internet companies
 serving a huge number of users. The data needs to be
 processed in an efficient way to preserve its effectiveness
 after it is generated. Search engines like Google need to
 crawl the web pages from a huge number of URLs, parse
 all of them to extract the feature information, then index the
 processed data for users to search. All of these jobs handle
 gigantic databases and need to be done in a timely manner.
 In view of these challenges, Google has designed MapRe-
 duce [4], a parallel processing model. It first decomposes
 each job into several independent parts with roughly the
 same size, each of which is processed by a Map task. To
 aggregate the generated intermediate results, several tasks
 of another type called Reduce are invoked to collect these
 results and generate the final results. The data to be pro-
 cessed or generated is stored in sequence files in the form of
 <Key, Value>. The outputs of Map tasks are also in <Key,
 Value> form. They are partitioned and shuffled to Reduce
 tasks according to different values of keys. Hadoop [5] is an
 Apache open-source implementation of MapReduce, where
 workers of the cluster run on top of JVMs. They can be
 configured as either Map slots or Reduce slots, which are
 the independent computing units running concurrently.
 The key module of MapReduce is the scheduler. It runs
 on the master server that performs the centralized control
 of MapReduce cluster. The scheduler manages all the slot
 resources on slave workers and allocates them to submitted
 jobs. For cloud computing service, MapReduce provides a
 sharing mechanism to multiple users. It is required by the
 service provider to allocate the resources in a fair way while
 guaranteeing the overall utilization. Even within a company,
 many products also need to share a MapReduce service to
 process their own huge amount of data. The simple FIFO
 scheduling is so cumbersome for a multi-job scheduler that
 it starves jobs. Fair scheduler works via polling. It favors
 the fair allocation of all slots, rigorously trying to guarantee
 fairness at every moment. Since this fair algorithm actually
 splits the cluster into slices, the real resources each job
 occupies are only a portion of the cluster. Therefore, the
 response time of each job will be much longer than when it
 monopolizes resources.
 Per-job performance is critical to each user or product.
 Users and products may be independent, and compete for
 the resources with each other. In reality, the job submitted
 by a user may only be a step of its workflow. For example,
 after the indexing job is completed, the indexed output data
 will be downloaded to front-end for the search use. The job
 taking a long time to finish can cause a bottleneck, which
 slows down the total flow time. For example, the bottlenecks
 of news search or micro blog search suggest the loss of
 effectiveness for the outcome.
 This paper addresses the problem on how to improve the
 per-job performance and accelerate the mean flow time of
 jobs without violating fairness. By showing another semantic
 of fairness, we can design a better scheduler to accelerate
 the mean flow time. It is enlightened by Pessimistic Fair
 2013 12th IEEE International Conference on Trust, Security and Privacy in Computing and Communications
 978-0-7695-5022-0/13 $26.00 © 2013 IEEE
 DOI 10.1109/TrustCom.2013.133
 1099
Schedule Protocol (PFSP) [6], which can be proven to
 dominate other online scheduling policies theoretically. It
 reduces the per-job flow, thereby reducing the mean flow
 time of the jobs.
 H-PFSP uses estimated job execution times as parame-
 ters by which it makes scheduling decisions. A imprecise
 estimation may make the whole scheduling invalid, or even
 degrade the overall performance. Thus, to achieve reasonable
 accuracy, we update the estimates at intervals during the
 running progress to capture the latest status of running jobs
 and clusters. Moreover, the frequent update needs efficient
 algorithms to reduce the complexity of the update routine.
 To achieve this goal, we design an incremental algorithm to
 make the update complexity acceptable.
 We implement H-PFSP scheduler, then embed it into Fair
 scheduler, so that jobs in each pool can be scheduled by
 H-PFSP. To evaluate the performance, we test two bench-
 mark MapReduce programs: Wordcount and Terasort. To
 validate it in large-scale environment, we also implement
 a simulation tool. We can generate a large amount of data
 and simulate the scheduler using this tool. By comparing H-
 PFSP to Fair scheduler using both experiment and simulation
 results, it is shown that H-PFSP has better performance than
 the Fair scheduling in most scenarios.
 This paper is organized as follows. Section II illustrates
 the core idea and the mechanisms adopted by our H-PFSP
 solution. Section III presents the design and algorithms
 of H-PFSP. We present the H-PFSP implementation and
 performance evaluation in Section IV. Section V surveys
 related work. Finally, we make final conclusion in Section
 VI.
 II. H-PFSP SCHEDULING
 In this section, we present the core idea and some key
 mechanisms adopted by the H-PFSP scheduler. Before we
 introduce H-PFSP, a simplified version (Parallel PFSP) is
 presented to illustrate the key idea of H-PFSP.
 A. Parallel PFSP
 FSP [7] maintains virtual execution states of each job
 by assuming that jobs are scheduled by PS scheduler. FSP
 always gives priority to the job that would finish first under
 PS. It is proven that the flow time of every job is no longer
 than that under PS, which is defined as the property of
 protective scheduling in [6]. The priority setting of FSP
 guarantees the completion order of jobs is the same as under
 PS. It is a sufficient condition for protective scheduling.
 However, it is too strict and unnecessary. Protective
 scheduling only implies the deadline of each job. Any com-
 pletion order of jobs is satisfactory provided their deadline is
 met. Thus, inspired by PFSP [6] which is another protective
 scheduling algorithm dominating FSP, we present Parallel
 PFSP. The idea of Parallel PFSP scheduling is to take
 parallel slots as a processor with the aggregated capacity.
 
 
 
 
 
   	 	 		
 

 
 

 

 
 
 	
 
 
 
 
 
 
 
 
 
    		 
    		 
 	

 
 
 	
 
	
 Figure 1. Comparison of three schedulers. (a) Fair Scheduler; (b) FSP;
 and (c) Parallel PFSP.
 Then, it mimics the shortest remaining processing time
 (SRPT) scheduling while guaranteeing the protectiveness
 property, by always scheduling a completable job with the
 shortest remaining process time. A job is completable [6]
 at time t if serving it to completion from t without being
 interrupted will not violate the protectiveness for other jobs,
 provided that no more jobs will arrive after t in the future.
 Suppose we have the relatively accurate estimation of
 job execution information first, we put an example here
 to illustrate the advantage of Parallel PFSP in Fig. 1. The
 algorithm of a modified version is presented in detail in
 Section III. We roughly show the capacity of resources as
 continuous without explicitly enumerating the slots and the
 task count. The real case using discrete slots and tasks can
 only produce a small deviation and does not have much of
 an impact on our conclusion. Fig. 1 shows that every job
 scheduled by Parallel PFSP finishes no later than scheduled
 by the FSP and the Fair scheduling, which implies the mean
 flow time jobs is reduced.
 We present several challenges and motivations of the
 Parallel PFSP in MapReduce system before we dig into the
 H-PFSP scheduling. First, precise estimation of the job size
 or the execution time is difficult. MapReduce job execution
 is volatile, especially in a heterogeneous environment. How
 to make a relatively accurate estimation becomes an issue.
 Second, multiple computing slots are used to execute tasks
 of MapReduce jobs concurrently. Traditional schedulers are
 mostly designed for scheduling indivisible jobs. However,
 the key principle of MapReduce is to parallelize job exe-
 cution using multiple resources by splitting each job into
 independent tasks. In this context, PS implementation turns
 into another form - Fair scheduling. So we use Fair and
 PS interchangeably in the rest of the paper. Finally, the
 complexity of our H-PFSP scheduling algorithm is critical
 and needs more consideration.
 1100
B. Job profiling and estimation
 Similar to FSP and SRPT-like schedulers, H-PFSP needs
 the estimates of the per-job flow time. The estimated flow
 time is used to initialize the scheduling algorithm at the start.
 During the execution, the job status should be re-estimated
 since server performance varies due to the background work-
 loads or failures. The remaining processing time of each job
 needs to be updated every once in a while. Profiling jobs by
 the historical job execution information is difficult due to
 the diversity of jobs and volatility of cluster environment.
 For example, the web parsing for a search engine has a
 computing-intensive Map phase, but does trivial work in the
 Reduce phase. In contrast, the page crawling has a network-
 intensive Reduce phase since crawler schedules the crawling
 tasks according to the hash values of site IPs, which can
 only be done after the partition and shuffle stages of Reduce
 phase, while the Map phase of crawling needs little work.
 Because of these reasons, we will not estimate the status
 of the current job by historical job profiling. Instead, the
 historical execution information of the current job itself
 will be used. First, we use the information of finished
 tasks of a job to estimate the average processing time of
 its tasks, including the average processing time of a Map
 task Avg(T )Mi and Reduce task Avg(T )Ri . These values
 are updated at intervals before we calculate the remaining
 execution time of the job. Suppose that between two updates,
 the Map progress for job i has increased from PMi to
 ˜PMi , the average task execution time can be estimated by
 computing the execution rate during this period.
 ∆PMi = ˜PMi ? PMi (1)
 ˜AvgMi = ? ·AvgMi + (1? ?) ·∆t/∆PMi (2)
 where ∆t/∆PMi is the average time of a Map task.
 The progress value can be obtained directly in Hadoop
 implementation. Exponentially weighted moving average
 (EWMA) method is used here to smoothly estimate the
 average value. The same estimation method also applies to
 a Reduce task. Note that we do not update the estimates
 if ∆PMi is too small, e.g., less than 0.02, because it is
 imprudent to make estimations when the job has little
 progress. However, we cannot obtain the accurate average
 time of Reduce task at the Map phase since Reduce phase
 has not started yet (or a Reduce task has launched but stuck
 in the shuffle phase because not all Map tasks have finished
 yet). Thereby, we can only use the average execution times
 of the Map tasks as estimates instead before the Map phase
 has completely finished.
 We consider the aggregate computing capacity of the
 parallel slots in MapReduce clusters as a single processor
 by normalizing the average task processing time as follows:
 ˆAvg(T )Mi = Avg(T )Mi /N(S)M (3)
 ˆAvg(T )Ri = Avg(T )Ri /N(S)R (4)
 where ˆAvg(T )Mi denotes the normalized average processing
 time of Map task of job i. N(S)M is the number of Map
 slots configured in the cluster. The notation is the same for
 Reduce.
 We use the average task execution time and job progress
 to estimate the remaining execution time of a job.
 wi = max
 {
 wmin, N(T )Mi ? (1? PMi ) ? ˆAvg(T )Mi
 +N(T )Ri ? (1? PRi ) ? ˆAvg(T )Ri
 }
 (5)
 where wi is the remaining execution time of job i. wmin
 is a small constant that marks job i as still running even
 though the value of the estimated remaining execution time
 reaches 0. N(t)Mi is the number of Map tasks of job i. The
 notation is the same with Reduce.
 C. Slack system update for parallel jobs
 To meet the protectiveness constraint, we use a slack
 vector [6] to indicate how much slackness there is for each
 job in the current state. We need to construct a virtual PS
 scheduler, and maintain the job states under the virtual PS
 scheduling, which is useful to H-PFSP. The slack value of
 a job is computed as the difference between the remaining
 flow time of the job under the virtual PS scheduling and the
 remaining flow time in a schedule serving only this job and
 the jobs which would finish before it under PS. It can be
 calculated as follows.
 si =
 ∑
 1≤j<i
 vj + (m? i + 1)vi ?
 ?? ∑
 1≤j≤i
 wj
 ?? (6)
 where m is the number of running jobs in the virtual PS
 scheduling queue. vi is the remaining processing time for
 job i under PS. The initial value of vi equals wi. si denotes
 the slack value for job i. The virtual PS scheduler sorts the
 jobs in order of their remaining execution time. If jobs are
 scheduled using the PS scheduler, the jobs queueing in front
 of i will finish before job i. As it takes vi to execute the
 remaining tasks of job i using the slots in the cluster, the
 jobs following job i in the queue will also have already been
 processed for time vi when job i finishes under PS. Thus, the
 remaining flow time of job i amounts to∑1≤j<i vj+(m?
 i + 1)vi. If only job i and the jobs that would finish before
 it under PS need to be served, job i will complete after time∑
 1≤j≤i wj . Therefore, the slack value of a job indicates
 how much slackness there is before violating protectiveness
 property. A schedule of jobs is protective if and only if the
 slack vector is non-negative at any time. The three variables,
 i.e., w,v and s constitute a slack system.
 Next, we illustrate how to update the slack system if the
 job information, i.e., w and v, is estimated in the beginning
 and not re-estimated again during the execution. We present
 an algorithm to update the slack system with re-estimation
 periodically during the processing in the next sub-section.
 1101
Within a time interval ∆t, one or more jobs are running
 on the slots. In our design, only one job occupies the slot
 resources in the beginning if the number of tasks in the job
 is greater than the number of slots. However, the last wave of
 this job may contain only few tasks and cannot fully utilize
 all the slots. In this case, the residual slots are assigned to
 other jobs. If job i has been executed during ∆t, assuming
 that no new jobs arrive in this short period, we update the
 slack system as follows.
 wi = wi ?∆ˆti (7)
 vj = vj ?∆ˆti/m, for all j (8)
 sj = sj ?∆ˆti, for all j < i (9)
 where m is the number of jobs in the virtual PS queue. Since
 under PS, the slots are shared all the time, the processing
 duration should be equally amortized to all the running jobs
 in the virtual PS queue. Thus, each job should take 1/m
 time to process under virtual PS. It can be proven that after
 updating the slack system, eq. (6) still holds. We use ∆ˆti
 instead of ∆t as follows:
 ∆ˆti = ∆t/N(S) (10)
 where N(S) is the total number of slots. Here, to consider
 parallel slots in MapReduce clusters as a processor with the
 aggregated capacity, we normalize the duration by dividing
 the time interval by the total number of slots. After updating
 job i, if we find any job j, for which vj = 0, we remove
 it from the virtual PS scheduling queue. If job i has been
 finished but is still in the virtual PS scheduling queue after
 updating its state, we set si =∞.
 D. Incremental estimations
 The FSP or SRPT scheduler always works statically with
 a priori job estimation that does not change during the execu-
 tion. However, the job execution time can be overestimated
 or underestimated since the system state varies over time.
 A relatively long-term estimation makes the job execution
 suffer from serious performance degradation because the
 scheduling policy made within a large interval may be totally
 wrong.
 A better alternative is to make incremental estimations at
 intervals. Before updating the slack system, we re-estimate
 the job remaining time first. After we re-calculate the
 remaining execution time of each job, i.e. w˜i, we update
 wi and vi in the slack system as follows.
 ∆wi = w˜i ? wi (11)
 wi = w˜i (12)
 vi = vi +∆wi (13)
 This modifies the previous estimation. After that, the jobs
 in the virtual PS scheduling queue need to update their slack
 values s according to (6). However, computing the slack
 value for every job is nontrivial. To scale well, we need
 to design a method to reduce such computing burden. In
 fact, we do not need to update all the jobs in virtual PS
 scheduling queue, but only the jobs affected by the change of
 estimated remaining time. We now discuss the two possible
 cases below.
 Case 1: After the estimation, Job i moves from position i
 to i? in the virtual PS scheduling queue, where i < i?.
 From eq. (6) we know, the remaining processing time of
 job i, i.e. wi and vi, only affect the slack values of jobs
 following it in the PS scheduling queue. From eq. (6) we
 know that the slack values of jobs between i and i? should
 involve the terms wi and vi but they do not. So only these
 jobs need to update their slack values. A simple calculation
 shows that,
 sj = sj + (vj ? vi + wi), for all i < j < i? (14)
 si = si +
 i??1∑
 ?=i+1
 v? + (m? i? + 1)v?i ? (m? i + 1)vi
 ?
 ?? i?1∑
 ?=i+1
 w? + w?i ? wi
 ?? (15)
 Case 2: After the estimation, Job i moves from position i
 to i? in the virtual PS scheduling queue, where i? < i.
 Based on the analysis similar to case 1, only jobs between
 i? and i should have been updated. For these jobs, a simple
 calculation shows that,
 sj = sj + (v?i ? vj ? w?i), for all i? < j < i (16)
 si = si ?
 i?1∑
 ?=i?+1
 v? + (m? i? + 1)v?i ? (m? i + 1)vi
 +
 ?? i?1∑
 ?=i+1
 w? ? w
 ?
 i + wi
 ?? (17)
 After the re-estimation of the job information and modi-
 fication of the slack system, we can update the slack system
 as usual by eqs. (7)-(9).
 E. Two-step hybrid scheduling
 Although we have designed an incremental estimation
 method to continuously approximate the real flow time of
 jobs, there is some lag time for the estimates to converge
 correctly. Thus, it is imprudent to make a scheduling deci-
 sion based on the estimation at the start of job execution,
 even though the incremental estimation is implemented.
 To address this issue, we design a two-step hybrid
 scheduling policy to make the Parallel PFSP more effective,
 before we dig into the details of the Parallel PFSP algorithm.
 Two steps are included in both Map and Reduce phases.
 When a job just starts Map phase or Reduce phase, a proper
 threshold K is initialized. Before the job finishes K Map
 tasks or Reduce tasks, it can preempt resources from other
 jobs that have finished the first step. By preempt, we actually
 1102
mean allocating slots to a job immediately when there are
 idle slots in the cluster. It does not support real preemption
 since it wastes resources, thereby decreasing the overall
 utilization in the system. This is called a fair phase, because
 for more than one job in the first step, we adopt a Fair
 scheduling policy, to assign the job with the largest deficit.
 After a job has finished K Map tasks in Map phase or K
 Reduce tasks in Reduce phase, it is scheduled according
 to the Parallel PFSP. As both the Fair scheduling and the
 Parallel PFSP are adopted in the scheduling strategy, we call
 our scheduling scheme Hybrid Parallel pFSP (H-PFSP). We
 present H-PFSP in detail in Section III instead of Parallel
 PFSP.
 The advantage of the two-step hybrid scheduling is that
 a job can start right after it is submitted without waiting
 for a long time. If the job has some unknown faults, it can
 throw exception in the first place. Programmers can quickly
 obtain the instructions and find bugs as a result of the early
 respond.
 When a new job arrives, it is put in the running pool
 as well as the virtual PS scheduling queue with PS related
 information initialized using default values, e.g. the average
 execution time of the tasks of the finished jobs. wi and vi
 are estimated by using these default values.
 wi = vi = max
 {
 0, N(T )Mi · ˆAvg(T )MDef
 +N(T )Ri · ˆAvg(T )RDef
 }
 (18)
 where ˆAvg(T )MDef is the default average execution time of
 a Map task. After a job is put in the PS scheduling queue , it
 will preempt the slot resource until a certain number of tasks
 have been scheduled. After that, we will obtain a relatively
 accurate estimate of the job execution time. Suppose that at
 that time, this job is at position i ordered by PS-remaining
 time v in the PS scheduling queue, the PS scheduling slack
 system must be updated following eqs. (14)-(17).
 III. DESIGN AND ALGORITHMS
 This section presents how we design the H-PFSP schedul-
 ing algorithms. We illustrate the data structure used in
 H-PFSP scheduler. Next, we describe the auxiliary steps
 needed by the scheduling algorithm, including job status
 re-estimation and slack system update. After that, H-PFSP
 algorithm is straightforward.
 A. Data structure
 Virtual PS scheduling. H-PFSP scheduling depends on
 the virtual PS scheduling to calculate and update the slack
 system. When scheduling a job according to our H-PFSP
 design, we need to update the actual job status under H-PFSP
 and also the status under virtual PS. A PS scheduling priority
 queue should be maintained. Once a job is submitted, a
 virtual copy of this job is pushed into the queue. It orders
 the job copies according to the remaining processing time
 under PS. In case of a tie, we place the job submitted earlier
 in the front.
 Slack System Data Structure. We maintain a slack system
 information structure for every job. It contains the job
 remaining time under PS, the remaining time under H-PFSP,
 and the slack value of the job. To facilitate the calculation
 of the slack system, we also need to store some auxiliary
 estimates and recalculate them at intervals. These estimates
 include the current progress of Map and Reduce phases, the
 average time of Map and Reduce tasks.
 B. Re-estimation and slack system update
 Every once in a while, we need to update the slack system
 since all the slack values will change as some jobs have
 occupied the slots and run during the interval. Moreover,
 the job status should be re-estimated before the slack value
 update since the estimation of job remaining time may vary
 based on the new information. For the sake of simplicity,
 we update the slack system synchronously with the re-
 estimation of the execution progress and the remaining time.
 Algorithm 1 sketches the algorithm of the updating process.
 Algorithm 1 Function: updateJobInfo(job)
 1: /* Job shuts re-estimation and slack system update */
 2: while Running do
 3: Thread.sleep(UPDATE INTERVAL)
 4: for all jobi executed in the interval do
 5: Update estimation infos using eqs. (1) - (5)
 6: Modify the slack system values according to (11) - (17)
 7: Update slack system using eqs. (6) - (10)
 8: end for
 9: end while
 C. H-PFSP algorithm
 The kernel of our design is an H-PFSP task assignment
 algorithm, implemented by H-PFSP comparators. When a
 slot becomes idle, our scheduler assigns a task to it according
 to the H-PFSP comparator, which always chooses the job in
 the fair phase first. If more than one job is in the fair phase,
 the scheduler chooses one with the largest deficits, i.e., the
 one with the minimum number of tasks scheduled. If all
 jobs have finished their fair phase, the scheduler assigns the
 completable job with the shortest remaining processing time
 under H-PFSP. The procedure is described in Algorithm 2.
 The function isFairPhase determines if a job is in its start
 phase. A job is in the fair phase under two circumstances.
 We discriminate between the two stages of a job. A job is
 running in the Map stage if the Map progress is less than
 0.95; otherwise, it is running in the Reduce stage. A job is
 in its fair phase if it is in the Map state and the number of
 scheduled Map tasks is under a certain threshold, or if it is
 in the Reduce state and the number of Reduce tasks is under
 the threshold. The procedure is given in Algorithm 3.
 By definition, the sufficient and necessary condition for a
 job i to be completable is that for all jobs j queued in front
 of i, i.e., vj < vi, sj ≥ wi, as shown in Algorithm 4.
 1103
Algorithm 2 H-PFSP Task Assignment
 1: while true do
 2: if exist idle slot then
 3: assign tasks using H-PFSP comparator
 4: end if
 5: end while
 6:
 7: H-PFSP Comparator:
 8: for all job for which isFairPhase(job)==true do
 9: assign job with the least tasks assigned yet
 10: return
 11: end for
 12: for all job for which isCompletable(job)==true do
 13: assign job with shortest remaining processing time
 14: return
 15: end for
 Algorithm 3 Function: isStartPhase(job)
 1: /* Decide if a job is in fair phase of Map or Reduce */
 2: if getStage(job) = MAP then
 3: return scheduled Map tasks number < FAIR LIMIT
 4: end if
 5: if getStage(job) = REDUCE then
 6: return scheduled Red tasks number < FAIR LIMIT
 7: end if
 8:
 9: Function: getStage(job)
 10: if job.getStatus().mapProgress() < 0.95 then
 11: return MAP
 12: else
 13: return REDUCE
 14: end if
 IV. PERFORMANCE EVALUATION
 We have implemented H-PFSP, and We test our algorithm
 on a cluster of 10 nodes. The OS is Red Hat Enterprise Linux
 AS release 4. Our implementation can be embedded into the
 Fair scheduler. We can use Fair scheduler to share the slot
 resources among different pools, while adopting our H-PFSP
 scheduler within each pool. The code is an independent
 plug-in module. Thus the delay scheduling and speculative
 execution policies are still in use. We have also developed
 a simulation tool to verify the H-PFSP algorithm in a large-
 scale environment. Workloads can be generated from the job
 generator and injected to the H-PFSP simulator.
 A. Results for mean flow time
 The main motivation of our work is to reduce the mean
 flow time of jobs in MapReduce like systems without
 undermining fairness. We test our algorithm and compare it
 with the Fair scheduling algorithm. In addition, we choose
 Algorithm 4 Function: isCompletable(job)
 1: /* Decide if a job is completable */
 2: for all jobi for which jobi.v < job.v do
 3: if job.w > jobi.s then
 4: return false
 5: end if
 6: end for
 7: return true
 
 
 
 
 
     	
 

 
 
 
 
 
 
 
 
 
 
   !   ! 
 Figure 2. Mean flow time of H-PFSP and Fair scheduling vs. job count.
 different values of FAIR LIMIT in the start phase of H-PFSP
 and observe the effect of this parameter on the performance.
 The results show the flow time of each job using H-PFSP
 is seldom longer than the corresponding flow time using
 the Fair scheduling policy, which means the fairness is
 guaranteed. Negative results appear in some cases since the
 system is volatile and the system status differs between two
 tests at different times.
 We have submitted several WordCount jobs in Hadoop
 with the same data size simultaneously. Fig. 2 shows the
 mean flow time under H-PFSP and Fair scheduling. With
 a small number of jobs, no significant difference exists
 between them. To explain this, consider an extreme case,
 there will be no difference at all if only a single job is
 submitted to the system. With more jobs submitted, the slot
 resources are shared by more jobs such that per-job flow time
 increases accordingly. As shown in Fig. 2, the difference
 between them increases with the number of jobs submitted.
 We have tested the performance of H-PFSP by setting two
 different FAIR LIMIT values. H-PFSP1 sets FAIR LIMIT to
 2 and H-PFSP2 sets a higher value 5. The results show that
 setting a lower value improves the performance. Intuitively,
 the result shows that the performance H-PFSP2 is between
 H-PFSP1 and the Fair scheduling policy since H-PFSP2 can
 be seen as a policy striking an average between H-PFSP1
 and the Fair scheduling. Although large FAIR LIMIT may
 degrade the performance, we suggest a moderately higher
 value of FAIR LIMIT in practice, especially when each split
 task is too small or the environment is volatile.
 We also test the TeraSort benchmark in Hadoop. In each
 test, we submit TeraSort jobs with the same size, from 1G
 to 5G respectively. The data is generated by the TeraGen
 program using Hadoop. As shown in Fig. 3(a), the mean
 flow time of jobs under H-PFSP is less than the time
 under the Fair scheduling policy. With larger job sizes, the
 improvement is also larger.
 Fig. 3(b) shows the mean flow time of five TeraSort jobs
 with different data sizes in a single test. We generate five
 data sets ranging from 1G to 5G using TeraGen tool. The
 result shows that small jobs have more chance to finish
 1104

 	
 
 	
 
     	
 
 
 
 	
 
 

 
 
 
 
   
 (a) Jobs with same size.
 
 "#	
 "	
 	"	
 #
     	
 

 
 
 
 
 
 
 !
   !   ! 
 (b) Jobs with different sizes.
 Figure 3. Mean flow time of H-PFSP and Fair scheduling vs. job size.
 earlier by our algorithm, while the flow time of the largest
 job using H-PFSP is almost the same as using the Fair
 scheduling policy. It agrees quite well with our design as
 we let small jobs have more opportunity to obtain the slot
 resources earlier so as to accelerate their running and reduce
 the mean flow time of all jobs. The large jobs queue behind,
 so that they will not block other jobs by occupying slots
 without releasing them for some time. No matter which
 scheduling policy is used, the largest job is always finished
 in the end, so that its completion time exhibits no significant
 difference.
 B. Results for mean slowdown
 The slowdown metric [8] is often used to measure how
 efficient the designed multi-job scheduler is. It is defined
 as the flow time of a job using a specific scheduling policy
 divided by the ideal flow time if it runs in the system without
 sharing resources with other jobs, i.e., it is the only job in
 the system. This metric can be seen as normalized flow time.
 We have done several tests with heavier workloads using
 our implemented simulation tools. We simulate the work-
 loads with Poisson arrivals and exponentially distributed job
 sizes. The average interval is set to 5 mins in this simulation.
 We generated 50 jobs using the workload generator. The slot
 count is 30. Fig. 4 summarizes the results by a slowdown
 profile plot. It compares the mean slowdown between H-
 PFSP and Fair. The number of tasks and task execution
 time of each job are pre-defined and fixed, i.e., each job
   	 
     
      	 
   
 

 
 
 
 
 $ $ $ $	 $
 $ $ $ $
 
 
 
 
 Figure 4. Mean slowdown profile plot of H-PFSP and Fair scheduling. We
 compare the mean slowdown by adjusting the FAIR LIMIT parameter. The
 number of tasks and the task execution time of each job are pre-defined
 and fixed. The execution time of all tasks for each job is equal.
    	 
  
   	     	
 
 
 
 
 	
 $ $ $	 $
 $ $ $
 
 
 
 
 
 
 
 Figure 5. Mean slowdown profile plot of H-PFSP. The task execution time
 follows normal distribution N (10, 2.236).
 has 200 tasks and each task takes 10 mins to execute. The
 number of tasks and the execution time of all tasks do
 not vary with time. We compare the mean slowdown by
 adjusting the FAIR LIMIT parameter. H-PFSP has smaller
 mean slowdown. With larger FAIR LIMIT, jobs run in
 the fair phase for more time which makes the scheduling
 algorithms inefficient.
 Next, we add randomness to the execution time of each
 task. We assume that it follows normal distribution. Fig.
 5 shows that as FAIR LIMIT becomes larger, the mean
 slowdown time first reduces. This is because the system can
 make more accurate estimation with longer fair phase.
 V. RELATED WORK
 MapReduce Scheduling. Google has developed a MapRe-
 duce programming model [4] to handle big data applications
 like search engines consisting of crawling and index process-
 ing [9]. Hadoop is an open-source Java implementation of
 MapReduce [5]. LATE [10], Mantri [11] and SAMR [12]
 scheduling algorithms take heterogeneous environment into
 consideration. Berkeley contributes a plug-in scheduler [13],
 [14]. A similar idea is used in designing Capacity scheduler
 [15]. Data locality [16] is considered to further accelerate
 the execution of MapReduce Job.
 Scheduling Optimization. Simple scheduling policies like
 FIFO and PS are not priority-based policies. They provide
 1105
low performance such as mean response time. Priority-based
 policies have advantages that they optimize mean response
 time as SRPT does, and provide service differentiation
 [17]. However, they usually cannot guarantee the fairness
 property. FSP uses job size information to achieve high
 performance while guaranteeing fairness [7]. Some other
 scheduling policies such as PFSP and OFSP [6] have been
 designed to improve FSP. They both belong to the protective
 scheduling category. However, none of them is applicable
 to parallel execution of jobs. Besides, they perform well
 only if job size information is known a priori [18]. FSP can
 be modified to improve MapReduce performance in [19].
 However, it only uses historical job profile for estimation,
 which is inaccurate when you have many diverse jobs,
 workloads and volatility of system status. Some scheduling
 algorithms [20] consider load balance to eliminate the long
 tail effect.
 VI. CONCLUSION
 We have designed H-PFSP, a hybrid protective sched-
 uler for MapReduce system. It is applicable to multi-
 user environments either within a company or on a cloud
 computing platform. H-PFSP improves per-job performance
 and reduces mean flow time with job execution information
 estimated during job progress, while guaranteeing that every
 job finishes no later than under PS as the protectiveness
 property guarantees. H-PFSP is implemented as a plug-in
 module of Hadoop and can be used within a scheduling
 pool. We have evaluated H-PFSP by the implementation
 using Hadoop and simulations. The results show that H-
 PFSP reduces the mean flow time of jobs proportionally to
 the job number and size without sacrificing the fairness.
 ACKNOWLEDGMENT
 This work is supported in part by the National Basic Re-
 search Program of China under Grant No. 2010CB328105,
 2009CB320504, the National Natural Science Foundation
 of China (NSFC) under Grant No. 60932003. We thank
 the anonymous reviewers for their suggestions that help us
 improve this paper.
 REFERENCES
 [1] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly, “Dryad:
 distributed data-parallel programs from sequential building
 blocks,” SIGOPS Oper. Syst. Rev., vol. 41, pp. 59–72, March
 2007.
 [2] D. G. Murray, M. Schwarzkopf, C. Smowton, S. Smith,
 A. Madhavapeddy, and S. Hand, “Ciel: a universal execution
 engine for distributed data-flow computing,” in NSDI’, 2011,
 pp. 9–9.
 [3] Amazon elastic compute cloud. [Online]. Available:
 http://aws.amazon.com/ec2/
 [4] J. Dean and S. Ghemawat, “Mapreduce: simplified data
 processing on large clusters,” Commun. ACM, vol. 51, pp.
 107–113, Jan. 2008.
 [5] Apache hadoop. [Online]. Available: http://hadoop.apache.org
 [6] E. Friedman and G. Hurley, “Protective scheduling,” Cornell
 University Operations Research and Industrial Engineering,
 Tech. Rep. 2007-11-09T21:01:23Z, March 2003.
 [7] E. J. Friedman and S. G. Henderson, “Fairness and efficiency
 in web server protocols,” SIGMETRICS Perform. Eval. Rev.,
 vol. 31, pp. 229–237, June 2003.
 [8] M. Gong and C. Williamson, “Simulation evaluation of hybrid
 srpt scheduling policies,” in MASCOTS, 2004, pp. 355–363.
 [9] D. Zhang, L. T. Yang, and H. Huang, “Searching in internet of
 things: Vision and challenges,” in ISPA, 2011, pp. 201–206.
 [10] M. Zaharia, A. Konwinski, A. D. Joseph, R. Katz, and I. Sto-
 ica, “Improving mapreduce performance in heterogeneous
 environments,” in OSDI’08, 2008, pp. 29–42.
 [11] G. Ananthanarayanan, S. Kandula, A. Greenberg, I. Stoica,
 Y. Lu, B. Saha, and E. Harris, “Reining in the outliers in
 map-reduce clusters using mantri,” in OSDI, 2010, pp. 1–16.
 [12] Q. Chen, D. Zhang, M. Guo, Q. Deng, and S. Guo, “Samr:
 A self-adaptive mapreduce scheduling algorithm in hetero-
 geneous environment,” in Proceedings of the 2010 10th
 IEEE International Conference on Computer and Information
 Technology, ser. CIT ’10, 2010, pp. 2736–2743.
 [13] M. Zaharia, D. Borthakur, J. S. Sarma, K. Elmeleegy,
 S. Shenker, and I. Stoica, “Job scheduling for multi-user
 mapreduce clusters,” University of California at Berkeley,
 Tech. Rep. Technical Report UCB/EECS-2009-55, April
 2009.
 [14] Apache hadoop fair scheduler. [Online]. Available:
 http://hadoop.apache.org/docs/r1.1.2/fair scheduler.html
 [15] Apache hadoop capacity scheduler. [Online]. Available:
 http://hadoop.apache.org/docs/r1.1.2/capacity scheduler.html
 [16] M. Isard, V. Prabhakaran, J. Currey, U. Wieder, K. Talwar,
 and A. Goldberg, “Quincy: fair scheduling for distributed
 computing clusters,” in SOSP, 2009, pp. 261–276.
 [17] A. Wierman, “Scheduling for today’s computer systems:
 bridging theory and practice,” Ph.D. dissertation, Carnegie
 Mellon University, 2007.
 [18] D. Lu, H. Sheng, and P. Dinda, “Size-based scheduling poli-
 cies with inaccurate scheduling information,” in MASCOTS,
 2004, pp. 31–38.
 [19] C. D. Michiardi Pietro, Barbuzzi Antonio, “Shared cluster
 scheduling: a fair and efficient protocol,” Networking and
 Security Deptartment, Institut Eurecom, Tech. Rep. Research
 Report 11-259, 2011.
 [20] K. Slagter, C.-H. Hsu, Y.-C. Chung, and D. Zhang, “An
 improved partitioning mechanism for optimizing massive data
 analysis using mapreduce,” The Journal of Supercomputing,
 pp. 1–17, 2013.
 1106

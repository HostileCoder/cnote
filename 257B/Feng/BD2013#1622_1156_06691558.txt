Agrios:  A Hybrid Approach to Big Array Analytics  
Patrick Leyshock, David Maier, Kristin Tufte 
Computer Science 
Portland State University 
Portland, Oregon, U.S.A. 
leyshocp, maier, tufte@cs.pdx.edu 
 
Abstract— Hybrid systems for analyzing big data integrate an 
analytic tool and a dedicated data-management platform.  The 
necessary movement of data between the components of a hybrid 
system can lead to performance problems, if that movement is 
not managed effectively.  We present Agrios, a hybrid analytic 
system for array-structured data, integrating R and SciDB.  
Agrios minimizes data movement between the two components of 
the hybrid, using techniques repurposed from relational database 
query optimization.  
Keywords—big-array analytics; query optimization; R; SciDB;  
I. INTRODUCTION 
 The analytic tools R, SAS, and Matlab are like home bak-
 ers’ equipment for data scientists.  Baking treats for home con-
 sumption using measuring cups and a rolling pin is relatively 
easy, but these tools are not sufficient to feed an army.  In the 
same manner, data scientists’ traditional analytic tools excel 
with small datasets, but are inadequate with big data, slowing 
to a crawl when the inputs’ size exceeds main memory.  
Scientists and businesses are hungry for analytics on array-
 structured big data, and data scientists need tools capable of 
quickly performing analyses on massive disk-resident datasets. 
 Data scientists have developed workarounds for analyzing 
big array-structured datasets using their traditional tools.  Some 
resort to sampling the data, others process the data “one loaf at 
a time,” dividing it into main-memory-sized chunks for 
iterative processing.  Though often effective, these ad-hoc 
solutions are often both slower – and more brittle – than 
systems explicitly designed for analyzing big data. 
 When workarounds fail, data scientists must switch to a 
dedicated big-data system.  There are three options.  In place of 
his or her existing analysis tool, a data scientist may: 
 
‚ Replace the analytic tool with a system explicitly 
designed to efficiently manage and analyze big data. 
Such systems range from traditional relational databases 
to newer data processing platforms based on a map-
 reduce processing paradigm. 
 ‚ Install an augmented version of the analytic tool, which 
has been extended to improve performance on big data 
through mechanisms such as parallelization or out-of-
 core libraries.  
 
‚ Adopt a hybrid analytic system, which integrates the 
analytic tool with a big-data tool, capturing the best of 
both worlds: sophisticated analytic abilities and func-
 tion calls from traditional tools plus the data-handling 
capabilities of big-data systems. 
     The speed of analysis is an important factor in selecting 
between these three alternatives, but there are important 
additional considerations.  A tool that minimally disrupts 
existing workflows has great value; such a tool allows the data 
scientist to use tried-and-true analytic scripts, with little or no 
modification, on datasets of any size.    
 That this requirement is a desideratum should not surprise 
us.  A home baker, tasked with feeding a crowd, would 
certainly prefer to do so in the comfort of his home, with his 
trusty tools, rather than purchase and master the operation of 
commercial-grade dough sheeters, proofing cabinets, and rack 
ovens.  While food engineers have yet to figure out how to 
make this easy sort of scaling possible in the baking world, in 
our field of computer science we are used to getting large, 
complex jobs done with familiar tools.  A single SQL query 
works on whatever data at which it is pointed, small or large.  
A single C program compiles into code executable on myriad 
machines.   
 Given this consideration, the hybrid approach stands out 
from the three as the best approach.  Hybrid systems present a 
familiar interface to data scientists, but have the power of big-
 data systems in handling disk-resident datasets.  The fact that 
hybrid systems consist of two components, however, raises 
unique complications.  In hybrid systems, operations are 
performed at both components of the hybrid, and data used in 
analyses can reside at either component.  It follows that data 
must move between components of the hybrid.  Hence, a good 
hybrid system satisfies a second constraint:  data scientists 
need not explicitly manage the movement of data between the 
components of the hybrid system. 
     Hybrid systems satisfying this second constraint relieve data 
scientists from the responsibilities of decomposing their analyt-
 ic task, reasoning about data movement within the system, and 
making (possibly incorrect) judgments about what choices 
yield the best performance. Freed from these responsibilities, 
data scientists need not manually determine the component on 
which particular operations are performed, or retool analytic 
scripts if the shape, size, or physical location of their data 
changes.  An ideal hybrid system lets the analyst focus on the 
analysis itself, not on where the analytic work is performed.   
     This second constraint resembles the concept of “physical 
data independence” in modern database systems.  In a system 
with physical data independence, a user need not analyze or 
consider the physical layout of the data when writing queries 
against it; the system automatically determines the fastest way 
to execute queries based on its knowledge of the physical facts 
about the data. 
 Our contributions include:  i)  an implementation of a hy-
 brid system – named Agrios – constructed using R and SciDB, 
2013 IEEE International Conference on Big Data
 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 85
ii)  a partial semantic mapping between the R language and 
SciDB's  Array  Functional  Language  (AFL),  iii)   use  of  a  cost  
model and expression transformations to minimize data 
movement in a hybrid analytic system, and iv)  test results 
quantifying the performance of this hybrid approach. 
There are additional considerations for improving the per-
 formance of hybrid systems, including per-component execu-
 tion times and resource availability.  Though we are address-
 ing these issues in ongoing research, in this paper we focus on 
data movement between hybrid components.  The manage-
 ment of data movement between components is not thorough-
 ly addressed by extant hybrid systems, and automatically 
managing data movement not addressed at all.  Our approach 
addresses this lacuna in a unique manner, utilizing proven 
techniques from relational database query optimization. 
II. HYBRID COMPONENTS 
The components of the Agrios system are R and SciDB. 
Agrios stands for A Generalized R Interface Over SciDB.1  
We present both components here, then examine how, and 
why, we manage data movement between the two.   
A. Overview of R 
R is a language and computing environment modeled after 
S-Plus, and is intentionally designed for analysis of structured 
data [1]. Its primary data object is the vector, a one-
 dimensional array, and it supports complex operations on ma-
 trices and multidimensional arrays as well. Each cell of a vec-
 tor  or  array  contains  a  single  value.   Since  vectors  are  first-
 class data objects, explicit control structures in the language, 
such as for and while loops, are not necessary for operating on 
arrays or vectors.  Functions simply take arrays and vectors as 
inputs; for example, arrays A and B are multiplied by: 
 
   A %*% B  
 
Their elementwise sum is computed as: 
 
      A + B 
 
     R is extensible, through the inclusion of user-developed 
packages.  There are thousands of such packages, their func-
 tionality ranging from database connectivity, to pretty-
 printing, to sophisticated machine-learning algorithms.   
     “Out of the box”, R has two related limitations: it can oper-
 ate  only  on  in-memory  data,  and  it  is  slow  processing  large  
datasets. If the size of intermediate results grows too large, R 
uses virtual memory to store them. At best performance is 
noticeably slowed, and at worst, execution crawls at a snail’s 
pace.  R’s interpreter, implemented as a single-threaded pro-
 cess, also limits performance.  
B. Overview of SciDB 
     SciDB is a scalable database system built explicitly to han-
 dle extremely large array-modeled datasets [2,3].  The funda-
                                                         
1 Agrios is a figure from Greek mythology:  a half-human, half-bear 
Thracian giant.  The human half handles the sophisticated analyt-
 ics, the bear half handles the big data.  The second author reverse-
 engineered the acronym from the name. 
mental data object in SciDB’s data model is the structured 
array, not the unordered relation of an RDBMS. Arrays are 
constituted of cells, each storing the values of attributes. All of 
SciDB’s components – including its optimizer, query proces-
 sor, storage manager – are designed around arrays. The stor-
 age and processing power of SciDB scales through the addi-
 tion of computing nodes, intended to be simple off-the-shelf 
commercial systems. 
     Arrays are operated upon in SciDB using one of its two 
data manipulation languages.  AQL is an SQL-like declarative 
query language; AFL is a functional language with similarities 
to relational algebra. As with R, both of SciDB’s languages 
treat arrays as simple objects; operations on them do not re-
 quire manipulation of the individual elements.  In SciDB’s 
AFL, arrays A and B are multiplied by: 
 
  multiply(A, B) 
 
Their elementwise sum is computed as: 
 
project(apply(join(A,B), result, A.val + B.val), result) 
 
SciDB’s ability to manage and process large collections 
of array-modeled data shows extraordinary promise, but its 
chief shortcoming are its languages and APIs.  AFL’s unfamil-
 iarity to data scientists is a significant obstacle to its adoption.  
AQL is touted by SciDB’s designers as more accessible lan-
 guage, but forcing users to learn a new language creates an 
barrier to adoption. Though the intentions of SciDB’s design-
 ers are good, the system would benefit from an interface more 
familiar to data scientists. 
C. Motivation Behind the Use of R and SciDB 
     R and SciDB are natural candidates for hybrid system 
components, for many reasons.  Their data models are similar, 
so translating expressions over data objects from one language 
to the other is relatively straightforward.  The functions ex-
 posed by both systems also include simple commands appli-
 cable  to  complex objects  such as  arrays.   The  code  of  R and 
SciDB is publicly available, and both systems are extensible.   
     Where the two systems are not similar, their differences 
often complement one another in the context of a hybrid sys-
 tem such as Agrios.  SciDB, unlike R, scales easily and simply 
to work with big data through the incorporation of additional 
computing nodes.  R’s language provides a higher level of 
abstraction than SciDB’s AFL.  R is the language used to 
write Agrios scripts.  Since the language is familiar to data 
scientists, Agrios lets data scientists utilize SciDB’s benefits 
through familiar syntax and semantics. 
     In addition to the two systems’ common and complimen-
 tary aspects, a handful of external factors argue for the utility 
of a hybrid system built with R and SciDB.  Both systems 
recognize as fundamental complex data objects such as vec-
 tors.  That both systems have such a data model is important 
because an extraordinary number of datasets in science and 
engineering are typically modeled as arrays of one or more 
dimensions [4].   Second, R is widely used, and very popular 
with data scientists.  Both the number of in-links to the R-
 Project’s website, and mean monthly email discussion traffic, 
are more than double those of competing tools SPSS and SAS 
86
[5].  Finally, the performance of SciDB for analyzing big data 
shows promise.  SciDB often outperforms relational systems 
on common analytic workloads. Benchmark comparison tests 
between SciDB and an RDBMS demonstrate that SciDB out-
 performs the relational system on many of the benchmark que-
 ries, and is competitive on queries in which the RDBMS out-
 performs SciDB [6]. 
III. STAGING 
Automatically managing the movement of data is key to a 
successful hybrid system.  Though R and SciDB perform the 
analytic heavy lifting in Agrios, neither one of them 
automatically manages the movement of data between the two 
components.  The process of managing data movement we call 
staging; the output of the staging process is an assignment of 
execution locations to the operations in an analytic script.  
(The output of staging is a staging:  while this usage may seem 
a situation ripe for confusion, in practice context always suffic-
 es to make the meaning clear.) 
Hybrid systems perform operations at both components of 
the hybrid, and can store data and intermediate results at either 
component.  As the example in Figure 1 shows, there are bet-
 ter and worse places to perform operations, when considering 
the movement of data.  Suppose vectors C and R are stored at 
component B, and their matrix product is required at compo-
 nent A. The product can be computed at A, requiring that C 
and R first be shipped from B to A.  Alternatively, the product 
can be computed at B, and then the result shipped to A.  The 
choice of execution location affects the amount of data moved 
by orders of magnitude.  If C and R are both vectors contain-
 ing 1000 elements, there is a difference of 998,000 elements 
moved between the option shown in (ii) and the option shown 
in (iii).  This example illustrates two facts about hybrid sys-
 tems:  (1)  we have choices about what data to move, and (2)  
some choices move less data than others.  The fact that there 
are decisions about data movement with significant conse-
 quences means that there are opportunities to build a better 
hybrid system through the automated management of data 
movement.   
For the example above, one possible staging is: 
 
multiply-at-B(C, R) 
another: 
multiply-at-A(C, R) 
 
Since many operations require all input data be located at the  
same   location,   a  staging  also  implies   what   data  must  be 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1:  The amount of data moved depends on the computation location. 
moved where, in order for an operation to be performed. The 
second staging indicates, for example, that vectors C and R 
must be moved from B to A, prior to execution. 
     This example contains only a single multiplication opera-
 tion, but consider that analytic scripts used by data scientists 
may run to dozens of lines, and contain hundreds of opera-
 tions.  When it comes to staging the operations in a script, the 
data scientist has three options; she can: 
 
‚ abandon optimality and resort to a simple staging 
policy, such as “do everything at R” or “do every-
 thing at SciDB”, 
 
‚ hand-stage the script, after reasoning about the opti-
 mal staging, redoing the staging if input properties 
change, or 
 
‚ utilize a system – such as Agrios – that automatically 
identifies the optimal staging. 
 
We demonstrate later that the first option performs poorly.  
Moreover, the first option may not even be possible, given that 
some operations in a script might be available at only one of 
the two components of the hybrid, or that some data objects 
may exceed the size of available memory in a component.  
The second option is labor-intensive; especially if there are 
many initial distributions, shapes, and sizes of the input data, 
or if the location, shape, and size change frequently.  Moreo-
 ver, the reasoning required in the second option can be chal-
 lenging and error-prone.  For nearly all applications, the final 
option – automation – is the desired choice.   
IV.  INTEGRATION 
A. High-Level Architecture 
     The Agrios middleware integrates R and SciDB.  There are 
four main components to Agrios:  parser, accumulator, stager, 
and executor.    
     The parser scans  each  line  of  an  R  program  and  converts  
the statements there to an Agrios Abstract Expression Tree 
(AAET), an internal Agrios data structure.  AAETs contain 
two types of nodes:  internal, corresponding to operations, and 
leaf, corresponding to data objects.  Both types of nodes are R 
objects, and contain important facts about the entities they 
represent.  A leaf node contains the size, shape, and storage 
location of a data object.  An internal node contains the opera-
 tion, size and shape of result, and input references.  After stag-
 ing, internal nodes also contain the execution location that the 
stager selected for the operation.  
     The accumulator collects and combines AAETs until exe-
 cution is forced by an operation such as “print”.  Accumulated 
expressions are then serialized and passed to the stager.      
     The stager performs two jobs:  it rewrites expressions, and 
it stages expressions, attempting to minimize data movement. 
     Once the stager has produced an optimized plan, the plan is 
serialized and passed to the final subsystem of Agrios:  the 
executor.  The executor unpacks the serialized plan and con-
 verts it back to an AAET.  This tree is traversed from the bot-
 tom up.  As each internal node is consumed, data objects are 
moved  as required  by the  staging.  Once the data objects are 
 
87
    
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2:  The architecture and workflow of Agrios.  The result returned to R 
may be the materialized result, or a reference to the result (if stored at SciDB). 
 
situated appropriately, the executor constructs the expression 
involving the data objects in the appropriate language:  R or 
SciDB's AFL.  The executor finally submits the expression to 
either R or SciDB for execution, and handles results and ex-
 ceptions from both components. 
B. Detailed Architecture 
1) Parser and Executor:  Though additional operators are 
slated for implementation, at present Agrios’ parser and 
executor handle four representative operations:  matrix 
multiplication, elementwise addition, aggregate sum, and 
subscripting.  These operations can be performed by either R 
or SciDB.   We selected these operations in part for that 
reason, and also because they:  i) are common building blocks 
of complex analyses, ii)  have different algebraic properties, 
and iii)  modify the shape of their inputs in important ways.  
An array’s shape differs from its size:  a 1 ? 100 vector and a 
20 ? 5  matrix  are  of  the  same size,  but  differ  in  shape.   The  
size alone of an array fails to provide information for effective 
staging; information on its shape is essential – different shapes 
interact with operators in different ways.  The optimal stagings 
may differ for a script with inputs of identical sizes, but 
different shapes. 
     Matrix multiplication can either increase or decrease the 
size of its output, depending on the shape of its inputs.  The 
product of an m ? 1  column vector  and an  1  ? n row vector 
results in a larger m ? n array, while the product of an m ? n 
array and an m ? 1 column vector results in a smaller m ? 1 
column vector. 
     The aggregate sum operation adds elements of an array 
along a specified dimension, which is input as a parameter.  It 
typically reduces the size of its input.   
      Like aggregate sum, the subscript operator also takes input  
parameters, in addition to an input array.  The parameters 
specify the range of values requested, for each dimen-
 sion.  Following the R language, given a 10 ? 10 array B, we 
can specify a specific part of the array with B[3:5, 1:2], a par-
 ticular row with B[4,], or a particular column with B[,3].   
     The result shape of elementwise addition is identical to that 
of any of the elementwise operations – the result takes the 
shape of the larger of the two inputs.  The result shape follows 
R’s convention for this situation.  A consequence of this con-
 vention is that when its inputs are of unequal sizes, the smaller 
object is “recycled” appropriately.  Elementwise adding a sca-
 lar to a vector provides a simple example:  adding the unit 
array [1] and vector [1 2 3 4 5] yields the vector [2 3 4 5 6]. 
 
2) Accumulator and stager:  The accumulator and stager 
are the two components of Agrios responsible for improving 
system performance by automating the movement of data.  
Agrios meets this goal in three ways.  It: i)  accumulates 
expressions prior to expression rewriting, enabling a greater 
number of rewrites, ii)  rewrites expressions during staging, 
exposing additional staging opportunities, and iii)  stages 
expressions, determining the best execution locations for each 
operation in an expression.   
     To illustrate accumulation, suppose these two lines are 
contained in an R script: 
 
B q D + E 
á á á 
A q B + C 
 
     Figure 3 shows representations of these two statements, 
plus an accumulation of the two expressions.  Consolidating 
these two expressions into one permits stagings and expres-
 sion rewrites not possible if the individual expressions are 
considered only in isolation.  With the application of a “left-
 to-right associate” transformation rule, the expression in (iii) 
can be rewritten into a logically equivalent expression – ex-
 pressions (i) and (ii) individually cannot.  
     Agrios’ stager is implemented by its Bonneville subsystem, 
an extension and modification of the Columbia relational da-
 tabase optimizer developed in the late 1990s [7,8].  Bonne-
 ville,   like      Columbia,      has      several      subcomponents,    
 
Figure 3:  Accumulation exposes opportunities for expression rewriting.  
Though neither (i) nor (ii) can be associated, the accumulated expression in 
(iii) may. 
 
 
 
 
 
 
 
 
 
 
 
Figure 4:  An enforcer rule inserts a Xfer operator, mandating data movement 
during execution.  The execution location of each addition operation is indi-
 cated with a subscript. 
 
R SciDB
 Parser
 Accumulator
 Stager
 Executor
 Q
 U
 ER
 Y
 R
 ES
 U
 LT
  
88
including a catalog, rule set, cost model, and search engine.  
While Columbia was intended for relational database systems, 
Bonneville is designed for hybrid systems with array data 
models. 
Columbia was a sensible starting point for Bonneville’s 
development because there are many analogues between stag-
 ing in hybrid systems and query optimization in relational 
systems.  In both cases, we try to identify the least costly que-
 ry or expression that is logically equivalent to the query writ-
 ten by the user. Traditional optimizers consider physical prop-
 erties such as sort order during optimization; stagers consider 
physical properties such as location.  A stager’s cost model is 
concerned with data movement between hybrid components, 
whereas the cost model of relational optimizers considers fac-
 tors such as disk blocks read. 
     The rule set defines what transformations are permitted, for 
each type of expression.  From a single expression, the stager 
uses these rewrite rules to generate multiple equivalent ex-
 pressions, each of which are then staged.  The stager then 
chooses the staging which moves the least amount of data.  
     There are several types of rule, one of which is the enforcer 
type.   Enforcer rules ensure that data is moved between com-
 ponents of the hybrid, by inserting new operations into an ex-
 pression.  Suppose the root operation of an expression must be 
performed at R.  If the input to the operation is not already 
located at R, the data object input must be transferred from 
SciDB to R.  An enforcer rule such as “Xfer” forces the data 
movement.    In Figure 4, application of the Xfer enforcer rule 
effects the change from expression (i) to (ii).  In (i), the ele-
 mentwise  addition  of  two  arrays  is  performed  at  R,  with  the  
left input to the operation located at R, and the right input at 
SciDB.  By inserting the Xfer operation (seen in (ii)), the Xfer 
rule “enforces” the fact that the root operation requires both 
operands to be at R. 
 
 
 
 
 
 
 
 
 
 
Figure 5:  Application of the “left-to-right associate” expression rewrite rule 
to the expression on the left yields the expression on the right.  This is an 
example of a consolidating transformation.  Data objects are leaf nodes, oper-
 ations internal nodes.  Grey nodes are located at one component of the hybrid, 
blue nodes at the other.  Data transfers are indicated with a red arrow. 
 
 
 
 
 
 
 
 
 
 
Figure 6:  Application of the “subscript pushdown” expression rewrite rule to 
the expression on the left yields the expression on the right.  This is an exam-
 ple of a reductive transformation.  Data objects are leaf nodes, operators inter-
 nal nodes.  Grey nodes are located at one component, blue nodes at the other.  
Data transfers are indicated with a red arrow. 
     A transformation can reduce data movement in one of two 
ways:  it reduces either the number of transfers required by an 
expression or the amount of data moved in a given transfer.  A 
reduction in the number of transfers usually results from trans-
 formations that consolidate objects at a given location.  Figure 
5 provides an example.  Suppose objects colored grey are lo-
 cated at one component of the hybrid, and objects in blue at 
the other.  Applying a “left-to-right association” transfor-
 mation rule to the expression on the left results in the expres-
 sion on the right.  This association groups like-located objects, 
reducing the number of transfers performed:  there are two red 
arrows indicating inter-component data movement prior to the 
rewrite, and only one after.  Note that the stager changed the 
execution location of the nested operation. 
     Figure 6 shows a “reductive” transformation that decreases 
the amount of data moved in a transfer.  This figure reflects 
the size of the data objects relative to one another.  Applying 
the “subscript pushdown” transformation rule to the expres-
 sion on the left results in the expression on the right.  Suppose 
the elementwise addition operation must be performed at the 
blue component.  By “pushing” the subscript operation 
through the elementwise addition operation, this expression 
rewrite reduces not the number of transfers, but the amount of 
data moved in the transfers. 
     The most complex component of the stager is the search 
engine.  The search engine identifies the plan with the lowest 
estimated cost, by creating and exploring the search space of 
logically equivalent plans.  In searching for the best plan, the 
search engine performs three main tasks:  
 
‚ It expands the collection of plans in the search space, via 
transformation rules.  For example, applying the “com-
 mute” rule to an expression adds the commuted expres-
 sion to the search space.   
 
‚ It calculates costs for plans, based on the cost model, 
properties of the data objects recorded in the catalog, and 
properties deduced for the plans’ operations. 
 
‚ It prunes plans that cannot be the optimal plan.  The cost 
of the least expensive plan is constantly updated by the 
stager.  If, during staging, the cost of a candidate plan’s 
subplan exceeds the total cost of the current best plan, the 
candidate plan is immediately discarded, or pruned. 
 
Agrios’ stager explores the search space of plans using a 
top-down memoization algorithm guaranteeing identification 
of a plan that minimizes data movement.  The potentially 
problematic space and time requirements for this process are 
kept in check through the use of techniques pioneered in Co-
 lumbia.  The memory footprint remains small through a com-
 pact representation of the search space, while the growth of 
the space is checked by aggressive rule-based pruning tech-
 niques that do not sacrifice optimality. 
Bonneville’s cost model and catalog contain the values 
used to compute the cost of expressions.  The catalog stores 
the logical properties required to perform these cost computa-
 tions – such as the shape of a data object – as well as the phys-
 ical properties of the object – such as its storage location.   
 
[ ]
 +
 +
 [ ] [ ]
  
89
The current cost used for staging is the total number of 
data elements moved, that is, the number of cells in the ar-
 ray.  The cost of data movement in Agrios is symmetric at 
present; we assume it costs the same to move an object from R 
to SciDB as vice versa.  If staging requires movement of an n 
? m matrix and a 1 ? p matrix, the cost of the staging is (n * 
m) + (1 * p).  A less expensive staging might require that only 
the 1 ? p matrix  be  moved;  this  second  staging  is  (n ? m) 
cheaper than the previous staging.   
The cost model is simple, but provides sufficient insight 
into how accumulation, expression rewriting, and staging im-
 proves performance by automating the movement of data in a 
hybrid system.  This cost model is reasonable, moreover, 
when data objects are dense and uncompressed, properties 
common in many applications.  We are currently augmenting 
the cost model of Agrios to include additional factors, includ-
 ing the compression status of data objects, estimated execution 
time at components, and network-transfer time.  Now that we 
have  examined  the  details  of  Agrios,  let  us  examine  the  sys-
 tem’s performance. 
V. EXPERIMENTAL RESULTS 
     The components of Agrios reduce data movement in hybrid 
systems through three related techniques:  accumulating mul-
 tiple expressions into one, rewriting expressions through the 
application of transformation rules, and staging expressions by 
identifying the optimal assignment of execution locations. 
     The experiments below evaluate some of the benefits of 
these techniques for reducing data movement. 
A. Methodology 
We conducted experiments on three test queries.  These 
queries contain operators common in analytic scripts that are 
supported by Agrios:  matrix multiplication, aggregate sum, 
elementwise addition, and subscript.  The number of operators 
and input data objects for each query are shown in Table I.  A 
query’s data objects form a collection.  To explore the effects 
of shape and size on Agrios, we input into each query several 
different collections, where the shape and size of the data ob-
 jects varied between collections.  The number of collection 
variants  is  also  shown  in  Table  I.   Figure  8  depicts  Query  2  
and its “standard” collection of input data objects – one of this 
query’s three collection variants.  
 
 
Figure 7:  Data movement of cost-staged queries compared to naïvely-staged 
“do it all at one place” queries – Query 1. 
 
In each experiment we consider all possible sitings of the 
data objects, where a siting defines the initial location of all 
the input data objects.  Each experiment considers, therefore, a 
siting where all data objects are located at R, a siting where all 
data objects are located at SciDB, and all combinations of 
sitings in-between.  In keeping with our cost model, we meas-
 ure the number of data elements moved in each experiment.  
Note that this metric is independent of the particular hardware 
on which R and SciDB run.  
B. Simple Staging 
     Our first claim is that staging alone substantially reduces 
the amount of data transferred.   Specifically, Agrios’ cost-
 staged queries transfer fewer data elements than queries 
staged by simpler staging policies.  
     For each of the three test queries, and for three alternative 
staging policies, we recorded the number of data elements 
moved.  These results we compared to the number of data el-
 ements moved by cost-staged queries.  The first two alterna-
 tive policies are simple:  they are “do everything at R” and “do 
everything at SciDB”. The third policy is a greedy policy.  For 
binary operations, the greedy policy performs an operation at 
the location of the larger input object, randomly breaking ties 
if  the  inputs  have  identical  sizes.   For  unary  operations,  the  
greedy policy performs the operation at the location of the 
input.  The greedy policy operates “bottom up”; its decisions 
on execution location consider only one operator at a time.  
For each operation, the greedy policy assigns it an execution 
location that guarantees that that particular operation moves 
the  minimal  amount of data;  this  choice  does  not guarantee 
 
+
 B
 A
 C D E
 F
 +
  
 
Figure 8:  Query 2 with its “standard” collection of six input data objects.  The 
relative shape and size of the lettered boxes reflects the relative shape and size 
of the data objects.  In one of the two alternative collections of data objects for 
this query, the column vector E is replaced with a square matrix with the same 
shape and size as D. 
 
 
 
 
 
 
 
 
 
 
TABLE I.          QUERY DETAILS 
Operators in 
query 
Data objects in 
query 
Collections of 
data objects  
Query 1 12 10 2 
Query 2 9 6 3 
Query 3 10 9 3 
90
 
 
 
 
 
 
 
 
 
 
 
that the amount of data moved by the entire query is mini-
 mized.   
     Results for one input collection of Query 1 are shown in 
Figure 7.  Each point on the plot shows a result for at least one 
different siting.  In both graphs, the vertical axes represent the 
number of data elements transferred by Agrios, the horizontal 
axes the number of data elements transferred by an alternative 
staging policy.  Points to the lower-right of the line indicate 
instances where Agrios transferred fewer data elements than 
the alternative, while points on the line represent instances 
where the two policies transferred the same number.  
     Results for all three queries are presented in Table II.  The 
table shows the percent reduction in the number of data ele-
 ments moved, between Agrios’ cost-based staging and one of 
the three alternate staging policies.  The first value shows av-
 erage reductions for all sitings, across all collections for the 
query; the value in parentheses shows average reductions 
across all collections, but only for cases where cost-based 
staging moves fewer data elements than the alternative policy.  
(Cost-based staging moves the same number of elements as 
alternative policies on average only 13, 25, and 21% of the 
time, for Queries 1, 2, and 3, respectively.)  Across all queries, 
Agrios moves substantially fewer data elements than both of 
the “All-at” policies.  Agrios also outperforms Greedy, though 
by smaller margins than the “All-at” policies.   In no cases 
does cost-based staging move more data elements than an al-
 ternative policy. 
    Examining all possible sitings helps bound the performance 
of Agrios, but one could argue that certain sitings are more 
likely to be found “in the wild” than others.  While hybrid 
systems store data at both locations of the hybrid, in practice 
one might expect to see the smaller data objects of a query 
stored at R, and the larger data objects stored at SciDB.  Intui-
 tively, such sitings lend themselves to an All-at-SciDB staging 
policy.  With this in mind, we hand-identified a number of 
sitings satisfying this expectation, and compared the number 
of  data  elements  moved  by  an  All-at-SciDB  policy  to  the  
 
 
 
 
 
 
 
 
 
 
Figure 9:  Staging and expression rewriting moves fewer data elements than 
staging alone.  Shown are results for Query 3, for two different collections of 
input data objects. 
 
number  of  data elements moved by Agrios.  Though in some 
instances the results were similar (with Agrios always moving 
no  more  data  elements  than  All-at-SciDB),  in  many  cases 
Agrios’ cost-based staging moved four- to ten-times fewer 
data elements than All-at-SciDB. 
C. Staging with Expression Rewriting 
     Expression rewriting can reduce the amount of data moved, 
over and above the reduction provided by cost-based staging 
alone.  Expression rewriting, which transforms the expression 
written by the user into logically equivalent alternatives, in-
 creases the number of plans considered by Agrios’ stager.  
The benefits of expression rewriting are illustrated by compar-
 ing the number of data elements moved by staging alone, to 
the number of data elements moved by staging augmented by 
expression rewriting.  Figure 9 shows results for Query 3, us-
 ing two different input collections. 
      The vertical axes for both graphs show the number of data 
elements moved when Agrios rewrites expressions during 
staging.   The  horizontal  axes  show  the  number  of  data  ele-
 ments moved when Agrios stages queries without rewriting 
expressions.  Though on some sitings expression rewriting 
provides no benefit over and above simple staging, in many 
cases expression rewriting reduces data movement. 
      Table III shows the percent reduction in the number of 
data elements moved by Agrios compared to the number 
moved by Greedy.  In all cases expression rewriting and stag-
 ing moves fewer data elements than staging alone.  The max-
 imum reductions presented in Table III deserve special atten-
 tion.  While on average the performance of Agrios versus 
Greedy may not be exceptional, the maximum reductions il-
 lustrate that there are cases where Agrios’ performance is re-
 markably better than Greedy’s.  In no cases does Agrios move 
more data elements than Greedy. 
 
 
 
 
 
 
 
TABLE III.          RESULTS, STAGING AND EXPRESSION REWRITING 
Average percent reduction in data elements moved: 
  all sitings (improved sitings) 
Maximum percent reduction in data elements moved: 
all sitings 
Agrios vs. Greedy, no 
expression rewriting 
Agrios vs. Greedy, with 
expression rewriting 
Agrios vs. Greedy, no ex-
 pression rewriting 
Agrios vs. Greedy, with ex-
 pression rewriting 
Query 1 19.9 (25.5) 27.1 (29.4) 63.0 83.3 
Query 2 1.1 (2.5) 8.3 (12.7) 33.3 66.7 
Query 3 17.4 (23.3) 46.1 (50.3) 65.5 99.9 
 
TABLE II.          AVERAGE PERCECNTAGE REDUCTION IN DATA 
ELEMENTS MOVED: ALL SITINGS (IMPROVED SITINGS) 
Agrios vs. 
All-at-R 
Agrios vs. All-
 at-SciDB 
Agrios vs. 
Greedy 
Query 1 35.6  (39.2) 41.9  (43.2) 19.9  (25.5) 
Query 2 70.0  (77.4) 68.8  (77.2) 1.1  (2.5) 
Query 3 32.7  (42.3) 34.3  (41.5) 17.4  (23.3) 
91
D. Staging with Expression Accumulation 
     Expression accumulation can also reduce data movement. 
To explore the utility of accumulation, we first ran our queries 
through Agrios, recording the amount of data moved with the 
optimal stagings.  We then divided each query into several 
subqueries, and independently staged each of the subqueries.  
In both cases, expression rewriting was enabled.  The total 
cost  of  the  unaccumulated  query  was  the  sum of  the  costs  of  
the individual subqueries, staged piecewise.   
    This experiment simulates cases where an analytic script 
contains many lines of code.  Figure 10 illustrates a test case, 
showing the complete (accumulated) Query 1, together with 
the “cut planes” which chop the query into smaller subqueries.  
This query could be expressed either as a single line of R 
code: 
 
 result q (A+((B%*%C)%*%D))[1:100,1:20] 
  %*%(sum(E)+(F+G)) 
  +(H%*%(I%*%J))[1:20,1:100]) 
 
or as several lines: 
 
 temp.1 q (B%*%C)%*%D 
 temp.2 q (sum(E)+(F+G)) 
  +(H%*%(I%*%J))[1:20,1:100] 
 result q (A + temp.1)[1:100,1:20] 
  %*% temp.2 
 
(Note that in the three-line version, substituting the values for 
temp.1 and temp.2 into result yields the single-line version of 
the  query.)   An analyst  might  prefer  the  latter  chunk of  code  
over the former for many reasons – legibility, coding stand-
 ards, or ease of debugging. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 10:  Query 1, logically subdivided into subexpressions along the dotted 
“cut planes”. 
 
 
Figure 11:  Data movement of accumulated queries compared to unaccumu-
 lated subqueries.   
 
      The benefit of accumulation is demonstrated by comparing 
the amount of data moved in the large single query to the total 
amount moved by all of the subqueries.  Representative results 
are shown in Figure 11.  An accumulated query moves no 
more data elements than its unaccumulated subqueries, and in 
many cases the accumulated query moves fewer.  The histo-
 gram adjacent to the scatter plot shows the frequency and im-
 pact of accumulation.  More often than not, accumulating que-
 ries reduces data movement, in many cases reducing the num-
 ber of data elements transferred by over 40%. 
  
VI. RELATED WORK 
A. Other Hybrid Systems Using R 
      Several hybrid systems already integrate R with a big data 
tool, including Ricardo, RICE, and RIOT.  Ricardo integrates 
R with the Hadoop stack, an open-source implementation of 
Google’s MapReduce system [9].  Large datasets are stored as 
replicated, partitioned objects in Hadoop’s HDFS file system.  
The analytic work is performed by both R and Hadoop nodes 
executing JAQL scripts written by the user. 
      RICE’s R-Op integrates R with SAP’s HANA, an in-
 memory parallel database [10]. Queries are written in a dedi-
 cated HANA-specific programming language, which may 
include embedded R scripts.  At runtime, the HANA executor 
runs the input program, parallelizing operations – if the script 
is written appropriately – by spawning R processes within 
HANA that execute subparts of the script.  The responsibility 
for identifying parallelization opportunities is exclusively that 
of the HANA user.    
     RIOT-DB – “R with I/O Transparency” – integrates R and 
a MySQL database [11]. Large datasets are stored in the 
RDBMS, with computations on the data performed either at R 
or within the database. RIOT is noteworthy in that it defers the 
evaluation of expressions until necessary, similar to Agrios’ 
expression accumulation.  The RIOT team is working to re-
 place the system’s RDBMS back-end with a dedicated array-
 based storage system, though results have not yet been pub-
 lished.  
      Unlike Agrios, none of these systems automate the move-
 ment of data between the hybrid’s components.  Though the 
systems provide mechanisms for moving data between the 
components, the burden of determining when and where to 
move data is exclusively shouldered by the data scientist. If 
the data scientist wishes to manage data movement with some-
 thing other than a naïve approach (e.g. “Do everything at the 
back-end of the hybrid”), he or she is responsible for decom-
 posing the analytic task, reasoning about data movement with-
 in the system, and manually assigning execution locations to 
the operations of the analysis.   
      
B. Query Optimization 
     Bonneville extends a long line of optimization research 
beginning with the Exodus Optimizer Generator [12].  Exodus 
pioneered use of a “top down” memoization algorithm to ex-
  
92
plore the query search space, as well as the extensibility 
mechanisms used by Agrios.   
     Volcano, Cascades, and Columbia further developed ideas 
initially explored by Exodus [7,8,13,14].  Volcano improved 
upon Exodus by pruning suboptimal plans, a process the au-
 thors described as “directed dynamic programming”.  Cas-
 cades and Columbia refined this pruning process to differing 
degrees, while still guaranteeing plan optimality. 
     A special subtopic of query optimization stems focuses on 
optimizing queries on distributed database systems.  Relevant 
work was conducted by Kossman, whose system made deci-
 sions about data movement based on a semi-random simulated 
annealing algorithm [15].   
     Cornacchia, Papadimos, and Maier address related issues in 
distributed database query optimization [16,17].   Cornacchia 
shows that a simple cost model is effective in determining 
how to distribute the constituent operations of a distributed 
database query.  Papadimos and Maier explored “mutating” 
query plans, dynamically adjusting the execution location of 
constituent operators, and anticipating aspects of our work.  
StatusQuo, while using different techniques than Agrios, is 
another example of optimizing data movement using automat-
 ic placement of functions [18]. 
     Since Volcano, the order of rule application is recognized 
as a factor in optimizer performance.  Pellenkoft et al. identify 
a methodology for avoiding the generation of duplicate plans, 
and we are considering their approach for Agrios [19].  
VII. CONCLUSION 
     Hybrid analytic systems integrate familiar analytic tools 
such as R, with dedicated platforms for managing big data.  
These hybrid systems present familiar functionality to data 
scientists, while extending the capability of the analytic tool to 
include analyses on large, disk-resident datasets.  Though the 
hybrid approach has benefits, a performance-oriented hybrid 
system requires effective management of data movement be-
 tween its two components.   
     Agrios integrates the analytic tool R and the array big-data 
management system SciDB.  Unlike other hybrid approaches, 
Agrios automates the management of inter-component data 
movement, minimizing the amount of data transferred through 
three techniques.  Agrios' stager minimizes data movement by 
using a top-down memoization algorithm to identify the opti-
 mal execution locations for the operations in an analytic script.  
Staging is rendered more effective through the accumulation 
of expressions, and the rewriting of expressions through trans-
 formation rules.   
     Experimental results reveal the benefits of staging when 
compared to alternate staging policies, such as Greedy or All-
 at-SciDB.  The positive effects of accumulation and expres-
 sion rewriting on staging were also demonstrated. 
ACKNOWLEDGMENT 
     This material is based upon work supported by National 
Science Foundation Grant Number 1110917, and the support 
of Intel’s Science and Technology Center for Big Data.  
Thanks to Portland State University’s Datalab, the SciDB 
team, and Paradigm4.  Remarks from our anonymous review-
 ers were helpful and appreciated. 
REFERENCES 
[1] R. Ihaka and R. Gentelman, “R:  A language for data analysis and 
graphics,” Journal of Computational and Graphical Statistics, 299-314, 
1996. 
[2] J. Becla, D. DeWitt, K. Lim, D. Maier, O. Ratzesberger, M. Stonebrak-
 er, and S. Zdonik, “Requirements for science data bases and SciDB,”  
CIDR Perspectives, 202-214,  2009. 
[3] M. Balazinska, J. Becla, P. Cudre-Mauroux, D. DeWitt, B. Heath, H. 
Kimura, K. Lim, D. Maier, J. Patel, J. Rogers, R. Simakov, E. Soroush, 
and S. Zdonik, “A demonstration of SciDB:  A science-oriented 
DBMS,”  VLDB, 87-100, 2009. 
[4] D. Maier and B. Vance, “A call to order,”  Proceedings of the 12th ACM 
Symposium on Principles of Database Systems, 1-16, 1993.   
[5] D. Smith and J. Rickert, “RevoScaleR:  Big data analysis for R using 
Revolution R Enterprise,” http://r4stats.com/popularity, 2010. 
[6] P. Brown, SciDB Developer’s Forum, http://lists.scidb.org, 2011. 
[7] K. Billings, “A TPC-D model for database query optimization in Cas-
 cades,” Master’s Thesis, Portland State University, 1997. 
[8] Y. Xu, “Efficiency in Columbia database optimizer,” Master’s Thesis, 
Portland State University, 1998.   
[9] S. Das, Y. Simanis, K.S. Beyer, R. Gemulla, P.J. Haas, and J. McPher-
 son, “Ricardo:  Integrating R and Hadoop,”  Proceedings of the 2010 In-
 ternational Conference on Management of Data.  987-998, 2011.   
[10] P. Grosse, W. Lehner, T. Weichert, F. Farber, and W.S. Li, “Bridging 
two worlds with RICE,”  Proceedings of the VLDB Endowment, 1307-
 1317, 2011.   
[11] H. Herodotou, J. Yang, and Y. Zhang, “RIOT:  I/O-efficient numerical 
computing without SQL,”  4th Biennial Conference on Innovate Data 
Systems Research, 1-11, 2009. 
[12] G. Graefe and D. DeWitt, “The Exodus optimizer generator,” Proceed-
 ings of the 1987 SIGMOD International Conference on Management of 
Data, 160-172, 1987. 
[13] G. Graefe, “Volcano:  An extensible and parallel query evaluation sys-
 tem,” IEEE Transactions on Knowledge and Data Engineering, 120-135, 
1994. 
[14] G. Graefe, “The Cascades framework for query optimization,” Data 
Engineering Bulletin, 19-28, 1995. 
[15] M. J. Franklin, B.T. Jonsson, and D. Kossmann, “Performance tradeoffs 
for client-server query processing,” SIGMOD Record, 149-160, 1996.   
[16] R. Cornacchia, A. van Ballegooij, and A.P. de Vries, “A case study on 
array query optimization,” Proceedings of the 1st International Work-
 shop on Computer Vision Meets Databases, 3-10, 2004. 
[17] V. Papadimos and D. Maier, “Distributed queries without distributed 
state,”  WebDB, 95-100,  2002. 
[18] A. Cheung, O. Arden, S. Madden, A. Solar-Lezama, A. C. Meyers, 
“StatusQuo:  Making Familiar Abstractions Perform Using Program 
Analysis”, Conference On Innovative Data Systems Research, 2013. 
[19] A. Pellenkoft, C. Galindo-Legaria, and M. Kersten, “The complexity of 
transformation-based join enumeration,” Proceedings of the 23rd VLDB 
Conference, 306-315, 1997.
  
93

 
Towards Reconfigurable Processors for
 Power-Proportional Computing
 Andrey Mokhov, Maxim Rykunov, Danil Sokolov, Alex Yakovlev
 School of Electrical and Electronic Engineering
 Newcastle University, UK
 Abstract—Today we witness a major shift in microelectronics
 engineering priorities. The cost of computation is no longer
 dominated by the cost of building computation hardware. Almost
 every consumer electronic device nowadays contains a powerful
 processor with one or, increasingly often, multiple computation
 cores that are cheap to manufacture. Growing demand for high-
 definition content and associated processing capabilities revealed
 the true cost of computation – energy. ‘Big data’ companies face
 large electricity bills; in other applications, such as healthcare
 and infrastructure monitoring, the energy constraint has a
 different form: instead of a large bill, one faces limited energy
 availability and has to resort to such measures as energy-
 harvesting. In both cases it is essential for the computation system
 to be adaptable to the inflow of energy and computation load
 which often vary dramatically during runtime.
 In this paper we discuss an approach to building processors
 with reconfigurable computation pipeline capable of changing
 the way they fetch and execute instructions depending on energy
 availability and application requirements. We show how to use
 Conditional Partial Order Graphs to model the microarchitecture
 of such a processor, discuss implementation details, and present
 preliminary experimental results on an example of Intel 8051
 microcontroller.
 I. INTRODUCTION
 From 2007 our society uses more energy for browsing
 the Internet than for air travel [3]. It is also predicted that
 the energy (and environmental) footprint of computation and
 data traffic will steadily increase in the near future: Data
 centres will grow and so will the network infrastructure
 together with the number of terminal nodes of the global
 information network such as computers, phones, gadgets and
 other connected cyber-physical devices (so called Internet of
 Things). Energy-efficiency of components at all levels of the
 computation hierarchy is thus becoming a major concern for
 the microelectronics industry. A serious factor impeding the
 progress in addressing this concern is a wide gap between
 the ways of how energy efficiency is approached by hardware
 and software engineers, and this gap is matched by the lack
 of common understanding between the two communities.
 In this paper we discuss an approach to bridging this
 gap by developing a shared design criterion, called power-
 proportionality, on the basis of which both electronics and
 programming solutions can be judged. A computing sys-
 tem, for it to be considered power-proportional, has to keep
 power consumption and computation load proportional to each
 other [8]. That is, an idle system would ideally consume no
 power, whereas given a small energy budget the system would
 adapt itself by reducing its computation flow and lowering
 the delivered Quality-of-Service (QoS), and still remain func-
 tional; see Section II for a detailed discussion. The state-of-
 the-art systems have generally poor power-proportionality; for
 example, the servers used in data centres typically consume
 50-60% of peak power under 10% of peak load [4]. Why are
 modern systems not power-proportional? We hypothesise that
 because they are designed to operate in a narrow scope of
 conditions, typically optimised for either high performance or
 low power consumption. This approach is inherently flawed
 because all the design effort is focused on one particular
 operation mode disregarding others.
 Applications
 Operating System
 Drivers/Libraries
 Protocols/Interfaces
 Microcontrollers
 Components
 Software layers
 Hardware layers
 Meters
 feedback control
 Figure 1: Cross-layer power proportionality
 Another issue that makes it difficult to handle unpredict-
 able environmental conditions is the absence of a cross-layer
 feedback-control loop between energy supply components (in
 hardware) and an operating system or an active application
 (in software); see Figure 1. As a result, software is unable to
 steer, or modulate, computation with the aim of maximising
 QoS by prioritising tasks, trading accuracy for energy, and, in
 general, by strategic planning of energy resource utilisation.
 In this paper we discuss cross-layer hardware-software mech-
 anisms for online energy monitoring and reconfiguration with
 particular focus on design and implementation of a power-
 proportional microprocessor, capable of adapting to varying
 operating conditions (such as low or even unstable voltage
 level) and application requirements in runtime.
 II. RECONFIGURABILITY AND POWER PROPORTIONALITY
 How can one build a reconfigurable and adaptable pro-
 cessor? The first thought could be to implement it in the field-
 programmable gate array (FPGA) technology allowing static
 and, sometimes, dynamic reconfiguration. Such a processor
 would be capable of adjusting its internal structure or beha-
 viour by rewiring interconnections between its components
 978-1-4673-6104-0/13/$31.00 ©2013 IEEE
 
System Service
 provision
 Service
 demand
 Power
 supply
 Power
 consumption
 Service provision
 Power supply
 saturation
 power-proportional
 interval of operation
 ideal power-proportionality
 minimum operating power
 interval of unutilised energy
 Power consumption
 Service demand
 saturation
 power-proportional
 interval of operation
 ideal power-proportionality
 minimum power
 interval of wasted energy
 Figure 2: Power proportionality: power supply/consumption and service demand/provision interplay
 or even by changing functionality at the level of individual
 gates. This technology can provide a very fine-grain control
 over a system at runtime, however, the associated overheads
 are extremely high. In particular, in terms of energy consump-
 tion, FPGAs are typically 10-15 times more expensive than
 application-specific integrated circuits (ASICs).
 Since the fine-grain reconfigurability offered by FPGAs is
 overly costly, one shall consider the coarse-grain reconfigur-
 able architectures in the ASIC realm. They significantly lower
 the overheads by dropping reconfigurability in datapath com-
 ponents and utilising their custom designed versions instead.
 The control logic and interconnect fabric, however, retain the
 capability to reconfiguration. In this setting, the key design
 and implementation challenge is to formally describe and
 synthesise the controller whose task is to coordinate hard
 system resources (the datapath components and interconnects)
 according to the runtime information on availability of soft
 system resources (energy, time); the latter can also include
 information on hardware faults in a system, thereby allowing
 the controller to bypass faulty components whenever possible.
 The conventional approach to control logic specification and
 synthesis is to employ Finite State Machines (FSMs) [7] or
 interpreted Petri Nets (PNs) [2] as an underlying modelling
 formalism. Within this approach the designer explicitly de-
 scribes the controller’s behaviour for each combination of
 available resources and operating conditions. The number
 of such combinations and corresponding behaviours grows
 extremely fast with the size and degree of adaptability of
 the system. This leads not only to the state space explosion
 problem, but also to explosion of the specification size [6],
 thus slowing down the synthesis tools, reducing productivity,
 and increasing the overall cost of ASIC development.
 Our approach is based on the crucial observation that the
 controller’s behaviours are strongly related to each other in
 different operating conditions. Indeed, when a system config-
 uration is changed incrementally, e.g., a datapath component
 goes offline and another one is used in its place, the overall
 behaviour of the controller is affected in the same incremental
 manner, therefore one would want to have a joint behaviour
 specification for these two configurations.
 It has been demonstrated that the FSM and PN formalisms
 are not well-suited to describing families of many related
 behaviours [6] and the design methodologies based on them
 have poor scalability in the context of reconfigurable systems.
 As an appropriate alternative, the Conditional Partial Order
 Graph model was introduced by Mokhov et al. [5][6]. The
 model was devised to allow implicit description of families of
 related behaviours in a compact form – see Section III.
 Figure 2 illustrates the concept of power-proportionality
 and its duality. On the leftmost chart one can see depend-
 ency of power consumption on service demand. Ideally, the
 former should grow proportionally to the latter: when there
 is no service demand, the system should consume (near) zero
 power, while any increase in service demand should cause
 a proportional increase in power consumption until the point
 of saturation, when the system can no longer serve all the
 incoming requests. Our design goal is to reduce the interval
 of wasted energy as much as possible.
 On the right hand side of Figure 2 one can see the dual chart
 illustrating dependency of service provision on power supply.
 Indeed, an ideal computation system should provide service
 proportionally to the inflow of energy. From this perspective,
 our design goal is to build computation systems that can start
 delivering certain level of QoS on as low power supply as
 possible, thus minimising the interval of unutilised energy.
 The duality of power-proportionality creates a four di-
 mensional space of power-service interplay: a computation
 system can be considered a black box which has two inputs
 (service demand and power supply) and two outputs (service
 provision and power consumption) as illustrated in the centre
 of Figure 2. Our combined design goal is therefore to achieve
 proportionality between the inputs and outputs of the system.
 III. CONDITIONAL PARTIAL ORDER GRAPHS
 A Conditional Partial Order Graph [6] (further referred to
 as CPOG or graph) is a quintuple H = (V;E;X;  ;  ) where:
  V is a set of vertices which correspond to events (or
 atomic actions) in a modelled system.
  E  V  V is a set of arcs representing dependencies
 between the events.
  Operational vector X is a set of Boolean variables. An
 opcode is an assignment (x1; x2; : : : ; xjXj) 2 f0; 1gjXj
 of these variables. An opcode selects a particular partial
 order from those contained in the graph.
   2 F(X) is a restriction function, where F(X) is the
 set of all Boolean functions over variables in X .  defines
 the operational domain of the graph: X can be assigned
 
a: 1
 d: 1
 b: 1
 c: x e: x_
 x 
x 
1
 x _
 x _
 x _
 x _
 ?(x)=1 
(a) Full notation
 a
 d
 b
 c: x e: x_
 x 
x 
x _
 x _
 x _
 x _
 ?(x)=1 
(b) Simplified notation
 Figure 3: Graphical representation of CPOGs
 only those opcodes (x1; x2; : : : ; xjXj) which satisfy the
 restriction function, i.e.  (x1; x2; : : : ; xjXj) = 1.
  Function  : (V [ E) ! F(X) assigns a Boolean
 condition  (z) 2 F(X) to every vertex and arc z 2 V [E
 in the graph. Let us also define  (z)
 df
 = 0 for z =2 V [E
 for convenience.
 CPOGs are represented graphically by drawing a labelled
 circle for every vertex and drawing a labelled arrow
 for every arc. The label of a vertex v consists of the vertex
 name, a colon and the vertex condition  (v), while every
 arc e is labelled with the corresponding arc condition  (e).
 The restriction function  is depicted in a box next to the
 graph; operational variables X can therefore be observed as
 parameters of  .
 Fig. 3(a) shows an example of a CPOG with jV j = 5
 vertices and jEj = 7 arcs. There is a single operational
 variable x; the restriction function is  (x) = 1, hence both
 opcodes x = 0 and x = 1 are allowed. Vertices fa; b; dg have
 constant  = 1 conditions and are called unconditional, while
 vertices fc; eg are conditional and have conditions  (c) = x
 and  (e) = x respectively. Arcs also fall into two classes:
 unconditional (arc c ! d) and conditional (all the rest). As
 CPOGs tend to have many unconditional vertices and arcs we
 use a simplified notation in which conditions equal to 1 are
 not depicted in the graph; see Fig. 3(b).
 The purpose of conditions  is to ‘switch off’ some ver-
 tices and/or arcs in a CPOG according to a given opcode,
 thereby producing different CPOG projections. An example
 of a graph and its two projections is presented in Fig. 4.
 The leftmost projection is obtained by keeping in the graph
 only those vertices and arcs whose conditions evaluate to 1
 after substitution of variable x with 1 (such projections are
 conventionally denoted by Hjx=1). Hence, vertex e disappears
 (shown as a dashed circle ), because its condition evaluates
 to 0:  (e) = x = 1 = 0. Arcs fa! d; a! e; b! d; b! eg
 disappear for the same reason; they are shown as dashed
 arrows . The rightmost projection is obtained in the
 same way with the only difference that variable x is set
 to 0; it is denoted by Hjx=0, respectively. Note that although
 the condition of arc c ! d evaluates to 1 (in fact it is
 constant 1) the arc is still excluded from the resultant graph
 because one of the vertices it connects, viz. vertex c, is
 excluded and naturally an arc cannot appear in a graph without
 one of its vertices. Each of the obtained projections can be
 regarded as specification of a particular behavioural scenario
 a
 d
 b
 c: x e: x_
 x 
x 
x _
 x _
 ?(x)=1 
a
 d
 b
 c e
 x=1 
a
 d
 b
 c e
 x=0 
x _
 x _
 Figure 4: CPOG projections: Hjx=1 (left) and Hjx=0 (right)
 of the modelled system, e.g. as specification of a processor
 instruction. Potentially, a CPOG H = (V;E;X;  ;  ) can
 specify an exponential number of different instructions (each
 composed from atomic actions in V ) according to one of 2jXj
 different possible opcodes.
 IV. IMPLEMENTATION
 In this section we give a brief outline for our power-
 proportional implementation of Intel 8051 microcontroller
 designed using the CPOG model.
 A. Design overview
 The instruction set of Intel 8051 processor contains 255
 instructions and our implementation supports all of them. We
 generally follow the standard 8051 architecture with two on-
 chip RAMs; the program is stored in a reprogrammable off-
 chip ROM. However several important changes were made:
  Our implementation is asynchronous, which gives certain
 advantages over clocked design in case of varying oper-
 ating conditions. The communication between the control
 logic and the datapath units is arranged by means of
 request and acknowledgement signals.
  We use adjustable delay lines to achieve robust operation
 in a wide range of supply voltages (Subsection IV-B).
  We can choose how to execute an instruction depending
 not only on application or environmental aspects, but also
 on the functional correctness of individual components,
 thus addressing the issues of fault tolerance.
 The overall CPOG specification of the microcontroller is
 shown in Figure 5. As explained in Section III, vertices cor-
 respond to datapath components and arcs model dependencies
 between them (see [5] for details on specific components).
 B. Adaptability features
 The most power and time consuming datapath compon-
 ents are adders, multipliers and dividers. To enable power-
 proportionality in a wide range of power supply and service
 demand conditions, we designed two sets of these arithmetic
 units: one optimised for low energy consumption and the other
 one optimised for high performance. Depending on whether
 
Figure 5: CPOG model of Intel 8051 control logic
 there is a shortage of energy or on any other restrictions
 imposed by an active application, we can choose the most
 appropriate datapath unit to be used during an instruction
 execution. The decision can be made at different levels of
 system control: software level, sensors, external signals, etc.
 Delay code Delay wrapper
 req ack
 variable delay line
 Inputs Outputs
 Register
 Datapath computational unit
 Figure 6: Adjustable delay lines
 Our implementation of self-timed datapath components is
 based on bundled-data approach, where each component is ac-
 companied with a matching delay line to signal completion of
 its computation. In order to correctly function in a wide range
 of operating conditions (e.g., supply voltage or temperature), a
 component needs to adjust the latency of its completion signal.
 We propose to address this issue by use of adjustable delay
 line, whose latency is selectable by the delay code in runtime,
 as shown in Figure 6.
 C. Experiments
 We implemented the design in STMicroelectronics 130nm
 technology process and conducted preliminary experiments to
 estimate the level of adaptability we can achieve using the
 discussed approach. Here we briefly report the results.
 The nominal voltage is 1.2V but the chip is capable of
 operating in the range from 0.2V to 1.5V of supply voltages:
  0.89V to 1.5V: full capability mode.
  0.74V to 0.89V: at 0.89V the RAM starts to fail, so the
 chip can only operate using internal registers.
  0.22V to 0.74V: at 0.74V the program counter starts
 to fail, so the chip gets stuck at executing the same
 instruction forever. Surprisingly, the rest of the control
 logic synthesised using the CPOG model continues to
 operate correctly down to 0.22V.
 In terms of performance, we measured the range from 67
 millions of instructions per second on nominal voltage (1.2V)
 down to ~2700 instructions per second at 0.25V.
 V. CONCLUSIONS
 We discussed an approach to architecting power-
 proportional computation systems. The approach was tested
 by designing and implementing the Intel 8051 microcontroller
 capable of operating in the range from 0.2V to 1.5V of
 supply voltages.
 Our future research is to push the boundaries of power-
 proportionality even further by using the self-timed SRAM
 capable of operating at low voltages [1], as well as by
 implementing the key control structures, such as program
 counter, using high-reliability logic with low fan-in gates.
 ACKNOWLEDGEMENT
 This work was supported by EPSRC grant EP/I038357/1
 (eFuturesXD, project PowerProp).
 REFERENCES
 [1] A. Baz, D. Shang, F. Xia, and A. Yakovlev. Self-Timed SRAM for Energy
 Harvesting Systems. In Journal of Low Power Electronics, 2011.
 [2] J. Cortadella, M. Kishinevsky, A. Kondratyev, L. Lavagno, and
 A. Yakovlev. Logic synthesis of asynchronous controllers and interfaces.
 Advanced Microelectronics. Springer-Verlag, 2002.
 [3] Cisco Systems Inc. Entering the Zettabyte Era, Visual Networking Index:
 Forecast and Methodology, 2010-2015, 2011.
 [4] David Meisner, Christopher M. Sadler, Luiz André Barroso, Wolf-
 Dietrich Weber, and Thomas F. Wenisch. Power management of online
 data-intensive services. In Proceedings of the 38th annual International
 Symposium on Computer Architecture (ISCA’2011), pages 319–330, 2011.
 [5] A. Mokhov, A. Iliasov, D. Sokolov, M. Rykunov, A. Yakovlev, and
 A. Romanovsky. Synthesis of Processor Instruction Sets from High-Level
 ISA Specifications. IEEE Transactions on Computers, 2013 (in print).
 [6] A. Mokhov and A. Yakovlev. Conditional Partial Order Graphs:
 Model, Synthesis and Application. IEEE Transactions on Computers,
 59(11):1480–1493, 2010.
 [7] Steven Nowick. Automatic Synthesis of Burst-Mode Asynchronous Con-
 trollers. PhD thesis, Stanford University, 1993.
 [8] Alex Yakovlev. Energy-modulated computing. In Design Automation and
 Test in Europe (DATE) conference, pages 1340–1345, 2011.

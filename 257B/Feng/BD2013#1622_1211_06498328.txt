 
Kanthaka: Big Data Caller Detail Record (CDR) Analyzer for Near Real Time 
Telecom Promotions 
 
Pushpalanka Jayawardhana 
Department of Computer Science and Engineering 
University of Moratuwa, UOM 
Moratuwa, Sri Lanka 
e-mail: pushpalankajaya@gmail.com 
Dhanika Perera 
Department of Computer Science and Engineering 
University of Moratuwa, UOM 
Moratuwa, Sri Lanka 
e-mail: dhanikaperera@gmail.com 
 
Ananda Kumara 
Department of Computer Science and Engineering 
University of Moratuwa, UOM 
Moratuwa, Sri Lanka 
e-mail: makuma87@gmail.com 
Amila Paranawithana 
Department of Computer Science and Engineering 
University of Moratuwa, UOM 
Moratuwa, Sri Lanka 
e-mail: amila1204@gmail.com 
 
Abstract – With the competitiveness that is growing in the 
telecom industry, the operators seek for ways to attract and 
keep the subscribers in their network. Giving away attractive 
promotions to their subscribers is a powerful and commonly 
used approach in that context. With the technical limitations 
and the scales of the rising subscriber base, these promotions 
have been limited to a very narrow pattern. In order to gain a 
competitive advantage, the operators need new technologies 
and methodologies to support more attractive newer patterns of 
promotions, despite the challenges involved in complex 
analyzing procedures. In this research study, we have come up 
with a system to select eligible users for a particular pre-
 defined promotion, analyzing the Caller Detail Records, while 
being scalable for a rapidly escalating subscriber base. 
Keywords-CDR; NoSQL; Big Data 
I. INTRODUCTION 
    In the present telecom industry, value added services and 
promotions play a great role in the tightening competition 
among the service providers in escalating the subscriber 
base. The promotions such as 4th minute is free after a 
3minutes of an IDD call represent a common pattern in 
currently available promotions. These are handled by the 
subscribers’ central office telecom switch. A switch is 
responsible for all basic features in placing a call, such as 
dial tone, signaling other telecommunication equipments 
about the call, and maintaining a record of the call, SMS or 
any other event type.  
Consider a promotion pattern example defined as; give a 
promotion notification from ABC snack bar, for all the 
subscribers who have called the ABC snack bar delivery 
number more than 5 times within a month. This involves 
counting the number of calls made by a subscriber to a 
particular number. The capabilities of telecom switch is not 
enough to execute this, but executing this can create a big 
competitive advantage for a telecom operator attracting 
subscriber with useful and interesting promotions while 
increasing the income received from commercial parties 
such as snack bars. The time taken to give away the 
promotion is a critical factor that a particular promotion may 
be effective only in a special season and should be made 
available on the correct time. 
When consider the general pattern of this new promotions, 
a technology should developed to identify who will be 
interested in receiving the notification of the promotion or 
who is eligible to receive the promotion. The CDRs 
generated by the telecom switch can be used to retrieve the 
relevant details and do the analysis. Also the dynamic 
definition and removal of the promotion should be available 
so that it can serve the need of the market. 
This research study is focused on developing a system to 
fulfill this need, answering the challenges of scale and speed. 
More specifically ‘Kanthaka’ is capable of analyzing 30 
million records per day as for the requirement of telecom 
operators’ subscriber base, in near real time (Maximum 30s 
delay). According to the current settings of telecom 
operators these records will be fed to ‘Kanthaka’ in 10 
minute intervals as they are generated. This paper presents 
the considered technologies in developing the system, a brief 
comparison of them, selected technologies, the 
implementation details of the system and final performance 
readings. 
II. TECHNOLOGY SELECTION 
Considering the scale and speed requirements of the 
system, a literature survey was conducted on possible 
technologies to use. The technologies used in current social 
network sites and search engines were analyzed to identify 
feasible technologies that are used for near real time analysis 
of data [1].  ‘Big Data’ technologies are a category emerged 
with the high volume data analysis needs raised with the 
hike of generation of information with the social network 
sites etc. So the major focus was on the technologies that fall 
into the category of ‘Big Data’ as the basic requirements of 
‘Kanthaka’ express similarities with them.  Following is a 
brief discussion of the findings, in the aspects of ‘Kanthaka’. 
A.  Complex Event Processing (CEP) 
    CEP is capable of giving a response in real time, after 
identifying meaningful patterns and relationships among 
apparently unrelated events. These engines accept queries 
2013 4th International Conference on Intelligent Systems, Modelling and Simulation
 2166-0662/13 $26.00 © 2013 IEEE
 DOI 10.1109/ISMS.2013.40
 534
 
provided by the user and match those queries against 
continuous event streams. Then it can trigger an event or an 
execution when the conditions specified in the queries are 
satisfied. The available CEP engines have proven to be up to 
the speed requirements of ‘Kanthaka’ [2].  
CEP engines analyze events that arrive within a limited 
amount of time (window) without keeping any persistent 
storage of analyzed data. So if a telecom promotion is 
defined to be given for one month and some failure occurs in 
the system, there is no way to recover the state. This cannot 
be tolerated in the context. Therefore CEP is not a good 
candidate for ‘Kanthaka’.  
B.   Apache Hadoop – Map Reduce 
Map-reduce is widely used in distributed computing in 
large clusters where the application is divided into many 
small fragments of work, each of which may be executed or 
re-executed on any node [3]. Apache Hadoop is an open 
source map-reduce implementation which has proven 
performance at Amozon, Facebook and eBay for search 
optimizations in PetaByte scales as mentioned in Hadoop 
site [4]. In scale-wise this is lot more than the needs of 
‘Kanthaka’. In speed-wise the latency involved in map-
 reduce is not effective for a data set smaller than few 
PetaBytes and not recommended for near real time execution 
needs [5], such as of ‘Kanthaka’.  
C.   Rules Engines 
As for the need of current business world several rules 
engines exists to support dynamic rules definition [6] which 
is promotion strategy definition in context of ‘Kanthaka’. 
From these, Drools was considered for ‘Kanthaka’, as it is a 
widely adapted, production rules engine optimized for 
pattern matching, using Rete algorithm which is faster in 
rule execution [7]. In benchmark results, Drools 
performance is above 100s for time taken to data loading and 
rule firing which is below the requirements of ‘Kanthaka’ 
[8].  
D.   Relational Database Management Systems (RDBMS) 
For data storing needs traditional solution is RDMS which 
provides ACID (Atomicity Consistency Isolation Durability) 
properties. When scaling these relational database systems to 
high volume data systems the system designer needs to 
address many aspects since traditional relational database 
systems are built to run on a single machine.  When scaling 
these systems sometimes the read and write throughput of 
the system becomes too much to handle for the system. This 
is mainly because relational database systems were not 
designed for distributed or shared systems and the additional 
overheads incur during these processed would trouble the 
normal operation of the system. The scaling techniques used 
on such a system would cost significant complexity and loss 
of fault tolerance [9]. Also when consider the data to be 
stored in ‘Kanthaka’ system; ensuring ACID properties is 
not essential that this cost become unreasonable [10]. 
E.   NewSQL 
This is the solution developed to scale the traditional 
RDBMS for the current high volume of data storage. It 
preserves ACID properties, while providing easy scale up 
using innovative mechanisms. VoltDB and NimbusDB are 
widely used implementations of NewSQL technology. When 
speed is concerned VoltDB has declared to be capable of 
executing 3.4 million transactions per second (at 10ms 
latency), which is satisfying the needs of ‘Kanthaka’ by far 
[12]. This higher speed has been achieved keeping all the 
data in-memory and supporting persistency via a commitlog 
written to the disk. As ‘Kanthaka’ needs to store data as long 
as a promotion is active while multiple promotions being 
active in the same time, it need lot of memory in such 
scenario. Considering this cost involved in high memory 
requirements, this technology was set beside. 
F.  NoSQL Databases 
NoSQL standing for “Not only SQL” which is an umbrella 
term to define a class of non-relational structured storage 
systems that differ from classic relational database 
management systems. These are focused on providing 
scalability and fast execution for growing volume of data, 
trading off in ACID properties. Instead these databases 
support BASE (Basically Available Soft-state Eventually-
 consistent) properties, achieving scalability in a limited data 
model. This is enough performance for ‘Kanthaka’. 
NoSQL databases were partly influenced by two key 
research papers: Google BigTable [13] which defines a 
specific data model focused on storing and querying multi-
 column data and uses a range-based partitioning scheme 
which allows data to be stored on multiple distributed nodes 
and Amazon Dynamo [14] which uses a simpler key-value 
data model, but the system is more resilient to failures, 
thanks to a looser consistency model. The concerns of both 
of these include serving throughput-oriented batch-
 processing jobs to latency-sensitive serving of data to end 
users which is more similar to the need of Kanthaka. Lot of 
open source implementations are available following these 
researches such as Cassandra, HBase, MongoDB and Hive 
which are widely adapted by social networking applications. 
To select between the options available in NoSQL arena 
Yahoo Cloud Service Benchmarks (YCSB) [15] was 
referred. Considering the read latency, update latency and 
elastic speed up properties Cassandra was selected as the 
data store for ‘Kanthaka’. 
III. KANTHAKA SYSTEM ARCHITECTURE 
With the pre-mentioned technology selection, the most 
optimized architecture selected for ‘Kanthaka’ is as Figure1.
 The complete system of ‘Kanthaka’ is divided into two main 
tiers as front-end and back-end. 
535
 
 
 
Figure 1. ‘Kanthaka’ architecture 
A.  Front-end 
 A web based UI (User Interface) is provided in the front-
 end which is used to define the promotions. When a business 
administrator adds a promotion via the Web UI, it is stored 
in the promotion database. These promotions in the database 
are converted in to Cassandra queries inside the Compiler 
module. Modular architecture pattern is followed here to 
ensure loose coupling within the system. Dynamic 
promotion addition and removal is supported to allow fast 
adjustments in the rules via the UI. 
B.   Back-end 
At the backend of the system, CDRs are received from the 
telecom operator periodically as .CSV (Comma Separated 
Values) files and they are read by the .CSV file reading 
Module. The preprocessor takes those CDRs from one side 
and the promotion queries from the other side. According the 
information extracted, pre-processing CDRs are stored in 
Hashmaps in the Memory module. The Hashmaps keeps a 
counter for each subscriber’s phone number found in CDRs 
in each interval and increment it according to the defined 
promotions. Then the increment is sent to Cassandra in a 
defined time interval as a batch, reducing the number of 
database updates. The objective of this is to filter the data 
according to the defined promotions and only store the 
useful data, avoiding useless usage of database space. Also 
as this reduces the amount of data, querying becomes faster.  
Selecting the eligible subscribers for a particular 
promotion is done from the Periodic Eligibility Checking 
module. This module takes the promotion queries from the 
compiler and periodically checks whether there are any 
matches for those queries in the Cassandra database. If there 
are any matches, the module sends the MSISDN (a number 
uniquely identifying a subscriber) to the telecom operators’ 
promotion server. At the same time it flags the eligible 
subscribers' entries of the Cassandra database to avoid 
reselecting the same subscribers for the same promotion.  
With this pattern of architecture, occurrence of 
concurrency issues is avoided. There is no need of locking 
the database for reads and writes as no harm is done even if 
both occurs at the same time. Once the rule is satisfied that 
entry is flagged and then that entry is not considered for that 
promotion, no matter it is updated or not after that. Also if 
the load for the system goes high in seasonal times etc. 
‘Kanthaka’ is flexible to face it adjusting the periods of rule 
execution and batch loading or elastically adding nodes to 
the cluster to balance the load. On the other hand, system is 
open for vast changes if needed, that Cassandra can be 
replaced any time, backing up its data, with few simple 
steps. Only the rule compiler and an adapter to load pre-
 processed data to the cluster are required for the new 
database. 
IV.   KANTHAKA SYSTEM IMPLEMENTATION 
The web UI of ‘Kanthaka’ system is implemented to add 
dynamic rules in a user friendly manner as a combination of 
‘AND’s of ‘OR’s. 
Eg. (Destination No = 011729729 || Destination No = 
081729729) && (No of Calls > 30 || No of SMSs > 10) && 
(Connection Type = “Prepaid”)  
The above promotion basically says if the subscriber has 
called more than 30 minutes or sent more than 10 SMSes to 
0112729729 or 0812729729; he/she is eligible for the 
promotion. At compiler this rule is analyzed and separates 
the conditions to be checked at pre-processing module and 
counters to be kept at database for the query. For the above 
example ‘Destination No’ and ‘Connection Type’ are 
considered conditions as they decide whether a CDR should 
increase a counter value in database. These are checked at 
pre-processor and the counter values are increased in a 
Hashmap in the memory as shown in Figure 2. 
536
 
Figure 2.  Pre-processor implementation details 
    ‘Kanthaka’ is implemented to co-operate with the current 
settings of telecom operators’ network. Java is the language 
used and provides interfaces to deal with outside. It receives 
a CDR file with 43 attributes in a row, in 10 minutes 
intervals. The length of the file depends on the actual events 
happened in the network. At .CSV file reading module it 
reads only the attributes needed for the active promotions in 
the system, checks for the identified conditions and 
increments Hashmap values accordingly.   
The incremented values in Hashmaps are then loaded to 
Cassandra in batch wise using the special features of 
database. As this is just an increment in the value, existing in 
the database, without reading it and updating most of the 
concurrency issues are avoided. With this database locking 
is not needed for read and writes and allows maximum use 
of Cassandra concurrent execution. Also as soon as an entry 
is selected for a promotion it is flagged and in query running 
only non-flagged entries are considered. Any chance of 
offering the same promotion twice for same subscriber is 
avoided with this. 
V. RESULTS AND FUTURE DIRECTIONS 
Test Environment 
Processor   
Model name-Pentium(R) Dual-Core CPU       T4200  @ 
2.00GHz Frequency-2000.000 MHz 
L2cache-1024KB 
Memory Total -2958 MiB 
OS Ubuntu - 10.04 Lucid  Kernel Linux 
2.6.32.33-generic GNOME 2.30.2 
Swap  - 4.7 GB 
 
Figure 3.  Performance in single node 
With the results in figure3, a latency increase can be 
observed, with the number of promotions increase for the 
same data set. This is because more processing is needed 
when number of promotions increases. Also latency 
increases with the number of records increases in the file. It 
is due to the time required to read the whole file and process 
it. 
Figure 4, shows the result comparison in a cluster of two 
nodes against single node. Two nodes of the cluster are with 
the same configuration as for single node, linked via a 
‘TIA/EIA 568-b.2 category 5e cable’ of 90cm length.  
For number of records below 1million, performance was 
worst in two node cluster than in single node due to network 
latencies introduced. But for larger loads cluster is effective, 
despite the network latencies, which proves that ‘Kanthaka’ 
can deal with seasonal hikes of network usage. 
Figure 4.  Performance in two node cluster 
 ‘Kanthaka’ architecture was developed keeping in mind, 
that the system should be highly scalable and elastic to 
satisfy the future needs of telecom operators’ with the 
growing subscriber base. This is already available for the 
data storage section, but not in data reading and pre-
 processing. It is possible to increase the speed of ‘Kanthaka’ 
by parallelizing the reading of CDR files and pre processing. 
If the CDRs can be fed into the system as several files rather 
than one, each file can be processed in separate servers. Each 
server can have its own Hashmaps in memory for counters 
and aggregation. At updating the Cassandra cluster from the 
Hashmap values, concurrency issues can occur if each server 
is allowed to update Cassandra at once.  If this could be 
537
 
implemented in ‘Kanthaka’ system with proper locking 
mechanisms it is doing all operations of reading, updating 
and querying in parallel, which in return will make the 
execution more near real time. 
Also the Cassandra cluster has its own optimizing 
techniques with memtable_limit adjustments and having 
separate disk for commitlogs to enhance the performance in 
hardware level. At deployment phase those should be 
considered to have most effective configuration in the 
cluster.  
ACKNOWLEDGMENT 
We are grateful to our internal supervisor Dr. Shahani 
Weerawarana, Visiting Lecturer, University of Moratuwa 
and Mr. Thilina Anjitha, Manager Engineering, hSenid 
Mobile Solutions (Pvt) Ltd for the support and guidance 
given at project proceedings. 
REFERENCES 
[1] A. Pavlo, E. Paulson, A. Rasin, D. J. Abadi, D. J. DeWitt, S. Madden, 
and M. Stonebraker, “A comparison of approaches to large-scale data 
analysis,” in Proceedings of the 35th SIGMOD international 
conference on Management of data, 2009, pp. 165–178. 
[2] I. L. Narangoda, S. Chaturanga, K. Gajasinghe, S. Perera, and V. 
Nanayakkara, “Siddhi: A Second Look at Complex Event Processing 
Architectures.” 
[3] J. Dean and S. Ghemawat, “MapReduce: Simplified data processing on 
large clusters,” Communications of the ACM, vol. 51, no. 1, pp. 107–
 113, 2008. 
[4]“PoweredBy - Hadoop Wiki.” [Online]. Available: 
http://wiki.apache.org/hadoop/PoweredBy/. [Accessed: 29-Jul-2012]. 
[5] N. Marz, “A new paradigm for Big Data,” in Big Data: principles and 
best practices of scalable real time data systems, MEAP ed. Manning, 
2012, ch.1, sec.3, pp. 8-12. 
[6] S. Liang, P. Fodor, H. Wan, and M. Kifer, “OpenRuleBench: an 
analysis of the performance of rule engines,” in Proceedings of the 
18th international conference on World wide web, 2009, pp. 601–610. 
[7] P. Nayak, “Comparison of the Rete and Treat Production Matchers for 
Soar (A Summary)* Pandurang Nayak Knowledge Systems Lab. 
Stanford Univ.” 
[8] “Illation LTDRules Engine» Illation LTD.” [Online]. Available: 
http://blogs.illation.com/category/rules-engine/. [Accessed: 29-July-
 2012]. 
[9] N. Marz, “A new paradigm for Big Data,” in Big Data: principles and 
best practices of scalable real time data systems, MEAP ed. Manning, 
2012, ch.1, sec.2, pp. 3-8. 
[10] R. P. Padhy, M. R. Patra, and S. C. Satapathy, “RDBMS to NoSQL: 
Reviewing Some Next-Generation Non-Relational Database‘s,” 
International Journal of Advanced Engineering Science and 
Technologies, vol. 11, no. 1, pp. 15–30, 2011. 
[11] N. Bonvin, “Linear Scalability of Distributed Applications,” École 
Polytechnique Fédérale De Lausanne, 2012, ch.3, pp. 47-53. 
[12] “Benchmarking | VoltDB.” [Online]. Available: 
http://voltdb.com/category/tags/benchmarking. [Accessed: 29-Jul-
 2012]. 
[13] G. Decandia, D. Hastorun, M. Jampani, G. Kakulapati, A. 
Lakshman,A. Pilchin, S. Sivasubramanian, P. Vosshall, and W. 
Vogels. Dynamo:amazon’s highly available key-value store. In Proc. 
of ACM Symposium on Operating Systems Principles, New York, 
NY, USA, 2007. 
[14] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach, M. Bur-
 rows, T. Chandra, A. Fikes, and R. E. Gruber. Bigtable: a distributed 
storage system for structured data. In Proc. of the Symposium on 
Operat-ing Systems Design and Implementation, Seattle, Washington, 
2006. 
[15] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears, 
“Benchmarking cloud serving systems with YCSB,” in Proceedings of 
the 1st ACM symposium on Cloud computing, 2010, pp. 143–154. 
538

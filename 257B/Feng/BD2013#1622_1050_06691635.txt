Large Scale Predictive Analytics for Real-Time 
Energy Management 
 
 
Natasha Balac, Tamara Sipes, Nicole Wolter, Kenneth Nunes, Bob Sinkovits, Homa Karimabadi 
University of California, San Diego 
La Jolla, CA USA 
natashab@sdsc.edu 
 
 
Abstract — As demand for cost-effective energy and 
resource management continues to grow, intelligent 
automated building solutions are necessary to reduce energy 
consumption, increase alternative energy sources, reduce 
operational costs and find interoperable solutions that 
integrate with legacy equipment without massive investments 
in new equipment and tools. The ability to analyze, 
understand and predict building behavior offer tremendous 
opportunities to demonstrate and validate increased energy 
efficiencies, which may ease many particular exorbitant 
pressures taxing the grid. In this paper, we describe a 
research platform driven by an existing campus microgrid for 
developing large scale, predictive analytics for real-time 
energy management. 
Keywords—data mining, smart grid, big data, time series. 
I. INTRODUCTION 
Advances in technology have led to an unprecedented 
ability to collect and store data. Harnessing the real-time 
streaming sensor data from modern electric microgrids and 
smart grids via advanced processing, modeling, 
optimization, real-time forecasting and analytics is a major 
challenge due to the sheer volume, complexity, and rate of 
acquisition. Managing and controlling actual energy 
delivery is an optimization and prediction problem that 
depends on many factors, some of which are present in 
monitoring within the grid itself, and some of which are 
only available outside the system, such as weather, 
residents’ behavior and economics. 
University campuses offer a comprehensive setting to 
deploy a microgrid and to maximize its operational benefits. 
In addition, it offers a unique assemblage of both 
intellectual and physical resources, including cutting-edge 
analytics and high performance computing resource needed 
to successfully analyze this vast amount of continually 
produced heterogeneous stream data. We present a forward 
thinking, innovation engine employing novel time series 
prediction algorithms to improve operational efficiency; 
lower operating costs, and reduces the overall carbon 
footprint of the microgrid. 
II. SMART GRID 
A. Smart Grid 
A smart grid is a modernized electrical grid that uses 
information and communications technology to gather and 
act on information about the behaviors of suppliers and 
consumers, in an automated fashion.  The goal of the smart 
grid is to improve the efficiency, reliability, economics, and 
sustainability of the production and distribution of 
electricity [1]. The present-day electric grid was originally 
built in the 1890s and improved upon as technology 
advanced through each decade. Today, it consists of more 
than 9,200 electric generating units with more than one 
million megawatts of generating capacity connected to 
more than 300,000 miles of transmission lines [2].  The 
digital technology enabling two-way communication 
between the utility and its customers, and the sensing along 
the transmission lines is what makes the grid smart.   
Similarly to the Internet, the Smart Grid consists of 
controls, computers, automation, new technologies and 
equipment working together.  In the case of the smart grid, 
these technologies need to work with the electrical grid to 
respond digitally to quickly changing electric demand.  The 
predictive power of the time series methods is of critical 
importance for enabling an efficient smart grid 
performance. 
B. UCSD Microgrid Environment 
The University of California, San Diego (UCSD) is the 
owner-operator of a 45 MW peak load Smart Grid and one 
of the first adopters of many new technologies, including 
multiple renewable and non-renewable energy generation 
resources, significant energy storage, and sophisticated 
monitoring for controlling flex-demand loads for a 
community of 54,000 energy consumers with an 
effectually, insatiable demand. With an aim of nurturing 
consumer adoption of energy-efficient vehicles, UCSD has 
made considerable progress in a adopting a campus-wide, 
electric-vehicle (EV) fleet that includes providing direct-to-
 solar plant connections and storage for solar plant power 
production [3]. UCSD’s 45 MW Microgrid includes a 
master controller and optimization system that self 
2013 IEEE International Conference on Big Data
 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 657
generates 92% of its own annual electricity load and 95% 
of its heating and cooling load. It also owns and maintains a 
69 kV substation, ninety-six 12kV underground feeder 
circuits and four 12kV distribution substations throughout 
the 1200-acre campus.  
C. System Architecture 
Current smart grid integration has been conducted 
through a public-private partnership consisting of UCSD 
researchers and staff, and the resources of Power Analytics’ 
and OSISoft’s hardware, software and expertise combined 
to create a smart grid master controller. In collaboration 
with our industry partners, UCSD has developed a 
microgrid master controller that can monitor and control 
real time operations of the microgrid, and conduct power 
system analysis to verify reliability constraints for the 
planning and operation of the microgrid. Integration of 
predictive analytics with the smart grid master controller’s 
optimization and scheduling capabilities is dramatically 
improving the energy management team’s ability to 
optimize indigenous resources and import energy, as well 
as export surpluses and shed loads when lucrative capacity, 
energy and ancillary price signals are received.  UCSD now 
saves more than $800,000 per month through use of its own 
system microgrid generation when compared to the 
alternative of being a direct access customer importing 
from the grid. 
The UCSD microgrid power system and building 
facilities are highly instrumented, and currently monitor 
approximately 84,000 data streams per second and is 
designed for expeditious integration of distributed energy 
resources (DER). The microgrid controller is integrated 
with OSI Soft’s PI data server on campus. Data collection 
and data analysis techniques are centrally managed by the 
on campus PI servers, which are interfaced with the Power 
Analytics’ microgrid controller. The UCSD microgrid also 
has one of the nation’s largest academia installations of 
synchrophasors (phasor measurement units [PMUs]) for 
data collection and data processing, which includes 
Gordon’s data intensive capabilities. Ultimately, the 
microgrid controller is also expected to utilize this data to 
provide the capability to operate the UCSD microgrid in an 
islanded condition. Local and national government as well 
as utility entities are collaboratively engaged to utilize the 
UCSD microgrid to improve management and efficiencies 
of the utilities and statewide grid operations such as 
demand response, excess generation, renewable supply, 
load balancing and power outages. 
D. Microgrid Data Collection 
Data collection and analysis software is centrally 
managed through eight servers interfaced with the Power 
Analytics’ master controlle. Two servers are dedicated to 
OSISoft interfaces; two are web servers for Power 
Anralytics master controller and OSISoft web services; one 
is an OSISoft PI server, and one is hosting Viridity’s pro 
Viridity’s software. See Figure 1. 
 
Figure 1. UCSD Microgrid Platform 
Since February 2011, the UCSD campus data 
acquisition system has collected continuous measurements 
from the heating, ventilation and air conditioning (HVAC) 
system at one-minute intervals. They include more than 
17,000 measurements from the network simulation model, 
nearly 10,000 additional measurements from electric power 
meters, and a large number of dependent variables 
computed in real time. The UCSD microgrid power system 
and building facilities are highly instrumented, and 
currently monitor approximately 84,000 data streams per 
second and is designed for expeditious integration of 
distributed energy resources (DER).  The steady state 
archive writes exceed 500,000 events per second. The 
simultaneous read rate is over 20,000,000 events per 
second and growing as more assets and more powerful 
sensors are added to the system daily. This very large, 
complex data holds enormous potential for discovering 
patterns and forecasts that can create significant energy 
savings.  Advanced forecasting and “big data” analytics 
techniques are employed in order to realize real 
improvements in energy efficiency and reductions in the 
energy cost of the campus microgrid, as presented below. 
III. BIG DATA PROCESSING POWER 
A. Computing Architecture 
The SDSC’s Gordon system was specifically designed 
for data-intensive workloads and careful attention was paid 
to data movement and capacity at all levels of the memory 
hierarchy. Each dual socket node contains two eight-core 
Intel Sandy Bridge processors and 64 GB of DDR3-1333 
memory. With four channels per socket, the Gordon 
compute nodes have a theoretical peak memory bandwidth 
of 85 GB/s. Groups of sixteen compute nodes are 
connected to pairs of 36 port switches using QDR 
InfiniBand (40 Gb/s) and these switches are then networked 
into a 4x4x4 dual rail 3D torus. Each switch is also 
connected to an I/O node, which serves both as a router to 
the Lustre parallel file system and houses sixteen 300 GB 
Intel 710 Series (Lyndonville) SSDs. Pairs of bonded 10 
GbE connections from the I/O nodes to the Lustre Object 
Storage Servers provide an aggregate bandwidth of up to 
100 GB/s into Gordon. 
658
The decision to place the SSDs in the I/O nodes and 
export using the iSER network protocol rather than 
physically installing on the compute nodes allows for 
significant flexibility in their deployment. For the majority 
of jobs using Gordon’s flash-based memory, one SSD is 
exported to each compute node and made available to user 
jobs as a 300 GB scratch file system. In some instances 
though, all sixteen SSDs are exported to a single “big 
flash” compute node and combined into a single RAID 0 
device with a raw capacity of 4.8 TB [4]. 
The other novel data intensive feature of Gordon is the 
use of ScaleMP’s vSMP foundation software to aggregate 
multiple compute nodes and the corresponding I/O nodes 
into large shared memory nodes. Although data exchange 
between physical nodes still takes place via the underlying 
IB network, the user is presented with a logical shared 
address space. The basic building block for the vSMP 
nodes is 16 compute nodes and one I/O node, with each 
unit contributing nearly 1 TB of DRAM and 4.8 TB of 
flash. Gordon currently has four of these 16+1 vSMP nodes 
in production, but has the flexibility to deploy more as 
needed, including 32+2 or larger configurations.  
B. Hadoop Cluster Path 
SDSC has deployed a persistent 16-node Hadoop 
cluster with the Hadoop file system (HDFS) stored on the 
flash drives of the Gordon supercomputer [5]. 
All Gordon users have temporary access to flash 
storage. Typically, at the start of a job, the Torque prologue 
creates a temporary directory on the flash file system that is 
available for the duration of the job and deleted once the 
job finishes. For projects requiring Hadoop clusters or 
frequently accessed databases, persistent access is 
preferred. To accommodate these workloads, a portion of 
the Gordon system is reserved for dedicated I/O node 
projects.  
In this case, the I/O node serves as the access point to 
the Hadoop cluster and as the management node. The I/O 
node hosts all metadata and resource management services, 
whereas the compute nodes host the Hadoop Distributed 
File System (HDFS) data and execute map tasks and reduce 
tasks. HDFS is created on the flash drives that are exported 
via iSER and are presented to HDFS as local devices.  This 
platform is used for data exploration, preparation and 
cleaning for the purpose of this study.  A number of big 
data and especially smart grid analytics projects are 
consuming tools like R[6], Mahout[7], Hive[8] and other 
Hadoop based technologies employed  on this platform due 
to the 4V’s [9] nature of the Smart grid data. 
C. Smart Grid Analytics Test Bed 
Figure 2 provides an overview of the analytics test bed. 
The key steps in the process are described below. Campus 
building and energy data are collected from 86,000 distinct 
data streams, of raw data deposited to the PI server. The PI 
server performs cursory analysis, provides dashboard 
capability for visualization, and offers other management 
functions such as alerts and basic reporting. 
 
Figure 2. Analytics Test Bed Data Flow 
From the PI server, the data is transferred to the Data 
Oasis high performance file system over the SDSC LAN 
where it is staged for the real-time analytics. Numerically 
intensive statistical analysis is performed on the dedicated 
set of the Gordon compute cluster (I/O nodes) and results 
are staged on the Data Oasis file system. The final analysis 
results will be transferred from Data Oasis to the PI server 
where models will be integrated into the existing 
architecture for real-time scoring and predictions. From the 
perspective of the PI system, the data generated on Gordon 
is simply another data stream. 
From Data Oasis, the original data and some subset of 
the derived data products can also be sent to the SDSC 
Cloud Storage environment for archive, reporting and 
future analysis. 
IV. DATA MINING 
Predictive data analytics have the potential to greatly 
enhance the smart grid and amplify its impact by enabling 
the understanding of an increasing wealth of data about 
energy usage and the kinds of demands placed on the grid. 
UCSD is committed to acquiring alternative forms of 
energy generation, with the goal of self-sufficiency and 
main grid independence by 2016. The campus utilizes 
smart grid data with advanced forecasting and “big data” 
analytics in order to realize real improvements in energy 
efficiency and reductions in energy cost. 
Buildings consume significant energy, and according to 
the DOE are responsible for 70% of the total electricity 
consumption of the US. UCSD, has over 450 buildings 
supported by a campus scale micro-grid.  In order to predict 
and manage energy use holistically across the buildings, we 
first need to create powerful predictive models for each 
building and then discover and consider their inter-
 dependencies by employing the vast amounts of 
heterogeneous data collected at the second level frequency 
from the smart grid.   Traditional time series models 
performance and scalability are innsuficient  for this large 
and compelx data.  We have employed scalable mutli-
 variate predictive analytics techniques to identify suspect 
and unforeseen performance behaviour.  This approach will  
enable the establishment of the baseline models for future 
measurement comparisons and creation of temporal and 
generalized building behavior consumption models.  
Applying such approaches to the available data in the 
microgrid environment enables informed, real-time 
decisions and enhancements, resulting in a truly intelligent 
and optimized micro grid. 
659
A. Data Description 
Campus building and energy data are collected for the 
UCSD Microgrid.  Data is currently collected from 86,000 
distinct data streams, from approximately 30 different 
buildings on the UCSD Campus.  Data Streams, include 
measures and set points, are collected from the Building 
Management system, Johnson Control systems, which 
includes HVAC systems, the central utility plant, Schneider 
Electric power meters, as well as from photovoltaic panels, 
network model output data and weather data stations at 
various location on campus.  The data collected includes 
measurements of airflow, carbon dioxide level, current, 
damper position, dew point, humidity, power, pressure, real 
power, speed, temperature, valve, voltage. 
Initial research focused on modeling three buildings, 
which are representative of the majority of campus 
buildings.  The three buildings selected are newer buildings 
representing the leading edge of campus energy efficiency 
and technology and more importantly contain the most 
instrumentation for data collection.   The buildings include 
the San Diego Supercomputer Center, which includes 
computer rooms, conference rooms and office space.  
EBU3B building is the home for the computer science 
department and it includes classrooms, conference rooms 
and offices.  The third building is called Pacific Hall and it 
consists of two large laboratory spaces.  In order to create 
highly accurate models, input attributes, algorithm 
parameters and environmental condition variables were 
chosen while exploring issues of seasonal variability and 
timing. 
The buildings sensor data ranges from February 2011 
and May 2013.  All three datasets contained the time stamp, 
the year, month, day, outside temperature and the power 
usage (in kW) variables.  We added the time of the week 
(Monday-Sunday) variable as well as a workday flag (=1 if 
yes, =0 if no). 
B. Predicting Energy Consumption using Time Series 
Approach 
Modeling the energy usage behavior of campus 
buildings is an important step to improving the efficiency 
of the system. An accurate model of future energy usage 
compared to actual energy usage enables anomalies 
discovery in the actual data that may signify wasteful usage 
of energy. This predictive information is used to combat the 
high future demand or optimize pre-heating or pre-cooling 
scheduling to maximize the energy efficiency.  The short-
 term predicted energy usage, can determine how much 
renewable energy should be used, help guide decisions 
regarding demand response, excess generation, renewable 
supply and load balancing and manage power outages.  
Ultimately the microgrid controller will utilize these 
predictions to provide the capability to enable the operation 
of the UCSD microgrid in an islanded condition, if 
necessary.  
The energy use in commercial buildings varies 
depending on many factors including weather, building 
insulation, equipment efficiencies, hours of equipment 
operation, system controls, building size, etc, overt time.   
There are various modeling techniques that could be 
employed to analyze and predict building energy 
consumption.  Most of the traditional predictive methods 
overlook, discretize or require oversimplification of the 
temporal characteristic of the data in order to produce a 
model [10][13].  In order to caputre the temporal nature of 
the Smart Grid data we used a time series approach was 
chosen for energy demand forecasting and building 
behavior modeling due to the temporal nature of the Smart 
Grid data.  The strength of this model is that it can work on 
any building with existing historical or real-time energy 
consumption data.  The goal of this study is to create highly 
accurate models predicting energy demand for the campus 
micro grid at multiple levels of granularity.  The study 
presented below focuses on modeling building level energy 
behaviors.     
Time series data containing multiple variables (i.e., 
multivariate time series data) commonly occurs in a wide 
variety of fields including biology, finance, science and 
engineering. A time series (or more generally temporal 
data) is a sequence of measurements that follow non-
 random orders and can be generated either from a fixed-
 point measurement at several time intervals or a convolved 
spatial-temporal variation as measured from a moving 
detector. Multivariate time series analysis is used when one 
wants to model and explain the interactions among a group 
of time series variables. Much of the scientific data is in the 
form of multivariate time series. Examples include ECG 
measurements, energy consumption data, temperature, and 
sign language hand movements, among others.   
Multivariate time series classification attempts 
classification of a new time series based on past 
observations of time series examples, rather than providing 
an analysis of a single variate time series. A data mining 
technique called MineTool-TS is introduced which captures 
the time-lapse information in multivariate time series data 
through extraction of global features and metafeatures [14].  
Unlike traditional approaches such as Hidden Markov 
Models, recurrent Artificial Neural Networks, Dynamic 
Time Warping [15][16][17] and most recently Tclass[18], 
MineTool can handle real-world, continuous, noisy data 
really well, and does not suffer from the large number of 
parameters, non-linear search and black-box model issues, 
as well as the assumptions difficult to obtain in energy data.    
If one could replace the time series by a static data 
consisting of variables that capture the relevant and 
interesting features (e.g., number of zero crossings, slope, 
extreme or average values) of the time series, then the 
standard MineTool technique could be used. Even though 
this approach is rather successful for a small number of 
simple datasets and problems, neither of these two 
approaches yields high accuracy results in modeling real 
life, complex time series data. Instead, we use a more 
sophisticated approach to extract features from multivariate 
time series data that yields much higher accuracy [14].    
C. MineTool for Static Data 
The core data-mining algorithm that underlies 
MineTool-TS is MineTool [14]. The advantages of 
MineTool over traditional algorithms such as support 
vector machine and artificial neural net (ANN) are its 
automated steps that make it more accessible and 
660
applicable in a variety of domains, its accuracy and 
robustness and the analytical form of the model at the 
output. 
An important algorithmic issue in data mining is how to 
find the optimal complexity of the model or the fitting 
function. Too much complexity in the model can result in 
model overfit, whereas not enough complexity can result in 
an underfit. The mathematical foundations of MineTool are 
based on considerations to balance the competing dangers 
of underfit and overfit to identify the level of model 
complexity that guarantees the best out-of-sample 
prediction performance without ad hoc modifications to the 
fitting algorithms themselves[19][20]. MineTool creates a 
predictive model architecture that is linear in the 
parameters. The algorithm searches for a model M that best 
relates rows of the input variable values Xij to the 
appropriate target value yi (yi = M(Xij)), where i = 1,…,N 
and j = 1,…,K. The model parameters are either linear 
combinations of the input (Xi’?, where prime indicates 
transpose of the vector, index i refers to the ith observation), 
linear transformations of the input variables (?(Xi)), or 
highly non-linear transformations of the input (?(Xi,?)). 
Equation (1) describes the general form of a MineTool 
model, and the specifics of the method in given in [14]:  
1 1
 ( ) ' ( , ) '
 QP
 i
 p q
 y ?
 = =
 = + +? ?i i p i q qX' ? X ? ? X ? ?
  
1. MineTool Model Equation. 
In its simplest form, the model would be a linear 
combination of the input parameters (i.e., a linear 
regression model). MineTool goes beyond a simple linear 
model by introducing the linear (such as level-1 and level-2 
transformations producing cross-products, ratios, squares, 
cubes etc.) and non-linear transformation of the input 
variables, if their addition increases the model accuracy. 
The non-linear transforms ? are single hidden layer feed 
forward ANN-like transforms, just like the ANNs of the 
same architecture, with the difference that the non-linear 
transformed inputs are combined into a linear model. 
Metafeature and Global Feature Detection: To be able 
to process a (time) series dataset (represented with multiple 
rows of data describing one instance or observation) using 
MineTool, the data needs to be “flattened,” or made static.  
This needs to be accomplished without losing the important 
information incorporated in sequential measurements 
varying with time. Historically, this has been done either by 
summarizing the data and writing only the mean of the 
different row values of one observation, or recording the 
difference between the pairs of rows and then treating them 
as single instance entries. These techniques work somewhat 
well on just a limited set of time series problems. For real 
life, complex scientific datasets, these approaches are most 
often too weak to incorporate the important time changes in 
the data. The MineTool-TS solution to this problem is to 
collect the important time-changing information that can 
occur in one of the time series variables. While a value 
varies with time, it most often increases, decreases or 
stagnates. There are other, more complex features one can 
record, that consist of the three basic changes, such as 
bipolar signature (relevant in case of flux transfer events), 
where a value goes up, then goes down crossing the axis, 
and goes up again (the sinusoid function demonstrates the 
bipolar behavior, for example). Global features, just like 
the metafeatures, are used to extract the information from 
all the rows representing one observation. Global features 
describe one instance of rows using one measurement, such 
as: the maximum value, minimum value, mean, mode or 
the number of zero crossings. The metafeatures and global 
features included in the MineTool-TS algorithm are listed 
here (all used in the analys except for the bipolar feature): 
Increasing Metafeature: An increasing metafeature is 
recorded for all the consecutive rising time-series 
measurements. For each increasing event, we record its 
start point, duration, gradient and average value, so that the 
increasing events can be used for analysis and comparison.   
Decreasing Metafeature: A decreasing metafeature is 
recorded for all the consecutive reducing time-series 
measurements. For each decreasing event, similar to the 
increasing events, we record starting point, duration, 
gradient (which is negative in this case) and average value.   
Plateau Metafeature: A plateau metafeature is 
recorded for all the consecutive non-changing time-series 
measurements. MineTool-TS allows for a small amount of 
noise to be ignored, so that the true plateaus are captured.     
Bipolar Signature Metafeature: A bipolar signature 
metafeature is recorded for all the consecutive time-series 
measurements that increase, decrease and cross the zero, 
and increase again.    
Global Minimum: For each single variable, the global 
minimum feature extracts the minimum value of all of the 
time observations belonging to one time series instance for 
the variable, and records it as the global minimum feature 
for that input channel. 
Global Maximum: The maximum value of all of the 
time observations belonging to one time series instance for 
the variable is recorded as the global maximum feature for 
that variable. 
Mean: The average value of all of the time 
observations belonging to one time series instance for the 
variable is recorded as the global mean feature for that 
specific variable. 
Mode: The mode value of all of the time observations 
belonging to one time series instance for the variable is 
recorded as the global mode feature for that specific input 
variable. 
Number of Zero Crossings: Lastly, the number of zero 
crossings occurring during the time observation recorded 
measurements is written down as the number of zero 
crossings global feature. 
Next, once all the requested features are collected, the 
MineTool-TS algorithm performs the feature space 
segmentation to group similar features and makes them 
have a higher predictive value for data mining. More details 
on the algorithm can be found in [14]. 
D. Modelling Results 
We provided a three-level predictive analysis of power 
usage: static analysis (where all the sensor information 
661
entries are thought of as independent instances) and time 
series (where the entries are considered a part of a time 
series, i.e. time dependent measurements), for all three 
datasets (CogSci, PAC Hall and SDSC Building data).  The 
three buildings have different usage profiles, different  
schedules, occupancy and energy needs.  
In the static case, we compared the results of static 
MineTool to a multilayer perceptron method (an artificial 
neural network (ANN) method)[13][22].  In the case of 
time series analysis, we used the MineTool-TS method 
described above.   
To prepare the data for time series analysis, the dataset 
was treated as a set of one-hour long streams of data.  
Below, we demonstrate the results of the static vs. time 
series analysis and show the accuracy achieved in 
predicting the energy usage for each of the different 
buildings. 
Dataset 1 – CogSci Building Data: Table 1 
demonstrates the results of the static vs. time series analysis 
for the first dataset, the CogSci building data.  We used the 
10-fold cross validation evaluation and the table lists the 
standard numerical prediction evaluation measures: CC 
(correlation coefficient), MAE (mean absolute error) and 
RMSE (root mean squared error). 
From the multivariate time series classification analysis 
point of view, out of all the standard metafeatures (such as 
increment, decrement and plateau features) and the global 
features (such as mean, number of zero crossings), the best 
performing feature chosen by the algorithm was the mean 
as well as the plateau feature with a relaxed noise level. 
This is due to the fact that the fluctuations within an hour 
are minimal, and there are no distinctive increases and 
decreases within the one hour long streams that have a high 
predictive capability.  
Figure 3 demonstrates a part of the time series 
prediction model represented in a tree form. We can see 
that workday, hour of the day, month of the year and the 
temperature all seem to have a high predictive ability in 
forecasting the usage, and that the workday seems to be the 
highest determining factor of the levels of power usage. 
Table 1 demonstrates that treating the data as time 
series indeed leads to stronger model accuracy. MineToo-
 TS achieved the correlation coefficient of 0.9, as tested 
using 10-cross validation, whereas the static analysis only 
yielding accuracy at the random guessing level of below 
(0.49 and 0.44).  
TABLE 1. COMPARATIVE ANALYSIS OF POWER USAGE 
MODELS FOR THE COGSCI DATASET 
Analysis Method CC MAE RMSE 
Static  MineTool 0.4939 3.4812 7.3371 
Static ANN 0.4436 6.8216 8.5553 
Time 
Series 
MineTool 0.9013 2.6971 3.8941 
 
 
Figure 3. Part of the CogSci Power Usage Prediction Model 
 
We believe that adding more measurements, such as the 
occupancy, class schedule and similar information to the 
current data would increase the accuracy of the models. 
Furthermore, campus events, cloud coverage and similar 
variables would, we believe would increase the predictive 
accuracy even further.  
Dataset 2 – PAC Hall Data: We provided a slightly 
different two-level analysis of the PAC Hall dataset. To be 
able to compare the time series analysis to the standard data 
mining, we preprocessed the second building data by 
averaging the values within each hour stream of the data. 
Table 2 illustrates the results of the static averaged data vs. 
time series analysis for this dataset.  We also used the 10-
 fold cross validation evaluation and the table lists the 
standard numerical prediction evaluation measures: CC 
(correlation coefficient), MAE (mean absolute error) and 
RMSE (root mean squared error). 
Table 2 above confirms that treating the data as time 
series leads to stronger model accuracy.  MineTool-TS 
achieved the accuracy of 0.95 correlation coefficient, tested 
using 10-cross validation, whereas the static analysis 
performed on the hour averages data yielded 0.64 CC 
accuracy.  
Figure 4 demonstrates a part of the time series 
prediction model represented in a tree form. We can see 
that in the case of PAC Hall data, the power usage has 
changed from year 2011 to 2012, and year 2013 as well, as 
the predictive analysis output shows a distinct submodel for 
2011. In the case of this building, particular day of the 
week seems to have a higher predictive ability as well, not 
just a workday as in the case of the previous dataset.  Hour 
of the day, month of the year and the temperature all still 
seem to have a high predictive ability in assessing the usage 
as in the CogSci dataset case. 
TABLE 2. COMPARATIVE ANALYSIS OF POWER USAGE 
MODELS FOR THE PAC HALL DATASET 
Analysis Method CC MAE RMSE 
Static ANN 0.6354 3.8081 5.1177 
Time 
series 
MineTool 0.9508 1.3368 2.0348 
 
662
TABLE 3. COMPARATIVE ANALYSIS OF POWER USAGE 
MODELS FOR THE SDSC DATASET 
Analysis Method CC MAE RMSE 
Static ANN 0.7589 63.927 95.2282 
Time 
series 
MineTool 0.9736 8.989 9.2737 
Dataset 3 – SDSC Building Data: Using the same two-
 level analysis as in the case of previous building, we 
obtained the SDSC power usage data analysis results 
shown in Table 3.  
Table 3 above reiterates that multi variate time series 
analysis is indeed over-performing static analysis of the 
data. The time series model achieved 0.97 correlation 
coefficient accuracy, while the static models achieved 0.76 
CC accuracy.   
Figure 5 demonstrates a part of the time series 
prediction model represented in a tree form. Power usage 
patterns changed from year 2011 to 2012 and 2013 in this 
building as well, and the day of the week, not just workday, 
as well as the hour are important predictors of power usage.   
In summary, predictive analytics of energy big 
historical usage data obtained excellent predictive results of 
power usage in kW, in three building of the UCSD micro 
grid system.  The multivariate time series classification 
produced excellent models (accuracy yielding 0.9-0.97 CC) 
as compared to the standard data mining analysis where the 
data is treated as static, independent rows, and instances are 
processed as independent examples, or averaged over an 
hour of streaming data.  The models demonstrate that the 
time of the day, workday, day of the week, month, hour and 
outside temperature are all excellent predictors of the 
power usage per building on the micro grid.  Put in the 
global perspective, this initial analysis of big data will 
enable us to be closer to predicting power usage and lower 
the cost of energy of the entire campus microgrid. 
FIGURE 4. PART OF THE PAC HALL POWER USAGE MODEL 
 
Figure 5. Part of the SDSC Power Usage Model 
V. CONCLUSION  
UCSD is committed to acquiring alternative forms of 
energy generation, with the goal of self-sufficiency and 
main grid independence by 2016. We are utilizing smart 
grid data with advanced forecasting and a “big data” 
analytics platform leveraging available large scale data and  
HP system in order realize tangible improvements in 
energy efficiency and cost.  We are further exploring 
clustering methods, real time and active learning models 
employed for predicting peaks and potential outages.   As 
data is rapidly growing, the stream mining and predictions 
become futher challenging due to the vast volume and 
speed of data.   Future work will involve scalling of the 
ongoing efforts to efficiently model the campus in its 
entirety and evalutae the system in real-time.  As model 
complexity and data granularity increase the need for large 
memory nodes and HDFS rise.  Investigating the 
parallelization of the worksflows and Minetool on a 
number of different platforms is be the focus of the future 
work.  This initial real-life case study has shown clear 
value through the big data analytics in both efficiencies and 
cost savings.    Applying such approaches to the available 
large scale sensor data in the microgrid environment 
enables informed, real-time decisions and enhancements, 
resulting in a truly intelligent and optimized microgrid. 
663
ACKNOWLEDGEMENT 
The authors would like to personally thank Byron 
Washom, director of Strategic Energy Initiatives; John 
Dilliott, manager of Energy and Utilities; and Robert 
Austin, administrator of Energy Management Systems at 
UCSD; Bob Caldwell, President of Centaurus Prime and a 
UCSD microgrid consultant who were instrumental in 
providing access to campus energy data; as well as our 
funding agencies, i.e., the National Science Foundation and 
the California Energy Commission.  
REFERENCES 
[1] U.S. Department of Energy. "Smart Grid / Department of 
Energy" 
[2] http://www.smartgrid.gov/the_smart_grid#smart_grid  
[3] Washom, B., Dilliott, J., Weil, D., Kleissl, J., Balac, N., 
Torre, W., Richter, C., "Ivory Tower of Power: Microgrid 
Implementation at the University of California, San Diego," 
Power and Energy Magazine, IEEE , vol.11, no.4, pp. 28,32, 
July 2013. doi: 10.1109/MPE.2013.2258278 
[4] Strande, S.M., Cicotti, P., Sinkovits, R.S., William, S., 
Young, W.S., Wagner, R., Tatineni, M., Hocks, E., Snavely, 
A., Norman, M. 2012. Gordon: Design, performance, and 
experiences deploying and supporting a data intensive 
supercomputer. In Proceedings of the 1st Conference of the 
Extreme Science and Engineering Discovery Environment: 
Bridging from the eXtreme to the Campus and Beyond 
(XSEDE '12). ACM, New York, NY, USA, Article 3, 8 
pages. DOI=10.1145/2335755.2335789. 
[5] Dean, J., Ghemawat, S. 2008. MapReduce: simplified data 
processing on large clusters. Commun. ACM 51, 1 (January 
2008), 107-113. doi: 10.1145/1327452.1327492 
http://doi.acm.org/10.1145/1327452.1327492  
[6] http://www.r-project.org/ 
[7] http://mahout.apache.org/ 
[8] http://hive.apache.org/ 
[9] "IBM What is big data? — Bringing big data to the 
enterprise". 01.ibm.com. 
[10] Solomon, D., Winter, R.L., Boulanger, A.G., Anderson, 
R.N., Wu, L.L., (2011); Forecasting Energy Demand in 
Large Commercial Buildings Using Support Vector Machine 
Regression. Technical Report Columbia University 
Computer Science Technical reports; CUCS-040-11; New 
York; http://hdl.handle.net/10022/AC:P:12171. 
[11] Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, 
P., Witten, I.A. (2009); The WEKA Data Mining Software: 
An Update; SIGKDD Explorations, Volume 11, Issue 1. 
[12] Breiman, L., Friedman, J., Stone, C. J., Olshen, R. A. (1984). 
Classification and regression trees. Chapman & Hall/CRC. 
[13] Kalogirou S.A. 2000. Applications of artificial neural-
 networks for energy systems, Applied Energy, Volume 67, 
Issues 1–2, September 2000, Pages 17–35. 
[14] Karimabadi, H., Sipes, T. B., White, H., Marinucci, M., 
Dmitriev, A., Chao, L.K., Driscoll, J., Balac, N. 
(2007).  Data Mining in Space Physics: 1. The MineTool 
Algorithm, J. Geophys. Res., 112, A11215, 
doi:10.1029/2006JA012136. 
[15] Myers, C. S. and Rabiner, L. R.  A comparative study of 
several dynamic time-warping algorithms for connected 
word recognition. The Bell System Technical Journal, 
60(7):1389-1409, September 1981. 
[16] Rabiner, L. R. and Juang, B. H.  An introduction to hidden 
markov models.  IEEE Magazine on Acoustics, Speech and 
Signal Processing, 3(1):4-16, 1986.    
[17] Schmidhuber, J., Graves, A., Gomez, F. and Hochreiter, S. 
Recurrent Neural Networks, Cambridge University Press, 
2012. 
[18] Kadous, M. W. Temporal Classification: Extending the 
Classification Paradigm to Multivariate Time Series. PhD 
thesis, School of Computer Science & Engineering, 
University of New South Wales, 2002.  
[19] Pérez-Amaral, T., Gallo, G. M. and White, H.,  A 
Comparison of Complementary Automatic Modeling 
Methods: RETINA and  PcGets,” Econometric Theory, 2005. 
[20] White, H., Personnel Readiness: Neural Network Modeling 
of Performance-Based Estimates, Final Report to the Office 
of Naval Research, Contract #: N00014-95-C-1078, 1999. 
[21] technique: Automated detection of flux transfer events using 
Cluster data, J. Geophys. Res., Vol 114, A06216, 
doi:10.1029/2009JA014202, 2009 
[22] White, H., Personnel Readiness: Neural Network Modeling 
of Performance-Based Estimates, Final Report to the Office 
of Naval Research, Contract #: N00014-95-C-1078, 1999. 
 
664

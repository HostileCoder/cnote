An Incremental Closed Frequent Itemsets Mining Algorithm Based on Shadow 
Prefix Tree 
Yun Li , Jie Xu,Xiaobing Zhang,Chen Li,Yingjuan Zhang 
College of Information Engineering   
Yangzhou University 
 Jiangsu,China 
E-mail:liyun@yzu.edu.cn 
Abstract—This paper introduces an incremental closed 
frequent itemsets mining algorithm, which is based on a 
shadow prefix tree to get closed frequent itemsets. By the use of 
shadow technology this algorithm can avoid the cost of 
generating and testing of candidate subsets. Shadow prefix tree 
can find nodes by virtual node without generating all nodes. 
The experiment results show that this algorithm can deal with 
big data more efficiently than others. 
    Keywords—Data mining; closed frequent itemsets;  prefix tree;  
incrementally update 
I. INTRODUCTION
 As an important branch of frequent patterns mining, 
closed frequent itemsets mining can take place of frequent 
itemsets mining in a compressive way without information 
lost[3][4][5][6].Closed frequent itemsets mining algorithms can 
be classified into two types :the first is based on Apriori 
algorithm[1][7] which adopt the strategy of candidate 
generating and testing; the other one is based on FP 
algorithm[8]which adopt the strategy of divide-and-conquer 
strategy, such as: CLOSET[9],CLOSET+[10], FPClose[11],
 DFCIM[14].
       NewMoment[12] is an incremental closed frequent 
itemsets mining algorithm, this algorithm uses bit vector to 
represent transaction database. A data structure like prefix 
tree is used to store critical data. But every item has a bit 
vector with fixed length in NewCet, when the dataset is 
scattered much space will be waste.   CFPL generates FPL 
structure from the preprocessing dataset[13][14] and gets the 
closed frequent itemsets by comparing these signature vertex. 
FPL is an algorithm dealing with vertical data structure, the 
waste of space will become obvious when the dataset is large. 
AFOPT algorithm[15] generates a data structure  derived 
from prefix-tree(PT) to mining closed itemsets. This 
structure is a kind of ascending prefix tree, but    it will 
generate much duplicate nodes, and the space has exponent 
relation to the number of items.  
This page raises an incremental mining algorithm 
SPTCI(Shadow Prefix Tree Closed frequent itemsets 
Incremental mining algorithm) based on Shadow Prefix 
Tree(SPT) which is  generated by using the merging 
technology and shadowing technology on PT. 
C. RELATED CONCEPTS
 A. Closed Frequent Itemsets 
Suppose that items set I={item1,item2,…,itemn} and 
transactions database D={t1,t2,…,tm},if X is a subset of I,the 
support count of X (supcount(X)) is the number of 
transactions in D which include X,and the support of X 
(sup(X)) is the percentage of supcount(X) with the number 
of transactions in D. The task of frequent itemsets mining is 
to find all these itemsets whose support  are no less than a  
mini support (minsup) assigned by the users. 
 Suppose that ,DT  Ii ,then 
f(T)={ tiTtIi  ,| };suppose , ,then
 g(X)={
 DT  IX 
 Dt | tiXi  , }.
 Definition 1:Suppose that X is an itemsets, if 
c(X)=f(g(X))= =X, then X is called a closed 
itemsets, the function c is the Galois operator
 )(Xgf 
 [2]
 .
 Definition 2:If two itemsets are supported by the same 
transactions, then the two itemsets belong to an equivalence 
class. Suppose ,if and only if 
(g(X))=f(g(Y))then X
 IYIX  ,
  Y,  is an equivalence relation. 
From definition 1 and 2 it is obviously that the c function 
divides the frequent itemsets into several equivalence classes 
and a closed frequent itemsets is the longest itemsets in a 
equivalence class, so the task of closed frequent itemsets 
mining is to find all the longest itemsets of each equivalence 
classes.
 B. Prefix Tree 
The nodes of Prefix tree(PT) consist of four parts: node 
label, node count, links of sub nodes, point to parent node. 
The value of node label is an element of I, it is used to 
distinguish the nodes; the value of node count is a number ,it 
represents the times the node appearances, written as 
PT(i).count; the links of sub nodes is a set of points which 
point to each children of this node; the parent point points to 
the parent node, all the prefix nodes can be find through this 
point easily[15].A path is a set of nodes in PT from root to leaf, 
and brother in PT represent nodes with the same parent node. 
PT has a root node, all the nodes in PT are sorted 
ascending in alphabet order. The node label of parent node 
are litter then all these node label of children, so the node 
label of root node is the smallest, written as PT(0);for a node 
2013 10th Web Information System and Application Conference
 978-0-7695-5134-0/13 $26.00 © 2013 IEEE
 DOI 10.1109/WISA.2013.89
 440978-1-4799-3219-1/13 $31.00 © 2013 IEE
PT(i),all the node label of the nodes in the left of it is smaller 
that i, the node label of the nodes in the right of it is bigger 
then i. For example, TABLE.I is a sample transaction 
database denoted as TD and fig.1 is a PT of TD. 
TABLE I. SAMPLE TTRASACTION DATABASE
 D. STRUCTURE AND GENERATE OF SHADOW PREFIX TREE
 SPT is derived form PT. The shadow technology of 
virtual nodes makes SPT avoiding the repeated nodes and 
uniting with an other SPT generated from a different dataset. 
By the uniting of SPT,SPT can update incrementally.  
C. The Structure and Features of Shadow Prefix Tree  
The structure of node in SPT is similar to PT, but the 
type of nodes in SPT can be classified into three classes: 
Real Node(written as RN(i)),Virtual Node(written as VN(i)) 
and Null Node(written as NN(i)).The structure of RN is 
same as the node of PT, RN(i) represent a node in SPT with 
the node label of i, the node count of RN(i) written as RN(i). 
count. SPT contains many sub SPT, these sub SPT take a RN 
as root node, written as SPT(RN(i)),this means that 
SPT(RN(i)) has a root node RN(i).A sub node of SPT(RN(i)) 
is a node appearing in a path of SPT(RN(i)).In SPT(RN(i)),if 
1 1
   then a node RN(iI1j) is the last node 
with node label j in the path of iI1j,SPT(iI1j) is the sub SPT 
with the root node of RN(iI1j).  
I, , i k jI k I    
 Fig. 1. The  prefix tree of  sample transaction database 
Definition 3:Suppose that node(i) is a node in SPT, its 
node label is i, if node(i) is made up of node label, parent 
point and node link(the point to a real node with the same 
node label),then this node is called Virtual Node, written as 
VN(i).If node(i) is made up of only node label then it is a 
Null Node written as NN(i). 
Virtual node is a node point to another RN with the same 
node label but has different prefix items. Null node would 
not be seen in the final SPT ,it was used to represent some 
node in the process of creating SPT.
 Definition 4:Suppose node(i) is a node in SPT, the set of 
node labels  of nodes from node(i) to root node(not include 
node(i) and root node) is the prefix set of node(i),written as 
PS(node(i)).
 Definition 5:If RN(i) is a real node in SPT, then take 
RN(i) as a root node, all these sub nodes and RN(i) is a sub 
SPT, written as SPT(RN(i)).For 
SPT(RN(i)), jiIj , .By the strategy of wide first, the 
first sub nodes RN(j) of SPT(RN(i)) finding in the order of 
right to left is called the right j node of SPT(RN(i)) written as 
R(SPT(RN(i))j);the sub tree with the root node of RN(j) is 
called right j sub tree of SPT(RN(i)),written as 
RT(SPT(RN(i))j). 
SPT has two important attributes, and the algorithm 
proposed by this page is based on the two attributes. 
Attribute 1:If RN(i) is not the root node then 
RN(i).count=|g(PS(RN(i)) )|. “||” is the number of 
elements in the set. 
{i}
 Attribute 2:SPT(RN(i)) is a sub SPT with the root node 
of RN(i), ,j i if there is a node with node label j in 
SPT(RN(i)) and RN(j)=R(SPT(RN(i))j),then 
RN(j).count=|g(PS(RN(i)) {i,j})|.
 The purpose of creating SPT is to make SPT has these 
two features and avoid generating duplicative nodes. There 
two important operation with SPT: Create SPT, unite SPT. 
D. Build Shadow Prefix Tree 
It is obviously that PT has the attribute 1 of SPT but not 
attribute2. The key step in the process of building SPT is the 
sub tree uniting which gives the attribute 2. Sub tree uniting 
takes the strategy of depth-first. Algorithm 1 is the process of 
building SPT. Create_SPT function operates on every 
brother from left to right, when the last brother finishes this 
operation then the unite operation in algorithm 1 will unite 
these nodes. Unite(SPT(RN(i))) operation will unite  each 
children with brother in the left. 
Algorithm 1: Create_SPT
 Input:RN(i)
 Output:shadow prefix tree SPT(RN(i)) 
Begin: 
{ For cn=each children of RN(i) 
 {If cn is the last children of RN(i) 
Unite(SPT(cn.node label)): 
{For j=each node label of node in RN(i) 
 {If j not in the children of RN(i) 
       Create NN(j) 
        If(TESTNN(NN(j))) 
           Delete NN(j) 
        Else  
        change NN(j) to a real node RN(j)} 
    Conjunction SPT(RN(j)) with each right tree
 ;Return;}}Create_SPT(RN(cn));} 
ID SETS ID SETS
 T1 A,B,C,E,F T6 D,E
 T2 A,C,E T7 A,B,D
 T3 B,D,E,F T8 C,F
 T4 B,E,F T9 B,D,F
 T5 C,E,F T10 B,F
 441
Theorem1: kjiXiIX ,,  ,
 then
 .
 |}){(||}),{(| kXgkjXg  
 }){(}),{( kXCkjXC  
 Proof: Suppose there is 
T1  D, 1 1{ } , { , }X k T X j k T   ,T1 
 }),{(}){( kjXgkXg 	 ,
 and  is inconsistent, so T
 1|}),{(||}){(| 
	 kjXgkXg 
 |}){(||}),{(| kXgkjXg   1 is not 
existing, so .According to 
definition 2,  belong to the same 
equivalence class, so .
 }){(}),{( kXgkjXg  
 { , }, { }X j k X k 
 }){(}),{( kXCkjXC  
 Lemma 1:Suppose the set of sub nodes of SPT(RN(i)) is 
SI, ,Iji , ji  ,
 { | (RN( )), RN( ) ( )( ), }I PS j j R j SI       
 N( )) { })C PS i j C PS i j   
 )
 ,if
 then
 .
 0||, 
 
 
 ( (RN( )) { }) ( (R
 Proof: Because exists in the path of right tree of 
SPT(RN(i)),so
 .According to theorem 
1, .Lemma 1 
explains when a null node can be delete and when change a 
null node to a real node. In fig.1,Create_SPT function will 
firstly operate on RN(ABCEF) on strategy of depth-firstly. 
RN(ABCEF) has no children, so operate on its parent 
RN(ABCE).Because RN(ABCE) has only one child so there 
is no new node generating after the unite function, so is its 
parent RN(ABC).RN(ABC) has a brother RN(ABD) in its 
left side, so when RN(ABC) finishes the Create_SPT 
operation, RN(ABD) will begin the Create_SPT  operation. 
RN(ABD) is the last child of STP(AB),so when the 
create_SPT operation finishes, the unite process will begin. 
E,F are node labels of sub nodes of SPT(RN(AB)),but not 
the node labels of any children of SPT(RN(AB)),so it is 
necessary to generate two null nodes NN(E) and NN(F) 
temporarily as children of SPT(RN(AB)) and then test them. 
Delete NN(E) and NN(F),because there is only one right E or 
F sub tree of SPT(RN(ABC)) and SPT(ABD).When 
SPT(RN(AB)) and SPT(RN(AC)) finish the Create_SPT 
operation, then  RN(A) will begin the unite operation. D,E,F 
are node labels of sub nodes of SPT(RN(A)),but not the node 
labels of any children of SPT(RN(A)),so it is necessary to 
generate three null nodes NN(D), NN(E) and NN(F) 
temporarily as children of SPT(RN(A)) and then test them. 
Delete NN(D) and NN(F),because there is only one right D 
or F sub tree of SPT(RN(AB)) and SPT(RN(AC)).NN(E) 
will be deleted too,because although there are two right E 
sub tree of SPT(RN(AB)) and SPT(RN(AC)),node with node 
label C appear in sub nodes of both the two right C sub trees. 

 |g(PS(RN(i)) {j})| |g(PS(RN(i)) {j})|    
 ( (RN( )) { }) ( (RN( )) { }C PS i j C PS i j   
     Conjunction function in unite process has two 
parameters SPT1  and SPT2.The action of conjunction 
function is to add the node count of sub nodes in SPT1 to 
node count of sub nodes in SPT2 and generate virtual at the 
right time. If a sub node N1 in SPT1  has a corresponding 
node N2 in SPT2 then add the node count of N1 to the node 
count of N2.If a sub node N1 in SPT1  has a corresponding 
node in SPT2 then generate a virtual node at the 
corresponding place in SPT2 The node label of the virtual 
node is the node label of N1 and the node point links to N1.   
Algorithm2: Conjunction 
Input: shadow prefix tree1:SPT1, 
       shadow prefix tree2:SPT2 
Begin 
  {SPT2.count=SPT2.count+SPT1.count; 
  For cSPT1=each children in SPT1 
     {If(SPT2 have a child cSPT2 
       {and cSPT2.itemid=cSPT1.itemid) 
        Conjunction(cSPT1,cSPT2)} 
Else
  { vn=new Virtual Node; 
  vn.link=SPT1;}} 
  SPT2.children.add Virtual Node(vn);} 
Continue to the previous sample,SPT(RN(AC)) will 
conjunction with right C sub tree of SPT(RN(AB)).The right 
C sub tree of SPT(RN(AB)) is SPT(RN(ABC)).The node 
count of RN(AC) will firstly add the node count of RN(ABC) 
and then the node count of RN(ACE) will add the node count 
of RN(ABCE). SPT(RN(ACE)) will add a virtual node 
VN(F) point to RN(ABCEF) because SPT(RN(ACE)) has no 
child with the node label F. When PT finishes all the 
operation, a SPT will be built. The SPT of table 1 is shown 
in Fig.2. 
Fig. 2. The shadow prefix tree of  sample database 
It is clear that the SPT has the two important attributes 
after the building process. All the closed frequent itemsets 
could be found easily by scanning the SPT. By the way, SPT 
has the basal attributes of PT, so SPT can be stored in the 
disk after serializing[16].It is convenient to read the SPT 
from the serialized SPT the next time to mining the same 
data set avoiding building process.  
E. Update Incrementally 
    SPTCI is an closed frequent itemsets mining algorithm 
which can be updated incrementally .SPTCI realizes 
updating incrementally by uniting two SPT generated from 
two different data sets.  
442
Theorem 2: If X is a closed itemsets in transaction data 
base D1, when D1 add another transaction data base D2 then 
X still be a closed itemsets in the new transaction data base.  
Proof :Let c(X)=X in D1.Suppose in the transaction data 
base consisted of D1 and D2 ,X is no longer a closed itemsets, 
then there is a itemset Y,  and c(X)=Y, so the 
support count of Y is equal to the support count of X. Let 
count_X and count_Y represent the support count of X and 
Y. The support count of X in D
 YXIY  ,
 1 is bigger than the support 
count of Y in D1, because X is a closed itemset in D1 .Let 
count_X1,count_X2 represent the support count of X in D1
 and D2 respectively and count_Y1,count_Y2 represent the 
support count of Y in  D1 and D2 respectively. In D2 the 
support count of X is no little than the support of Y. So 
count_X=count_X1+count_X2,count_Y=count_Y1+count_Y2
 and count_X>count_Y, it is inconsistent with 
count_X=count_Y. So X is still a closed itemsets in the new 
transaction data base. 
    Theorem 2 indicates that if an itemsets is closed 
itemsets it will be still closed itemsets when add new 
transactions. In order to find all the closed itemsets from data 
set consisted of two sub data sets, it is necessary to 
conjunction each real nodes of two SPT generated from the 
two sub data sets. Algorithm 3 shows how to unite two SPT 
of different data sets. The unite_SPT operation is similarly to 
the union operation in algorithm 1.The unite_SPT has two 
parameters: SPTA and SPTB. In this operation each real 
node in SPTA and SPTB will be united to the new SPT. At 
the last of unite_SPT operation , all the point of virtual node 
should be redirected to the corresponding node in the new 
SPT because the input SPT will be delete when the new SPT 
is generated.  
Algorithm3: unite_SPT 
Input: shadow prefix tree1: SPT_A, 
shadow prefix tree2: SPT_B 
Output: SPT_C
 BEGIN: 
 { For ch=each children of SPT_B 
   If ch in SPT_A   
   Conjunction(SPT_A.ch,SPT_B.ch) 
   Else   
   SPT_B.addchildren(SPT_A.ch) 
For each  VN point to node of SPT_A 
  REDIRECT TO nodes of SPT_B 
Delete SPT_A  
 Return SPT_B} 
E. CFI MINING ALGORITHM
 SPTCI algorithm scans the SPT according to the mini 
support count?denoted as minsup. Algorithm 4 shows the 
process of SPTCI. 
Theorem 3: In the SPT generated from data set D, there 
is a path corresponding to each closed itemsets of D.  
Proof: Suppose ,I is the  set of items appears in the 
transaction of D, if X is a closed itemset, then C(X)=X. If 
,it is obvious that X has a corresponding path 
in SPT, because T has a corresponding path in PT and the 
process of building SPT does not reducing the node of PT. If 
IX 
 XTDT  ,
 XTDT  , ,then suppose g(X)=D1, ,let i 
be the first element in X sorted by alphabet order. If i is not a 
root node of a sub SPT, suppose the right i tree of SPT(0) is 
SPT(RN(i)) and right i node of SPT(0) is RN(i),then 
|g({i}|=RN(i).count according attribute 2 and 
TXDT  ,
 |g(PS(RN(i)) { }) | ( ).i RN i count  according to attribute 
1,so )))(((}){))((( iRNPSciiRNPSc   because theorem 1.It 
is conflicting with the definition of closed itemset. So SPT(0) 
has a child with nod label i. If i and j are two adjacent 
elements in X and  ji  ,then SPT(RN(i))(with the root 
node of RN(i)) has a child with node label j. Let 
RN(j)=R(SPT(RN(i))j),by the two attributes of 
SPT , |g(PS(RN(j)) { }) | ( ). | ( (RN( )) |j RN j count g PS j  ,if
 RN(j) is not a child of 
SPT(RN(i)),then  }|{ jliIl  ,
 ( (RN( )) { }) ( (RN( )) { })C PS i j C PS i j    ,it is 
conflicting with that i and j are adjacent in X. So elements in 
X are corresponding to nodes in a path of SPT.  
Algorithm 4: SPTCI
 Input: shadow prefix tree: SPT, 
Mini support count: minsup 
Output:closed frequent itemsets :  CFIS 
Begin: 
{Close Frequent itemsets CFIS; 
CFIS=null; 
For ( ep=each path of SPT) 
{For (en=each node of ep) 
{If (en.count<minsup) 
break; 
If (en have no children OR  
en.children.count <en.count OR 
en has two or more children) 
CFIS.add(PS(en)+i)}} 
Return CFIS} 
Theorem 4:If node(i) is a real node in SPT and the node 
count of node(i) is bigger than the node count of its children, 
then P is a closed itemset. S(node(i)) {i}
 Proof: Suppose is not a closed itemset, 
then exist and
 PS(node(i)) {i}
 ik 
 |}){))(((||}),{))(((| iinodePSgkiinodePSg   .There is a 
path corresponding to by 
theorem3.Let ,
 (node( )) { } { }PS i k i 
 1 2PS(RN(i)) {i} I I {i}  
 1 2PS(RN(i)) { } {i} I { } I {i}k k     ,
 1,1,,2 IkInlkIl   ,j is the first element in I2,then 
RN( ) has a brother RN(j),and there is a path to i in 
SPT(RN(j))(with the root node of RN(j)).  Because 
node( ) is a real node, so the node count of 
RN( ) is bigger than the node count of 
RN( ).It is conflicting with 
}{1 kI 
 1 2I I {i  }
 1 2I I {i} 
 (node( )) { } { }PS i k i 
 443
| ( (RN( )) { } { }) | | ( (RN( )) { }) |g PS i i k g PS i i   ,so
  is a closed itemset. PS(node(i)) {i}
  Theorem 3 and 4 declares that all the frequent closed 
itemsets can be found by a scanning of SPT .SPT stores all 
the closed itemsets, SPTCI finds all frequent itemsets from 
SPT by mini support count. 
F. EXPERIMENT
 In order to evaluate the performance of SPTCI, the 
experiment compares with FPL and AFOPT  on the data 
set generated by IBM data generator .  TABLE.II describes 
the information of the test datasets. Both of the two test 
datasets are very large, and the experiment is finished on a 
cluster in 
[13] [15]
 [17]
 Yangzhou University. The cluster is consist of 
fourteen blades of IBM. Each blade has two Intel Xeon 
processors and 24GB RAM. These algorithms are realized 
by C++ on Ubuntu. Fig.3 and 4 are the results of the 
experiment. 
TABLE II. THE INFORMATION OF TEST DATASET
 Both dataset 1 and 2 are dense datasets with long patterns. 
The time consumed by SPTCI is little than the time 
consumed by FPL and AFOPT because there is no sub set 
testing process in SPTCI.SPTCI avoids the scanning of sub 
tree whose root node has a node count little than mini 
support count, so the mini support count would not effect 
SPTCI as much as FPL and AFOPT.   
2
 42
 62
 82
 :2
 322
 342
 362
 382
 3:2
 7 32 37 42 47 52
 okpkowo"uwrrqtv*'+
 wu
 kp
 i"
 vk
 og
 *u
 +
 URVEK
 HRN
 CHQRV
 Fig. 3. Result of dataset 1 
2
 722
 3222
 3722
 4222
 4722
 5222
 5722
 7 32 37 42 47 52
 okpkowo"uwrrqtv*'+
 wu
 kp
 i"
 vk
 og
 *u
 +
 URVEK
 HRN
 CHQRV
 Fig. 4. Result of dataset 2 
CONCLUSIONS
 SPTCI algorithm gets closed frequent itemsets directly 
by scanning the SPT once, improves the performance by 
reducing the searching space and avoiding the testing of sub 
sets. SPT is an important data structure used by SPTCI. The 
two attributes of SPT let it store all the closed itemsets, and 
the virtual nodes makes SPT avoiding to generate duplicate 
nodes and reducing the space.  
ACKNOWLEDGMENT 
This research was supported in part by the Natural 
Science Foundation of Education Department of  Jiangsu 
Province under contract 11KJB520011 and 
12KJB520018,the High-level talent Project of "Six talent 
peaks" of Jiangsu Province under contract 2012-WLW-024 
and Joint innovation fund project of industry,education and 
research of Jiangsu Province (prospective joint research) 
under contract BY2013063-10. 
REFERENCES
 [1] A grawal R, Imielinsk iT, Swam I A. “”“Mining association rules 
between sets of items in large database .” Proceeding of the 1993 
ACM S IGMOD International Conference on Management of Data. 
New York:ACM Press, 1993. 207- 216 
[2] ClaudioLucchese,SalvatoreOrlando,and Raffaele Perego, “Fast and 
Memory Efficient Mining of Frequent Closed itemsets,” Knowledge 
and Data Engineering,Jan.2006,21-36 
[3] R.Belohlavek,V.Vychodil, “Discovery of optimal factors in Boolean 
factor Analysis via novel method of binary matrix decomposition,” 
Comp.System Sciences 76(1):2010,3  
[4] Petr Krajca,Jan Outrata,Vilem Vychodil , “Using frequent closed 
itemsets for data dimensionality reduction,” Proceeding of the 11th 
International Conference on Data Mining (ICDM),Dec. 2011,1128 - 
1133
 [5] R.Godin,R.Missaoui, “An incremental concept formation approach 
for learning from databases,” Theoret.Comput.Sci.133,1994,378-
 C419
 [6] N.Pasquier,Y.Bastide,T.Taouil,L.Lakhal, “Efcient mining of 
association rules using closed itemsets lattices,” 
Inform.SySPTems24(1),1999, 25-C46 
[7] Y.Bastide,R.Taouil,N.Pasquier,G.Stumme,and L.Lakhal.,“Mining 
frequent patterns with counting inference,” ACM SIGKKDD 
Explorations Newsletter.vol,2,December 2000,66-75 . 
[8] Jiawei Han, Jian Pei, Yiwen Yin. “Mining frequent patterns without 
candidate generation,”  Proceedings of the ACM SIGMOD 
International Conference on Management of Data, v 29, n 2,2000: 1-
 12. 
[9] J.Pei,J.Han,and R.Mao. “CLOSET:An Efficient Algorithm for 
Mining Frequent Closed itemsets,”  In SIGMOD Int’l Workshop on 
Data Mining and Knowledge Discovery(DMKD00),21-30. 
[10] J.Y.Wang,J.Han,and J.Pei. “CLOSET+:Searching for the Best 
Strategies for Mining Frequent Closed itemsets,” Proc of ACM 
SIGKDD(KDD 03),ACM Press,2003,24-27  
[11] G.Grahne,J.Zhu. “Efficiently using Prefix-trees in Mining Frequent 
itemsets. ,” Proc.Of FIMI’03,2003,123-132 
[12] Hua-Fu Li,Chin-Chuan Ho, Fang-Fei Kuo and Suh-Yin Lee,“A New 
Algorithm for Maintaining Closed Frequent itemsets in Data Streams 
by Incremental Updates,” Proc.of IWMESD,Hong Kong,2006.672 - 
676
 [13] Qin Li, Sheng Chang , “Generating Closed Frequent itemsets with the 
Frequent Pattern List,” Database Technology and 
Applications,2010,1-4 
Datasets Size  Records Items Average length 
T1000I10T
 10000I100 
5G
 15G
 1000k 
10000k 
10k
 1000k 
20
 40
 444
[14] Chun Liu, Zheng Zheng, Kai-Yuan Cai, Shichao Zhang, “Distributed 
Frequent Closed itemsets Mining,” Signal-Image Technologies and 
Internet-Based System, 2007. SITIS '07. Third International IEEE 
Conference on,2007,43-50 
[15] Gui mei Liu Hong jun Lu, “Ascending Frequency Ordered Prefix-tree: 
Efficient Mining of Frequent Patterns,” Database Systems for 
Advanced Applications, 2003. (DASFAA 2003). Proceedings. Eighth 
International Conference on,2003,65-72 
[16] Jie Xu,Yun Li,Bo Liu, “ A Parallel Frequent Itemsets Mining 
Algorithm Based on Vertical FP-tree,” Computer and Digital 
engineering,vol.40 ,2012,12-16 
[17] IBM Almaden Research Center, "Synthetic Data Generation Code for 
Associations and Sequential 
Patterns,"URL:http://www.almaden.ibm.com/ software/quest/, 2006.4 
445

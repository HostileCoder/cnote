Tree Labeled LDA: A Hierarchical Model for Web Summaries 
Anton Slutsky, Xiaohua Hu and Yuan An 
College of Information Science and Technology 
Drexel University  
Philadelphia, PA, USA  
{as3463, xh29, yuan.an}@drexel.edu 
 
 
Abstract—We study the applications of hierarchical topic 
models to represent the content of website summaries. We 
concentrate on the DMOZ collection of Web extracts and 
propose a novel Tree Labeled LDA (tLLDA) algorithm to infer 
topic models using its manually compiled ontology.  The 
algorithm takes advantage of the ontology structure and infers 
topic models by jointly modeling word and ontology node 
assignments for documents.  We evaluate the performance of our 
topic modeling approach against that of four state-of-the-art 
algorithms (Labeled LDA, Hierarchically Labeled LDA, 
Hierarchically Supervised LDA and Supervised LDA) and show 
improvement in terms of perplexity and accuracy.  Our 
evaluation shows that topic models produced by tLLDA 
outperform other algorithms in terms of perplexity for all test 
sets and all but one test case in terms of accuracy. 
I. INTRODUCTION  
The Open Directory project (also known as DMOZ) aims to 
organize web content by compiling a comprehensive directory 
of public web sites available on the Web.  This Web directory 
compilation is accomplished through the hard work of 
thousands of human volunteers who inspect and manually map 
websites based on their content into a well-defined ontology 
[1]. 
Unfortunately, DMOZ ontology elements provide no 
information beyond their textual labels as to the meaning of 
associated documents.  Even though much work has been 
done by the DMOZ project towards organizing and classifying 
collections of Web documents, the project makes no attempt 
to extract path-level and statistical views of content.  Such 
additional information may facilitate tasks such as browsing, 
searching, and assessing document similarity [2].   
Therefore, in this work, we attempted to leverage the 
manually created ordered ontology structures published by the 
Open Directory project to model the content of underlying 
document collections.  The aim was to produce a view of the 
content that would enable individuals to quickly grasp 
underlying themes of documents associated with ontology 
nodes.  Using topic modeling techniques, we developed an 
approach for constructing statistical views of ontology nodes 
by associating them with topics, which were probability 
distributions over a fixed vocabulary.  Our technique 
estimated distribution parameters as word-multinomials and 
used these multinomials to produce sorted lists of vocabulary 
terms for all nodes in the ontology.  Qualitative evaluation of 
these lists suggested that top most probable terms, as specified 
by the corresponding word-multinomial parameters, were 
indicative of the underlying general theme of Web documents.   
We propose a new probabilistic generative model based on 
the Labeled-LDA [3] approach, which is a supervised variant 
of the well-known LDA [4] model.  The new Tree Labeled 
LDA model takes advantage of the hierarchical nature of 
DMOZ ontology and jointly models word and ontology node 
assignments as a generative process.  
Quantitative evaluation of our approach conducted by 
comparing predictive power of resulting topic models with 
that of topic models produced by other state-of-the-art 
algorithms in terms of perplexity and accuracy for held-out 
data showed that, for datasets used in the study, the new 
tLLDA model outperformed Labeled LDA and Hierarchically 
Labeled LDA topic modeling approaches in terms of 
perplexity in all cases and Supervised LDA and Hierarchically 
Supervised LDA in terms of classification accuracy in all but 
one case where Supervised LDA beats our model by a small 
margin.  The subsequent qualitative evaluation of the resulting 
topic models as compared to topic models produced by 
Labeled LDA and Hierarchically Labeled LDA suggested that 
topic models produced by the new tLLDA were more 
semantically indicative of the underlying content. 
The rest of the paper is organized as follows.  Section 2 
presents the review of notable related works.  In section 3, we 
introduce the generative Tree Labeled LDA (tLLDA) model in 
detail.  Section 4 discusses procedures for estimate distribution 
parameters from data.  In section 5, we discuss experimental 
setup and evaluate language models produced by the tLLDA 
approach as compared with other algorithms.  In sections 6 and 
7 we discuss conclusions and outline future work. 
II. RELATED WORKS 
The now classical work on Latent Dirichlet Allocation 
(LDA) [4] has provided an extensible modular framework for 
many topic modeling approaches.  In LDA, the basic idea is 
that documents are represented as random mixtures over latent 
topics with each topic characterized by a distribution over 
words [4].  While Latent Dirichlet Allocation has served as 
basis for many approaches [2], it is fully unsupervised and 
may not be the best choice when the goal is prediction.  That 
is, for instance, in a system concerned with movie ratings 
intuitively good predictive topics could differentiate between 
134
 2013 IEEE International Conference on Big Data
 978-1-4799-1293-3/13/$31.00 ©2013  IEEE 
“excellent”, “terrible” and “average” ignoring the genre.  
Unsupervised machinery of the basic LDA, however, may 
estimate topics that correspond to genres if that is the intrinsic 
structure of the corpus [2]. 
The Supervised LDA model (sLDA) [2] extends LDA by 
taking supervision into account and adding a response variable 
associated with each document. This variable is usually 
associated with the supervisory labels for documents, such as 
the film rating adjectives in the above example.  The sLDA 
model jointly models documents and responses with the goal 
of finding latent topics that will best predict responses for test 
data [2].  The response values come from a normal linear 
model, which covariates in the sLDA model with empirical 
frequencies of topics in documents [2]. 
A further refinement on the Supervised LDA model – the 
Hierarchically Supervised LDA (HSLDA) [5] – extends sLDA 
to take advantage of hierarchical supervision.  The HSLDA 
model is based on the intuition that hierarchical context of 
labels provides valuable information about labeling.  As in 
sLDA, HSLDA jointly models documents and responses by 
drawing response variable realizations from a Normal 
distribution, but unlike sLDA it generates label responses 
using a hierarchy of conditionally dependent probit regressors 
[5].  In the joint modeling of each document, both empirical 
topic distribution and whether or not the parent label is applied 
to the document determine whether or not a label is to be 
applied.  The HSLDA model views word-multinomials 
(topics) as global constructs and links them to hierarchy nodes 
through per-label topic distributions.  This makes HSLDA 
output difficult to interpret, as the global topics do not directly 
correspond to nodes. 
While Supervised LDA and Hierarchically Supervised 
LDA have been shown to work well in some applications, 
they have the limitation of allowing only a single label to be 
applied to a document and are thus not applicable to document 
collections where multiple labels can be assigned to texts [3].  
Therefore, a radically different way of providing supervisory 
input to topic modeling was developed.  Named Labeled LDA 
(L-LDA), this model aimed at joining the multi-label 
supervision frequently found in modern text databases with 
word-assignment disambiguation of LDA family of models 
[3].  In L-LDA, each unique label is viewed as a topic and the 
goal of the model is to restrict the generative process to 
operate over a subset of topics, thus allowing for each 
document to be supervised. 
Similarly to L-LDA, Hierarchically Labeled LDA (or 
hlLDA) [5] constricts the general LDA model to operate over 
a subset of label-bound topics.  While L-LDA is a general 
model for corpora where documents may be associated with 
multiple topics that may or may not be related to each other in 
any way, hlLDA considers each document to be associated 
with a set of topics which correspond to the set of nodes on 
the hierarchical path for each document.  The hlLDA model 
relies on some of the formalisms described in yet another 
LDA successor model called Hierarchical LDA (hLDA) [6], 
which is an unsupervised generative model which infers 
hidden hierarchical structures from data.  The hlLDA variant 
extends the hLDA approach by assuming that the hierarchical 
structure -- which is hidden from hLDA -- is known and 
restricts topics accordingly.  Unfortunately, the hlLDA model 
is limited as it only considers supervision from observed 
hierarchy paths for each document and does not make use of 
the hierarchy structure as a whole. 
It is important to note that hierarchical topic modeling 
techniques such as hlLDA and HSLDA place statistical 
pressure on the posterior to have more general terms in topics 
towards the root of the hierarchy as there are more paths 
through nodes at higher levels than there are through lower 
level nodes [7].  While this is desirable in some cases, the 
statistical pressures may become overwhelming for sparsely 
populated hierarchies.  The proposed tLLDA model attempts 
to combat the tendency to favor higher level nodes by offering 
a statistical counterbalance derived from the structure of the 
hierarchy. 
III. MODEL 
In this section we introduce the Tree Labeled LDA 
(tLLDA), which is a generative probabilistic model that 
describes the process of generating a document collection 
where each document is associated with a distribution over 
hierarchy nodes.  It improves upon hlLDA by jointly modeling 
word and node label assignments, which allows it to take the 
hierarchy structure as a whole into consideration.  Further, the 
proposed tLLDA model estimates a single word-multinomial 
for each node of the target ontology, which allows for an 
easier interpretability when compared with the HSLDA 
model.    
The tLLDA model is intuitively motivated by the nature of 
manually constructed taxonomies, where conceptual meanings 
of hierarchy labels often represent more general concepts at 
levels closer to the root node and increase in specificity 
towards the leaf nodes [7].  tLLDA attempts to take advantage 
of hints as to the specific meaning of each document by 
biasing the generative process towards hierarchy nodes 
representing narrower conceptual meanings while still 
allowing words to be generated from all hierarchy levels.  
Further, we are motivated by the intuition that hierarchical 
nodes with edges to large number of children are 
representative of more general concepts whereas fewer child 
elements may be indicative of narrower meanings.  The 
tLLDA model takes the later intuition into account by relaxing 
the bias towards lower hierarchy levels in those cases where 
paths are comprised of relatively barren nodes. 
 The tLLDA model aims to incorporate both the multi-label 
supervision and supervision derived from the structure of the 
target hierarchy.  Similar to other topic modeling techniques [5, 
8], it adopts the mixed  membership formalism [4] where a 
document is thought of as a mixture over a set of word-
 multinomials [8].  The tLLDA approach combines the multi-
 label supervision of hlLDA with hierarchical supervision by 
jointly modeling word and label assignment generation.  Also, 
unlike HSLDA which generates label responses using 
conditional hierarchy of probit regressors [5] assuming a 
Normal distribution, tLLDA draws a path  through the 
hierarchy for each topic directly from a distribution 
135
parameterized by a global vector of multinomial parameters 
without assuming any underlying distribution shape. 
A. Notation 
Let each document d be represented by a tuple consisting of 
list of word indices ???? ? ???? ? ? ????  where each ?? ???? ? ? ???? , ?  is the vocabulary and ??  is the document 
length. Let hierarchy ? ? ??? ?? ???  be a tree which is a 
directed acyclic graph with known structure where ? ?
 ???? ? ? ???? is the set of vertices (or nodes) of size ??, ? is the 
set of edges, ?? ? ? is the root node and each node ?? ? ? may 
have at most one parent.  Let ? be the total number of topics 
equal to the size of the set  ?.  Note that since the number of 
topics equals the number of vertices in the hierarchy, topics 
and vertices (nodes) may be used interchangeably in this 
context. Let a path to a node ?? ? ?  be the list ????? ?
 ???? ? ? ??? such that ?? ? ????? ? ? and each subsequent node 
in the list ?????  is a child of its predecessor.  For each node 
?? ? ?  let ????????????  be a set such that 
??? ? ¸?œ??º?ºßœ???? ? ı?ß??? "? "??? ? 8??? ? ı?ß?? "? ?? ?
 ¸?œ??º¸?ºßœ??.  Let ??  represent the sub-tree of node ? such 
that ?? ? ?and ??? ? ????? ? ????????????? "? "?????. 
B. Theory 
The tLLDA approach models document creation by 
imagining a process that randomly generates two strings of 
equal length – a string of words and a string of hierarchy node 
labels.  The string of words is generated in a way common to 
other topic modeling approaches [3, 5, 6, 8].  That is, as in 
other related approaches, tLLDA draws a topic from a 
distribution parameterized by a topics proportions vector and 
then generates a word by drawing it from a distribution 
parameterized by a word-multinomial vector.  At this stage, 
tLLDA diverges from other topic modeling algorithms [3, 5, 
6, 8] and adds a new step to generate a label by randomly 
selecting a node from the distribution over labels conditioned 
on the topic drawn earlier.  The probability of drawing node 
label ?? from some node (topic) ?? is 
???????? ? ?????
 ??"??"?? ? ??
 ?"?????????  
Because probabilities of drawing a word from a topic and 
drawing a label from a distribution conditioned on the topic 
are independent, the chance of drawing a word string ? and a 
label string ? of length ??? is 
???? ????? ?????????????????????????
 ????
 ???
 ???
 ???
  
where ?  is the mixture proportion vector and  ? ?? ????????
 ??"?? [8].  The generative process of the algorithm is outlined 
in Figure 1: 
 
 
 
 
 
 
 
 
1. For each topic ? ? ???? ? ??: 
2.     Generate ?? ? ?????? ? ? ???????????? ?"? 
3. For each topic ? ? ???? ? ??: 
4.     For each topic ?? ? ??? ? ???: 
5.         Deterministically set ?????? ? ????? using Eq. 1 
6.     Generate ???? ? ???? ? ? 
7.     Generate ?? ? ?????? ? ? ????????????? ?????? 
8. For each document ?: 
9.     For each topic ? ? ???? ? ??: 
10.        Generate ????? ? ????????????? ???? 
11.   Generate ???? ? ???? ? ? 
12.   Generate ???? ? ????? ? ? ??????????? ?????? 
13.   For each word ? ? ???? ? ???: 
14.        Draw ?? ? ??????? ? ? ?????
 ??? ? ??????? ?????? 
15.        Draw ?? ? ??? ? ? ????"?"?????? ????? 
16.        Draw ?? ? ??? ? ? ??"?"?????? ????? 
Fig. 1. tLLDA generative algorithm 
Steps 1 and 2 where the multinomial topic distributions ?? 
over vocabulary for each topic ? ? ???? ? ?? conditioned on 
the Dirichlet prior ? are drawn remain identical to the standard 
LDA, L-LDA and hlLDA.   
To use the hierarchy structure as a supervising agent for 
the generative process we deterministically construct vector 
???? ? ????? ? ? ????  such that ??? ? ???? ? ?????  for each 
topic ? in steps 4-5:   
????? ? ? ?? ??"?? ? ???? ?????????    (1) 
Because the hierarchy structure is known apriori and is fixed 
for all documents, this deterministic step does not jeopardize 
the generative nature of the approach.  Having thus generated 
vector ???? we define a node-specific matrix ???? over ?? ? ? 
for each node where ?? ? ????? ? ???????? ? ??? ; for each 
row ? ? ??? ? ? ??? and column ? ? ??? ? ???: 
 ?????? ? ??? ??"??
 ??? ? ?
 ?? ?????????   
The matrix ??????  is used to project parameter vector of the 
Dirichlet prior  ? ? ???? ? ? ????  to lower dimensional vector 
???? in step 6: 
 ???? ? ???? ? ? ? ???????? ? ? ????????  
In step 7, path assignment proportion vector ?? is drawn for 
each topic with parameter vector ???? . Since ????  is 
constructed by the use of ??, the makeup of vector ?? depends 
on where the node corresponding to topic ? is in the target 
hierarchy in relation to other nodes. Therefore, the parameter 
vector ????  encodes the supervisory input of the hierarchy 
structure as it is a projection of K-dimensional vector ? onto 
the lower dimensional space defined by ?’s sub-tree. 
Then, for ? number of topics let ???? ? ???? ? ? ???  be the 
list of binary topic presence/absence indicators for document 
?  such that  ??" ? ????? .   As in L-LDA, vector ????  is 
generated in steps 9-10 by using a binomial distribution for 
each topic ? with prior ?? [3].  With that, a document-specific 
136
label projection matrix ????  over ?? ? ? is defined for each 
document where ?? ? ????? ? ???????? ? ??? ; for each row 
? ? ??? ? ???? and column ? ? ??? ? ? ??: 
 ?????? ? ??? ??"??
 ??? ? ?
 ?? ????????? [3] 
The matrix ???? is used to project the parameter vector of the 
Dirichlet prior ? ? ???? ? ? ???? to lower dimensional vector 
????: 
 ???? ? ???? ? ? ? ???????? ? ? ??????? ? [3] 
Step 12 draws ????  by parameterizing a Dirichlet 
distribution with ???? computed in step 11.  Then, to generate 
a word, topic ? is sampled from a distribution parameterized 
by ???? and a word is sampled from distribution over words 
parameterized by the word proportion vector ??.   
Unlike other algorithms that repeat the word generation 
process at this point, tLLDA draws a label assignment ?? ? ?? 
from a distribution parameterized by vector ?? where ? is the 
topic drawn in step 7.  As the vector is projected onto the 
lower dimensional space defined by ?’s subtree (see definition 
of ??), it is in this step that the structure of the hierarchy is 
included into the supervision -- makeup of ??  and 
consequently the values of vector ?? depend on where the ?th 
node is in the target hierarchy in relation to other nodes. 
 
 
Fig. 2. hLLDA graphical model showing the plate diagram with solid lines 
representing probabilistic links and dashed lines representing 
deterministic relationships.  The shaded circles represent observed nodes 
whereas un-shaded nodes are latent. 
IV. PARAMETER ESTIMATION 
The model of document generation described in the 
previous section outlines a process that is not directly 
observed.  Instead, an observer is presented with the output of 
the process, which may be used to infer latent parameters that 
governed the algorithm responsible for producing the output.  
In much of the topic modeling literature, the output is limited 
to word strings that constitute documents in a given corpus.  
The tLLDA model, however, posits that there exists a second 
output detectible to an outside observer – namely, the 
document-length string of label assignments. 
And indeed, such additional output can be detected by 
considering the placement of each document in the target 
hierarchy.  Since document is associated with a hierarchy 
node, it is easy to construct a document-length string of label 
assignments by repeating the node’s label as many times as 
there are words in the document.  That is, in the tLLDA 
context, we say that in addition to observing a randomly 
generated string of words, we observe the eventuality of the 
randomly generated string of labels being entirely made up of 
identical elements.  
We used Gibbs sampling to estimate topic-word 
distribution parameter ???? since compared to other parameter 
estimation methods, Gibbs sampling yields a relatively simple 
and computationally efficient algorithm [9].  Sampling 
equation for a topic for document d and path leading to ?? 
(notation outlined in Table 1) : 
???? ? ?????? ?? ? ? ??? ?? ?? ?? "?
 " ?????
 ?? ??""
 ?????
 ??? ????? ?
 ?????
 ??? ??
 ?????
 ??? ??????????
 ? ?????
 ?? ??
 ?????
 ??? ??????
   
 
TABLE I.  NOTATION 
?? ?? ? Dirichlet hyperparameters 
???????  The number of times word ?? assigned to topic ? not 
counting the current instance 
????????  Number of times any word is assigned to topic ? 
excluding the current instance 
????????  The number of times topic ? is assigned to document ? excluding the current instance 
????????  Number of times any topic is assigned to document ? 
???????  ???????
 ??? ? ??"? ? ??????
 ?? ?????????  
????????  ? ????????
 ??
 ???
  
?? Observed node assignment for document ?; 
?? ? ???? ? ??
  
After a predetermined number of iterations of the sampling 
process based on distributions estimated using the above 
equation, parameters can be estimated for any single sample as 
using the following equations: 
????? ? ??
 ?????
 ???????????????
  , ???? ?
 ??
 ??"??
 ??
 ????????", ??
 ?? ? ????????????????? 
V. EXPERIMENTAL RESULTS 
A. Experimental Data 
We tested the tLLDA on the Web summary dataset 
provided by the Open Directory project.  The Open Directory 
project publishes four distinct data dump pairs (structure file 
and content file).  These are ‘World’, ‘Kids and Teens’, 
137
‘Adult’ and ‘AOL’. The well-structured DMOZ ontology is 
published in a separate RDF documents devoid of review 
content.  Review content RDF files are made up of sections 
corresponding to ontology identifiers (topics).  Each section 
contains a set of URL references to websites, their titles and 
description strings that are relatively short with the average of 
4.2 and 18.1 words per string respectively. 
The ‘World’ RDF download contains the ontology 
structure and corresponding reviews for all web sites reviewed 
by the project excluding those found in ‘Kids and Teens’, 
‘Adult’ and ‘AOL’ repositories. The ‘Kids and Teens’ 
download contains reviews of web sites related to children and 
teenagers, such as reviews of cartoons and primary school 
activities.  In our experiments, we excluded the ‘AOL’ 
repository as the latest file published at [10] contained no 
review content.  We also excluded the ‘Adult’ section from 
our experiments for ethical reasons and tested our approach on 
the ‘World’ and the ‘Kids and Teens’ datasets.  As both the 
‘World’ and ‘Kids and Teens’ repositories were large (~2.1 
million records and ~26,000 respectively) and due to hardware 
and time constraints we tested our approach using a set of 
smaller subsets of records extracted from each repository. 
For our experiments, we used the English language 
portions of the ‘World’ and the ‘Kids and Teens’ DMOZ 
datasets.  As preprocessing steps, each raw record, title and 
description strings were extracted and concatenated into a 
single document.   Resulting strings where case-folded and 
tokenized using simple tokenization rules followed by non-
 letter characters (numbers and punctuation) removal.  
Stemming and lemmatization was not applied.  Following the 
process in related works [8], vocabulary was extracted by a 
single pass through the data and documents were regenerated 
by replacing English terms with numerical identifiers of terms 
in the vocabulary. 
Because of the sparse nature of data sets which are made 
up of relatively small number (average of 5.4 documents per 
hierarchy node) of short review documents and in order to 
preserve the underlying relationships within the hierarchy, we 
extracted six fixed size data windows of various sizes from 
each of the two data sets. Further, we reserved 10% of the data 
in each data window for testing by applying simple random 
sampling without replacement.  The maximum number of 
records used for testing the approach was limited to 6000 
elements.  This number was chosen as it was empirically 
determined to be the largest number of records that the 
available hardware was able to process in one twenty-four 
hour period for each algorithm. 
B. Comparison Models 
We compared the predictive power of the language models 
produced by our approach to four related models against the 
Open Directory (DMOZ) dataset.  The four comparison 
models included the Hierarchically Labeled LDA [8] and the 
Labeled LDA [3], Hierarchically Supervised LDA [5] and 
Supervised LDA [2].  We considered two distinct comparison 
approaches as the evaluation criteria.  We used perplexity to 
compare tLLDA performance with that of hlLDA and L-LDA 
as perplexity is a common way to compare language models 
[3].  To compare tLLDA predictive power with that of sLDA 
and HSLDA we used multilabel classification Exact Match 
(MR) as the evaluation criteria.  We chose to use multilabel 
classification as opposed to perplexity for sLDA and HSLDA 
because sLDA-type approaches associate supervisory topics 
with distributions over language models rather than with 
single language model, making evaluation in terms of 
perplexity not feasible. 
We compared language models produced by our approach 
to those learned by hlLDA and L-LDA as the output of these 
models lends itself to per-topic language model comparison.  
The perplexity measure used for comparison over held-out 
subset of data ?? ? ????? ? ? ???? given language model ? and 
the training data [8] is computed via the following formula: 
???????? ? ? ?®ı"??
 ?
 ??
 ?
 ?????? Øæ?"??????????
 ?????
 ???
 ?
 ???
 ? 
where ? ? ??? ?, ?? ? ?? , ??  is the jth term in the ith string in 
the held-out collection and ???? ? ??? is the probability of 
term ? as per the learned language model ?. 
    The tLLDA approach outperformed L-LDA and hlLDA 
algorithms in terms of perplexity.  Tables 2 and 3 summarize 
experimental results for ‘World’ and ‘Kids and Teens’ data 
subsets respectively in terms of perplexity values.  We set 
? ? ???? ? ? ????" and ? ? ?  for all experiments as these 
values are commonly chosen in the related literature [9].  We 
set the number of iterations to 1000 as the algorithm appeared 
to converge past that number of iterations. 
TABLE II.  ‘WORLD’ RESULTS 
  1000 2000 3000 4000 5000 6000 
L-LDA 2642.6 3952.4 4661.7 6016.8 6740.4 8142.8 
hlLDA 3159.3 4598 5714.7 7046.4 8264.21 9787.72 
tLLDA 690.25 795.99 779.99 877.47 1077.13 1356.14 
‘World’ dataset perplexity values for L-LDA, hlLDA and tLLDA 
(rows) and 1000-6000 record data windows (columns) 
TABLE III.  ‘KIDS AND TEENS’ RESULTS 
  1000 2000 3000 4000 5000 6000 
L-LDA 4170.66 4958.17 6232.11 7450.27 8226.14 8976.4 
hlLDA 4081.14 6247.47 7767.5 8809.1 9421.62 10035.43 
tLLDA 838.26 965.94 1165.07 1297.19 1400.46 1473.13 
‘Kids and Teens’ dataset perplexity values for L-LDA, hlLDA and 
tLLDA (rows) and 1000-6000 record data windows (columns) 
Since sLDA-type approaches associate supervisory labels 
with distributions over language models rather than with 
single language model per label as in the case of hlLDA and 
L-LDA, it is difficult to meaningfully compare language 
models of tLLDA with those of sLDA and HSLDA using the 
perplexity quantifier.  Therefore, in addition to perplexity 
evaluation we compared performance of tLLDA to that of 
138
other related models by conducting multilabel classification, 
which is a task of predicting the set of labels appropriate for 
each document given a training set of documents with 
multiple labels [3].   
In this study, the classification algorithm remained the 
same for tLLDA, hlLDA, L-LDA, sLDA1 and HSLDA2 with 
the only variation being the posterior probability 
approximation procedure, which was specific to each 
algorithm.  Since the goal of the classification task is to 
predict the set of labels (a path) for each test document, 
document-topic approximation routine used during testing 
must differ from that which was used during model training 
for topic modeling approaches that take supervision into 
account.  Therefore, we employed the inference procedure 
described in [3] to estimate document-topic distribution 
parameters.   There, global parameters that were estimated 
during training were used to initialize the state of the 
unsupervised LDA sampling procedure, which was then used 
to estimate document-level distributions. 
Recalling that our ultimate goal was to help accurately 
classify web documents and associate them with correct 
hierarchical paths, we measured the results in terms of 
example-based classification accuracy which is defined as 
?
 ? ? ???? ? ???????  where ?  is the indicator function, ?  is the 
number of test documents and ?? and ?? are sets of predicted 
and actual labels for the ith test document respectively [11].  
Figures 3 and 4 show accuracy results for tLLDA, hlLDA, L-
 LDA, sLDA and HSLDA for each of the test data windows in 
the ‘World’ and ‘Kids and Teens’ datasets respectively. 
 
Fig. 3. Accuracy results for each test data window in the ‘World’ dataset 
                                                           
1  Implementation available at 
https://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/chongw/slda/ 
 
2  Implementation in Python graciously provided by authors of 
HSLDA in personal communication 
 
 
Fig. 4. Accuracy results for each test data window in the ‘Kids and Teens’ 
dataset 
C. Topic Visualization 
Recalling that the goal of this study was to produce a view of 
the content which would enable individuals to quickly grasp 
the underlying theme of documents associated with each 
ontology node and realizing that no clear means existed for 
quantifying the quality of topic multinomials [9], we evaluated 
the topics discovered by our model by examining the top 
words assigned to each topic.  We observed that the top word 
assignments produced by the tLLDA model (Table 4) 
appeared semantically more meaningful as compared to those 
produced by hlLDA and L-LDA, as the language models 
produced by the later algorithms seemed to favor proper nouns 
for language models associated with lower-level hierarchy 
nodes and marginalize other terms that added semantic 
consistency to evaluation.  
TABLE IV.  TOPIC VISUALIZATION 
Sample topic visualizations for top performing evaluated algorithms 
(rows) and several hierarchy levels (columns).  Highlighted terms 
indicate words that appeared semantically indicative of the content 
theme during qualitative evaluation by the authors. 
VI. DISCUSSION 
Similar to L-LDA, one of the advantages of tLLDA is the 
document-specific topic mixture ? .  In the context of Web 
documents, the topic mixture can be inferred for each Web 
page and since each topic is associated with a node in the 
target ontology, insight as to the subject matter of each 
document can be gained by evaluating the topic proportions.  
Such insight may be instrumental in helping organizations 
such as the Open Directory project in their important tasks of 
digesting the Web content and presenting it to the public in an 
accessible way. 
0
 0.1
 0.2
 0.3
 0.4
 1000 2000 3000 4000 5000 6000
 Ac
 cu
 ra
 cy
 Number of records
 sLDA
 HSLDA
 LLDA
 hLLDA
 tLLDA
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 1000 2000 3000 4000 5000 6000
 Ac
 cu
 ra
 cy
 Number of records
 sLDA
 HSLDA
 LLDA
 hLLDA
 tLLDA
 Composers Classical Beethoven,_Ludwig_v
 an 
tLLDA and with 
includes for a  his 
samples 
biography … 
of composer the 
on work classical 
grieg edvard in a 
information ... 
biography beethoven 
van ludwig and brief 
works of a includes list 
L-LDA biography brief 
composer the of 
with for key  … 
mozart amadeus 
life wolfgang 
work strings 
mozarts … 
van beethoven  ludwig 
includes compositions  
139
As node-specific word-multinomials are learned by the 
tLLDA approach from the underlying collections of 
documents, it may be possible to consider words in terms of 
their probability vis-à-vis the corresponding node-level 
language model and revisit the structure of the target 
hierarchy.  As target ontologies, such as the one used by 
DMOZ, are manually compiled, they may contain omissions 
or overgeneralizations.  Such omissions and 
overgeneralizations may become prominent when viewed 
through the prism of their learned language models. 
To exemplify, consider the sample cited in Table 4.  There, 
the set of terms “biography beethoven van ludwig and brief 
works of a includes list” are top most probable for the 
hierarchy node with the path ‘Beethoven,_Ludwig_van’ as per 
the node-specific language model learned by the tLLDA 
approach.  Examination of this list of most probable terms 
may suggest that the underlying collection of documents 
related to the famous composer may be further partitioned into 
two more specific subcategories – one containing documents 
related to the biography of the composer and the other related 
to his works. 
VII. CONCLUSIONS AND FUTURE WORK 
In this paper, we proposed a new topic modeling approach 
of Web summaries using a popular Web ontology.  This 
approach took advantage of the hierarchical structure of the 
ontology to improve predictive power of resulting topic 
models.  While we focused on the Web summary data 
provided by the Open Directory project, the topic modeling 
approach introduced here can be easily adopted to any 
hierarchically organized content.  This type of model can be 
useful in identifying key notions in large collections of text. 
In this work, we took advantage of tree-like hierarchical 
structures exhibited by the Open Directory ontology.  In 
addition to the hierarchical relationships, however, the 
ontology also provides lateral links between related nodes that 
often breach hierarchical boundaries of parent-child 
relationships.  Understanding how to take advantage of these 
links to further improve predictive power of topic models is 
one area of future research. 
Since much of our evaluation was hindered by the 
computational complexity of parameter estimation algorithms, 
in future works we will attempt to leverage hierarchical 
structures to reduce time required to estimate parameters for 
the proposed model for larger datasets by distributing the 
parameter estimation process.  We will attempt to build on 
earlier works on distributed topic modeling algorithms (e.g.: 
[12]) to improve performance of the tLLDA parameter 
estimation procedure. 
REFERENCES 
[1] DMOZ. Available: http://www.dmoz.org/ 
[2] D. Blei and J. McAuliffe, "Supervised topic models," Advances in 
Neural Information Processing Systems 21, 2007. 
[3] D. Ramage, et al., "{Labeled LDA}: A supervised topic model for 
credit attribution in multi-labeled corpora," in Proceedings of the 
2009 Conference on Empirical Methods in Natural Language 
Processing, 2009, pp. 248-256. 
[4] D. Blei, et al., "Latent Dirichlet Allocation," Journal of Machine 
Learning Research, vol. 3, pp. 993-1022, 2003. 
[5] F. W. Adler J. Perotte, Noemie Elhadad, Nicholas Bartlett, 
"Hierarchically Supervised Latent Dirichlet Allocation," NIPS'11, 
pp. 2609-2617, 2011. 
[6] D. Blei, et al., "Hierarchical topic models and the nested Chinese 
restaurant process," in Neural Information Processing 
Systems(NIPS), ed, 2003. 
[7] T. Weninger, et al., "Document-topic hierarchies from document 
graphs," presented at the Proceedings of the 21st ACM 
international conference on Information and knowledge 
management, Maui, Hawaii, USA, 2012. 
[8] Y. Petinot, et al., "A hierarchical model of web summaries," 
presented at the Proceedings of the 49th Annual Meeting of the 
Association for Computational Linguistics: Human Language 
Technologies: short papers - Volume 2, Portland, Oregon, 2011. 
[9] C. Lu, et al., "The topic-perspective model for social tagging 
systems," presented at the Proceedings of the 16th ACM SIGKDD 
international conference on Knowledge discovery and data mining, 
Washington, DC, USA, 2010. 
[10] D. RDF. Available: http://rdf.dmoz.org/rdf/ 
[11] G. Tsoumakas, et al., "Mining Multi-label Data," in Data Mining 
and Knowledge Discovery Handbook, O. Maimon and L. Rokach, 
Eds., ed: Springer US, 2010, pp. 667-685. 
[12] D. Newman, et al., "Distributed Algorithms for Topic Models," J. 
Mach. Learn. Res., vol. 10, pp. 1801-1828, 2009. 
 
 
140

On Access Control Schemes for Hadoop Data Storage 
Chunming Rong 
Department of Science and Technology 
 University of Stavanger 
Stavanger, Norway 
chunming.rong@uis.no  
Zhou Quan 
School of Mathematics and 
Information Science 
Guangzhou University 
Guangzhou, China 
zhouqq@gzhu.edu.cn 
Antorweep Chakravorty 
Department of Science and Technology 
University of Stavanger 
Stavanger, Norway 
antorweep.chakravorty@uis.no 
Abstract—Hadoop is a distributed Big Data storage and 
processing framework hugely adopted in different sectors from 
online media, education, government and social media to 
handle the enormous growth of information in their respective 
domains. However, the core architecture of the solution is 
based on a trusted cluster. It lacks native methods for 
protecting sensitive data that cross over enterprises and are 
exposed or accessed illegally. In this paper, propose an access 
control scheme for data stored in Hadoop, based on concepts 
from BitTorrent and the secure sharing storage over the cloud. 
The proposed schemes can impose access control for data 
owners, and prevent unauthorized access and illegal 
authorization. 
Keywords- access control; Hadoop; authorization; BitTorrent; 
secure sharing storage. 
I. INTRODUCTION
 Research in newer ways of storing and processing data 
has led to the development of various No-SQL based 
solutions [1]. These newer technologies enable shift in 
paradigm from traditional relational solutions. Unlike 
traditional DBMS/RDBMS solutions, they offer capabilities 
of storing and processing enormous amounts of data. They 
enable aggregation of different data sources from structured, 
semi-structured and unstructured. With distributed & parallel 
architectures, they offer competencies of processing large 
historical datasets (often in TeraBytes), as well as real time 
continuously gather data. Among others, the Hadoop 
platform has emerged as a forerunner for big data processing 
and storage [1, 2]. Major organizations such as Facebook, 
Yahoo, IBM and Microsoft as well as other government and 
educational institutions have widely accepted it as key part 
of their data architecture. Amazon & Oracle wrapped 
Hadoop as one of their key businesses by elevating it to the 
“cloud” and offering “Platform as a Service” [3]. However, 
the core architecture of the Hadoop framework was 
conceived for a trusted cluster environment, with a small 
elite set of advanced programmers as its user base. Until 
recently, security aspects of the Hadoop framework 
remained widely unaddressed [4]. However with its rapid 
acceptance in different sectors and its use for handling 
critical data, the need for proper data protection and access 
mechanisms have become quite prevalent.  
In the paper, we propose two novel access control 
schemes for the Hadoop data storage, addressing 
unauthorized access and illegal authorization attacks on data 
stored in the different nodes throughout the Hadoop cluster. 
The first scheme is based on virtual piece concepts from 
BitTorrent [5]. The other is designed with the secure sharing 
storage over the cloud [6] methodology.  
The remainder of this paper is organized as follows: 
Section 2 provides a background on the Hadoop framework, 
its architecture, process flow and security risks. We also 
introduce concepts from the BitTorrent and secure sharing 
storage over the cloud. Next we introduce our Access
 Control Schemes for Hadoop data storage in Section 3. 
Section 4 concludes the paper with discussion of future work. 
II. BACKGROUND
 The following subsection sections, provides an 
overview of the Hadoop architecture, its process flows, 
associated security risks, overview of the BitTorrent 
protocol for peer-to-peer file sharing and the secure sharing 
storage over the cloud methodology. 
A.  Hadoop Architecture 
Hadoop is an open source framework for distributed 
storage and data-intensive processing, first developed by 
Yahoo. It offers a file system called Hadoop Distributed File 
System (HDFS) [7, 8] and computing paradigm MapReduce 
[9, 10]. HDFS is a distributed file system that splits and 
stores data on nodes throughout a cluster, with a number of 
replicas. It provides an extremely reliable, fault tolerant, 
consistent, efficient and cost-effective way to store large 
amounts data. The MapReduce paradigm provides a model 
for computation on huge amounts of data spread across a 
cluster. It consists of two key functions: Mapper and 
Reducer. The Mapper processes input data splits in parallel 
through different map tasks and sends sorted, shuffled 
outputs to the Reducers that in turn groups and processes 
them using reduce tasks for each group. 
The HDFS stores the data splits into multiple nodes 
called DataNodes. The namespace & meta-data of each file 
and its blocks are stored in a master node called NameNode. 
The MapReduce model enables distributed processing of 
data stored throughout the HDFS. A JobTracker node 
schedules & coordinates a number of map & reduce tasks on 
multiple data processing nodes called TaskTrackers. The 
2013 International Conference on Cloud Computing and Big Data
 978-1-4799-2829-3/13 $26.00 © 2013 IEEE
 DOI 10.1109/CLOUDCOM-ASIA.2013.82
 6441978-1-4799-2830-9/14 $31.00 © 2014 IEE
architecture and process flow is given in Figure 1. The 
TaskTrackers are always deployed in the DataNodes. 
Figure 1. Hadoop Architecture and Process Flow 
B. Process flows  
There are two types of process flow for accessing & 
processing data in the Hadoop framework: one being direct 
operations on the HDFS and other being processing data 
using the MapReduce paradigm. 
 HDFS Operations 
Process flows of HDFS include the read and write 
operations. HDFS read operations on a data file, from 
Hadoop clients need locations of its data blocks stored 
across different nodes. An RPC request to the NameNode 
returns the addresses (metadata) of DataNodes that has a 
block of the requested file. The returned addresses of the 
DataNodes are used directly by Hadoop clients to read the 
data blocks. Write operations from Hadoop clients, sends a 
request to the NameNode for writing a new file. The 
NameNode performs checks to ensure that the file does not 
already exist and that the client has the right permissions. It 
next makes a record of the new file and returns a list of 
suitable DataNodes to store the file blocks. The Hadoop 
client then, splits and writes the data blocks to the respective 
DataNodes. 
 MapReduce Jobs 
MapReduce jobs are scheduled and coordinated by the 
JobTracker. Hadoop clients submit jobs to the JobTracker. It 
in-turn consults the NameNode to learn the addresses of 
DataNodes that have the blocks of the input files for a job. 
The JobTracker initiates the TaskTrackers running near or 
on those DataNodes that contains the blocks of the input 
files. The TaskTrackers start a Map task and monitor its 
progress and report its status to the JobTracker. Each node 
stores the intermediate data on local disk after completion of 
a Map task. Simultaneously, the JobTracker also starts 
Reduce tasks on TaskTrackers. All intermediate data from 
Map tasks that have same output keys are sent to be 
respective Reduce tasks for final computation. Outputs are 
written back into the HDFS. 
C. Security Risks  
The Hadoop framework is designed for a trusted 
environment. Its operations are based on assumptions that 
clusters would consist of cooperating, trusted machines used 
by trusted users in a trusted environment. There aren’t any 
native Hadoop mechanisms to provide authentication, 
encryption, authorization and privacy [11, 13, 14]. This 
creates serious security vulnerabilities as malicious users can 
read, corrupt or even delete data by bypassing access 
control’s rules. All users have direct access to all data stored 
in DataNodes without any fine-grain authorized access 
control mechanisms. A job to the JobTracker could be 
executed without proper validation or authentication. 
Without encryption, sensitive data in the DataNodes may be 
leaked. Yahoo proposed addressing these concerns using 
Kerberos [12]. However, it lacks requirements of role-based 
access control, attribute-based access control and active 
directory [11, 13, 14] and is unable to provide a 
comprehensive solution. Bellow some of the key security 
risks and challenges are summarized. 
 Authentication 
A typical Hadoop cluster is based on a trusted 
environment. Such a setting does not require any 
authentication of users or services.  Hadoop integrated with 
Kerberos can provide some form of authentication 
mechanism. The NameNode and JobTracker can ensure that 
any HDFS or MapReduce requests are being executed with 
the required authentication. This configuration, realizes 
service-to-service and user-to-service authentications. 
However, Kerberos fails to control operations and 
permission of user submitting jobs. Further it does not 
provide any security while directly accessing the data blocks 
store in the DataNodes. 
 Authorization 
The core authorization mechanism in Hadoop is through 
its file-system. The HDFS enables authorization mechanisms 
based upon access control lists (ACL) similar to that used in 
POSIX file-systems. However, the authorization mechanisms 
in Hadoop are still in their infancy. Increase in the number of 
users and data files, makes it difficult to control and manage 
the access control lists and permissions [13, 14]. Hadoop 
clients may directly read or write a data block bypassing the 
NameNode as long as it can supply an appropriate block id 
[13, 14]. Moreover, the Hadoop clients may register 
themselves as another TaskTracker or DataNodes leading to 
data corruption, task interruptions and other malicious 
activities. 
 Confidentiality 
Hadoop natively doesn’t implement any cryptographic 
framework to support encryption & decryption of files in 
HDFS. This enables users with administrative rights read 
data that are stored in the DataNodes. Moreover, RPC 
communication protocols between the Hadoop services 
6452
transports data over an unprotected network vulnerable to 
leaks and eavesdropping. 
 Audit 
The audit mechanisms of Hadoop are the similar to that 
in Unix, where in it is difficult to keep track of audit events. 
Data owners do not have a mechanism to track access's 
histories to their data. 
D. BitTorrent 
BitTorrent is a popular peer-to-peer transport protocol 
commonly used for sharing large files [5]. BitTorrent splits 
the data files into virtual pieces that are shared between peers 
(nodes participating in sharing of a file). It maintains a 
torrent seed file with indexes and hashes for each virtual 
piece of a file. The torrent file also includes other meta-
 information such as addresses of the Trackers (servers that 
assists in the communication between peers), communication 
ports and block sizes. The tracker maintains and provides a 
list of peers involved in sharing a file. Peer acts as a client as 
well as a server. Once it receives a complete data block, it 
acts as a server for that block. Each peer discovers other 
peers via one or more trackers from the torrent file. When the 
multiple peers download the same file concurrently, they 
upload to each other making it possible to support very a 
large number of downloaders with only a modest increase in 
load. The advantage of the BitTorrent is fast distribution and 
transportation of large files. There have been multiple-use 
cases for sharing data using the BitTorrent protocol as 
surveyed in [15]. 
E. Secure Sharing Storage over the Cloud 
A secure sharing storage scheme based on elliptic curve 
cryptography over the cloud was proposed in [6]. The 
scheme can imperatively impose the access control policies 
of data owners, preventing the cloud storage providers from 
unauthorized access and making illegal authorization to 
access the data. The advantage of this scheme is that only the 
data owner has control of who has access to his data.
 In this scheme, assuming that Alice has the private key 
ka and the public key kaG, Bob has the private key kb and the 
public key kbG, and the Cloud Storage Provider has the 
private key kc and the public key kcG. Alice stores a 
message m encrypted as me in the cloud. The encrypted 
message me = m + r·kcG + t·G, where r and t are random 
numbers. G is a point that satisfies some conditions on the 
elliptic curve. In order for Bob to access the message, he 
sends a request to Alice with his public key kbG. Alice 
computes tcG = -rb·kbG - rc·kcG - r·kcG - t·G, where rc and 
rb are also some random numbers. Alice then sends (rc·G,
 tcG) to the cloud storage provider and rb·G to Bob. The 
cloud storage provider re-encrypts the data me via the 
formula mc = me + kc·rc·G + tcG. Finally, Bob receives mc
 from the cloud storage provider and decrypts mc with the 
formula mb = mc + kb·rb·G to gain the message m (mb = m).
 III. NOVEL ACCESS CONTROL SCHEMES
 In the section, we propose two novel access control 
schemes for Hadoop data storage. One is based upon 
concepts from BitTorrent protocol and data encryption. The 
other is derived using the secure sharing storage over the 
cloud methodology. 
A. Access Control Scheme with BitTorrent and encryption 
BitTorrent uses virtual pieces to distribute a data file. 
Users download data files with their torrent seeds. The 
notion of virtual pieces, allows creation of access tokens for 
the metadata information such as addresses of blocks stored 
on DataNodes. Only users with right access tokens can 
access block addresses, and the respective blocks stored in 
the DataNodes. This scheme includes four phases: creating 
and distributing access token, gain access token and access 
blocks. Figure 2 shows the process of creating and 
distributing the access token over a Web server.
 Figure 2. Creating and Distributing Access Token over a Web Server 
6463
 Creating and Distributing Access Token 
A Hadoop client that is owner of a data file can create 
and distribute the access token for the file. The steps 
involved are as follows. 
Step 1: Request the NameNode: When a Hadoop client 
uploads or accesses its data file in HDFS, it sends a request 
to the NameNode. 
Step 2: Return the metadata: After receiving a request, 
the NameNode returns the metadata of the requested data 
file. The metadata includes block ids and the address of 
DataNodes that store the blocks. The Hadoop client creates 
a record of metadata for each block. 
Step 3: Generate the metadata file: The Hadoop client 
generates a metadata file with the records of metadata 
returned by the NameNode. 
Step 4: Create the access token data: The Hadoop 
client encrypts each record of metadata. If a block is 
authorized to a user, this block’s record in the metadata file 
is encrypted with a shared key with the Hadoop client and 
the user. If a block is authorized to a group, this block’s the 
record is encrypted with a shared key with the Hadoop 
client and the group. 
Step 5: Create the access token file: The Hadoop client 
uses the access token data and other file information, such 
as file head, timestamp, size and so on, to create an access 
token file of the data file.
 Step 6: Create the torrent file: The Hadoop client uses 
a BitTorrent tool, to split the virtual pieces of the access 
token file, create indexes and hash for each virtual piece to 
generate a torrent file. 
Step 7: Distribute the access token file and torrent file: 
The Hadoop client uploads the torrent file to a Web server 
to distribute the access token seed. The access token file is 
also uploaded to the server of announce list in the .torrent 
file.
  Gain Access Token 
When a user wants to access the blocks of a data file, it 
downloads its respective torrent file from the Web server. It 
also uses a BitTorrent tool, to download the access token 
file. Then, it decrypts the access token data with its shared 
key to gain the authorized access token of blocks. Without 
the shared key, a user cannot decrypt the access token data 
and is not able to gain the authorized access token, thus 
unable to identify addresses of blocks in the DataNodes. 
 Access Blocks 
The authorized access token of blocks, allows access to 
the metadata of the blocks and their addresses on the 
DataNodes. 
B. Access Control Scheme with Secure Sharing Storage 
In the earlier scheme, a Hadoop client authorizes users 
access to the blocks stored in DataNodes via an access token 
and shared key. Although only users with a shared key may 
decrypt the access token data to gain the access token, data 
owners do not gain knowledge about who downloads the 
access token file and accesses the blocks. A secure sharing 
storage over the cloud addresses this concern. This scheme 
also includes four phases: creating and distributing access 
token, gain access token and access blocks. Figure 3 shows 
the steps of the creating and distributing access token over 
the cloud. 
Figure 3. Creating and Distributing Access Token over the Cloud 
 Creating and Distributing Access Token The steps 1 to 5 are same as discussed in “Access 
Control Scheme with BitTorrent and Encryption”. The 
additional steps involved are given bellow. 
6474
Step 6: Encrypt the access token file: The Hadoop 
client uses the secure sharing scheme over the cloud to 
encrypt the access token file. 
Step 7: Distribute the encrypted the access token file: 
The Hadoop client distributes the encrypted access token 
file to the cloud storage provider. 
 Gain Access Token 
A user needing to access to blocks of a data file, 
requests the Hadoop client about the owner of the data file. 
The Hadoop client sends a response to the user as well as 
the cloud storage provider according to concepts from the 
secure sharing storage scheme over the cloud. The user 
downloads the re-encrypted token file from the cloud 
storage provider and decrypts it. Finally, the user decrypts 
the access token data to gain the authorized access token. 
 Access Blocks 
When a user gains the authorized access token, they 
know the metadata of the blocks authorized by the data 
owner, including addresses of the blocks stored in the 
DataNodes. The user may access the data blocks stored in 
the DataNodes via their metadata information. 
IV. CONCLUSION
 In our work, we identified some of the security risks of 
Hadoop system and proposed two novel access control 
schemes for storing data. It aimed at allowing the data 
owners to control and audit access to their data. However, 
there could be a burden on data owners to update the access 
token and torrent files when the metadata of their file blocks 
changes. A flexible key sharing key management solution 
for the access control would be investigated in the future. 
We also aim to implement the complete solution, evaluate 
its performance and application for access control. 
ACKNOWLEDGMENT
 This work has been supported by the Province Natural 
Science Foundation through the Guangdong 
(S2012040007370 and S2012010009950), the Key National 
Science and Technology through the China 
(2011BAD21B01), the National Natural Science Foundation 
through the China (11271003), and Science and Technology 
Program through Guangdong (2011B020313023) projects.  
REFERENCES
 [1] Watson R T, Lind M, Haraldson S. The Emergence of Sustainability 
as the New Dominant Logic: Implications for Information 
Systems[C]//ICIS. 2012. 
[2] Wei W, Du J, Yu T, et al. Securemr: A service integrity assurance 
framework for mapreduce[C]//Computer Security Applications 
Conference, 2009. ACSAC'09. Annual. IEEE, 2009: 73-82. 
[3] Lal K, Mahanti N C. A Novel Data Mining Algorithm for Semantic 
Web Based Data Cloud[J]. International Journal of Computer Science 
and Security (IJCSS), 2010, 4(2): 160. 
[4] Faruk Berksöz, http://zh.scribd.com/doc/124221312/hadoop.
 [5] Mahesh Somani, BitTorrent for Package Distribution in the 
Enterprise, http://www.ebaytechblog.com/2012/01/31/bittorrent-for-
 package-distribution-in-the-enterprise/. 
[6] Zhao G, Rong C, Li J, et al. Trusted data sharing over untrusted cloud 
storage providers[C] Cloud Computing Technology and Science 
(CloudCom), 2010 IEEE Second International Conference on. IEEE, 
2010: 97-103. 
[7] Tom White, Hadoop: The Definitive Guide (second edition[M], 
O’Reilly Press, 2010:41-72. 
[8] Brad Helund, Understanding Hadoop Cluster and the network, 
http://bradhedlund.com/2011/09/10/understanding-hadoop-clusters-
 and-the-network/. 
[9] Tom White, Hadoop: The Definitive Guide (second edition[M], 
O’Reilly Press, 2010:15-38. 
[10] Jeffrey Dean and Sanjay Ghemawat, MapReduce: Simplified Data 
Processing on Large Clusters, http://labs.google.com/papers/ 
mapreduce.html. 
[11] Andrew Becherer, Hadoop Security Design Just Add Kerberos? 
Really? http://media.blackhat.com/bh-us-10/whitepapers/Becherer/ 
BlackHat-USA-2010-Becherer-Andrew-Hadoop-Security-wp.pdf 
[12] Owen O’Malley, Kan Zhang, Sanjay Radia  and el.at., Hadoop 
Security Design, https://issues.apache.org/jira/secure/attachment/ 
12428537/ security-design.pdf. 
[13] Hadoop Poses a Big Data Security Risk: 10 Reasons Why, 
http://www.eweek.com/security/slideshows/hadoop-poses-a-big-data-
 security-risk-10-reasons-why/. 
[14] Andrew Brus, The Odd Couple: Hadoop and Data 
Security, http://www.zdnet.com/the-odd-couple-hadoop-and-data-
 security-7000018468/.
 [15] Yu L, Susilo W, Safavi-Naini R. X2bt trusted reputation system: a 
robust mechanism for p2p networks[M]//Cryptology and Network 
Security. Springer Berlin Heidelberg, 2006: 364-380. 
[16] Securing Big Data: Security  Recommendations for Hadoop and 
NoSQL Environments, https://securosis.com/assets/library/reports/ 
SecuringBigData_FINAL.pdf. 
[17] Zhou Quan, Deqin Xiao, Daixian et al. TSHC: Trusted Scheme for 
Hadoop Cluster[C], The 4-th International Conference on  Emerging 
Intelligent Data and Web Technologies, EIDWT-2013:344-349.
 6485

Top-K aggregation over a Large Graph Using
 Shared-Nothing Systems
 Abhirup Chakraborty?
 School of Informatics and Computer Science
 Indiana University, Bloomington, Indiana 47408
 Email: achakrab@indiana.edu
 Abstract—Analyzing large graphs is crucial to a variety of
 application domains, like personalized recommendations in social
 networks, search engines, communication networks, computa-
 tional biology, etc. In these domains, there is a need to process
 aggregation queries over large graphs. Existing approaches for
 aggregation are not suitable for large graphs, as they involve
 multi-way relational joins over gigantic tables or repeated mul-
 tiplications of large matrices. In this paper, we consider top-K
 aggregation queries that involve identifying top-K nodes with
 highest aggregate values over their h-hop neighbors. We propose
 algorithms for processing such queries over large graphs in a
 shared nothing environment. Using the notion of graph parti-
 tioning, we propose an update-based algorithm that minimizes
 network overhead by propagating updates in the neighborhood
 information.The algorithm partitions a graph across a number of
 processing nodes, and uses an iterative join algorithm within each
 node. We present a hybrid scheme to further reduce the network
 overhead during a few initial iterations. We develop a baseline
 algorithm based on distributed joins. Our experimental results
 validate the effectiveness of the proposed algorithms in reducing
 the aggregation time and in scaling the aggregation computation
 over a number of distributed hosts.
 I. INTRODUCTION
 Analyzing large graph data has seen renewed importance
 due to increased interests in a number of applications such as
 chemical data, biological data, XML data, social network data,
 communication networks, etc. In each of these applications,
 the underlying graphs are very big in size. There is current
 trends in advanced analysis of social network graphs aiming
 at evaluating the network values of customers [5], link predic-
 tion [15], etc. In large graph analysis tasks all the vertices
 and edges of the entire graph are accessed multiple times
 in a random fashion. Examples include Page-rank [2], social
 network influence analysis [13], recommender systems [1], etc.
 In social and biological network graphs, each node is often
 associated with an attribute set. The value of each attribute
 for a node represents some features of the entities represented
 by the node. For example, a node representing a Facebook
 user may have an attribute indicating his/her interest in a
 particular online game. Within the applications domains, there
 is a growing need to process standard queries efficiently over
 large graphs. For example, for each node find the aggregate
 value of an attribute for all its neighbors lying within h-hops
 in the graph. Such a query could identify the popularity of a
 game within one’s social circle. Also, such queries could help
 ?The research work was partly done while the author was working as a
 post-doctoral researcher at IBM T.J. Watson Research Center, NY, USA.
 finding the influential nodes (based on some attributes) in the
 social network and help in placing online advertisements.
 The issue of processing aggregate queries over graphs
 has been addressed in [24]. However, this approach assumes
 a pre-computed data structure storing the h-hop neighbors
 of each graph. The algorithms only save computations in
 calculating the aggregate score of each node by pruning nodes
 selectively. We observe that computing h-hop neighbors is a
 crucial operation in the graph aggregation, and the aggregate
 scores can be computed or materialized during neighborhood
 discovery; therefore, a separate phase for computing aggregate
 scores from the h-hop neighborhood information for each node
 is redundant. Moreover, the proposed algorithms work only in
 a single node, and does not consider the issue of distributing
 aggregate processing loads over a shared nothing system.
 Parallelizing or distributing processing loads of the transitive
 closure algorithm (i.e., Floyd-Warshall Algorithm [6]) over
 a shared nothing system is infeasible due to a large num-
 ber of synchronization barriers. Reference [22] improves the
 Floyd-Warshall algorithm by proposing a blocking algorithm
 to reduce random memory accesses and to increase cache
 utilization. Reference [12] extends the Floyd-Warshall algo-
 rithm in a GPU-based system using a block-based algorithm;
 the algorithm attempts to increase the instruction execution
 throughput using the GPU. However, the approaches do not
 show significant performance gains even in the multi-threaded,
 shared-memory system.
 In this paper, we consider the issue of parallelizing the ag-
 gregation queries over a shared-nothing system. Neighborhood
 aggregation queries are computed in three phases (section II):
 h-hop neighbor computation, aggregate score evaluation and
 top-k collection. Computing h-hop neighbors for all nodes
 in the graph is a computationally expensive operation. If the
 average degree of each node in the graph is m, the total number
 of edges to be accessed while computing h-hop neighborhood
 for a node would be around mh|V |. In applications with a
 large graph (with millions of nodes) such a computational
 cost is prohibitive. Though h-hop neighborhood within a graph
 can be found using Floyd-Warshall algorithm [6], such an
 algorithm is difficult to parallelize even within a single node
 due to a large number of synchronization barriers. Crecelius
 and Schenkel [4] propose algorithms to incrementally maintain
 an index structure—that stores the nearest neighbors for all
 nodes, i.e., all-pairs shortest path—with changes in the graph.
 However, the paper does not present efficient, parallel tech-
 niques for the initial computation of the index structure.
 In this paper, we propose distributed algorithms to process
 2013 IEEE International Conference on Big Data
 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 448
h-hop neighborhood information of a graph within a shared-
 nothing system. We divide the graph into a number of parti-
 tions and assign a partition to each of the host machines. We
 propose a join-based algorithm that computes h-hop neighbor-
 hood by applying distributed joins in an iterative fashion. Such
 an algorithm needs to shuffle a large volume of data across the
 host machines and also requires a large number iterations for
 a large h-value. Considering the limitations, we propose an
 update-based algorithm, that allows each host machine com-
 pute h-hop neighborhood locally. The algorithm propagates
 updates across the machines in an iterative fashion until the
 system converges (i.e., no host has new updates to propagate
 to other hosts). At an inter-partition level, the algorithm uses
 a technique similar to that with route discovery in a large
 network. The algorithm proceeds in phases, and at the end
 of a phase, each host propagates updates (observed within its
 partition) to the relevant hosts. The update-based algorithm
 incurs high communication volume during a few initial phases.
 Considering the limitation, we propose a hybrid algorithm that,
 instead of sending h-hop neighborhood information of each
 node across the hosts, transfers the graph partitions during
 initial/start-up phases; the receiving hosts regenerates the h-
 hop neighbors for each graph node from the received partition.
 II. PRELIMINARIES
 In this section, we present a formal definition of aggre-
 gation queries over graphs, and discuss graph partitioning
 concepts and various data structures used in the paper.
 A. Aggregation over Graphs
 The aggregation queries over graphs are relevant to a
 number of emerging applications in online social communities
 such as book recommendation on Amazon, target marketing
 on Facebook, etc. A top-k aggregation query needs to solve
 three issues [24]:
 1) Compute the h-hop neighbors of the all the nodes in
 the graph.
 2) Evaluate the individual relevance function f(u) and
 the aggregate score or collective strength of a node
 F (u). f(u) could be binary value 0/1 (e.g., if a user
 plays a game or not). F (u) is an aggregate function
 (e.g., SUM, AVG, etc.) over the h-hop neighbors of
 a node u.
 3) Find and collect the top-k nodes with the highest
 scores.
 In a graph G(V, E), SUM aggregation of h-hop neighbors of
 a node v (i.e., aggregate score) can be given as,
 F (v) = ∑
 u?Sh(v)
 f(u)
 Here, Sh(v) is the h-hop neighbors of node v.
 B. Partitioning and Data Structures
 Given a graph G = (V, E), a graph partitioning algo-
 rithm divides the node-set V into disjoint subsets of nodes
 {V1, V2, . . . , Vn} such that ?iVi = V , and Vi ? Vj = ? for
 i = j. A partition Pi of the graph denotes the subgraph
 J
 P
 P
 P3
 2
 1
 A
 B
 EG
 F
 C
 D
 L
 N
 O
 M
 H
 I
 K
 Fig. 1. Entry nodes in a partition are marked with green color
 Gi(Vi, Ei) induced by nodes in Vi; Ei is the set of intra-
 partition edges (u, v) ? E, such that u ? Vi and v ? Vi.
 The (edge) cut-set Ec is the set of edges whose vertices
 lie in different partitions. Inter-partition cut-set Eci,j is the
 set of cut edges incident from partition Pi to Pj . Eci is
 the set of all cut edges emerging from partition Pi, i.e.,
 Eci = ?(1≤j≤n)?(j =i)Eci,j . We use the term out-table to denote
 the inter-partition cut-set Eci , and refer to Eci,j as a fragment
 of the out-table.
 An adjacency representation, AL, of a graph consists of a
 number of lists, one for each of the nodes in the graph. We
 refer to such nodes of the lists as source nodes. We denote the
 set of source nodes in an Adjacency structure AL as V (AL).
 An adjacency list for a source node u, denoted as AL(u) is the
 set of nodes that are descendants of node u is the graph. For
 a partition or graph Gi(Vi, Ei) with Ni nodes, the adjacency
 structure contains Ni lists. For each descendant nodes v, the
 list keeps the distance (hop-count) to the node v from the
 source node u. So, each entry in AL(u) is a pair (v, d), that
 represents a shortest path from node u to node v with a distance
 d. The adjacency list provides a few functions. It provides a
 method DISTANCE(v) to denote the distance of the node v
 from u as stored in AL(u). We assume that the operation
 gives ∞ when v is not reachable from u. Also, the adjacency
 list AL(u) provides an operation INSERT(v, d?) that replaces
 the entry (v, d) in AL(u) with (v, d?) if d? is less than d;
 the INSERT operation adds the entry (v, d?) in the list AL(u)
 when there is no such entry in the list. The list AL(u) provides
 another operation EXIST(v) that checks for the membership of
 v within the list, and returns TRUE if node v exists in the list
 AL(u), and FALSE otherwise.
 For each partition Pi of the graph, we identify the Entry
 nodes in a partition Pj(j = i)—denoted as ENi,j—which
 are the end vertices of the inter-partition cut-set (edges in
 a segment of an out-table) Eci,j incident to the partition Pj .
 Figure 1 shows a graph with three partitions; nodes marked
 with green color are the entry nodes within a partition. For
 the partition P1, the edges marked as blue are the (cut-)edges
 in the out-table Ec1; the two segments (of the out-table E1)
 Ec1,2 and Ec1,3 have two edges and one edge, respectively.
 449
3
 P3
 P0
 P1
 P2
 H2H0
 H1
 H
 Fig. 2. Distributed join processing among partitioned adjacency lists
 III. JOIN-BASED ALGORITHM
 The top-k aggregation query we consider in this paper is
 processed in two phases: neighborhood discovery and top-k
 computation. In the first phase, every node u in the graph
 discovers its h-hop neighbors and computes the relevance
 function F (u) of the node. Discovering the h-hop neighbor-
 hood for all nodes in the graph is a (h ? 1)-way self-join
 operation. We present an algorithm based on iterative joins and
 use it as a baseline algorithm. The join operations should be
 carried out over the partitioned graph in a distributed fashion.
 The relevance function of the nodes can be computed either
 incrementally during the neighbor discovery process or off-line
 after discovering all the h-hop neighbors of every nodes in
 the graph. In the second phase, the top-k computation process
 identifies the top-k nodes among all the nodes distributed
 within the networked host machines. In the subsequent part
 of this section, we describe the two phases of the baseline
 algorithm in details. As the relevance function, we consider
 only a count operation; however, supporting other relevance
 functions, like sum, average, etc., is straightforward.
 We present the baseline algorithm to process top-k aggre-
 gation query within a distributed system with multiple host
 machines connected through a network. Such a distributed join
 framework is similar to cyclo-join [7]. In the subsequent part of
 this section, we discuss various components of the algorithm to
 process top-k aggregation based on multi-way joins. We start
 with a simple horizontal graph decomposition scheme, and
 then present the distributed join processing technique that is the
 core operation of the top-k aggregation. The top-k collection
 scheme (i.e., the second phase of the aggregation operation) is
 similar to the one presented in section VI.
 A. Horizontal Partitioning
 We distribute the graph among the set of host machines
 in the system using a simple decomposition of the graph that
 partitions the vertices among the host machines such that each
 host owns all the outgoing edges of a node assigned to it. Such
 a decomposition can be regarded as a row-wise (horizontal)
 decomposition of the adjacency matrix representing the graph,
 and we refer to such a decomposition as a horizontal decom-
 position of the graph. While distributing the nodes across the
 Algorithm 1 DISTRIBUTEDJOIN(ALi, ALRi , h)
 Require: An adjacency structure ALi in host Hi, a parameter
 h specifying the depth/levels of joins
 Ensure: ALRi contains the result of h-way joins among ALi
 and all n partition
 1. ALRi ? ALi
 2. for k ? 1 to h? 1 do
 3. Bsnd ? ALi
 4. for iter ? 1 to n? 1 do
 5. NOBLOCKSEND((i + 1)%n, Bsnd)
 6. ALRi ? ADJJOIN(ALRi , Bsnd)
 7. Brcv ? BLOCKRECEIVE((i ? 1 + n)%n)
 8. Bsnd ? Brcv
 9. if iter = n? 1 then
 10. ALRi ? ADJJOIN(ALRi , Brcv)
 11. Barrier()
 hosts, we balance the number of edges assigned to the hosts
 by considering the out-degree of the nodes. Such a distribution
 eliminates skews in workloads among the hosts, as the loads
 of a join operation depends on the size of the table (i.e., total
 edges). Uniformly distributing the nodes results in a skew in
 total edges within the hosts, as nodes with higher out-degrees
 might be assigned to a host.
 Algorithm 2 ADJJOIN(ALa, ALb, h)
 Ensure: An adjacency structure ALR with the results of the
 join between ALa and ALb
 Definitions:
 V (ALi): set of source nodes for adjacency structure ALi
 1. ALR ? ALa
 2. for ur ? V (ALR) do
 3. for ub ? V (ALb) do
 4. if ALR(ur).EXIST(ub) then
 5. for (v, db) ? ALb(ub) do
 6. dr ? ALR(ur).DISTANCE(ub)
 7. if dr + db ≤ h and v = ur then
 8. ALR(ub).INSERT(v, dr + db)
 9. return ALR
 B. Distributed Joins
 Using the horizontal partitioning scheme, we distribute the
 nodes of the graph into multiple partitions, assign a partition
 to each of the hosts, and maintain an adjacency representation
 ALi for the partition at a host Hi. To process joins over the
 distributed adjacency lists, we arrange the hosts in a circular
 ring as shown in Figure 2. Each of the hosts communicates
 only with two neighboring hosts, and data is sent over the links
 only in one direction. A host Hi receives partition data from
 its left neighbor (i? 1+ n)%n and sends the data to its right
 neighbor (i+1)%n, where n is the total hosts (or graph parti-
 tions) in the system. Each host Hi maintains a result adjacency
 structure ALRi , initialized to ALi, to store the join results.
 After receiving a partition from its left neighbor, the host
 machine joins the received adjacency structure with its local
 result ALRi using an adjacency-join algorithm (Algorithm 2)
 450
3
 2H1
 1 2 3
 pmap
 3
 :
 1 2 3
 32 1
 1 2 3
 32 1
 1 2 3
 32 1
 H1
 H2 P3 P1H3
 H2 H3
 pmap
 2:
 pmap
 2: pmap3:
 pmap
 1
 pmap
 1:
 2: 3 1
 1 2 3 1 2
 P
 (a)
 B
 1 H2 H3
 N
 C F
 1,2
 A 3,1
 3,2 N
 JH
 JH
 EN
 EN EN
 1
 2 3
 2,3
 EN
 EN
 3,2
 3,1 F
 BEN
 EN2,1
 2,3 N
 N
 EN
 EN1,3EN
 1,3EN
 EN EN
 A
 C
 H
 (b)
 Fig. 3. pmap and EN lists before and after all-to-all communication
 described later in this subsection. After the join operation, the
 host forwards the received partition to its right neighbor. Once
 a partition in Hi is propagated all way through the ring to
 its left neighbor host H(i?1+n)%n, the respective partition get
 joined with all the n partitions stored in n hosts, and this
 completes a cycle of the propagation process. After such a
 cycle, each host stores the output data ALRi , which is the result
 of joining its local partition with all the partitions of the graph.
 To process h-way joins, we need to carry out the h?1 cycles of
 the join operations. Algorithm 1 shows the process in details.
 The algorithm uses two buffers Bsnd and Brcv to pipeline
 the join processing with data communication by using non-
 blocking send and receive primitives. The algorithm has h? 1
 cycles (loop in Line 2), and in each cycle, a host Hi receives
 n ? 1 partitions from its left neighbor and forwards n ? 1
 partitions to its right neighbor. Line 8 copies the received data
 to Bsnd to overlap data transfer (receive) with computation in
 the subsequent iteration. Line 10 joins the received data in the
 last iteration to complete the join cycle.
 Algorithm 2 presents the details of the ADJJOIN method.
 Line 4 checks if a node ub is reachable from ur in the
 adjacency structure ALR. If reachable, the method updates the
 list ALR(ur) by inserting into it all the nodes in list ALb(ub)
 that are within a distance h from the node ur(line 5–8). Line 7
 checks the distance limit (first part in the expression) and
 ensures that addition of the node ub does not form any loop
 in ALR (the second part in the expression).
 IV. UPDATE-BASED ALGORITHM
 The iterative, join-based approach to process h-hop ag-
 gregation over a distributed system requires (h ? 1) cycles;
 in each of the cycles, the algorithm computes joins over the
 whole graph. Such an approach is costly both in terms of
 computation and communication. To efficiently process h-hop
 neighborhood, we develop an incremental approach based on
 update propagation, that converges to equilibrium after only
 Algorithm 3 INCRDISCOVERY(ALRi , ENi, h)
 Require: An adjacency structure ALRi storing the h-hop
 neighborhood information in host Hi, entry-node infor-
 mation ENi for the partition in the host Hi, a parameter
 h specifying the depth/levels of joins
 Ensure: ALRi contains, for all the local nodes within the
 partition in the host Hi, the global h-hop neighbors across
 all the partitions.
 Definitions:
 Eci,j : out-tables listing the intra-partition cut-edges from
 host Hi to host Hj
 ULi, ULnewi , ULsnd, ULrcv: Adjacency structure for
 updates
 rcvH, sndH : host number to receive updates from and
 send updates to, resp.
 1. cycle ? 0
 2. while flag do
 3. for u ? ?j =iENi,j do
 4. ULi(u)? ALRi (u)
 5. rcvH ? (i? 1 + n)%n
 6. sndH ? (i + 1)%n
 7. while sndH = i do
 8. for u ? ENi,sndH do
 9. ULsnd ? ULi(u)
 10. EXCHANGEUPDATES(sndH, rcvH, ULsnd, ULrcv)
 11. PROCESSUPDATES(cycle, rcvH , ULrcv, h, ALRi ,
 ULnew)
 12. rcvH ? (rcvH ? 1 + n)%n
 13. sndH ? (sndH + 1)%n
 14. CLEANUP(ULi)
 15. ULi ? ULnew
 16. CLEANUP(ULnew)
 17. cycle ? cycle + 1
 18. flag ? CHECKCONVERGENCE()
 a few cycles. This approach partitions the graph across the
 processing nodes. The algorithm proceeds in two phases: local-
 compute phase and incremental discovery phase. During the
 local-compute phase, each node applies an algorithm to com-
 pute h-hop neighbors locally over its own partition. During an
 incremental discover phase, each partition (or node), sends the
 updates to relevant partitions. A separate preprocessing phase
 partitions the graph and stores the intra-partition out-tables. We
 describe the preprocessing and neighborhood processing phase
 (local-compute and incremental phase) in the subsequent part
 of this section.
 A. Graph Preprocessing
 We partition the graph into n partitions using a graph parti-
 tioning algorithm that minimizes the number of cut-edges. Our
 proposed algorithms are oblivious to graph partitioning tech-
 niques, and hence can use any graph partitioning algorithms.
 (However, as we illustrate later in the paper, the performance
 of our algorithms depends on the graph partitioner.) We store
 two files for each partition: an intra-partition file and an inter-
 partition file. An intra-partition file stores the edges within
 a partition; an inter-partition file for partition Pi maintains
 451
all cut-edges emanating from partition Pi, grouping the cut-
 edges based on their destination partitions. Each host stores
 the mapping from host ID to partition number. During the
 initialization phase, each host performs an all-to-all broadcast
 to distribute its local partition number to all other nodes. After
 the all-to-all shuffling, each host gets the same global mapping
 table pmap, where pmap[j] gives the host ID that stores the
 partition Pj .
 A host Hi reads the intra-partition file for the partition
 assigned to it and creates an adjacency structure ALi. The host
 then reads the inter-partition file, and using the mapping array
 pmap, groups the cut edges in the out-table Eci according to
 host ID and maintains one fragment (in the out-table) for each
 of the hosts. An out-table fragment Eci,j contains the set of
 inter-partition cut-edges emanating from the partition in host
 Hi and incident to the partition in Host Hj .
 Now, we need to determine the entry nodes for each
 partition in a host Hj . From Figure 1, we observe that the
 entry nodes for the partition in host Hj can be obtained from
 out-table Eci,j , which is located in the host Hi. So, each host
 Hi should compute the list of entry nodes ENi,j for all other
 hosts Hj (j = i). Now, nodes in the list ENi,j are the entry-
 nodes within host Hj for the cut-edges incident from partition
 in host Hi. Hence, host Hi should send the information to
 host Hj . Therefore, after computing the entry-nodes, each host
 participates in an all-to-all communication phase to shuffle
 the entry-node information across the hosts. Considering the
 graph and its partitions shown in Figure 1, Figure 3(a) and
 Figure 3(b) show, respectively, the lists pmapi and ENi
 within each host before and after the all-to-all shuffling of the
 information about partition mappings and entry nodes within
 each host.
 B. Local-Compute Phase
 In a local-compute phase, each host computes the h-hop
 neighbors for the nodes within the partition assigned to the
 host. We use multi-way joins to compute the h-hop neighbors.
 At a first glance, the Floyd-Warshall Algorithm to calculate
 transitive closures appears to be the better candidate from the
 perspective of computational complexity. However, we find
 that given a particular graph and h-values within a limit ( e.g.,
 h ≤ 35), an iterative multi-way join performs better that the
 Floyd-Warshall algorithm while computing h-hop neighbors.
 So, each host applies the ADJJOIN algorithm iteratively to
 compute (h ? 1)-way self-joins over the local adjacency list
 ALi, and stores the results in an adjacency structure ALRi .
 C. Incremental Discovery Phase
 The Incremental discovery phase consists of multiple
 cycles. In each cycle, the hosts participate in an all-to-all
 propagation operation to shuffle, among the hosts, the local
 updates in the adjacency lists. Updates within each host Hi
 are stored in an adjacency structure called Update List ULi,
 and this ULi stores the updates in neighborhood information:
 before the onset of the incremental discovery phase, each host
 Hi initializes ULi with with adjacency lists in ALRi , and each
 host creates a new update list ULnewi during each cycle. The
 new update list ULnewi records all the updates (v?, d?), incurred
 within a cycle, in the adjacency list ALRi (u) for each node
 Algorithm 4 PROCESSUPDATES(cycle, rcvH , ULrcv, h,
 ALRi , ALinvi , ULnew)
 Require: An adjacency structure ALRi storing the h-hop
 neighborhood information in host Hi, ALinvi storing the
 inverted adjacency lists, ULrcv containing the updates
 from host with ID rcvH , a parameter h specifying hop-
 count for aggregation
 Ensure: Append to ULnew any changes in h-hop neighbors
 within ALRi
  if it is the first cycle
 1. if cycle = 0 then
 2. for ?uo, vo, do? ? Eci,rcvH do
 3. ALstage(uo).INSERT(vo, do)
  Stage the updates from the remote host
 4. for ur ? V (ULrcv) do
 5. for ?uo, vo, do? ? Eci,rcvH do
 6. if ur = vo then
 7. for ?vr, dr? ? ULrcv(ur) do
 8. if do + dr ≤ h and uo = vr then
 9. ALstage(uo).INSERT(vr, do + dr)
  Propagate the updates
 10. for us ? V (ALstage) do
 11. for ?vs, ds? ? ALstage(us) do
 12. for ?v?, d?? ? ALinvi (us) do
 13. if d? + ds ≤ h and v? = vs then
 14. ALstage(v?).INSERT(vs, d? + ds)
  Incorporate updates within result structures
 15. for us ? V (ALstage) do
 16. for ?vs, ds? ? ALstage(us) do
 17. ALRi (us).INSERT(vs, ds)
 18. if ds < h and us ? ?jENi,j then
 19. ULnew(us).INSERT(vs, ds)
 u ? V (ULi); (v?, d?) is regarded as an update to the adjacency
 list ALRi (u) if (a) there is no entry in the list for node v?, or(b) (v?, d?) replaces an old entry (v?, d) and d? < d. Note that
 V (ULi) is the set of all entry nodes in ENi (i.e., ?j =iENi,j),
 and is also the set of source nodes of the adjacency structure
 ULi.
 Algorithm 3 shows the pseudo-code for incremental dis-
 covery operations. Each iteration of the outer loop (line 2–
 18) represents a cycle. Within a cycle, each host propagates
 updates in its adjacency lists to all other hosts through an all-
 to-all propagate phase (line 7–13). For the all-to-all propagate
 phase, we use a ring-based algorithm as proposed in [3].
 We arrange the hosts in a ring; in each iteration within a
 cycle (line 7–13), each host receives updates from a host
 HrcvH and sends its updates to host HsndH . Each host Hi
 collects, from ULi, the updates for the entry nodes ENi,sndH
 and creates the adjacency structure ULsnd (line 8–9). The
 Procedure EXCHANGEUPDATES sends the updates in ULsnd
 to the host HsndH , and stores in ULrcv updates received
 from the host HrcvH (line 10). The procedure marshals the
 adjacency structures over the network link at the sending
 end, and reconstructs the structure at the receiving end. To
 facilitate the overlapping of computation with communication,
 452
the procedure employs a non-blocking send and a blocking
 receive. Also, while transferring the updates across the hosts,
 the procedure uses a bounded buffer to prohibit buffer overflow
 in case the aggregate updates across all the partitions become
 very large. Algorithm 5 shows the details of the procedure.
 Algorithm 5 EXCHANGEUPDATES(sndH , rcvH , ULsnd,
 ULrcv)
 Require: ULrcv containing the updates to be sent to sndH ,
 rcvH to receive updates from
 Ensure: ULrcv contains the update entries/tuples received
 from host with ID rcvH
 Definitions:
 Sbuf : (send or receive) buffer size
 Bs, Br: send and receive buffer, resp.
 1. scount ? 0
 2. for us ? V (ULsnd) do
 3. Bs[count]? ?us, NAD?
 4. for ?vs, ds? ? ULsnd(us) do
 5. Bs ? ?vs, ds?
 6. scount ? count + 1
 7. if scount = Sbuf ? 2 then
 8. send scount to host HsndH
 9. send (non-blocking) Bs to host HsndH
 10. if rF lag = True then
 11. receive rcount from host HrcvH
 12. if rcount > 0 then
 13. receive update tuples from HsndH and store
 in Br
 14. INSERT(ULrcv, Br, rcount)
 15. else
 16. rF lag ? False
 17. wait for non-blocking send
 18. scount ? 0
 19. send scount to host HsndH
 20. send (non-blocking) Bs to host HsndH
 21. if scount > 0 then
 22. send (non-blocking) 0 to host HsndH
 23. if rF lag = True then
 24. repeat
 25. receive rcount from host HrcvH
 26. if rcount > 0 then
 27. receive contents from HsndH and store in Br
 28. INSERT(ULrcv, Br, rcount)
 29. until rcount = 0
 30. wait for non-blocking send
 After receiving updates from a remote host, the receiving
 host Hi invokes a procedure PROCESSUPDATES (described
 later in this subsection) that incorporates within its adjacency
 structure ALRi the newly published neighbor information
 ULrcv from the remote host HrcvH . The procedure also
 appends to ULnew any updates to the adjacency lists of entry
 nodes within the partition. After the all-to-all shuffle, each
 host Hi cleans up the current update list Ui and initializes
 it with the recent update list ULnew (line 15), and use the
 ULi to propagate the updates in the subsequent cycle. Line 16
 garbage collects the list ULnew destroying all the updates in
 the adjacency structure. Line 18 checks for the convergence or
 termination of the incremental phase. The incremental phase
 converges when no host in the system has new updates to send.
 During the incremental phase, once the system converges, all
 the nodes of the graph have discovered all of its neighbors
 lying within the proper distance (h). Is should be noted that
 the maximum number of cycles in the incremental discovery
 phase is bounded by a parameter pmax. Suppose p indicates
 the number of partitions spanned by a shortest path (with the
 distance ≤ h) between two nodes in the graph. The parameter
 pmax is the maximum p over all pairs of nodes within the
 graph. Although there are pmax total iterations before the
 convergence, total volume of updates shuffled across the nodes
 will decrease drastically after the first few iterations. Devising
 a graph partitioner to minimize the the total iterations in the
 aggregation algorithms outside the scope of the paper, and is
 a topic of future research.
 Algorithm 4 presents the details of the procedure PROCES-
 SUPDATES used in the incremental discovery phase. Instead
 of directly updating the result structure ALRi , the procedure
 first store and manipulate the tentative updates within an
 intermediate storage ALstage, and then propagates the updates
 to the adjacent nodes (within the partition) in a separate phase;
 the procedure then incorporates the finalized updates within the
 result structure ALRi . During the fist cycle of the incremental
 discovery phase, the procedure scans the edges of the out-
 table, and adds the destination nodes, with the proper distance
 do(= 1), as the updates to the source nodes (Lines 1–3).
 Lines 4–9 join the adjacency structure ULrcv, that contains
 updates received from the remote host, with the respective
 segment Ei,rcvH in the out-table Ei, and stores the result in an
 staged adjacency list ALstage. This join operation propagates
 the updates published by the entry nodes (of the remote host)
 to the source nodes of the out-table segment. The loop in line 7
 scans the updates published by an entry node ur. Line 8 checks
 whether the destination node of an update lies within a distance
 h from the source node uo in the out-table segment; the second
 part of the expression eliminates loop in the adjacency list of
 the source node uo. Line 9 inserts the update tuple into the
 adjacency list of the source node uo. Note that, the update
 entry ?vr, do + dr? is added to the adjacency list of uo if and
 only if there is no entry with the destination node vr or the
 new distance metric in the update entry is smaller than the
 distance metric of an existing entry for the destination node
 vr.
 After the join operation (in Lines 4–9), the intermediate
 (staged) adjacency list ALstage contains a list of update
 entries for the respective source nodes. Lines 10–14 propagate
 the updates to the other adjacent nodes within the partition.
 The propagation process uses an inverted adjacency structure
 ALinvi , where each list ALinvi (u) for a source node u stores
 every node within the partition that reaches the node u through
 a shortest path of distance less than h. Once the updates are
 propagated to the nodes within the partition, the new updates
 for each node in the partition are added to the result adjacency
 structure ALRi (Line 15–19).
 453
Algorithm 6 INNETWORKTOPK(F Listi, k)
 Require: An list F Listi storing the ?node,F-value? pairs for
 all the nodes within the partition in Host Hi
 Ensure: compute local top-K and propagate in the network;
 the host H0 stores the global top-k
 Definitions:
 Loc: local top-k ?node, F ? value? pairs
 Left: top-k pairs received from left sub-tree
 Right: top-k pairs received from right sub-tree
 i: host ID
 n: total hosts in the network
 1. Loctopk ? COMPUTETOPK(F Listi, k)
 2. if (2 ? i + 1 < n) then
 3. receive topk pairs from H2?i+1 and store in Left
 4. Loc ? COMPUTETOPK(Loc ? Left, k)
 5. if (2 ? i + 2 < n) then
 6. receive topk pairs from H2?i+2 and store in Right
 7. Loc ? COMPUTETOPK(Loc ?Right, k)
 8. if i = 0 then
 9. send Loc to host Hi/2
 V. A HYBRID APPROACH
 In the update-based algorithm, during a few initial cycles,
 the total volume of updates generated by each host could
 be very large. Shuffling the raw updates across the hosts
 increase the communication loads. During the first few cycles,
 the total update volume within each host might be a few
 order of magnitude larger than its partition size. In such a
 situation, sending the graph partition instead of the update
 lists might reduce the communication loads on the network.
 With such a partition shipment, the receiving host iteratively
 applies, as in the local compute phase (of the update-based
 algorithm) in Subsection IV-B, the ADJJOIN algorithm to
 compute the (h ? 1)-hop neighbors of each node within the
 partition received from the host HrcvH . The receiving host Hi
 uses the entry-node list of the host HrcvH to regenerate the
 update list ULrcv, and processes the incoming updates using
 the PROCESSUPDATES algorithm as described in Algorithm 4.
 The hybrid algorithm works in two modes: update-
 shipmentmode and partition-shipmentmode. Initially the algo-
 rithm is set in partition-shipment mode, and the INCRDISCOV-
 ERY algorithm (Algorithm 3) shuffles partitions (in line 10),
 instead of update logs, across the hosts. The receiving host
 regenerates the (h?1)-hop neighbors of each node and creates
 the update lists as described above. At the end of each cycle,
 the hosts participate in all-to-all communication to exchange
 the size of the update entries ULnew generated within the
 hosts. The hosts switch to update-shipment mode if the size
 of the generated updates within each of the hosts falls below
 a predefined threshold. Once in update-shipment mode, the
 incremental discovery phase proceeds on the way as given in
 the update-based algorithm (Algorithm 3).
 VI. TOP-K COLLECTION
 Once the neighborhood discovery phase is finished, each
 node computes top-k results locally. To compute the global
 top-K over all the nodes, we use a tree-based collection
 TABLE I. SYNTHETIC GRAPHS
 Name |V | |E| Cut Description
 A 22.5K 44.7K 972 2D Grid (n=200,m=200)
 B 10K 19.8K 620 2D Grid (n=100,m=100)
 C 20K 160K 38114 Small World Model(n=20K,k=8,p=0.2)
 D 20K 80K 44000 Erdos Renyi Model(n=20K,m=80K)
 E 20K 160K 6295 Small World Model(n=20K,k=8,p=0.01)
 F 40K 40.3K 4231 Power-Law graph(n=40K,p=3)
 TABLE II. REAL-WORLD GRAPHS
 Name |V | |E| Cut Description
 S 18772 198050 61248 Collaboration Graph
 T 23133 93499 20554 Collaboration Graph
 U 34546 420877 82583 Citation graph
 V 27770 352285 95215 Citation graph
 W 26518 65369 27170 Peer-to-peer network
 mechanism, where all the nodes are organized in a binary
 tree structure. Each node receives top-K values from its two
 children and computes top-K nodes locally combining its own
 local top-K with the data received from the children. The node
 then sends the final local top-K to its parent. The global top-K
 over the whole graph can be found at the root node. The code
 for in-network processing of top-K is given in Algorithm 6.
 VII. EXPERIMENTAL RESULTS
 In this section, we evaluate our implementation of the Top-
 k aggregation system. All experiments are carried out in a
 20-node compute cluster. Each compute node has a quad-core
 AMD Opteron system with 4GB of RAM, runs 64-bit Red Hat
 Enterprise Linux, and is connected via 10 Gigabit Ethernet.
 We implement the three algorithms in C++. We contrast the
 performance of various approaches by collecting processing
 (CPU) time, communication time and total time to process
 aggregation queries over a wide range of synthetic and real-
 world graphs, as listed in Table I and Table II, respectively. We
 generate the synthetic graphs using the SNAP graph generation
 software [20], and take the real-world graphs from SNAP
 datasets [20], [14]. All synthetic graphs are undirected graphs;
 among the real-world graphs, S and T are undirected, and U ,
 V , and W are directed graphs. Unless stated otherwise, the
 default number of partitions or hosts is taken as 12, and the
 default value for h is set to 10. We set the value of k to 200
 nodes. Table I and Table II show the 12-way cut for the graphs
 considered in the experiments. We partition the graph using an
 off-the-shelf partitioning algorithm, METIS [11].
 In Figure 4a, Figure 4b and Figure 4c, we plot the average
 CPU time, average communication time (across the host ma-
 chines) and total time for different graphs. Each of the graphs
 shows the comparative data for three algorithms—update-
 based, join-based and hybrid. The average communication time
 across the host machines (Figure 4b) for the hybrid algorithm
 is minimum among the three algorithms, however this hybrid
 algorithm incurs higher CPU time (Figure 4a) than the update-
 based algorithm. Among the three algorithms, the join-based
 algorithm usually requires the highest CPU time for the given
 hop-length of 12. As shown in Figure 4c, except for graphD,
 the total time to complete the aggregation query is the lowest
 454
 1000
  10000
  100000
  1e+06
  1e+07
  1e+08
 Time (msec
 )
 Graph
 graph
 A
 graph
 B
 graph
 C
 graph
 D
 graph
 E
 graph
 F
 update-based
 join-based
 hybrid
  100
  1000
  10000
  100000
  1e+06
  1e+07
  1e+08
 Time (msec
 )
 Graph
 graph
 A
 graph
 B
 graph
 C
 graph
 D
 graph
 E
 graph
 F
 update-based
 join-based
 hybrid
  1000
  10000
  100000
  1e+06
  1e+07
  1e+08
 Time (msec
 )
 Graph
 graph
 A
 graph
 B
 graph
 C
 graph
 D
 graph
 E
 graph
 F
 update-based
 join-based
 hybrid
 (a) Average CPU time (b) Average Communication time (c) Total time
 Fig. 4. Synthetic graphs (a) Average CPU time within the participating hosts (b) Average communication time across the host machines, and (c) total time to
 finish the aggregation query for the different graphs (x-axis is plotted on a log-scale)
  10000
  100000
  1e+06
  1e+07
  1e+08
 Time (msec
 )
 Graph
 graphS graphT graphU graphV graphW
 update-based
 join-based
 hybrid
  10000
  100000
  1e+06
  1e+07
  1e+08
 Time (msec
 )
 Graph
 graphS graphT graphU graphV graphW
 update-based
 join-based
 hybrid
  10000
  100000
  1e+06
  1e+07
  1e+08
 Time (msec
 )
 Graph
 graphS graphT graphU graphV graphW
 update-based
 join-based
 hybrid
 (a) Average CPU time (b) Average Communication time (c) Total time
 Fig. 5. Real-world graphs (a) Average CPU time within the participating hosts (b) Average communication time across the host machines, and (c) total time
 to finish the aggregation query for the different graphs (log-scale along the x-axis)
 in case of the update-based algorithm. For the graphD, update-
 based algorithm does not perform well due to a large fraction
 of (12-way) cut edges in the partitions of the graph. For such
 a graph, the communication volume dominates the total time
 to finish the query, and hence the hybrid algorithm fares well
 for such a graph.
 Figure 5a, Figure 5b and Figure 5c show the CPU time, the
 communication time and the total time for different real-world
 graphs. As shown in Figure 5c, in all the real-worlds graphs,
 the total time to process the aggregation query is minimal for
 the update-based algorithm. For some graphs, the completion
 time for the update-based algorithm is two order of magnitude
 lower than that with the join-based algorithm (please note the
 logarithmic scale along the x-axis).
 A. Varying h-values
 Figure 6 shows the total time to compute top-k nodes (in
 graph E) with varying h-values. As the hop-count increases,
 the time to compute the h-hop neighbors also increases; hence,
 the total time to compute top-k nodes in the graph also
 increases. As observed in the Figure 6, the total time for the
 update-based algorithm is up to two order of magnitude lower
 than that in the algorithm based on distributed joins (the join-
 based algorithm); please note that the plot uses a logarithmic
 scale long the x-axis.
 B. Varying host machines
 Figure 7 and Figure 8 present the scalability results for
 the top-k computation (in Graph A) within a shared-nothing
  10000
  100000
  1e+06
  1e+07
  1e+08
  4  6  8  10  12  14  16  18  20
 Total time (msec)
  
h-value
 Update-based
 Join-based
 Hybrid
 Fig. 6. Total time to compute top-k nodes with varying h-values: total time
 increases linearly with h-value (note the log-scale along the x-axis)
 system. As shown in Figure 7, the total time to compute top-
 k nodes decreases linearly with the increase in the number
 of machines (note the log-scale along the x-axis). Within
 the range of host population, the delay for the update-based
 algorithm is an order of magnitude smaller than the join-
 based algorithm (the figure uses a log-scale along the x-
 axis). Figure 8 shows the total iterations with varying host
 population. As we increase the number of hosts (partitions),
 total number of iterations also increases. As the number of
 partitions (or hosts) increases, a shortest path of length h might
 be split across a larger number of partitions, which leads to a
 higher pmax value (Section IV-B) for the system.
 455
 10000
  100000
  1e+06
  1e+07
  8  10  12  14  16  18  20
 Total time (msec)
  
Total hosts
 Update-based
 Join-based
 Hybrid
 Fig. 7. Total time to compute top-K nodes with varying hosts: the proposed
 algorithms scales with the number of hosts and achieves an order of magnitude
 speed-up, while compared to the base-line algorithm (log-scale on the x-axis)
  2
  3
  4
  5
  6
  7
  8
  9
  10
  8  10  12  14  16  18  20
 Total iteration
 s
 Total hosts
 Update-based
 Join-based
 Fig. 8. Total cycles/iterations with varying hosts: with the base-line algorithm,
 the total number of iterations is fixed at h; with the update-based algorithm,
 it varies with the number of hosts (or partitions)
 VIII. RELATED WORK
 There is an increasing trend in processing graphs within
 large-scale, distributed systems, resulting in a number of
 commercial and open-source systems being developed, for ex-
 ample, Pegasus [9], Pregel [17], GraphLab [16] etc. A number
 of techniques to manage partitions across distributed systems
 have been proposed [18], [25], that aim at minimizing inter-
 machine communications while supporting simple queries that
 access or manipulate a set of nodes. These techniques dynam-
 ically maintain the partitions of graphs with changes in query
 workloads. The key idea is to replicate a set of nodes across
 the machines based on cross-partition query loads, query hot-
 spots, and read/write patterns of the queries. Contrary to their
 partitioning scenario, the type of queries we consider are both
 compute and communication intensive in nature, and they
 need to access the whole graph. PowerGraph [8] considers
 graphs with a power-law degree distribution and proposes a
 graph partitioning or distribution abstraction exploiting the
 structure of the power-law graphs. The authors observe that a
 balanced p-way edge-cut technique to distribute nodes across
 the machines does not perform well with a power-law graph.
 Our work is orthogonal to graph partitioning or distribution
 framework, and our proposed algorithms can be incorpo-
 rated within any graph partitioning or distribution abstraction.
 Supporting compute- and communication-intensive algorithms
 (e.g., aggregation over h-hop neighborhood) with a coarse-
 grained model of computation is the main theme of our work.
 Crecelius and Schenkel [4] presents an algorithm to dy-
 namically maintain nearest neighbor lists for all the nodes
 in a graph under updates to the graph (i.e., node insertions
 and decrease in edge weights). Their algorithm is relevant
 to a centralized or single-machine system, and it does not
 consider the initial computation of the index structure. Though
 such a computation of the index structure is an one time
 and off-line operation, for a large graph such a computation
 requires enormous processing time; thus, an efficient approach
 to create the index structure storing the all-pairs shortest
 paths is necessary even within a single-machine system. We
 consider the problem of parallelizing, across a set of distributed
 machines, the process of computing all-pairs shortest paths
 and top-k aggregation over h-hop neighborhood within a large
 graph.
 There is a body of work on graph aggregation and summa-
 rization. Tian et al. [21] develop a graph summarization oper-
 ation, SNAP, that groups graph nodes based on user-specified
 node attributes and relationships. The users can drill-down and
 roll-up through the summaries using the node attributes and
 relationships, that captures the hierarchy information of the
 summaries. Navlakha et al. [19] apply a Minimum Description
 Length (MDL) principle to generate coarse-level representation
 of a input graph. The compressed graph representation consists
 of two parts: a graph summary and a set of corrections. Wu
 et al. [23] present graph aggregation or clustering techniques
 based on multi-level geodesic approximation. These papers
 address the aggregation issue that is related to cluster formation
 or hierarchical summarization over graphs, which is different
 from the neighborhood aggregation problem addressed in this
 paper.
 Yan et al. [24] present techniques to prune nodes of a
 graph while computing the top-K nodes with the highest
 aggregate scores. The paper does not consider the issue of
 parallelizing the neighborhood aggregation process across mul-
 tiple machines. Also, the paper assumes that the neighborhood
 information is already computed, and takes the neighborhood
 information of the nodes as input. We maintain that computing
 the neighborhood information, a compute-intensive operation,
 is the crux of the aggregation problem, and present algorithms
 to parallelize the neighborhood computation and top-K aggre-
 gation across a distributed system.
 IX. CONCLUSION
 In this paper, we present algorithms to process aggrega-
 tion in graphs exploiting computational resources within a
 number of networked machines. The update-based algorithm
 incrementally propagates the updates across the host machines.
 Such an incremental propagation reduces network loads and
 total iterations while compared to a baseline algorithm based
 on distributed joins. The algorithm reduces synchronization
 barriers by allowing each node to proceed with the local
 computation independent of the others. We propose a hybrid
 algorithm that minimizes communication volumes (during a
 few initial iterations) for a graph with a large number of
 cut edges. Our experimental results show the update-based
 algorithm achieves an oder of magnitude speed-up while
 computing top-k aggregation over a wide variety of graphs.
 456
There exists a number of open issues that we plan to
 investigate further. First, developing a partitioning algorithm
 that minimizes the parameter pmax (maximum number of
 partitions spanned by the shortest paths in the graph) is an
 open research issue. Such an algorithm might decrease total
 iterations before convergence, reducing the synchronization
 overheads in a system with a large number of processing nodes.
 Second, in a distributed environment with a large number
 of nodes, allowing synchronization barriers at the onset of
 each iteration results in high overheads due to unbalanced
 computation within nodes or noises within computing nodes
 or networks. We plan to devise a lazy protocol for propa-
 gating updates eliminating barrier synchronization. Third, we
 are working towards developing incremental algorithms to
 dynamically maintain the top-k nodes or h-hop neighborhood
 information with changes in the graph. We have plan to
 incorporate the distributed graph processing techniques within
 the framework of a workflow provenance tool, Karma [10].
 X. ACKNOWLEDGMENTS
 The author thanks the reviewers for their helpful comments.
 The author is grateful to Dilma Da Silva for her comments and
 feedback on the paper. This work has been funded in part by
 National Science Foundation under grant ACI 1148359, and
 the Pervasive Technology Institute, Indiana University.
 REFERENCES
 [1] G. Adomavicius and A. Tuzhilin. Toward the next generation of
 recommender systems: A survey of the state-of-the-art and possible
 extensions. IEEE Trans. on Knowl. and Data Eng. (TKDE), 17(6):734–
 749, June 2005.
 [2] S. Brin and L. Page. The anatomy of a large-scale hypertextual web
 search engine. In Proc. Int’l. Conf. on World Wide Web, pages 107–117,
 1998.
 [3] A. Chakraborty, E. Schenfeld, and D. D. Silva. Switching optically-
 connected memories in a large-scale system. In Proc. Int’l Prallel and
 Distributed Processing Symposium, pages 727–738, 2012.
 [4] T. Crecelius and R. Schenkel. Pay-as-you-go maintenance of precom-
 puted nearest neighbors in large graphs. In Proc. ACM Int. Conf. on
 Information and knowledge management, pages 952–961, New York,
 NY, USA, 2012.
 [5] P. Domingos and M. Richardson. Mining the network value of
 customers. In Proc. ACM SIGKDD Int’l. Conf. on Knowledge discovery
 and data mining (KDD), pages 57–66, 2001.
 [6] R. W. Floyd. Algorithm 97: Shortest path. Commun. ACM, 5(6), 1962.
 [7] P. W. Frey, R. Goncalves, M. Kersten, and J. Teubner. A spinning join
 that does not get dizzy. In Proc. Int’l Conf. on Distributed Computing
 Systems, pages 283–292, Washington, DC, USA, 2010.
 [8] J. E. Gonzalez, Y. Low, H. Gu, D. Bickson, and C. Guestrin. Pow-
 ergraph: distributed graph-parallel computation on natural graphs. In
 Proc. USENIX Conf. on Operating Systems Design and Implementation
 (OSDI), pages 17–30, Berkeley, CA, USA, 2012.
 [9] U. Kang, C. E. Tsourakakis, and C. Faloutsos. Pegasus: A peta-scale
 graph mining system implementation and observations. In Proc. IEEE
 Int. Conf. on Data Mining, pages 229–238, Washington, DC, USA,
 2009.
 [10] Karma Provenance Tool. http://d2i.indiana.edu/
 provenance_karma.
 [11] G. Karypis and V. Kumar. A fast and high quality multilevel scheme
 for partitioning irregular graphs. SIAM J. Sci. Comput., 20(1):359–392,
 Dec. 1998.
 [12] G. J. Katz and J. T. Kider, Jr. All-pairs shortest-paths for large graphs
 on the gpu. In Proc. ACM SIGGRAPH/EUROGRAPHICS symposium
 on Graphics hardware, pages 47–55, 2008.
 [13] D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the spread of
 influence through a social network. In Proc. ACM SIGKDD Int’l. Conf.
 on Knowledge discovery and data mining (KDD), pages 137–146, 2003.
 [14] J. Leskovec, J. Kleinberg, and C. Faloutsos. Graph evolution: Densifi-
 cation and shrinking diameters. ACM Trans. Knowl. Discov. Data, 1(1),
 2007.
 [15] D. Liben-Nowell and J. Kleinberg. The link prediction problem for
 social networks. In Proc. Int’l. Conf. on Information and knowledge
 management (CIKM), pages 556–559, 2003.
 [16] Y. Low, D. Bickson, J. Gonzalez, C. Guestrin, A. Kyrola, and J. M.
 Hellerstein. Distributed graphlab: a framework for machine learning
 and data mining in the cloud. Proc. VLDB Endow., 5(8):716–727, 2012.
 [17] G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn, N. Leiser,
 and G. Czajkowski. Pregel: a system for large-scale graph processing.
 In Proc. Int. Conf. on Management of data (SIGMOD), pages 135–146,
 New York, NY, USA, 2010.
 [18] J. Mondal and A. Deshpande. Managing large dynamic graphs effi-
 ciently. In Proc. Int. Conf. on Management of data (SIGMOD), pages
 145–156, 2012.
 [19] S. Navlakha, R. Rastogi, and N. Shrivastava. Graph summarization with
 bounded error. In Proc. Int. Conf. on Management of data (SIGMOD),
 pages 419–432, New York, NY, USA, 2008.
 [20] SNAP(Stanford Network Analysis Platform) Software. http://
 snap.stanford.edu.
 [21] Y. Tian, R. A. Hankins, and J. M. Patel. Efficient aggregation for graph
 summarization. In Proc. Int. Conf. on Management of data (SIGMOD),
 SIGMOD ’08, pages 567–580, New York, NY, USA, 2008.
 [22] G. Venkataraman, S. Sahni, and S. Mukhopadhyaya. A blocked all-pair
 shortest-paths algorithm. volume 8, Dec 2003.
 [23] A. Y. Wu, M. Garland, and J. Han. Mining scale-free networks using
 geodesic clustering. In Proc. ACM SIGKDD Int. Conf. on Knowledge
 discovery and data mining, KDD ’04, pages 719–724, New York, NY,
 USA, 2004.
 [24] F. Z. J. H. Xifeng Yan, Bin He. Top-k aggregation queries over large
 networks. In Proc. Int’l. Conf. on Data Engineering (ICDE), pages
 377–380, Long beach, California, April 2010.
 [25] S. Yang, X. Yan, B. Zong, and A. Khan. Towards effective partition
 management for large graphs. In Proc. Int. Conf. on Management of
 data (SIGMOD), pages 517–528, 2012.
 457

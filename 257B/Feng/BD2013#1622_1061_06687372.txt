Hysteresis Re-chunking Based Metadata Harnessing
 Deduplication of Disk Images
 Bing Zhou and Jiangtao Wen
 State Key Laboratory on Intelligent Technology and Systems
 Tsinghua National Laboratory for Information Science and Technology (TNList)
 Department of Computer Science and Technology,
 Tsinghua University, Beijing, China
 Email: zhoubing09@mails.tsinghua.edu.cn, jtwen@tsinghua.edu.cn
 Abstract—Metadata-related overhead can significantly impact
 the performance of data deduplication systems, including the real
 duplication elimination ratio and the deduplication throughput.
 The amount of metadata produced is mainly determined by the
 chunking mechanism for the input data stream. In this paper, we
 propose a metadata harnessing deduplication (MHD) algorithm
 utilizing a duplication-distribution-based hysteresis re-chunking
 strategy. MHD harnesses the metadata by dynamically merging
 multiple non-duplicate chunks into one big chunk represented by
 one hash value while dividing big chunks straddling duplicate and
 non-duplicate data regions into small chunks represented with
 multiple hashes. Experimental results show that the proposed
 algorithm achieves a lower metadata overhead and a higher
 deduplication throughput for a given duplication elimination
 ratio, as compared with other state-of-the-art algorithms such
 as the Bimodal, SubChunk and SparseIndexing algorithms.
 Keywords—Data Deduplication, Metadata Harnessing
 I. INTRODUCTION
 Data deduplication refers to the process of optimizing a
 data storage system by dividing the data into chunks so as to
 identify and eliminate redundant storage of identical chunks.
 The performance of a data deduplication system is mainly
 measured in terms of the duplication elimination ratio, as well
 as the deduplication throughput. Generally the smaller the
 average chunk size, the more the redundancies that can be
 identified and removed, at a cost of higher metadata related
 overheads. Because fixed-sized chucking algorithms such as
 those used in the Venti [1] and OceanStore algorithms [2] are
 not able to handle the boundary-shifting problem [3], content-
 defined-chunking (CDC) techniques dividing a data stream
 into variable-sized chunks based on fingerprints computed
 using a sliding window were proposed in systems such as
 LBFS [4] and have become the basic chunking algorithm
 in virtually all state-of-the-art deduplication algorithms or
 systems, including [5] [6] [7] [8] [9] [10] [11].
 In a typical deduplication system, metadata are generated
 and used for two purposes, i.e. metadata for data storage man-
 agement (MS), and metadata for content description (MD),
 including the hash values, as well as the address and size of
 each chunk. The MD that can not fit into the main memory will
 be stored on disk. The related disk I/O and look-up operations
 would reduce deduplication speed, a.k.a the disk bottleneck.
 To address the disk bottleneck, the Data Domain [12] and
 SparseIndexing [13] algorithms utilized inherent data locality
 to improve the throughput without explicitly addressing the
 issue of metadata generation. In the Data Domain algorithm,
 the amount of the metadata grows linearly with respect to the
 number of chunks. The rate of growth is even higher for the
 SparseIndexing algorithm, as the hash value of a chunk could
 be stored multiple times if the chunk is present in multiple
 segments.
 To reduce the size of the MS, Fingerdiff [14] coalesce
 contiguous non-duplicate chunks up to a maximal number into
 one big chunk stored on the disk. The Bimodal [15], FBC [16]
 and SubChunk [17] algorithms first search for duplications
 using a large chunk size, with the non-duplicate big chunks
 selectively processed again using a small chunk size. Although
 such algorithms are capable of reducing the MS, they do not
 necessarily reduce the MD or the overall amount of metadata
 and I/Os. In addition, in Fingerdiff, a database is needed to
 index each chunk. The assumption that the database can fit
 into the RAM might not be realistic in practical systems.
 In the Bimodal, FBC and Subchunk algorithms, before re-
 chunking a big chunk, a query for the non-duplicate big chunk
 is introduced.
 Metadata related overhead also greatly impacts the dedu-
 plication performance in distributed systems related applica-
 tions such as large scale data backup. To improve the trade-
 off between duplication elimination ratio and deduplication
 overhead including the storage space occupied by the extra
 metadata and the corresponding I/Os, in this paper, we propose
 a duplication-distribution-based metadata harnessing dedupli-
 cation (MHD) algorithm. It uses hysteresis re-chunking taking
 into account both data that have been processed, as well as in-
 coming data.
 As an illustrative example, consider the case in Fig. 1 with
 3 files. When File-1 is the only file to be processed, the
 optimal chunk should be set to File-1. But when a second
 file File-2 that matches Slice-1 of the first file is present,
 the first file should be re-chunked into two chunks, namely
 Slice-1 and Slice-2. Similarly, if there is a third file
 File-3 which matches Slice-3 of the second file, the
 second file should be re-chunked into three chunks, Slice-3,
 Slice-4 and Slice-1. In this example, only 5 hash values
 are required.
 2013 42nd International Conference on Parallel Processing
 0190-3918/13 $26.00 © 2013 IEEE
 DOI 10.1109/ICPP.2013.48
 389

 
 	
 
 	
 	 

 
 
 Fig. 1. Illustration for Hysteresis Re-chunking.
 While utilizing tools such as manifests [18] and hooks as in
 the SparseIndexing algorithm, MHD differs from the Bimodal,
 FBC and SubChunk algorithm in the following two aspects:
 • MHD reduces unnecessary queries for non-duplicate
 big chunks by replacing the widely used big-chunk-
 first-small-chunk-second deduplication strategy with a
 bi-directional-match-extension mechanism. Using this
 mechanism, when a small duplicate chunk is detected,
 duplication detection is conducted using its neighboring
 data and a relatively large chunk size.
 • Big chunks would be re-chunked when and only when
 they straddle duplicate and non-duplicate data.
 The rest of this paper is organized as the following. Section
 II presents a brief overview of related work. The proposed
 algorithm is described in detail in Section III, followed by
 analysis in Section IV and experimental results obtained using
 real-world disk image data and the proposed algorithm as well
 as the Bimodal, SubChunk and SparseIndexing algorithms are
 given in Section V. Section VI contains the conclusions.
 II. RELATED WORK
 The widely used Rabin Fingerprint [19] chunking algorithm
 uses a sliding window to scan the input data stream and
 calculate a fingerprint at each position. The current position is
 recorded as a cut point if either of the following is true: 1) the
 fingerprint matches with a pre-defined value and the length of
 the current data block is larger than a lower bound; or 2) when
 the length of the current data block is equal to or larger than
 a upper bound and no matching fingerprint is found. The data
 stream between two cut points are extracted as chunks.
 An improved chunking algorithm named TTTD was pro-
 posed in [3]. In TTTD, candidate cut points corresponding
 to chunk sizes between the lower and the upper bounds
 are recorded, and used as cut points only if no pre-defined
 fingerprints are detected when the corresponding chunk size
 reaches the upper bound.
 Nohhyun and David [20] evaluated the characteristics of a
 dataset using a metrics for the variation of data patterns in the
 dataset.
 Considering the processing overhead of CDC for mobile
 devices, Lee and Park [21] proposed a chunking method
 adaptively selecting the CDC and FSP algorithms based on
 the file type and the computational capabilities of the devices.
 
 
 
 
 
  !
  !
 "
 
 
 ##
 ! !
 
 ! !
  $!
 
 
 
 %
 &
 & 
 
 $ '
 (
  ! !
 (
  !
 !
 Fig. 2. System Architecure.
 Extreme Binning [22] uses one chunk from each file to
 represent the corresponding file. If the representative chunk
 is found to be a duplicate, data locality information of the
 corresponding file is loaded into the RAM. As only one
 disk access is needed per file, the throughput of the Extreme
 Binning algorithm is comparatively high.
 Data Domain uses the Bloom Filter [23] to avoid unneces-
 sary disk lookups. A stream-dependent segment layout is used
 to maintain high spatial locality [24], so that each retrieval
 from the disk is more fully utilized for multiple data chunks
 as opposed to requiring a separate disk access for each chunk.
 In SparseIndexing, instead of building a full index for every
 chunk, a small portion of chunks are extracted as hooks. Each
 hook was mapped to a few manifests or recipes [18], which
 preserve the data locality present in the original input chunks.
 When deduplicating, SparseIndexing divides the incoming
 stream into large segments, and processed each segment with
 reference to several most similar existing segments referenced
 by the hooks. By keeping only hooks in the RAM and by
 requiring only a few disk seeks per large segment, the disk
 look-up bottleneck is avoided.
 Meister et al. [25] proposed an entropy coding based post-
 process compression method for file recipes. It should be
 noted that file receipts is only one of many types of metadata
 generated during deduplication.
 The Bimodal, FBC and SubChunk algorithms first divide
 the input stream into big chunks for duplication detection.
 Bimodal then re-chunks the non-duplicate big chunks adjacent
 to duplicate chunks (termed “transition points”) into smaller
 chunks, followed by the deduplication process on the smaller
 chunks. FBC performs selective re-chunking using several
 strategies based on the frequency information of chunks
 estimated from data that have been previously processed.
 SubChunk on the other hand, re-chunks every non-duplicate
 big chunk into small chunks for deduplication. The non-
 duplicate small chunks belonging to one big chunk would then
 be coalesced together.
 III. METADATA HARNESSING DEDUPLICATION
 The MHD algorithm consists of two phases, 1) the sampling
 and hash merging phase, in which large chunks are formed by
 merging multiple smaller chunks and represented by a single
 390
$)$ 
*+,
 
 *-,
 .
 *-,
 $!&
 *,
 $)$ 
*+,
 *,/  
 ! !
 *(,/  
 $!
 $)$ 
*+,
 
 *-,
 .
 *-,
 *,/  
 
 Fig. 3. Formats of Metadata.
 #01
  !
 23
 4 
  !
 
 0
 2
 3
 0
 $
 5
 6
 6
 0$$#3
 0$$#3
 0
 $$# 5
 $$# 5
 5
 0
 Fig. 4. MHD process.
 hash value; and 2) the hysteresis hash re-chunking phase, in
 which large chunks at the boundary of duplicate and non-
 duplicate regions are adaptively divided into smaller chunks.
 Fig. 2 shows the architecture of the proposed system. The
 input to the system is the byte stream created by concatenating
 the content of the files in the unprocessed file system. This byte
 stream is divided into chunks by the chunker and sent to the
 deduplicator which are optimized (deduplicated) by the sys-
 tem. The outputs of the system include non-duplicate chunks
 saved in the DiskChunkStore, the DiskChunkManifests, the
 FileManifests used for reconstructing each of the files in the
 original system, and the Hooks. In the remainder of the paper,
 the DiskChunkManifest is referred to as simply the Manifest.
 The Manifest is a sequence of hash values representing the
 data blocks within the corresponding DiskChunk, while a
 Hook is a hash value sample extracted from the Manifest. Each
 DiskChunk, Manifest, or Hook is implemented as a separate
 hash addressable file and the formats of the metadata are
 shown in Figure 3. Each Hook (named using its hash value) is
 mapped to only one Manifest and each Manifest is mapped to
 one DiskChunk. The DiskChunk and the Hook files that have
 been written to disk will not be further modified. Each entry
 in the Manifests introduces a one-byte Hook flag to indicate
 whether this entry is a Hook. Re-chunking is only carried out
 for data blocks corresponding to the non-Hook hash values,
 with the original hash values replaced by multiple new values
 created from data blocks after re-chunking. The only files
 updated during the deduplcation process are the Manifest files.
 In terms of RAM access, besides the Manifest which might
 be reloaded into the RAM to take advantage of data locality,
 some data blocks from the DiskChunks might also be reloaded
 into the RAM during hysteresis hash re-chunking.
 The deduplication process of MHD is shown in Fig. 4.
 

 
 

 -
 

 
 

 	
 

 7
 

 8
 

 
 

 

 

 +
 

 9
 
 
 
 
 
 
 ! !
 
  $!
 
 
 Fig. 5. Example for SHM.
 For each new chunk extracted from the input byte stream
 using the Rabin Fingerprint algorithm, a SHA-1 hash value
 h is calculated and used for duplication detection using an in-
 memory bloom filter and cache. The cache contains a number
 of Manifests, each of which is organized as a hash table. An
 incoming duplicate chunk is detected, if its hash h matches a
 Manifest in the cache. When no such hit has been found, h
 would be checked in the bloom filter to determine whether it
 is a new value. If the bloom filter indicates that the hash is
 probably duplicate, MHD continues to query whether there is
 a duplicate Hook on disk. When such a Hook is found, the
 Manifest it points to would be loaded into the RAM. If the
 cache becomes full during this process, one Manifest would
 be freed following the Least-Recently-Used (LRU) [26] policy.
 A Manifest that has been set dirty, is written back to the disk
 before it is freed.
 After the above steps, an incoming chunk detected as non-
 duplicate would be temporary buffered in the RAM until
 it is confirmed as a non-duplicate after backward direction
 match extension. Although the size of the DiskChunk can
 be arbitrarily large, we only merge the non-duplicate chunks
 belonging to one file into one DiskChunk and write it to the
 disk. Each DiskChunk has a corresponding Manifest, each of
 which with at least one Hook that serves as the entry point
 to the Manifest. The Hooks for a Manifest are selected by
 uniformly sub-sampling of the sequence of hash values in
 the Manifest. The period for the sub-sampling is called the
 Sample Distance (SD). Since the SD?1 hash values between
 two neighboring Hooks would not be directly accessed by the
 system, they are replaced by a single hash calculated over
 the corresponding SD?1 chunks of the SD?1 hash values,
 effectively merging these SD?1 hash values into one value
 representing a larger chunk. This process is termed Sampling
 and Hash Merging (SHM). In our implementation, we set an
 in-memory chunk buffer with the size of two times of SD.
 When the buffer is full, the first half of SD chunks which can
 not be backward extended as duplicates would be written to
 the DiskChunk with two hash values created, corresponding
 to the first and the last SD?1 chunks respectively. It should
 be noted other SHM strategies also exist, for example, SHM
 can be performed on the contiguous non-duplicate chunks of
 the original input stream, to guarantee each non-duplicate data
 slice of the input stream “owns” at least one Hook.
 Fig. 5 shows an example for SHM using 10 non-duplicate
 chunks with SD = 5 hashes. The original Manifest of 10 hashes
 Hash 1 to Hash 10 (corresponding to Chunk 1 to Chunk
 391


 0	
 

 
 

 

 

 09
 

 -
 

 8
 

 7
 2& !
 2
 
 
 
 
 
 
 
 :
 6 6
 
 
 
 
 
 
 
 0;
 
 	
 
 
 
 
 
 
 Fig. 6. Example for HHR.
 10) are converted to a new Manifest containing 4 hash values
 after the chunks between Chunk 2 and Chunk 5 as well as
 between Chunk 7 and Chunk 10 are merged into two big
 chunks, each with a single hash value.
 When an incoming chunk is detected as a duplicate, Bi-
 Directional Match Extension would be performed on the
 Manifest. We term the hash that has been hit on the Manifest
 as the HitHash and the incoming duplicate chunk as HitChunk.
 For Backward Match Extension (BME), new hash values are
 calculated for the buffered chunk bytes before the HitChunk
 and compared with the hash values for the corresponding
 byte before the HitHash in the Manifest until a mismatch
 is found. If the old chunk represented by the mismatched
 hash (which is not a Hook on the Manifest) covers an edge
 between the duplicate and non-duplicate chunks in the buffer,
 data from the old chunk would be loaded into the RAM
 from the disk for byte comparison, followed by the Hysteresis
 Hash Re-chunking (HHR) process. At least one hash named
 the EdgeHash representing the non-duplicate border chunk,
 one hash representing the duplicate chunk(s), and one hash
 representing the remaining bytes of the old chunk would be
 re-calculated after the HHR. In our system, the EdgeHash is
 created to prevent the same duplicate data slice from triggering
 identical and duplicate HHR in the future. We introduce byte
 comparisons to remove the duplicate data between two chunks
 with different sizes, as the exact boundary in the big chunk is
 not knowm. Each reloading of chunk bytes would one HHR
 process, with the Manifest on which HHR is performed set
 dirty, forcing it to be written back on to the disk as an update
 for the Manifest.
 After BME, a similar process called the Forward Match
 Extension (FME) is carried out in the forward direction. New
 chunks would be pre-fetched into the buffer to calculate new
 hashes for hash comparison with the hashes after HitHash on
 the Manifest until one mismatched hash is found, followed
 by the forward direction HHR. Chunks that have been pre-
 fetched but not extended as duplicates during the HHR, are
 again processed using the deduplication process as showed in
 Fig. 4. The duplicate chunks detected via Bi-Directional Match
 Extension and HHR are removed from the buffer.
 As shown in the example of Fig. 6, the BME process stops
 at Hash 2-5, as it is not a Hook and its byte size is larger
 than the total size of the three chunks before the HitChunk
 Chunk 6 in the buffer. The data of Chunk 2-5 represented
 by Hash 2-5 are then reloaded from the disk into the RAM
 for byte comparison. The buffered Chunk 4 and Chunk
 5 are detected duplicate while Chunk N3 is detected as
 non-duplicate after byte comparison. Then Hash 2-5 is re-
 chunked into a Hash 4-5 after duplicates in Chunk 4 and
 Chunk 5 are removed, an EdgeHash Hash 3 representing
 the boundary data block with the same size of Chunk N3,
 and a Hash 2 representing the remaining bytes in the old
 Chunk 2-5 are created. After the FME and HHR, Hash
 7-10 is re-chunked into another three new hash values.
 In MHD, a new entry will only be written into the File-
 Manifest at the terminating point of neighboring chunks of
 duplicate or non-duplicate data slices within one file, or when
 a data slice reaches the end of the file.
 IV. ANALYSIS AND COMPARISON
 In this section, we present an analysis of the trade-off
 between the duplication elimination ratio and the deduplica-
 tion overhead for the MHD, and CDC algorithms such as
 SubChunk, Bimodal, and SparseIndexing. In our comparison,
 widely used deduplication acceleration methods such as the
 bloom filter and data locality preservation are considered
 implemented in the algorithms.
 We consider the total amount of metadata generated and
 the disk access times required by different algorithms when
 the same duplication elimination ratio is achieved by all
 algorithms on the input data. The sampling distance used by
 SHM in MHD is denoted as SD, while the expected big chunk
 size in the Bimodal and SubChunk algorithms is assumed to be
 ECS?SD, where ECS is the basic expected chunk size, so
 as to keep the deduplication efficiency of different algorithms
 at the same granularity.
 Given an ECS, we use N and D to denote the final number
 of the non-duplicate and duplicate chunks respectively in all
 algorithms,regardless of how chunks are generated, processed
 and stored by different algorithms. The corresponding duplica-
 tion elimination ratio can be roughly calculated as (D+N)/N .
 We use L to denote the number of detected duplicate data
 slices, each of which is a sequence of continuous chunks in
 the input stream. Let F denote the final number of the input
 files which are not completely duplicate, which is also the
 number of Manifests.
 We assume further that each Hook contains a 20-byte SHA-
 1 address to the Manifest it belongs to, each metadata file
 requires one inode for storage management, each inode costs
 256 bytes and each entry in the Manifest costs 36 bytes
 including the hash value, byte start position and byte size
 of a chunk. Each input file which cannot be deduplicated
 completely would generate one DiskChunks with a corre-
 sponding Manifest pointed to by one or more Hooks in all the
 algorithms analyzed. In SubChunk, the non-duplicate small
 chunks generated by dividing the same big chunk would be
 coalesced into one DiskChunk stored on the disk, resulting in
 at least N/SD DiskChunks, each of which has the maximal
 expected size of SD?ECS. Since each Manifest containing
 the small-chunk-to-container-chunk mappings would cover
 392
Algorithms MHD SubChunk Bimodal CDC
 Inodes for DiskChunks F N/SD F F
 Inodes for Hooks N/SD F N/SD + 2L(SD ? 1) N
 Bytes for each Hook 20 20 20 20
 Inodes for Manifests F F F F
 Bytes for Manifests 74N/SD + 148L 36N + 28N/SD 36N/SD + 72L(SD ? 1) 36N
 summary 512F + 424N/SD 532F + 280N/SD + 36N 512F + 312N/SD + 624L(SD ? 1) 512F + 312N
 TABLE I
 METADATA SIZE COMPARISON (SD>=2)
 Algorithms MHD SubChunk Bimodal CDC
 Chunk Output Times F N/SD F F
 Chunk Input Times 2L 0 0 0
 Hook Output Times N/SD F N/SD + 2(SD ? 1)L N
 Hook Input Times L L L L
 Manifest Output Times F + L F F F
 Manifest Input Times L L L L
 Big Chunk Query Times 0 (N +D)/SD N/SD 0
 Small Chunk Query Times N + L N + L (2SD + 1)L N + L
 Summary without Bloom Filter 2F + 6L+N + NSD 2F + 3L+N +
 2N+D
 SD 2F + (4SD + 1)L+ 2NSD 2F + 3L+ 2N
 Summary with Bloom Filter 2F + 6L+ NSD 2F + 3L+
 N+D
 SD 2F + (2SD + 1)L+ NSD 2F + 3L+N
 TABLE II
 DISK ACCESSING TIMES COMPARISON
 multiple DiskChunks, the entries for the small chunks be-
 longing to the same DiskChunk in the Manifests need to
 share 28 bytes to indicate the address and the number of the
 chunks contained in the same DiskChunk. Each Manifest is
 conservatively allocated with one Hook. In Bimodal, the big
 chunks adjacent to the duplicate data slices would be divided
 into small chunks. Assuming all the resulting small chunks
 are non-duplicates, the numbers of the big and small chunks
 will be N/SD?2L and L?SD respectively. Bytes from non-
 duplicate chunks belonging to the same file are stored in one
 DiskChunk. Because each chunk, big or small, is represented
 by one entry in the Manifests as well as one Hook, the number
 of inodes for the Hooks is identical to the number of chunks
 while the size of the Manifests is the number of chunks
 multiplied by 36. On the other hand, the total size for metadata
 in the CDC algorithm can be calculated as 512F+312N bytes.
 In MHD, the number of inodes for the Hooks is N/SD
 and the number of entries in the Manifests is 2N/SD. Each
 entry each requires one more byte for the Hook flag, resulting
 in a total of 74N/SD bytes plus 148L bytes introduced by
 HHR. We ignore the metadata for the FileManifests in the
 analysis of different algorithms, as the amount is identical for
 all algorithms under our assumption. A detailed quantitative
 comparison between the metadata needed for different algo-
 rithms are given in TABLE I. From TABLE I, it can be easily
 calculated that when SD is set sufficiently high, the amount of
 metadata required by MHD becomes much smaller than those
 required by the other algorithms.
 The disk bottleneck is resulted from frequent disk accesses
 for duplication queries and metadata I/O during deduplication.
 When a new hash hits a Hook on the disk, the address of the
 Manifest will be read into the RAM, followed by the entire
 Manifest, which is also loaded into the RAM. During this
 process, three disk accesses are required, triggered by every
 detection of duplicate data slice. As a result, the total number
 of disk accesses is 3L, on top of F disk accesses required for
 Manifest output, as required by all the algorithms. As shown
 in TABLE II, SubChunk has at least N/SD DiskChunks to
 be written and requires (N+D)/SD disk look-up for big
 chunk duplication queries. The Manifests in SubChunk record
 only the small-chunk-to-container-chunk mapping without pre-
 serving data locality between the big chunks. This loss of
 data locality lead to additional disk accesses for duplicate big
 chunks. We assume each duplicate data slice would also cause
 one Manifest loading in SubChunk in the worst case. In MHD,
 on the other hand, for each HHR operation, each duplicate data
 slice detected would produce at most three disk accesses, two
 for reloading chunk bytes in Bi-Directional Match Extension
 and one for writing the updated Manifest back to disk, for
 a total of 3L disk accesses in the worst case. TABLE II
 compares the disk accesses in detail. Assuming the bloom
 filter eliminates all queries for non-duplicate hash values, when
 3L<D/SD, the number of disk accesses for MHD is lower
 than all other algorithms compared.
 In SparseIndexing, instead of being written onto the disk,
 all sampled Hooks are buffered in the sparse index. Different
 from the bloom filter, sparse index is another in-memory data
 structure used for checking if a hash value is a duplicate. If the
 sparse index indicates the hash is duplicate, no confirmation by
 disk look-up is needed. Since the Manifests in SparseIndexing
 preserve data locality within the backup byte stream, one hash
 may be recorded for multiple times if the corresponding chunk
 appears multiple times in the stream. Because the Hooks in
 SparseIndexing are also sampled based on the Manifests, each
 Hook may point to one or more Manifests. Compared with
 SparseIndexing, MHD generates fewer bytes for the Manifests.
 393
The Manifests will only retain the hash values for non-
 duplicate chunks and are utilized by SHM for deduplication.
 Because the number of the Hooks produced in SparseIndexing
 are larger than in MHD using the same sample distance, in
 the situation when a Hook stored in the spare index needs
 to be written to the disk, more disk I/Os are needed for the
 Hooks than in MHD. We don’t give the quantitative results
 for SparseIndexing in this paper.
 Comparing the duplication elimination ability of the algo-
 rithms, the maximal sizes of the data blocks represented by
 a single SHA-1 hash in the MHD, SubChunk, Bimodal and
 CDC algorithms are ECS?(SD?1), ECS?SD, ECS?SD
 and ECS respectively.
 The actual metadata and throughput comparisons using real-
 world disk image data are given in the next Section. In our
 analysis, the I/O overhead is compared on the basis of the
 number of I/Os required, without considering the amount of
 data accessed in each I/O.
 V. EXPERIMENTAL EVALUATION
 Experiments were designed to examine the following:
 • Utilization of metadata by MHD
 • The best deduplication efficiency achieved by the algo-
 rithms compared
 • The trade-off between deduplication efficiency and the
 amount of metadata required by the algorithms
 • The trade-off between deduplication efficiency and dedu-
 plication throughput for the algorithms
 • Whether the Hooks and Manifests generated by MHD fit
 into the RAM
 • Characteristics of the real-world disk images used in the
 experiments
 In the experiments, MHD as well as the SubChunk, Bimodal
 and SparseIndexing algorithms were implemented as prototype
 programs in the user space of the Ext3 file system. In our
 simulations, algorithms read data from and write the outputs
 to local directories.
 The test dataset were taken from the disk image backups
 of a group of 14 PCs running the Windows, Linux or Mac
 operating systems with NTFS, FAT, FAT32, Ext3, Ext4, or
 MFS file systems over a period of two weeks. The size of the
 dataset was 1.0 TB.
 The deduplication efficiency was measured using two num-
 bers. The real Duplication Elimination Ratio (DER) is defined
 as the size of input data divided by the size of output data
 from the perspective of the file system, including all metadata,
 whereas the data-only DER does not consider the metadata.
 To measure the metadata overhead and the deduplication
 speed, we define the MetaDataRatio as the ratio between the
 size of the total amount of metadata generated by the algorithm
 and the size of input data, and the ThroughputRatio as the
 time to pass the input data through the deduplication system
 without deduplication operation (e.g. by simply copying data)
 divided by the time taken for deduplication. A larger Through-
 putRatio means a higher deduplication speed. In this work, the
 deduplication throughput refers to the write throughput.
 In order to measure the the duplication distribution of the
 dataset, we define the Duplication Aggregation Degree (DAD)
 as the number of duplicate bytes divided by the number
 of the duplicate data slices. The larger the DAD, the more
 concentrated the duplicate data are.
 For more accurately evaluating the throughput improvement
 achieved by the proposed algorithm, we compared MHD with
 the improved bloom filter based Bimodal and SubChunk al-
 gorithms incorporating widely used deduplication acceleration
 methods, including the bloom filter and data locality.
 We note that the MHD algorithm can also be implemented
 in conjunction with the sparse index data structure in Spar-
 seIndexing. In order to distinguish a sparse index based MHD
 implementation, we denote the bloom filter based implemen-
 tation used in the experiments the BF-MHD algorithm.
 We set the parameters in accordance with those used in the
 analysis above for different algorithms so as to achieve similar
 granularities for deduplication by the different algorithms.
 Specifically, when the BF-MHD algorithm was configured by
 the parameters ECS and SD, the expected sizes of small and
 big chunks in the Bimodal and SubChunk algorithms were set
 as ECS and ECS?SD respectively. We used an in-memory
 bloom filter of 100 MB for the Bimodal, SubChunk and BF-
 MHD algorithms. In SparseIndexing, the sample distance for
 the Hooks is set as SD based on the input, the segment size
 was set to ECS?SD?5 and the maximal allowable number
 of champions for each segment was set as 10 as in [13]. The
 maximal number of the Manifests one Hook can point to was
 set to 5, following the LRU cache policy. The RAM used for
 spare index was not limited and reported in the experiments.
 A. Metadata Harnessing
 Fig. 7 demonstrates the comparison for the amount of
 metadata required to achieve similar deduplication efficiency
 by the different algorithms, with SD = 1000 hashes, and ECS
 = 512, 1024, 2048, 4096, and 8192 bytes. The HHR process of
 BF-MHD only increases the sizes of the Manifests but does not
 introduce new inodes, whereas every time Bimodal performs
 re-chunking, new inodes are introduced along with new bytes
 in the Manifest.
 The number of inodes required per MB of input data
 v.s. the ECS is showed in Fig. 7(a), in which the curves
 for BF-MHD and SubChunk overlapped, while the curve
 of Bimodal was comparatively higher. Since SparseIndexing
 sampled the Hooks based only on the input chunks but not on
 the non-duplicate chunks, the curve for SparseIndexing was
 the highest.
 The relationship between the number of bytes contained
 in the Hook and Manifest files normalized by the input
 data size and the ECS is given in Fig. 7(b). As expected,
 SparseIndexing produced the most bytes for the Manifest and
 Hooks. SubChunk produced the second most bytes due to
 the small-chunk-to-container-chunk mappings which recorded
 information for all small chunks. The number of bytes for
 Bimodal was lower than SubChunk, while BF-MHD produced
 the least bytes. This is because the HHR process in MHD was
 394
512 1024 2048 4096 8192
 100
 101
 102
 ECS (Bytes)
 Number of Inodes per M
 B
 BF?MHD
 Bimodal
 SubChunk
 SparseIndexing
 (a) Number of Inodes vs. ECS
 512 1024 2048 4096 8192
 10?4
 10?3
 10?2
 10?1
 100
 ECS (Bytes)
 Manifest MetadataRati
 o
 BF?MHD
 Bimodal
 SubChunk
 SparseIndexing
 (b) Manifest and Hook Size vs. ECS
 512 1024 2048 4096 8192
 10?4
 10?3
 10?2
 ECS (Bytes)
 File Manifest MetadataRati
 o
 BF?MHD
 Bimodal
 SubChunk
 SparseIndexing
 (c) FileManifest Size vs. ECS
 512 1024 2048 4096 8192
 10?3
 10?2
 10?1
 ECS (Bytes)
 Total MetadataRati
 o
 BF?MHD
 Bimodal
 SubChunk
 SparseIndexing
 (d) Total Metadata size vs. ECS
 Fig. 7. Metadata comparison (ECS in Bytes, SD=1000 hashes).
 performed only when duplicate data might be found in a big
 chunk, and with the corresponding hash value replaced by at
 most three new hash values. In contrast, in Bimodal, the big
 chunks at the transitional points were always broken up. In
 addition, the number of small chunks then generated, which
 was more or less equal to ratio between the expected big and
 small chunk sizes, was usually larger than 3. These two factors
 combined led to a much higher storage requirement for the
 Manifests.
 Fig. 7(c) shows the MetaDataRatio of the FileManifests
 as a function of the ECS. BF-MHD attempts to represent
 contiguous data block in the same DiskChunk with one entry
 in the FileManifest. therefore, as can be seen, BF-MHD
 generated the least bytes for the FileManifests.
 Fig. 7(d) depicts the overall MetaDataRatio combining bytes
 for the inodes, the bytes contained in the Hook and Manifest,
 and the FileManifest files. The overall performance of the BF-
 MHD algorithm was the best among the algorithms compared.
 B. Trade-Off between Deduplication Efficiency and Dedupli-
 cation Overhead
 Fig. 8 shows the comparison of the trade-offs between
 the DER and the amount of metadata required as well as
 the reduced deduplication throughput. We can see from the
 figure, that BF-MHD achieved the best real DER. For the
 other algorithms, the peak DERs were reached under various
 conditions. It can be more clearly observed in Fig. 8(a),
 which shows that the maximal metadata required by the
 SparseIndexing, SubChunk, Bimodal and BF-MHD algorithms
 were about 3.8%, 1.7%, 1%, and 0.2% of the size of the input
 respectively.
 Smaller ECS values usually lead to more duplicate. How-
 ever, the real DER taking into account the metadata generated
 will peak when the amount of metadata generated equals to
 the duplication in data found. Fig. 8(a) and Fig. 8(b) show the
 data-only DER and the real DER as a function of the Meta-
 DataRatio. As expected, more metadata lead to better data-
 only DER for all the algorithms. However, when the amount
 of metadata is considered, the rapid growth of metadata of the
 Bimodal, SubChunk and SparseIndexing algorithms negated
 the increase in data-only DER, resulting in decreasing real
 DER as the metadata continues to grow. The Bi-Directional
 Match Extension and HHR process in MHD on the other hand,
 contributed to elimination of duplicate data inside big chunks
 using less metadata, and with new metadata generated only
 when duplicate data were found, making the introduction of
 extra metadata more “worthwhile”.
 Although SparseIndexing can find more duplicate chunks,
 395
0 1 2 3 4
 3.4
 3.5
 3.6
 3.7
 3.8
 3.9
 4
 4.1
 4.2
 MetaDataRatio (%)
 Data?only DE
 R
 BF?MHD
 Bimodal
 SubChunk
 SparseIndexing
 (a) Data-only DER vs. Metadata
 0 1 2 3 4
 3.4
 3.5
 3.6
 3.7
 3.8
 3.9
 4
 MetaDataRatio (%)
 Real DE
 R
 BF?MHD
 Bimodal
 SubChunk
 SparseIndexing
 (b) Real DER vs. Metadata
 0.2 0.25 0.3 0.35 0.4 0.45 0.5
 3.4
 3.5
 3.6
 3.7
 3.8
 3.9
 4
 4.1
 4.2
 ThroughputRatio
 Data?only DE
 R
 BF?MHD
 Bimodal
 SubChunk
 SparseIndexing
 (c) Data-only DER vs. Throughput
 0.2 0.25 0.3 0.35 0.4 0.45 0.5
 3.4
 3.5
 3.6
 3.7
 3.8
 3.9
 4
 ThroughputRatio
 Real DE
 R
 BF?MHD
 Bimodal
 SubChunk
 SparseIndexing
 (d) Real DER vs. Throughput
 Fig. 8. The comparison of trade-off between deduplication efficiency and deduplication overhead (SD=1000 hashes).
 because it does not regulate the the size of the Manifests, as
 more Manifests need to be reloaded into the RAM, and as the
 computational complexity related to computing the champions
 for each segment increases, the real DER achieved became
 lower than BF-MHD, even though the data-only DER for the
 two were similar (Fig. 8(c) and Fig. 8(d)).
 For the Bimodal algorithm, duplicate data inside the dedu-
 plicated big chunks that were not at the transition points will
 be missed. Similarly in SubChunk, when one small-chunk-to-
 container-chunk mapping was not hit, the duplicate data inside
 the big chunks covered by the mapping would be missed.
 As a result, for a given ThroughputRatio, both the Bimodal
 and SubChunk algorithms provided the worst DER with and
 without taking the metadata into account.
 Fig. 9 shows the performance of BF-MHD for different SD
 values. The DER of BF-MHD is determined by the value of
 ECS?SD. From Fig. 9(a), we can see smaller SD led to bet-
 ter trade-offs between the real DER and MetaDataRatio. This
 is because the growth rate of the metadata became very small
 as SD decreased, while the size of the duplicate data detected
 rapidly increased. Although smaller value of ECS?SD also
 resulted in more disk I/Os , overall the tradeoff between the
 real DER and the ThroughputRatio improved as SD decreased
 due to the much improved real DER, as seen in Fig. 9(b).
 TABLE III
 RAM USED FOR SPARE INDEX IN SPARSEINDEXING (SD=1000 HASHES).
 ECS (Bytes) 1024 2048 4096 8192
 RAM (KB) 106618 102638 101183 100132
 C. RAM
 TABLE III lists that the RAM space used for sparse index
 was about 0.01% of the input data size in our experiments,
 consistent with the results in the work of Lillibridge et al. [13].
 TABLE IV shows that the size for all the Hooks and Manifests
 in BF-MHD was between 0.007% to 0.02% of the input data
 size. If all Hooks and Manifests were stored in RAM, the
 RAM space for the bloom filter and the disk accessing time
 for reading the Manifest from disk into the RAM showed in
 TABLE V could be avoided. The BF-MHD algorithm requires
 less RAM, which also helps relieving the disk bottleneck.
 D. Characteristics of the test dataset
 The characteristics of the test data affects the deduplication
 speeds of different algorithms. The maximal data-only DER
 of our dataset is about 4.15 as detected by SparseIndexing.
 For a comprehensive understanding of the dataset we used,
 the DAD detected by BF-MHD versus ECS is plotted in Fig.
 396
0.24 0.26 0.28 0.3 0.32
 3.5
 3.6
 3.7
 3.8
 3.9
 4
 4.1
 MetaDataRatio (%)
 Real DE
 R
 BF?MHD?SD?1000
 BF?MHD?SD?500
 BF?MHD?SD?250
 (a) Real DER vs. Metadata
 0.25 0.3 0.35 0.4 0.45 0.5
 3.5
 3.6
 3.7
 3.8
 3.9
 4
 4.1
 ThroughputRatio
 Real DE
 R
 BF?MHD?SD?1000
 BF?MHD?SD?500
 BF?MHD?SD?250
 (b) Real DER vs. Throughput
 Fig. 9. BF-MHD performance at different SD values (SD= 1000, 500, and 250 hashes respectively).
 512 768 1024 2048 4096 8192
 0
 50
 100
 150
 200
 250
 ECS (Bytes)
 DAD (KB
 )
 DAD
 (a) DAD detected by BF-MHD vs. ECS (SD=1000 hashes).
 512 768 1024 2048 4096 8192
 0
 2
 4
 6
 8
 10
 x 106
 ECS (Bytes)
 HHR Cost and Number of Dup. Slic
 e
 HHR Cost
 Dup. Slice
 (b) The extra disk accessing times caused by HHR vs. The
 number of detected duplicate data slice (SD=1000 hashes).
 Fig. 10. Dataset characteristics and HHR cost statistics.
 TABLE IV
 BYTE SIZE FOR ALL THE HOOKS AND MANIFESTS IN BF-MHD.
 ECS (Bytes, SD=1000) 1024 2048 4096 8192
 Size (KB) 136512 98340 83940 72996
 ECS (Bytes, SD=500) 1024 2048 4096 8192
 Size (KB) 174883 112160 90199 75897
 ECS (Bytes, SD=250) 1024 2048 4096 8192
 Size (KB) 226151 139154 103162 82067
 TABLE V
 DISK ACCESSING TIMES FOR MANIFESTS LOADING IN BF-MHD.
 ECS (Bytes, SD=1000) 1024 2048 4096 8192
 Times (K) 4439 3289 3073 2985
 ECS (Bytes, SD=500) 1024 2048 4096 8192
 Times (K) 5024 3390 3094 2996
 ECS (Bytes, SD=250) 1024 2048 4096 8192
 Times (K) 5706 3530 3134 3009
 10(a), from which we can see the DAD of the dataset was
 between 90 bytes to 220 bytes. Smaller ECS helped to find
 shorter duplicate data slices, resulting in a smaller detected
 DAD. As analyzed previously, the performance of BF-MHD
 is related to the distribution of the duplications. The more
 concentrated the duplication distribution is, the fewer the disk
 accesses required by HHR. Even though the theoretical upper
 bound for the number of disk assesses brought by HHR in the
 worst case is 3L times, the actual disk accessing times caused
 by HHR for our test data as shown in Fig. 10(b) was much
 smaller than L, the number of duplicate data slices detected.
 VI. CONCLUSIONS
 In this paper, we propose an in-line metadata harness-
 ing deduplication scheme named MHD using hysteresis re-
 chunking. The SHM, Bi-Directional Match Extension and
 the HHR modules of the algorithm lead to better overall
 real deduplication ratio with lower metadata and I/O related
 overheads and better throughputs, as compared with other
 algorithms such as Bimodal, SubChunk and SparseIndexing.
 ACKNOWLEDGMENT
 We would like to thank Youyou Lu of Tsinghua University
 and our reviewers for their kindly, helpful feedback.
 397
REFERENCES
 [1] Sean Quinlan and Sean Dorward, “Venti: a new approach to archival
 storage,” 2002, FAST’08.
 [2] John Kubiatowicz, David Bindel, Yan Chen, Steven Czerwinski, Patrick
 Eaton, Dennis Geels, Ramakrishan Gummadi, Sean Rhea, Hakim Weath-
 erspoon, Westley Weimer, Chris Wells, and Ben Zhao, “Oceanstore: an
 architecture for global-scale persistent storage,” SIGPLAN Not., vol. 35,
 no. 11, pp. 190–201, Nov. 2000.
 [3] Kave Eshghi and Hsiu K. Tang, “A framework for analyzing and
 improving contentbased chunking algorithms,” in Tech. Rep. HPL-2005-
 30(R.1), Hewlett Packard Laboratories, Palo Alto, 2005.
 [4] Athicha Muthitacharoen, Benjie Chen, and David Mazie`res, “A low-
 bandwidth network file system,” SIGOPS Oper. Syst. Rev., vol. 35, no.
 5, pp. 174–187, Oct. 2001.
 [5] Jeffery C. Mogul, Yee Man Chan, and Terence Kelly, “Design,
 implementation, and evaluation of duplicate transfer detection in http,”
 Berkeley, CA, USA, 2004, NSDI’04, pp. 4–4, USENIX Association.
 [6] Fahad R. Dogar, Amar Phanishayee, Himabindu Pucha, Olatunji
 Ruwase, and David G. Andersen, “Ditto- a system for opportunistic
 caching in multi-hop wireless networks,” Mobicom, , no. SI, 2008.
 [7] Siddhartha Annapureddy, Michael J. Freedman, and David Mazie`res,
 “Shark: scaling file servers via cooperative caching,” Berkeley, CA,
 USA, 2005, NSDI’05, pp. 129–142, USENIX Association.
 [8] Cezary Dubnicki, Leszek Gryz, Lukasz Heldt, Michal Kaczmarczyk,
 Wojciech Kilian, Przemyslaw Strzelczak, Jerzy Szczepkowski, Cristian
 Ungureanu, and Michal Welnicki, “Hydrastor: a scalable secondary
 storage,” Berkeley, CA, USA, 2009, FAST ’09, pp. 197–210, USENIX
 Association.
 [9] Ashok Anand, Vyas Sekar, and Aditya Akella, “Smartre: an architecture
 for coordinated network-wide redundancy elimination,” SIGCOMM
 Comput. Commun. Rev., vol. 39, no. 4, pp. 87–98, Aug. 2009.
 [10] Cristian Ungureanu, Benjamin Atkin, Akshat Aranya, Salil Gokhale,
 Stephen Rago, Grzegorz Calkowski, Cezary Dubnicki, and Aniruddha
 Bohra, “Hydrafs: a high-throughput file system for the hydrastor content-
 addressable storage system,” 2010, FAST ’10, pp. 225–238.
 [11] Jiansheng Wei, Hong Jiang, Ke Zhou, and Dan Feng, “Mad2: A scal-
 able high-throughput exact deduplication approach for network backup
 services,” may 2010, pp. 1 –14.
 [12] Benjamin Zhu, Kai Li, and Hugo Patterson, “Avoiding the disk
 bottleneck in the data domain deduplication file system,” Berkeley, CA,
 USA, 2008, FAST’08, pp. 18:1–18:14, USENIX Association.
 [13] Mark Lillibridge, Kave Eshghi, Deepavali Bhagwat, Vinay Deolalikar,
 Greg Trezise, and Peter Camble, “Sparse indexing: large scale, inline
 deduplication using sampling and locality,” Berkeley, CA, USA, 2009,
 FAST ’09, pp. 111–123, USENIX Association.
 [14] Deepak R. Bobbarjung, Suresh Jagannathan, and Cezary Dubnicki,
 “Improving duplicate elimination in storage systems,” Trans. Storage,
 vol. 2, no. 4, pp. 424–448, Nov. 2006.
 [15] Erik Kruus, Cristian Ungureanu, and Cezary Dubnicki, “Bimodal
 content defined chunking for backup streams,” Berkeley, CA, USA,
 2010, FAST’10, pp. 18–18, USENIX Association.
 [16] Guanlin Lu, Yu Jin, and D.H.C. Du, “Frequency based chunking for
 data de-duplication,” in Modeling, Analysis Simulation of Computer
 and Telecommunication Systems (MASCOTS), 2010 IEEE International
 Symposium on, aug. 2010, pp. 287 –296.
 [17] Romanski Bartlomiej, Heldt Lukasz, Kilian Wojciech, Lichota
 Krzysztof, and Dubnicki Cezary, “Anchor-driven subchunk deduplica-
 tion,” New York, NY, USA, 2011, SYSTOR ’11, pp. 16:1–16:13, ACM.
 [18] Niraj Tolia, Michael Kozuch, Mahadev Satyanarayanan, Brad Karp,
 Thomas Bressoud, and Adrian Perrig, “Opportunistic use of content
 addressable storage for distributed file systems,” in In Proceedings of
 the 2003 USENIX Annual Technical Conference, 2003, pp. 127–140.
 [19] Michael O. Rabin, “Fingerprinting by random polynomials,” Tech.
 Rep., TR-15-81, Center for Research in Computing Technology, Harvard
 University, 1981.
 [20] Nohhyun Park and D.J. Lilja, “Characterizing datasets for data dedupli-
 cation in backup applications,” in Workload Characterization (IISWC),
 2010 IEEE International Symposium on, dec. 2010, pp. 1 –10.
 [21] W. Lee and C. Park, “An adaptive chunking method for personal data
 backup and sharing,” in Proceedings of the 8th USENIX conference on
 File and storage technologies post session, 2010, FAST’10.
 [22] D. Bhagwat, K. Eshghi, D.D.E. Long, and M. Lillibridge, “Extreme
 binning: Scalable, parallel deduplication for chunk-based file backup,”
 in Modeling, Analysis Simulation of Computer and Telecommunication
 Systems, 2009. MASCOTS ’09. IEEE International Symposium on, sept.
 2009, pp. 1 –9.
 [23] Andrei Broder and Michael Mitzenmacher, “Network applications of
 bloom filters: A survey,” in Internet Mathematics, 2002, pp. 636–646.
 [24] Andrew S. Tanenbaum, “Structured computer organization,” chapter
 4,THE MICROARCHITECTURE LEVEL. Prentice Hall, 5 edition
 (June 25, 2005).
 [25] Dirk Meister, Andre Brinkmann, and Tim S., “File recipe compression
 in data deduplication systems,” 2013, FAST’13.
 [26] Andrew S. Tanenbaum, “Modern operating systems,” chapter 4,MEM-
 ORY MANAGEMENT. Prentice Hall PTR, SECOND EDITION, 2001.
 398

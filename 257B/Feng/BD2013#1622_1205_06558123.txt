A Comparative Study of Enterprise and Open 
Source Big Data Analytical Tools 
 
1Udaigiri Chandrasekhar 2Amareswar Reddy3Rohan Rath 
School of Information Technology and Engineering  
VIT University  
Vellore, India  
1u.chandrasekhar@vit.ac.in  
2amareshnlr@gmail.com  
3rohanrath1105@gmail.com 
 
 
Abstract: In this paper, we bring forward a comparative study 
between the revolutionary enterprise big data analytical tools and 
the open source tools for the same. The Transaction Processing 
Council (TPC) has established a few benchmarks for measuring 
the potential of software and its use. We use similar benchmarks to 
study the tools under discussion. We try to cover as many different 
platforms for big data analytics and compare them based on 
computing environment, amount of data that can be processed, 
decision making capabilities, ease of use, energy and time 
consumed, and the pricing. 
 
Keywords: Big data, enterprise, open source, analytical tools, 
Hadoop, business intelligence, metadata, MapReduce, SQL, 
security, reliability. 
1 INTRODUCTION 
We are in a flood of data today. Statistics show that 90% of 
the world’s data was generated in the last two years itself, and 
it is growing exponentially. 
To tackle such data and process it, we need to leave the 
traditional batch processing behind and adopt the new big data 
analytical tools. The data generated everyday exceeds 2.5 
quintillion bytes which is a mind-boggling figure. 
The growth of data has affected all fields, whether it is the 
business sector or the world of science. To process such huge 
amounts of data various new tools are being introduced by 
companies like Oracle and IBM, while on the other handle 
Open Source Developers continue their work in the same 
field. 
1.1 What is Big data? 
Big Data is the massive amounts of data that collect with 
time and are difficult to analyze using the traditional database 
system tools. Big Data includes business transactions, photos, 
surveillance videos and activity logs. Scientific data from 
sensors can reach massive proportions over time, and Big 
Data also includes unstructured text posted on the Web, such 
as blogs and social media. 
1.2 Managing and Analyzing Big Data 
For the past two decades most business statistics have been 
created using structured data produced from functional 
techniques and combined into a information factory or data 
warehouse. Big data significantly improves both the number 
of information resources and the variety and number of 
information that is useful for research. A significant number 
of this data is often described as multi-structured to 
differentiate it from the arranged functional data used to fill a 
data warehouse. In most companies, multi-structured data is 
growing quicker than structured data. 
Two important information management styles for handling 
big data are relational DBMS products enhanced for 
systematic workloads (often known as analytic RDBMSs, or 
ADBMSs) and non-relational techniques (sometimes known 
as NoSQL systems) for handling multi-structured data. A non-
 relational system can be used to produce statistics from big 
data, or to preprocess big information before it is combined 
into a data warehouse. 
1.3 Need for Big Data Analysis 
When a business can make use of all the information 
available with large data rather than just a part of its details 
then it has a highly effective benefit over the market 
opponents. Big Data can help to gain ideas and make better 
choices.   
Big Data provides an opportunity to create unmatched 
company benefits and better service distribution. It also needs 
new facilities and a new way of thinking about the way 
company and IT market works. The idea of Big Data is going 
to change the way we do things today.  
The International Data Corporation (IDC) research forecasts 
that overall details will develop by 50 times by 2020, 
motivated mainly by more included systems such as receptors 
in outfits, medical gadgets and components like structures and 
connects. The research also identified that unstructured details 
- such as data files, email and video - will account for 90% of 
Proceedings of 2013 IEEE Conference on Information and Communication Technologies (ICT 2013)
 978-1-4673-5758-6/13/$31.00 © 2013 IEEE 372
all details designed over the next several years. But the 
number of IT experts available to handle all that details will 
only develop by 1.5 times the present levels. 
The electronic galaxy is 1.8 billion gigabytes in dimension 
and saved in 500 quadrillion data files. And its dimension gets 
more than dual in every two years’ period of time. If we 
evaluate the electronic universe with our actual universe then 
it's nearly as many pieces of details in the electronic universe 
as stars in our actual universe. 
1.4 Characteristics of Big Data 
A Big Data platform should give a solution which is 
designed specifically with the needs of the enterprise in the 
mind. The following are the basic features of a Big Data 
offering-  
• Comprehensive - It should offer a wide foundation and 
address all three size of the Big Data task -Volume, Variety 
and Velocity.   
• Enterprise-ready - It should include the performance, 
security, usability and reliability features.     
• Incorporated - It should easily simplify and speeds up the 
release of Big Technological innovation to business. 
• Open Source based - It should be start resource technology 
with the enterprise-class overall performance and 
incorporation.  
• Low latency flows and updates 
• Solid and fault-tolerant 
• Scalability 
• Extensible 
• Allows adhoc queries 
• Little maintenance 
2 ENTERPRISE BIG DATA ANALYTICAL TOOLS 
VERSUS OPEN SOURCE BIG DATA ANALYTICAL 
TOOLS 
Ever since the research on big data started, companies like 
IBM, Google and Oracle have been leading the race of 
enterprise analytical tools and have been occupying the major 
section of the market. Little is known about the new age 
analytical tools that have been developed recently and are 
spreading faster than expected. In this paper, we bring to light 
the working characteristics of many such tools for big data. 
Keeping aside all the expensive and closed source 
applications, we also strive to explain the working and 
advantages of open source analytical tools, which are as good 
as their counterparts in the enterprise world. We explain 
where these can be used and what are the issues involved in 
the same.  
2.1 Enterprise Analytical Tool 
2.1.1 Pentaho 
Pentaho is a very useful tool to visualize high volumes of 
data and analyze it to draw conclusions. It supports all the 
right set of tools for the entire processing lifecycle of big data. 
It provides exploration and visualization features for the 
business sectors; also performs predictive analysis.  
It provides a 15 times boost to scripting and coding. It is 
very useful for interactive reports, time series forecasting, 
statistical learning, evaluation and visualization of predictive 
models. It supports Predictive Modeling Markup Language. 
It provides visual tools that define instant access to data. It is 
built on a modern high performing, lightweight platform. This 
platform fully works on a modern 64 bit multi-core processor 
and harnesses the power of new-age hardware. Pentaho is 
unique in leveraging external data grid technologies such as 
Infinispan and Memcached to load vast amounts of data into 
memory.  
The Instaview feature on Pentaho enables us to instantly 
view the reports generated by careful analysis of the data in a 
multi-dimensional and interactive format. 
 
 
Fig1. Instaview feature on Pentaho 
 
The problem is with the pricing. During the survey, it was 
found that a pricing request needs to be sent to Pentaho Big 
data Analytics in order to view the quotation. But reviews 
show that it is priced at a much lesser price as compared to 
other commercial BI tools.  
2.1.2 TerraEchos 
TerraEchos, a world leader in innovative security 
alternatives utilizing Big Data in Movement statistics, was 
known as the champion of a 2012 as it received the IBM 
Beacon Award for Outstanding Information Management 
Innovation. 
TerraEchos, a next-generation big-data statistics company, 
has implemented the first foundation to blend and narrow 
large volumes of live complicated structured and unstructured 
data on the fly — and at the same time to draw out, evaluate, 
and act upon the data, in the moment. 
Unlike database-centric techniques to high-speed, high-
 volume data research, the TerraEchosKairos foundation is not 
restricted by the amount, type, or rate of the data: It 
consistently examines data while it is still moving, without 
requiring storing it. Based on a streaming operating system, 
and with statistics and creation segments updated to specific 
Proceedings of 2013 IEEE Conference on Information and Communication Technologies (ICT 2013)
 978-1-4673-5758-6/13/$31.00 © 2013 IEEE 373
programs, the TerraEchosKairos foundati
 protection, intellect, facilities, and growing
 other marketplaces where understanding an
 now is crucial. 
 
Fig2. Annual Lobbying by TerraE
 2.1.3 Cognos 
Cognos is known for business intel
 performance and strategy management as w
 to work for analytics applications. IBM's C
 known to provide an organization with wha
 become top-performing and analytics driv
 the individual, workgroup, division, mids
 large business, Cognos application has b
 ensure that companies make better choices 
development. This tool is for those who w
 company's intellect or performance.  
Cognos solutions are designed to help 
decisions and manage performance for o
 results. Combine your financial and opera
 single, seamless source of information in t
 your choice for transparent and timely repo
 action. 
IBM offers a full range of customer sup
 certification, and services for Cognos an
 Analytics customers. 
 
Fig3. Query Analyser for Cog
  
An investment of over a lakh in Indian cur
 for the effective use of Cognos. It seems leg
 to invest in it, but for personal usage it may 
the general public. 
 
on is perfect for 
 professional, and 
d performing right 
 
chosInc 
ligence, financial 
ell. It also extends 
ognos Software is 
tever they need to 
en.With items for 
ize company and 
een developed to 
for their future and 
ish to grow their 
you make smarter 
ptimized business 
tional data into a 
he environment of 
rting, analysis and 
port, training and 
d other Business 
 
nos 
rency is needed 
it for a company 
not be possible for 
2.1.4 Attivio 
The AIE or Active Intelligenc
 huge impact on the business
 customers' information assets. 
the situation and help them in w
 helps to fulfil a strategic goal wh
 AIE has the capabilities to 
platforms and brings together bo
 data content for an efficient an
 power to incorporate and rela
 content silos - without any 
requirement. It has a very a
 search capability for BI which
 analytical tools. 
It offers both intuitive search
 powers, making structured and 
in ways never been thought of. 
of big data and can be useful f
 varied technical skills and priori
 2.1.5 Google BigQuery 
Google BigQuery allows y
 against very large datasets, wit
 This can be your own data, o
 shared for you. BigQuery work
 of very large datasets, typically
 large, append-only tables.  
It’s a high speed tool which c
 seconds. It can handle trillio
 terabytes of data. It’s simplicit
 close reference to SQL. It ha
 groups and user based Google 
secure SSL access method. It 
through BigQuery browser, the
 Script. 
It has a very distinguished pri
 storage, interactive queries and 
(per GB/month), $0.035 (per G
 processed) respectively. 
They all have default limit
 example is a fine example of a 
and optimized for processing 
analytics. 
 
Fig4. Workplace for
 e Engine of Attivio has had a 
 through the availability of 
It helps in a quick analysis of 
hatever ways they need it. It 
ich has been established. 
analyse anything on many 
th structured and unstructured 
d agile BI. It is equipped with 
te all the available data and 
advance modelling of data 
dvance intuitive Google like 
 incorporates all the needed 
 capabilities along with SQL 
unstructured data more useful 
It has great insight in the field 
or all kinds of users and their 
ties. 
ou to run SQL-like queries 
h potentially billions of rows. 
r data that someone else has 
s best for interactive analysis 
 using a small number of very 
an analyse billions of rows in 
ns of records pertaining to 
y stands out as it works with 
s a powerful sharing through 
Accounts. It works on a very 
has multiple access methods 
 REST API or Google Apps 
cing range which varies as per 
batch queries which are $0.12 
B processed), $0.02 (per GB 
s assigned too.This Netezza 
system that has been designed 
a specific workload: business 
 
 Google BigQuery 
Proceedings of 2013 IEEE Conference on Information and Communication Technologies (ICT 2013)
 978-1-4673-5758-6/13/$31.00 © 2013 IEEE 374
2.1.6 Netezza 
A revolutionary data analytical tool was e
 and was named Netezza. It is an advanced
 warehouse engine that incorporates a dat
 storage components into a single environme
 run predictive analysis, business intelli
 applications needed in every field. 
It is based on IBM blade architecture
 processors; an Asymmetric Massively P
 (AMPP) approach to process workloads; a
 (field programmable gate arrays — speciali
 a means to filter data before it is process
 used to speed the process of queries and p
 which preprocesses. In this, computati
 processed by FPGAs as opposed to making
 the work.   
The Netezza design has led to an expone
 field of big data analytics and is very u
 commodity or specialty environment.  
It is not only ideal for processing comp
 queries such as those found when performin
 It preprocesses data, and then feeds the C
 fashion. Further, it is not burdened by
 structures and online transaction proces
 resulting in a simple code path for faster per
 2.2 Open Source Analytical Tools 
2.2.1 Apache Hadoop 
Licensed under Apache v2, Apache Ha
 source system framework that not only r
 platform using data intensive processing. F
 it supports running on large clusters. Suppo
 and provides security and reliability for data
 its own Hadoop computation paradigm c
 where in the work is divided into vario
 processed on a clustered system or a grid. 
It has a capacity to handle petabytes of d
 files within seconds and provides a very h
 computational processing. 
Cloudera is the leader in Apache Hado
 and services and offers a powerful new 
enables enterprises and organizations to lo
 — structured as well as unstructured. 
Hadoop is written in the Java programmin
 a top-level Apache project being built and
 community of contributors Hadoop. 
stablished by IBM, 
 high performance 
abase, server and 
nt. This is used to 
gence and many 
 which uses x86 
arallel Processing 
nd it uses FPGAs 
zed processors) as 
ed. This FPGA is 
erform a function 
onal kernels are 
 the CPU do all of 
ntial growth in the 
seful as a hybrid 
lex, scanning type 
g deep analytics.  
PU in a balanced 
 legacy database 
sing features — 
formance. 
doop is an open 
uns on distributed 
or commodity use, 
rting data mobility 
 processing. It has 
alled MapReduce, 
us units and then 
ata in millions of 
igh bandwidth of 
op-based software 
data platform that 
ok at all their data 
g language and is 
 used by a global 
Fig5. Work Flow o
 2.2.2 Zettaset 
One of the most flexible op
 works on any Apache Hadoop d
 availability and security of the d
 system and is very cost effec
 second name. 
Zettaset has created Orchestr
 an enterprise management too
 issues of Hadoop deployment w
 are sophisticated tools. 
It has the capacity to automa
 installation of Hadoop on a clus
 is ready for enterprise usage
 Orchestrator is that it is not bas
 is a very secure open system. 
2.2.3 HPCC Systems 
Abbreviated as HPCC, Hi
 Cluster, as the name suggests is
 It was developed by LexisNexis
 versions to this tool, paid as w
 structured and unstructured c
 performing scalability from 
processing makes it even strong
 It is commercially available a
 tool so easily available to the m
 selecting this tool is a platform
 includes a highly integrated
 capabilities from raw data pr
 queries and data analysis using 
Working as an optimized clu
 high performing system resultin
 of Ownership) along with secu
 with a very good processing sp
 centric programming language 
programmer productivity for d
 this platform.  
It has a good tolerance for 
processing in case of system fa
 warehouses, and high volume o
 security analysis of massive am
  
f Apache Hadoop 
en source analytical tools, it 
istribution. Its features include 
ata. It is easy to deploy on any 
tive. Simplicity is Zettaset’s 
ator, software solution that is 
l that addresses the common 
ith easy-to-use interfaces that 
te, simplify and accelerate the 
ter management system which 
. The outstanding feature of 
ed on Hadoop distribution but 
gh Performance Computing 
 a clustered computing system. 
 Risk Solutions. There are two 
ell as free and both work on 
ontent data. It has a high 
1-1000s of nodes. Parallel 
er a tool. 
nd offers a lot of features for a 
asses. A major advantage of 
 for data-intensive computing 
 system environment with 
ocessing to high-performance 
a common language.  
ster, it is a very low costing 
g a very low TCO (Total Cost 
rity, scalability and reliability 
eed. It has an innovative data-
 incorporated which increases 
evelopment of applications on 
faults and capabilities for re-
 ilures. It can also manage data 
nline applications to network 
ounts of log information. 
Proceedings of 2013 IEEE Conference on Information and Communication Technologies (ICT 2013)
 978-1-4673-5758-6/13/$31.00 © 2013 IEEE 375
2.2.4 Dremel 
An interactive ad-hoc query system, it was developed by 
Google to offer analysis of nested readable data. Scalable to 
extremes; up to 1000s of computer systems and petabytes of 
data; it has the power to process trillions of rows together in 
just a matter of seconds by multi-level execution of trees and 
columns. It is not meant for replacing the old system, MR and 
is often used for analysis of crawled web documents, tracking 
install data for applications on Android Market and also for 
crash reporting for Google products. 
Other important uses are spam analysis, debugging of map 
tiles on Google Maps, tablet migrations in managed Bigtable 
instances. 
2.2.5 Greenplum HD 
Greenplum HD allows customers to start with big data 
statistics without the need to develop an entire new venture. It 
is provided as application or can be used in a pre-configured 
Data Handling Equipment Component. Greenplum HD is a 
100 percent open-source qualified and reinforced edition of 
the Apache Hadoop collection that contains HDFS, 
MapReduce, Hive, Pig, Hbase and Zookeeper. IT prevails of a 
finish information research foundation and it brings together 
Hadoop and Greenplum data resource into only one Data 
Handling Equipment.  Available as application or in a pre-
 configured Data Handling Equipment Component, Greenplum 
HD provides a finish foundation, such as set up, training, 
international support, and value-add beyond simple 
appearance of the Apache Hadoop submission. Greenplum 
HD makes Hadoop quicker, more reliable, and easier to use. 
Greenplum HD facilitates Isilon’sOneFS Scale-Out NAS 
Storage space for Hadoop. EMC Isilon scale-out NAS is the 
first and only Business NAS remedy that can natively include 
with the Hadoop Allocated Data file System (HDFS) part. By 
dealing with HDFS as an over the cable method, you can 
quickly set up a extensive big data statistics remedy that 
brings together Greenplum HD with Isilon scale-out NAS 
storage systems to provide a very effective, extremely 
effective and versatile information storage and statistics 
environment. 
The Greenplum HD DCA Component easily combines the 
Greenplum HD application into a product, offering an 
enhanced setting designed for performance and stability. The 
Greenplum Data Handling Equipment marries the 
unstructured batch-processing power of Hadoop with the 
Greenplum Database and the cutting-edge Extremely Similar 
Handling (MPP) structure. This allows businesses to draw out 
value from both arranged and unstructured data under only 
one, smooth foundation. 
2.2.6 HortonWorks 
Hortonworks is an authentic and free Hadoop Distribution 
system. It is developed on top of Hadoop and it allows clients 
to capture process and perform together at any broad variety 
and in any framework in a simple and cost-effective way. 
Apache Hadoop is a key of the Hortonworks framework.  It is 
ideal for organizations that want to combine the power and 
cost-effectiveness of Apache Hadoop with the amazing 
alternatives and balance required for organization 
deployments. 
Hortonworks is the latest organization of Hadoop system 
and expert support, but it's an old element when it comes to 
working with the platform. The organization is a 2011 spinoff 
of Search engines, which remains one of the greatest clients of 
Hadoop. Actually, Hadoop was usually developed at Search 
engines, and Hortonworks managed an extensive broad 
variety of nearly 50 of its very first and most well-known 
associates to Hadoop. 
There’s differentiator between Hortonworks and the other 
suppliers. Hortonworks products are 100% Begin Source and 
are free contrary to some of and Cloudera’s Company 
amazing and/or value-adding Hadoop products, which are not. 
2.2.7 ParAccel 
The open source tool, ParAccel data analytics platform has 
been used by organizations for its interactive capabilities to 
analyze big data in an enhanced fashion. It offers a high 
storage along with compression of adaptive capabilities. In-
 memory processing and compilation on the fly is also 
important, making it easy to work with and adapt to. 
 
 
Fig6. ParAccel Query Analyzer 
2.2.8 GridGrain 
An enterprise open source system, GridGain, as the name 
suggests is for grid computing. This was specially made for 
Java and is compatible with Hadoop DFS and offers an 
alternative for Hadoop's MapReduce. It offers a distributed, 
in-memory and scalable data grid, which is the link between 
data sources and different applications. An open source 
version is available on Github or a commercial version can be 
downloaded from their homepage. 
Proceedings of 2013 IEEE Conference on Information and Communication Technologies (ICT 2013)
 978-1-4673-5758-6/13/$31.00 © 2013 IEEE 376
3 COMPARISON OF SECURITY BETWEEN OPEN 
SOURCE AND ENTERPRISE TOOLS 
Although you can take an open source project, compare it 
against a closed source project, and say that one is more 
secure than the other based on some number of observations 
or measurements; this determination will probably be based 
on factors other than the nature of the project's open or closed 
source code. Secure design, source code auditing, quality 
developers, design process, and other factors, all play into the 
security of a project, and none of these are directly related to a 
project being open or closed source. 
It is shocking to see the vulnerabilities in some closed 
source systems.  And although it certainly wouldn't mean 
open source software is quantitatively "more secure" than 
closed source software, it just means that there is a doubt in 
the source code auditing principles and otherwise the general 
security practices of certain closed source operating system 
vendors. However, the issue here isn't specifically related to 
the operating system being open or closed source, but to the 
processes with which the vendor approaches security. 
Although this might seem to imply that open source projects 
are going to have less vulnerabilities than closed source 
projects, that's not really the case either; the number of 
vulnerabilities present in a given system can’t be simply 
associated with the openness of its source code. Ultimately, 
it's about the way the project and its developers handle and 
integrate security. 
4 SELECTING THE RIGHT TOOLS FOR DATA 
ANALYTICS 
 
The factors discussed in the paper have a significant impact 
on technology selection.Organizations are not ready to make 
risky investment strategies in expensive alternatives just in 
case there is something more to be discovered. This is where 
multiple alternatives come into play. Existing exclusive – and 
generally expensive – storage space and data resource 
alternatives are being formulated by some of the more cost-
 effective growing technology, generally from the Apache 
Hadoop atmosphere. Initial discovery and research of large 
information amounts, where the "nuggets" are well invisible, 
can be performed in a Hadoop atmosphere.  
Once the "nuggets" have been discovered and produced, a 
decreased and more organized information set can then be fed 
into a current information factory or statistics system. 
From that viewpoint, it makes overall sense for providers of 
current storage space, data resource, and information 
warehousing and statistics software to provide connections 
and APIs to Hadoop alternatives. And also put together 
incorporated promotions that work with both the exclusive 
and free components. While some of them hurry to accept 
Hadoop, there is no evidence that it is a sensible and suitable 
move. As already described, many of the new big data 
technology are not ready for popular business utilization, and 
organizations without the IT abilities of the trailblazers or 
common early adopters will welcome the support from 
recognized providers. 
5 CONCLUSION 
To conclude, after the analysis of both closed and open 
source Big Data Tools, it is pretty evident that it's all about the 
usage and needs of an individual or the company. It is 
impossible to afford a few tools at a personal level because of 
the prices and complications, while using open source systems 
might pose an outdating and modifications problem. There is 
also the security issues involved in choosing the tool. Open 
source promotes development and innovation and supports 
developers. 
Big data is on every CIO’s mind and for good reasons 
companies have spent more than $4 billion on big data 
technologies in the year 2012.  
These investments will in turn trigger a domino effect of 
upgrades and new initiatives that are valued for $34 billion for 
2013.  
 
References 
 
[1] Colin White, BI Research, January 2012:MapReduce and the Data 
Scientist. 
[2] Anthony M. Middleton, Ph.D. LexisNexis Risk Solutions  
Date: May 24, 2011HPCC Systems:Introduction to HPCC 
[3] US Department of Energy Labs, and TerraEchos, Inc. 
www.terraechos.com 
[4] hhtp://workloadoptimization.com 
[5] IBM Systems and Technology Group 
[6] (Solution Brief; TerraEchosKairos on IBM 
PowerLinux servers) 
[7] http://www.paraccel.com/technology/paraccel-analytic-
 platform.php 
[8] http://www.greenplum.com 
[9] www.ibm.com/software/analytics/cognos 
[10] http://www.pentaho.com/ 
[11] http://community.pentaho.com/ 
[12] http://www.cloudera.com 
[13] Google, Inc.: Dremel Interactive Analysis of WebScale Datasets 
Sergey Melnik, AndreyGubarev, Jing Jing Long, Geoffrey 
Romer,Shiva Shivakumar, Matt Tolton, Theo Vassilakis 
[14] http://forum.gridgain.com/index.html 
[15] http://hadoop.apache.org/ 
[16] http://incubator.apache.org/drill/ 
[17] http://www.zettaset.com/ 
[18] http://hortonworks.com/ 
Proceedings of 2013 IEEE Conference on Information and Communication Technologies (ICT 2013)
 978-1-4673-5758-6/13/$31.00 © 2013 IEEE 377

A Study on Classification in Imbalanced and
 Partially-Labelled Data Streams
 R. J. Lyon, J. M. Brooke, J. D. Knowles
 School of Computer Science
 University of Manchester
 Manchester, UK
 B. W. Stappers
 Jodrell Bank Centre for Astrophysics
 University of Manchester
 Manchester, UK
 Abstract—The domain of radio astronomy is currently facing
 significant computational challenges, foremost amongst which
 are those posed by the development of the world’s largest
 radio telescope, the Square Kilometre Array (SKA). Preliminary
 specifications for this instrument suggest that the final design will
 incorporate between 2000 and 3000 individual 15 metre receiving
 dishes, which together can be expected to produce a data rate
 of many TB/s. Given such a high data rate, it becomes crucial
 to consider how this information will be processed and stored
 to maximise its scientific utility. In this paper, we consider one
 possible data processing scenario for the SKA, for the purposes
 of an all-sky pulsar survey. In particular we treat the selection
 of promising signals from the SKA processing pipeline as a
 data stream classification problem. We consider the feasibility of
 classifying signals that arrive via an unlabelled and heavily class
 imbalanced data stream, using currently available algorithms and
 frameworks. Our results indicate that existing stream learners
 exhibit unacceptably low recall on real astronomical data when
 used in standard configuration; however, good false positive
 performance and comparable accuracy to static learners, suggests
 they have definite potential as an on-line solution to this particular
 big data challenge.
 Index Terms—Data Streams, Classification, Imbalanced Learn-
 ing, Unlabelled Data, Astroinformatics
 I. INTRODUCTION
 Data streams have arisen naturally from the ever increasing
 volumes of data being generated by modern computational
 systems. Typically rapidly generated, temporally ordered, and
 infeasible to store in their entirety, data streams pose a sig-
 nificant challenge to those seeking to unlock the knowledge
 they contain. In recent years, considerable research effort has
 been expended towards identifying and solving these problems,
 leading to the development of many effective data stream
 learners. However the main focus of this work has been
 upon learning from streams that possess a reasonably balanced
 class distribution, and completely labelled or in some cases
 partially labelled data. In situations where the class balance
 is heavily skewed, or where it is unrealistic to expect more
 than a small fraction of the stream to be labelled, we face
 a new set of challenges that undermine the effectiveness of
 existing approaches. In this paper, the results of an empirical
 investigation are presented which demonstrate how the per-
 formance of Hoeffding bound based data stream classifiers,
 can degrade when faced with an increasingly imbalanced data
 stream. Motivated by a real world problem faced by the radio
 astronomy community, this paper seeks to add to data stream
 research efforts, by drawing increased attention to a learning
 scenario that is likely to become commonplace as data streams
 appear in ever more diverse domains.
 II. MOTIVATION
 Radio Pulsars are extremely dense, rapidly rotating stellar
 remnants formed during the collapse of massive stars. They
 are a somewhat rare phenomena, which happen to provide
 a unique environment in which to perform numerous astro-
 physical experiments [1]. These scientific opportunities have
 motivated the search for pulsars using large radio telescopes.
 In order to find pulsars, the signals arriving at the receiver
 of a radio telescope must be fed into a computational pipeline,
 designed to search for periodic broadband radio emission.
 Those periodic signals possessing a signal-to-noise ratio above
 a predefined threshold value (based on domain knowledge), are
 considered to be pulsar ‘candidates’ worthy of further investi-
 gation. The search pipeline itself is made up of many process-
 ing components. Broadly speaking these can be separated into
 those that act as data processors which clean and correct the
 data (e.g. excision of radio frequency interference) and search
 routines tasked with isolating signals of interest. Crucially the
 volume of data moving through such search pipelines has
 been increasing steadily for some time. Indeed as observed
 in [2], pulsar survey data capture rates have exhibited a level
 of growth closely describable by Moore’s Law. This growth
 reflects advances in technology that have enabled various
 improvements in survey specifications, from increased survey
 sampling rates to finer frequency resolution. However there is
 an inverse relationship between the sampling rate of a pulsar
 survey and the data capture rate. As the sampling period
 decreases (thereby increasing sensitivity to shorter period
 emissions) the data capture rate increases. When coupled with
 finer frequency resolution and wider observational bandwidths,
 this relationship creates successive data storage and processing
 challenges. Similarly an inverse relation exists between the
 sampling rate of a survey, and the number of signals meeting
 the candidate selection criteria. As the majority of signals
 captured at the receiver of a telescope are attributable to noise
 2013 IEEE International Conference on Systems, Man, and Cybernetics
 978-1-4799-0652-9/13 $31.00 © 2013 IEEE
 DOI 
1506
 DOI 10.1109/SMC.2013.260
or interference; nearly the entire set of selected candidates are
 of no scientific value. Separating those candidates which are
 likely to lead to discovery from those which are spurious is
 therefore becoming increasingly difficult to do.
 A. The Square Kilometre Array
 The next generation of radio telescope, the Square Kilo-
 metre Array (SKA) [3] is currently under development by an
 international consortium. Due to begin science operations in
 the next decade, the SKA will produce data at an unprece-
 dented rate, firmly ushering in the era of exascale computing.
 For pulsar survey operations alone the data rate is predicted
 to be between 0.43 ? 1.45 TB/s [4]. This is many orders
 of magnitude greater than previous surveys. Using existing
 technology, it is practically impossible to acquire the hardware
 and supporting infrastructure required for storing all this data
 permanently due to financial restrictions. Hence future SKA
 surveys must either a) utilise technological advances that make
 it feasible to store such large quantities of data at low cost, or b)
 process the data in real-time, storing only a fraction of the data
 for off-line analysis. Although one cannot say with certainty,
 it appears that real-time analysis is the only tractable and cost
 effective long term solution to this problem [4]. If this is indeed
 the case, then in the future candidate selection will have to
 be performed on-line, and in real-time. In the absence of an
 existing real-time selection system or the means to deal with
 the impending increase in candidates; the possibility exists that
 important discoveries could be delayed or possibly missed due
 to inadequate supporting computational infrastructure. Given
 that present day searches for radio transients1 are already being
 conducted in real time [5], it seems likely that the real-time
 scenario will win out.
 III. PROBLEM DEFINITION
 We consider the selection of candidate pulsars as a two-class
 data stream classification problem. We define the completely
 unlabelled input stream S = {(xi), ..., (xn), ...}, i = 1, ..., n
 as the stream of candidates emerging from a pulsar search
 pipeline, under a discrete time model. Each candidate in the
 stream xi ? X is defined as xi = {(y1i ), ..., (ymi )}, where
 each yji ? R for j = 1, ..., m is a summary statistic or
 measure that describes the candidate xi. We specify binary
 candidate labels L = {0, 1}, where l is an individual label
 such that l ? L (i.e. uninteresting = 0, interesting = 1). Our
 goal is to learn a function f : X ? L which maps each
 candidate to its correct label producing the set of labelled
 candidates C = {(xi, f(xi)), ..., (xn, f(xn))}. From C the set
 of positively labelled candidates (those instances with l = 1)
 can be obtained, which should be recommended for inspection.
 As the stream is completely unlabelled, only those candidates
 selected are ever likely to receive their correct labels (with the
 remainder necessarily discarded). Even then only a fraction
 of these may receive labels given the candidate volume, and
 certainly only after a substantial delay. Thus this scenario
 1Typically transients are short non-repeating bursts of high energy radio
 emission.
 can be thought of in terms of two distinct streams. The
 first from the telescope, the second a biased feedback stream
 containing data labelled by experts. Crucially the primary
 stream emerging from the telescope will be heavily imbalanced
 in favour of the negative class. The Parkes Multibeam Pulsar
 Survey (PMPS) for example has so far yielded 742 pulsar
 discoveries [6], from ? 8 million candidates [7]; giving rise
 to a class balance of roughly +1 : -11,000. If we make the
 conservative assumption that the class balance of the PMPS is
 maintained by the SKA, then we can expect to collect some
 ?200,000,000 negative candidates (given that current estimates
 suggest the SKA will detect approximately 20,000 new pulsars
 in total [4] ). A hypothetical classifier with perfect recall (1)
 and an accuracy (2) of .99 applied to these SKA candidates,
 would still select 2 million for further analysis.
 Recall = True Positives
 True Positives + False Negatives (1)
 Accuracy = True Positives + True NegativesPositives + Negatives (2)
 Precision = True Positives
 True Positives + False Positives (3)
 False Positive Rate = False Positives
 False Positives + True Negatives (4)
 This is far too many for an expert to analyse, and infeasible
 to follow up on given the costs associated with telescope
 time. An ideal classifier must therefore maintain a high rate
 of recall (as missing positive instances is acutely costly given
 their rarity) and significantly reduce the false positive rate (4).
 We summarise the five key data mining challenges for our
 candidate classifier as follows:
 1) Imbalanced class distribution: the ratio of positive to
 negative instances will be greatly imbalanced.
 2) Nonstationarity : Although the negative class is com-
 prised primarily of noise, various types of interference
 will be subject to gradual changes over long time scales,
 and abrupt changes over short ones.
 3) Unlabelled data: the data in the stream is completely
 unlabelled. Drift detection must be performed on un-
 labelled data, without the prospect of obtaining correct
 labels within a reasonable time.
 4) Real-time processing: The SKA will produce a high
 throughput data stream. It is unrealistic for much more
 than a small sample of this data to be stored, hence
 processing must likely be done in real-time.
 IV. RELATED WORK
 A. Candidate Selection
 Candidate selection has typically involved the expert se-
 lection of candidate signals via some form of summary user
 interface. During the Swinburne Intermediate Latitude Pulsar
 Survey [8] and PMPS [9], graphical tools were developed
 which allowed large numbers of candidates to be filtered. A
 later reprocessing of the PMPS spawned the more sophisticated
 REAPER tool, which enabled candidates to be viewed via
 a customizable graphical plot [10] ultimately leading to the
 1507
discovery of 128 pulsars. A new version of this tool called
 JREAPER was then developed and led to the discovery of 28
 pulsars [11]. More recently machine learning techniques have
 been used to filter candidates. In [7] a multi-layered perceptron
 (MLP) was used to perform an automated re-analysis of a
 data sample taken from the PMPS. The implementation was
 capable of recalling up to 93% of the pulsars present in PMPS
 test samples. Other methods utilised include Gaussian Mixture
 models as applied in [12], to rank pulsar candidates from the
 Fermi 2FGL catalogue.
 B. Imbalanced & Unlabelled Streams
 The problem of class imbalance is usually tackled by either
 assigning different costs to training examples thereby reweight-
 ing the balance, or by resampling the original dataset to achieve
 a desired balance. Resampling can be done in many ways,
 either by over-sampling the minority class or under-sampling
 the majority class [13]. The SMOTE algorithm developed by
 Chawla et al. [13] mixes both of these approaches. Empirical
 results suggest that SMOTE can successfully improve the
 accuracy of classifiers such as C4.5 and Naı¨ve Bayes on the
 minority class. In [14] Chen et al. develop their recursive
 weighted ensemble approach (REA) for classifying nonstation-
 ary imbalanced data streams. REA adaptively pushes minority
 class examples into the current data chunk to balance the class
 distribution. An in-depth discussion of the problems associated
 with learning from imbalanced data can be found in [15].
 In terms of unlabelled streams, the Semi-supervised learning
 paradigm [16] has typically been applied when large data
 sets are continually produced or when the labelling of the
 data is prohibitively costly [17]. Some representative works
 in this area include: the decision tree based algorithm for
 classifying concept drifting and unlabelled data streams called
 SUN developed in [18], [19]; the relational K-means based
 transfer semi-supervised SVM (RK-TS3VM) developed for
 classifying unlabelled drifting data streams [20], and the CSL-
 Stream algorithm (Concurrent Learning of Data Streams) that
 clusters and classifies data at the same time [21].
 V. EXPERIMENTS
 Amongst the most successful of the single model data
 stream classifiers has been the very fast decision tree (VFDT)
 [22]. VFDT is an incremental any time decision tree learner
 that uses constant memory, and permits model updates in
 time proportional to the tree depth and data dimensionality.
 VFDTs low runtime and space complexity therefore make it
 ideal for use on data streams, particularly as VFDT’s can be
 dynamically pruned if required to operate under strict memory
 constraints. The output of a VFDT is also guaranteed to be
 asymptotically similar to a conventional learning algorithm.
 VFDT achieves this by using the Hoeffding bound (5) to
 choose with a high probability, those split attributes that would
 have been selected given access to all the data as in the non-
 streamed scenario. By calculating the observed mean r¯ of an
 attribute, the bound is able to determine with confidence 1? ?
 Dataset Instances Attributes / Type Balance
 Pulsar 11,219,848 22 / Continuous, numerical +1 : -7500
 Skin 245,057 3 / Discrete, numerical +1 : -5
 MiniBoone 130,065 50 / Continuous, numerical +2 : -5
 Magic 19,020 10 / Continuous, numerical +1 : -2
 TABLE I
 CHARACTERISTICS OF THE DATA SETS USED.
 (where ? is user supplied), that the true mean of an attribute
 r is at least r¯ ?  where,
  =
 √
 R2 ln(1/?)
 2n
 . (5)
 Although not intended to operate on unlabelled data, we
 begin looking at ways to solve the candidate selection problem
 by testing the effectiveness of existing algorithms such as the
 VFDT. Using the MOA stream mining framework [23], we test
 an implementation of the VFDT called the HoeffdingTree [23]
 on imbalanced and unlabelled streams. We compare the perfor-
 mance of the HoeffdingTree to three static classifiers including
 a standard decision tree, a SVM and Naı¨ve Bayes. Our tests
 were designed to reveal how well the HoeffdingTree performs
 if i) made to learn from only the examples in the stream and
 ii) pre-trained before being taken on-line. Tests of other VFDT
 tree based algorithms including the HoeffdingOptionTree [24],
 AdaHoeffdingOptionTree2 , and finally OzaBag[25] and Oza-
 Boost[25] both using the HoeffdingTree have also been com-
 pleted (in preparation). In each case the parameters used for
 these algorithms were set to the defaults provided by MOA.
 A. Data
 In order to test the various data stream classifiers, four
 datasets were utilised (see Table I). The first and largest
 dataset consisted of pulsar candidates obtained during the High
 Time Resolution Universe Survey (HTRU) [26]3. Candidates
 are represented by 22 continuous numerical attributes, each
 of which is a summary statistic that describes it in some
 way. The data set contains only 1,611 positive and 2,593
 negative examples correctly labelled by human annotators. The
 remainder of the dataset is naı¨vely assumed to be negative.
 However we can constrain the number of actual positives
 incorrectly labelled in this dataset based on Monte Carlo
 simulations undertaken by Keith [26] and later by Levin [27],
 to some small fraction of approximately 879-916 instances.
 Also of the known 1,108 pulsars in the survey region, 725
 have been re-detected in this data. Thus together the number
 of re-detections and expected discoveries falls within the
 range of the 1,611 certain positive examples already labelled.
 The remaining datasets used included the Skin Segmentation,
 MiniBoone and the Magic Gamma Telescope datasets. These
 are obtainable from the UCI machine learning repository [28].
 B. Methodology
 Three distinct types of test were undertaken for this work.
 The first established a baseline level of performance on our
 2This is a HoeffdingOptionTree with the leaves modified so they store an
 estimation of the current error.
 3This data is currently not publicly accessible.
 1508
data, using static classifiers under a traditional static learning
 scenario. Here performance on our data sets was assessed
 using all of the data and stratified 5-fold cross validation (train
 on 4 folds, test on 1). To test the algorithms under the two
 streaming scenarios on the other hand (no pre-training vs.
 pre-trained), the data sets were firstly shuffled (except for the
 temporally ordered pulsar data) and then randomly sampled in
 order to generate different levels of class imbalance 4, whilst
 also varying the proportion of instances labelled 5. The no
 pre-training scenario tested only the stream classifiers under
 a test then train model (test on an instance, then train on an
 instance), such that each of the sampling permutations was
 treated as a single stream of data. Each test was repeated ten
 times for a given balance and labelling, allowing results to
 be averaged over multiple runs. For the pre-trained scenario,
 the same sampling and test procedure was used except that
 training and test sets were generated. The learners were then
 trained prior to being taken on-line for testing following the
 same test then train approach. All training sets were uniform
 in size and configuration, containing 200 positive and 1000
 negative instances (+1 : -5 balance). The class distribution of
 these training sets is similar to the test distribution of three
 out of our four data sets (see Table I), the exception being the
 pulsar data which has a very different test distribution. For the
 pre-trained tests both static and stream classifiers were used,
 allowing for a comparison between the performance of static
 and streamed learners trained on the same small sample of data.
 The test framework which executed these tests is summarised
 in Figure 1. As we assume a discrete time model, for each
 instance xi arriving at time step t, a class prediction is made
 at time step t + 1. The learner is trained at time step t + 2
 only if the instance was labelled. Following each prediction
 a count of true positives, false positives, false negatives and
 true negatives is updated, by checking the prediction against
 the correct label of xi. If xi was unlabelled in the stream, then
 the correct label is obtained from a meta data file, and then
 compared to the prediction as before. Thus each prediction
 is evaluated during testing. However overall statistics are not
 computed until the end of an individual test run.
 Evaluating classifier performance on imbalanced data is
 known to be difficult, particularly as many metrics are sensitive
 to the underlying class distribution [15]. We therefore keep
 track of multiple assessment metrics which include amongst
 others the F-Score (6) and the G-Mean [15] (7). The F-Score
 is sensitive to the class distribution given its dependence on
 precision (3). Thus when the data set is large and imbalanced,
 the numerator of the F-Score equation remains small while the
 denominator becomes increasingly large. Thus low precision
 can cause the F-Score to obtain a small value even given a
 high level of recall (e.g. with recall = .99 and precision = .01,
 the F1 = .0099) . As the G-Mean is not sensitive to the class
 distribution in the same way, we present the two together to
 4The levels used were: 1.0, 0.5, 0.3, 0.25, 0.2, 0.1, 0.04, 0.02, 0.01, 0.008,
 0.004, 0.002, 0.001, 0.0004, 0.0002, 0.00013, 0.0001 (e.g. 0.1 = +1 : -10).
 5The labelling proportions used: 0%,1%,5%,10%,25%,50%,75%,99% and
 100%.
 Fig. 1. An overview of the test framework.
 provide a representative impression of performance.
 F-Score = 2? Precision ? RecallPrecision + Recall (6)
 G-Mean =
 √
 TP
 TP + FN ?
 TN
 TN + FP (7)
 C. Results
 In Table II baseline accuracy results for static classifiers
 tested in a traditional learning scenario, are compared to those
 achieved by the stream classifiers under some representative
 imbalanced class scenarios. Static learners perform well on our
 data sets, achieving higher levels of accuracy than the stream
 learners when the class distribution is reasonably balanced.
 As the class imbalance increases however, the accuracy of the
 stream learners tends towards 1, surpassing the accuracy levels
 achieved by the static classifiers. This increase in accuracy be-
 comes statistically significant for all data sets (using a signifi-
 cance level of 0.05) when the imbalance reaches ? 1+ : ?125.
 Accuracy tending towards 1 in this manner, appears to happen
 due to the presence of Naı¨ve Bayes classifiers at each leaf
 of the Hoeffding tree. As each leaf contains a count of those
 positive and negative examples arriving there, a fully labelled
 data stream which is heavily imbalanced, will cause the count
 of negatives at these leaves to increase disproportionately to the
 positives. This skews the Naı¨ve Bayes predictions towards the
 negative class. As most examples in the stream are negative,
 by consistently predicting negative in this way the Hoeffding
 tree is effectively optimising classifier accuracy.
 Stream Accuracy
 Balance 1.0 0.1 0.01 0.001 0.0001
 Pulsar .9174 .9759 .9949 .9992 .9999
 Skin .9892 .9860 .9940 .9987 .9996
 MiniBoone .9949 .9995 0.9996 .9999 .9998
 Magic .7154 .9083 .9897 .9990 -
 Static Accuracy
 DT SVM NB
 .9999 .9999* .9676
 .9992 .9291 .9239
 .9999 .9279 .2827
 .8503 .7916 .7269
 TABLE II
 ACCURACY OF THE HoeffdingTree WITHOUT PRE-TRAINING ON A 100%
 LABELLED DATA STREAM, VERSUS ACCURACY OF STATIC CLASSIFIERS
 TRAINED AND TESTED USING STRATIFIED 5-FOLD CROSS VALIDATION
 (ALL DATA). * HERE A 90% TRAIN, 10% TEST STRATEGY WAS USED.
 However the improvement in accuracy comes at the expense
 of reduced recall. Across all of the datasets tested, recall rates
 1509
Recall
 Balance 1.0 0.5 0.1 0.01 0.001 0.0001
 Pulsar .86/.87 .83/.86 .81/.82 .74/.74 .64/.61 .06/.04
 Skin .99/ .99 .98/.98 .93/.93 .45/.32 .08/.07 .03/0
 MiniBoone .98/.99 .99/.99 .98/.96 .96/.84 .74/.46 .73/.02
 Magic .84/.53 .13/.31 .008/.008 .007/0 0/0 -
 TABLE III
 RECALL RESULTS FOR THE HoeffdingTree WITH AND WITHOUT
 PRE-TRAINING (WITH/WITHOUT), ON A 50% LABELLED DATA STREAM.
 consistently dropped as the class imbalance increased. This is a
 side effect of almost always predicating the negative class. This
 effect can be seen clearly in Table III. Even when 50% of the
 stream is labelled (which is unrealistic for a SKA scenario)
 recall rates tend toward zero which is unacceptable for our
 problem domain. Figure 2 also shows the same drop in recall
 rates observed when testing a non pre-trained Hoeffding tree on
 pulsar data. In this case recall rates are zero when no labels are
 available, and slowly increase as more labelled data appears in
 the stream. Static classifiers trained on only a small sample of
 data (200 positive and 1000 negative instances) achieved higher
 recall rates when treating the complete pulsar data stream as a
 static data set. Recall rates were ? .91 for C4.5 and ? .88 for
 the SVM respectively. Though both of these static classifiers
 return far too many false positives. C4.5 returned on average
 ? 138, 000 and the SVM ? 87, 000.
 Fig. 2. Recall rates of the non pre-trained Hoeffding tree on pulsar data as
 the labelling and class balance is altered.
 Compared to a static classifier, the Hoeffding tree has a very
 low false positive return rate when the stream is completely
 labelled. The previously described preference for applying neg-
 ative classifications when the stream is imbalanced, means that
 few positive predictions are ever made. Thus false positives
 become extremely rare. For the most imbalanced streams only
 1
 37000 instances became a false positive. As the proportion
 of labelled items in the stream is reduced, the false positive
 rate increases - slightly for the non pre-trained classifiers, but
 greatly for those pre-trained. This can be seen in Table IV
 which shows the false positive rates obtained on unlabelled
 streams. In row one a false positive return rate of .02 equates
 to ? 225, 000 false positives, much worse than the static
 classifiers. It appears that without labels to learn from, the
 Hoeffding tree maintains an imprecise model similar to the
 static classifiers, as both are trained on the same sample of
 data.
 False positive rate
 Balance 1.0 0.5 0.1 0.01 0.001 0.0001
 Pulsar .03 .02 .01 .01 .03 .02
 Skin .02 .02 .02 .03 .02 .02
 MiniBoone .01 .005 .02 .004 .004 .1
 Magic 0 0 0 0 0 -
 TABLE IV
 FALSE POSITIVE RATES FOR THE HoeffdingTree ON UNLABELLED
 STREAMS, TRAINED ON 200 POSITIVE & 1000 NEGATIVE INSTANCES.
 In comparing the pre-trained and non pre-trained stream
 classifiers, a statistically significant difference in accuracy
 was observed. The results are summarised in Tables V and
 VI. Non-pre-trained classifiers cannot build a model from
 an unlabelled stream, thus their accuracy for these streams
 is zero. On the remaining streams, the pre-trained classifiers
 initially outperform the non pre-trained, particularly in terms
 of recall. However as the the size of the data sets and the
 proportion of labelling increases, the non-pre-trained classifiers
 achieve higher levels of accuracy, once again by favouring
 negative classifications. Despite this difference in accuracy,
 the pre-trained classifiers actually maintain higher recall rates
 throughout. Though asymptotically their performance does
 appear to approach that of the non pre-trained classifiers as the
 number of instances classified increases. Thus it appears that
 pre-training a classifier only delays the inevitable favouring
 of the negative class if a large class imbalance exists. In
 summary these results show that those stream learners we
 tested, optimise accuracy on imbalanced data sets by almost
 always predicting the majority class. The net effect of this
 preference for the majority negative class in our case is to
 reduce recall and false positive rates, given that the minority
 class is very rarely predicted. Pre-training a classifier off-line
 on a sample of training data does in the short term improve
 recall rates, however this effect is only short lived.
 VI. CONCLUSION
 In this paper we have found that the recall capabilities of
 the Hoeffding tree classifier, deteriorate when faced heavily
 class imbalanced data streams. The imbalance appears to skew
 the class predictions of the Naı¨ve Bayes classifiers at the
 leaves of the tree, toward the majority class. This has the
 effect of improving classifier accuracy, whilst also degrading
 recall. We have found that pre-training a classifier before
 taking it on-line does initially improve the recall rate, however
 the class imbalance makes this effect inherently short term.
 Although these result suggests that VFDT based classifiers
 such as the Hoeffding tree are unsuitable for solving the
 candidate selection problem, their capacity to maintain low
 false positive return rates makes them appealing, particularly
 as we explore ways in which the VFDT approach could be
 modified to accommodate imbalanced streams and thereby
 increase the recall rate. Future work will expand on the results
 of this investigation, and test other data stream classifiers not
 exclusively based on the Hoeffding bound. We also intend to
 1510
Balance +1 : -10 +1 : -100 +1 : -1,000 +1 : -10,000
 Labelling (%) 0 50 75 100 0 50 75 100 0 50 75 100 0 50 75 100
 Pulsar 0/0 .9/.87 .9/.86 .91/.97 0/0 .86/.76 .85/.75 .86/.76 0/0 .78/.59 .79/.6 .79/.63 0/0 .29/.14 .37/.22 .42/.26
 Skin 0/0 .96/.90 .96/.92 .97/.93 0/0 .56/.45 .62/.52 .68/.60 0/0 .22/.13 .21/ .12 .23/.10 0/0 0/0 .02/0.1 .07/0.1
 MiniBoone 0/0 .98/.98 .99/.99 .99/.99 0/0 .92/.9 .93/.92 .95/.95 0/0 .67/.56 .77/.69 .75/.69 0/0 .05/.04 .05/0.4 0/0
 Magic 0/0 .07/.02 .07/.02 .07/.02 0/0 .05/.03 .04/.03 .07/.03 0/0 0/0 0/0 0/0 - - - -
 TABLE V
 G-MEAN/F1 SCORE RESULTS FOR THE HoeffdingTree CLASSIFIER ON THE TEST DATASETS WITHOUT PRE-TRAINING.
 Balance +1 : -10 +1 : -100 +1 : -1,000 +1 : -10,000
 Labelling (%) 0 50 75 100 0 50 75 100 0 50 75 100 0 50 75 100
 Pulsar .92/.87 .9/.86 .89/.85 .92/.87 .92/.57 .86/.77 .86/.75 .88/.75 .92/.1 .8/.6 .77/.6 .8/.61 .92/.02 .23/.08 .29/.12 .32/.15
 Skin .9/.83 .96/.89 .97/.89 .97/.93 .9/.42 .67/.55 .7/.58 .73/.64 .91/.09 .28/.01 .24/.08 .2/.07 .91/.01 .11/.01 .13/.03 .12/.04
 MiniBoone .79/.94 .99/.99 .99/.99 .99/.99 .76/.83 .98/.97 .98/.97 .98/.98 .59/.54 .83/.76 .95/.91 .9/.85 .84/.56 .83/.74 .96/.92 .71/.72
 Magic 0/0 .06/.02 .05/.01 .1/.04 0/0 0/0 .01/.03 .01/.03 0/0 0/0 0/0 0/0 - - - -
 TABLE VI
 G-MEAN/F1 SCORE RESULTS FOR THE HoeffdingTree CLASSIFIER WHEN TRAINED BEFORE CLASSIFYING THE STREAM.
 analyse the attributes that describe a candidate pulsar, with
 the aim of removing redundant features which may improve
 classifier accuracy, and reduce the dimensionality of the data.
 ACKNOWLEDGMENT
 This work was supported by grant EP/I028099/1 from the
 UK Engineering and Physical Sciences Research Council (EP-
 SRC). Computational resources were provided by the Jodrell
 Bank Centre for Astrophysics (JBCA), which has support from
 the UK Science and Technology Facilities Council (STFC).
 Experiments were carried on upon data obtained by the
 Parkes Observatory, which is funded by the Commonwealth of
 Australia and managed by the Commonwealth Scientific and
 Industrial Research Organisation (CSIRO). We would also like
 to thank Dan Thornton (JBCA) for his help in obtaining the
 HTRU survey data.
 REFERENCES
 [1] J. Cordes, M. Kramer, T. Lazio, et al., Pulsars as tools for fundamental
 physics and astrophysics, New Astronomy Reviews, Vol. 48, 2004.
 [2] S. D. Bates, Surveys of the Galactic plane for pulsars. PhD thesis,
 University of Manchester, Jodrell Bank Centre for Astrophysics School
 of Physics and Astronomy, September 2011.
 [3] C. Carilli, S. Rawlings, Science with the square kilometre array, New
 Astronomy Reviews, Vol. 48, no. 11-12, 2004.
 [4] R. Smits, M. Kramer, B. Stappers, et al., “Pulsar searches and timing with
 the square kilometre array”, Astronomy and Astrophysics, Vol. 493, no. 3,
 pp. 1161–1170, 2009.
 [5] J.-P. Macquart, M. Bailes, N. D. R. Bhat, et al., The CRAFT Collabora-
 tion, “The commensal real-time ASKAP fast-transients (CRAFT) survey”,
 Publications of the Astronomical Society of Australia, Vol. 27, no. 3,
 p. 272, 2010.
 [6] D. R. Lorimer, A. J. Faulkner, A. Lyne, et al., “The Parkes multibeam
 pulsar Survey - VI. Discovery and timing of 142 pulsars and a Galactic
 population analysis,” Monthly Notices of the Royal Astronomical Society,
 Vol. 372, pp. 777–800, 2006.
 [7] R. P. Eatough, N. Molkenthin, M. Kramer,et al.,“Selection of radio pulsar
 candidates using artificial neural networks”, Monthly Notices of the Royal
 Astronomical Society, Vol. 407, no. 4, pp. 2443–2450, 2010.
 [8] R. T. Edwards, M. Bailes, W. van Straten, et al., The Swinburne
 intermediate-latitude pulsar survey, Monthly Notices of the Royal Astro-
 nomical Society, Vol. 326, no. 1, pp. 358–374, 2001.
 [9] R. N. Manchester, A. Lyne, F. Camilo, et al., “The Parkes multi-beam
 pulsar survey - I. Observing and data analysis systems, discovery and
 timing of 100 pulsars”, Monthly Notices of the Royal Astronomical
 Society, Vol. 328, no. 1, pp. 17–35, 2001.
 [10] A. J. Faulkner, I. H. Stairs, M. Kramer, et al.,“The Parkes Multibeam
 Pulsar Survey: V. finding binary and millisecond pulsars”,Monthly Notices
 of the Royal Astronomical Society, Vol. 355, no. 1, pp. 147–158, 2004.
 [11] M. J. Keith, R. P. Eatough, A. Lyne, et al., “Discovery of 28 pulsars
 using new techniques for sorting pulsar candidates”, Monthly Notices of
 the Royal Astronomical Society, Vol. 395, no. 2, pp. 837–846, 2009.
 [12] K. J. Lee, L. Guillemot, Y. L. Yue, M. Kramer, D. J. Champion,
 Application of the gaussian mixture model in pulsar astronomy pulsar
 classification and candidates ranking for the fermi 2fgl catalogue, Monthly
 Notices of the Royal Astronomical Society, Vol. 424, no. 4, 2012.
 [13] N. V. Chawla, K. W. Bowyer, L. O. Hall, et al.,“Smote: synthetic minor-
 ity over-sampling technique,” Journal of Artificial Intelligence Research,
 Vol. 16, pp. 321–357, 2002.
 [14] S. Chen and H. He, “Towards incremental learning of nonstationary im-
 balanced data stream: a multiple selectively recursive approach,” Evolving
 Systems, Vol. 2, 2011.
 [15] H. He and E. Garcia, “Learning from imbalanced data,” Knowledge and
 Data Engineering, IEEE Transactions on, Vol. 21, no. 9, 2009.
 [16] T. M. Mitchell, Machine Learning, First Edition, McGraw-Hill Sci-
 ence/Engineering/Math, 1997.
 [17] J.-N. Vittaut, M.-R. Amini, and P. Gallinari, Learning classification with
 both labeled and unlabeled data, in: T. Elomaa, H. Mannila, H. Toivonen
 (Eds.), Machine Learning: ECML 2002, Vol. 2430 of Lecture Notes in
 Computer Science, Springer Berlin / Heidelberg, pp. 69–78, 2002.
 [18] P. Li, X. Wu, and X. Hu, Learning from concept drifting data streams
 with unlabeled data, in: Proceedings of the 24th AAAI Conference
 on Artificial Inteligence, Association for the Advancement of Artificial
 Intelligence, pp. 1945–1946, 2010.
 [19] X. Wu, P. Li, X. Hu, Learning from concept drifting data streams with
 unlabeled data, Neurocomputing, Vol. 92, pp. 145–155, 2012.
 [20] P. Zhang, X. Zhu, and L. Guo, Mining data streams with labeled and
 unlabeled training examples, in: ICDM ’09. Ninth IEEE International
 Conference on Data Mining, pp. 627–636, 2009.
 [21] H. Nguyen, W. Ng, Y. Woon, et al., Concurrent Semi-supervised learning
 of data streams , in: A. Cuzzocrea, U. Dayal (Eds.), Data Warehousing
 and Knowledge Discovery, Lecture Notes in Computer Science, Springer
 Berlin / Heidelberg, pp. 445–459, 2011.
 [22] G. Hulten, L. Spencer, and P. Domingos, Mining time-changing data
 streams, in: KDD ’01: Proceedings of the seventh ACM SIGKDD
 International Conference on Knowledge Discovery and Data Mining,
 2001.
 [23] The University of Waikato, MOA - massive online analysis, Web
 Accessed (12/10/2012), http://moa.cms.waikato.ac.nz (2012).
 [24] B. Pfahringer, G. Holmes, and R. Kirkby, “New options for hoeffding
 trees,” in Proceedings of the 20th Australian joint conference on Advances
 in artificial intelligence, AI’07, pp. 90–99, 2007.
 [25] N. Oza, “Online bagging and boosting,” in IEEE international Confer-
 ence on Systems, Man and Cybernetics ’05, Vol. 3, pp. 2340–2345, 2005.
 [26] M. J. Keith, A. Jameson, W. van Straten, et al., “The high time resolution
 universe pulsar survey - I. System configuration and initial discoveries,”
 Monthly Notices of the Royal Astronomical Society, Vol. 409, no. 2,
 pp. 619–627, 2010.
 [27] L. Levin, A search for radio pulsars: from millisecond pulsars to
 magnetars. PhD thesis, Faculty of Information and Communication
 Technology Swinburne University, June 2012.
 [28] K. Bache, M. Lichman, UCI Machine Learning Repository, University
 of California, Irvine, School of Information and Computer Sciences, Web
 Accessed (07/01/2013), http://archive.ics.uci.edu/ml, July 2013.
 1511

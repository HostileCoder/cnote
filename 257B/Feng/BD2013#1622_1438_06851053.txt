Logic-in-memory based Big-data Computing by Nonvolatile
 Domain-wall Nanowire Devices
 Yuhao Wang, Pingfan Kong and Hao Yu
 School of Electrical and Electronic Engineering,
 Nanyang Technological University, Singapore 639798
 haoyu@ntu.edu.sg
 Abstract—As one recently introduced non-volatile memory
 (NVM) device, domain-wall nanowire (or race-track) shows not
 only potential as future memory, but also computing capacity
 in big-data processing under unique domain-wall manipulation
 ability. In this paper, domain-wall nanowire device is studied
 for a NVM-based big-data computing platform, where all three
 parts: general purpose logic in LUT, special logic of XOR
 and data storage are all implemented by domain-wall nanowire
 devices. As one application, matrix multiplication, which is widely
 deployed in big-data applications such as machine learning or
 web searching, is studied in the proposed big-data computing
 platform. Experiment shows that when compared to CMOS based
 multi-core platform, the proposed computing platform exhibits
 37x higher performance at the cost of 9x silicon area under the
 same power budget; and 4.2x better performance and 88.77%
 less power consumption under the same area constraint.
 I. INTRODUCTION
 The big-data applications have raised the need to rethink
 the architecture of the current computing platforms. The data-
 oriented applications have significantly increased demands
 on memory capacity and bandwidth. The current computing
 platform has well-known memory-wall issue with limited
 accessing bandwidth but also large leakage power at advanced
 CMOS technology nodes, which hinders the scalability for big-
 data computing [1], [2], [3], [4], [5], [6], [7], [8], [9], [10],
 [11], [12], [13], [14]. As such, a scalable yet power-efficient
 memory-oriented computing platform is highly desirable for
 the future big-data processing.
 There are many recent explorations by the newly discovered
 non-volatile memory (NVM) technologies at nano-scale [7],
 [8], [9]. Specifically, domain-wall nanowire device [3], [4], [5]
 has been recently utilized to perform logic-in-memory based
 computation, which may erase the memory-wall of the limited
 I/O bandwidth. In this paper, we have explored the study
 of domain-wall nanowire based big-data computing platform.
 Firstly, the domain-wall nanowire based look-up table (DW-
 LUT) is configured to realize general logic functions required
 by workload for big-data applications. What is more, the
 domain-wall nanowire is also explored in specific logic (DW-
 XOR) such as XOR and shift due to the unique domain-wall
 shifting property. Because of the low power consumption of
 DW-LUT and DW-SL, the proposed computing system can
 be rich of computing resources that are specific to big-data
 workloads. As a case study, the proposed computing system
 is applied to execute large-scale matrix multiplication in a
 MapReduce fashion. Compared to the conventional CMOS
 based multi-core platform, the proposed computing platform
 shows 37x higher performance at the cost of 9x silicon
 area under the same power budget; and shows 4.2x better
 performance and 88.77% less power consumption when under
 the same area constraint.
 The rest of the paper is organized as follows. Section II
 introduces the overall memory-based computing platform by
 domain-wall nanowire. Section III discusses the domain-wall
 nanowire operation, domain-wall nanowire based LUT and
 XOR logics in details. Section IV shows the case study of
 how to apply the proposed computing platform to execute
 MapReduce-based matrix multiplication. Experiment results
 are presented in Section V with conclusion in Section VI.
 II. NVM-BASED BIG-DATA COMPUTING PLATFORM
 Non?volatile"Memory
 General"purpose"reconfigurable"
 computing"engine"by"NV?LUTs
 Logic"1
 LUT
 Logic"2
 LUT
 Logic"N
 LUT
 …Logic"3
 LUT
 Data"switch"network"by"NV?crossbar
 M
 e
 m
 o
 ry
 "&"
 fu
 n
 ct
 io
 n
 "se
 qu
 e
 n
 ce
 "co
 n
 tr
 o
 lle
 r"
 DW?XOR DW?adder DW?shifter
 Accelerators"resources"by"Non?volatile"DW?logic
 …………
 Data"storage"and"exchange
 d
 ata"b
 u
 s
 Fig. 1. System overview of the proposed NVM-based logic-in-memory
 computing platform
 As shown in Figure 1, the proposed non-volatile memory
 (NVM) based computing platform is composed of three parts.
 Firstly, non-volatile domain-wall nanowire based lookup-table
 (DW-LUT) is utilized for configuring general logic. In this
 part, multiple LUTs are configured as different functions
 according to the program to be executed. Conventionally,
 program that intends to achieve complex functionality will
 be decomposed into basic instructions that the ALU can
 take by compiler. Therefore, it needs multiple clock cycles
 to be executed due to the decomposition. This compromises
 978-1-4799-4110-0/13/$31.00 ©2013 IEEE
efficiency in order to gain better generality. However, in big-
 data applications, programs are usually intensive with a set
 of domain-specific functions without generality such that the
 coarse granularity can be introduced. With the basic functions
 implemented by LUTs, programs can be executed with greatly
 augmented execution performance as well as power efficiency.
 Secondly, the physics of domain-wall nanowire device is
 exploited for special logic (DW-SL). Although most of the
 functions are covered by LUTs, some commonly executed
 functions such as XOR and shift can be economically imple-
 mented by domain-wall nanowire device directly. Thirdly, non-
 volatile memory is deployed for the data storage. Obviously,
 the three main parts of the proposed computing system are
 rich of non-volatile devices, which can be significantly helpful
 to achieve high power efficiency and high bandwidth. In the
 following, we will explore detailed design by non-volatile
 domain-wall nanowire device for each part.
 III. DOMAIN-WALL NANOWIRE BASED LOGICS
 A. Domain-wall nanowire operations
 Domain-wall nanowire, also known as racetrack memory
 [3], [4], [5], is a newly introduced non-volatile memory device
 in which multiple bits of information are stored in single
 ferromagnetic nanowire. As shown in Figure 2(a), each bit is
 denoted by the leftward or rightward magnetization direction,
 and adjacent bits are separated by domain-walls. By applying
 a current through the shift port at the two ends of the nanowire,
 all the domain-walls will move left or right at the same velocity
 while the domain width of each bit remains unchanged, thus
 the stored information is preserved. Such a tape-like operation
 will shift all the bits similarly like a shift register.
 Magnetic
 tunnel
 junction
 Insulator
 Fixed layer
 Write/read
 port
 Shift
 port
 Contact
 Domain
 wall
 Fig. 2. The diagram of domain-wall nanowire device
 In order to access the information stored in the domains, a
 strongly magnetized ferromagnetic layer is placed at desired
 position of the ferromagnetic nanowire and is separated by
 an insulator layer. Such a sandwich-like structure forms a
 magnetic-tunnel-junction (MTJ), through which the stored
 information can be accessed.
 Depending on the alignment of the fixed layer and free layer,
 parallel or anti-parallel, the MTJ exhibits different states, high
 resistance state or low resistance state. This is called giant
 magnetoresistance (GMR) effect. By detecting the resistance
 of MTJ junction, the direction of magnetization can be known
 thus the stored bit is read out. The write operation is achieved
 by current induced magnetization reversal of MTJ free layer.
 Based on the current direction, the electron spin will force the
 free layer to be parallel or anti-parallel to fixed layer, thus the
 free layer magnetization is altered. Because the write and read
 operation can only occur in the MTJ junction, thus the bit to
 be operated needs to be shifted and aligned with fixed layer.
 Shift operation is like a shift register, and the domain-wall
 motion direction and velocity can be controlled by the current
 direction and amplitude.
 B. Special XOR logic
 Obviously, the current induced shift operation of domain-
 wall nanowire makes it a natural candidate to achieve shift
 logic. Figure 3(a) shows the structure of shift logic obtained
 by domain-wall nanowire and access transistors. This structure
 contains multiple access ports for write and read operation,
 and one port for shift operation. Therefore, the shifter can be
 assigned with binary value, shifted leftward or rightward, read
 out the binary value by manipulating the current through dif-
 ferent port. Unlike traditional shift registers, in which the logic
 gates built by complementary PMOS and NMOS will bring
 the inevitable sub-threshold leakage, the access transistors for
 non-volatile devices are able to avoid leakage current.
 SHF1SHF1
 RD
 RDWR1
 WR1 SHF2SHF2
 WR2
 WR2
 Operand
 A
 Operand
 B
 XOR op
 (a)
 SHF
 SHF WR1
 WR1
 WR2
 WR2
 WRn
 WRn
 Bit_1 Bit_2 Bit_n
 (b)
 Fig. 3. Logic devised at device level (a) shift logic achieved by multiple bit
 domain-wall nanowire with access transistors; (b) the XOR logic achieved by
 two domain-wall nanowires with access transistors.
 The GMR-effect can be interpreted as the bitwise-XOR
 operation of the magnetization directions of two thin magnetic
 layers, where the output is denoted by high or low resistance.
 In MTJ structure, however, the XOR-logic will fail as there is
 only one operand as variable since the magnetization in fixed
 layer is constant. Nevertheless, this problem can be overcome
 by the unique domain-wall shift operation in the domain-wall
 nanowire device, which enables the possibility of DWL-based
 XOR-logic for computing.
 Figure 3(b) shows a bitwise-XOR logic implemented by two
 domain-wall nanowires. A new read-only-port is constructed
by two free layers and one insulator layer stacked in the
 middle, where proposed bitwise-XOR logic can be performed.
 Thus, the two operands, denoted as the magnetization direction
 in free layer, can both be variables with values assigned
 through the MTJ of the according nanowire. To perform
 XOR operation, first the operand A and B are written into
 left and right MTJ respectively; then A and B are shifted
 to the constructed port, followed by a read operation at the
 constructed port.
 C. General LUT logic
 nanowire
 nanowire
 nanowire
 nanowire
 nanowire
 nanowire
 nanowire
 nanowire
 nanowire
 nanowire
 nanowire
 nanowire
 BL BLB
 Bit-line Mux
 W
 o
 rd
 -
 lin
 e
  
de
 co
 de
 r
 Bit 1 Bit 2 Bit n
 Bit nBit 1 Bit 2
 Parallel output
 Serial output
 SHF
 SHF
 WL
 WL
 BL
 BLB
 MTJ
 as access port
 Data 
segment
 Reserved
 segment
 Fig. 4. LUT by domain-wall nanowire array with parallel output and serial
 output
 Figure 4(a) shows the structure of cell in the LUT array. The
 access-port lies in the middle of the nanowire, which divides
 the nanowire into two segments. The left-half segment of
 nanowire is used for data storage while the right-half segment
 is reserved for shift-operation in order to avoid information
 lost.
 Figure 4(b) shows the domain-wall nanowire based LUT
 array. The input of the function implemented by LUT is
 represented as binary address. The address is fed into word-
 line decoder and bit-line mux to find the target domain-wall
 nanowire cell, where the multiple-bit result is kept. The LUT
 array size depends on the domain, range and precision of the
 function to perform.
 Based on the way data is organized, the result can be output
 in serial manner or parallel manner. In serial output scenario,
 the binary result is stored in single domain-wall nanowire that
 is able to hold multiple bits of information. Assume each
 cell has only one access port and the first bit of result is
 initially aligned with access port, the way to output result
 is to iteratively readout and shift one bit until the last bit is
 output. In parallel output scenario, the multiple-bit result is
 distributed into different nanowires. Because each cell has their
 own access port, the multiple bits can be output concurrently.
 The design complexity of parallel output scheme is that, to
 find the relative position of the result within the nanowire, a
 variable access time will be introduced. For example, if luckily
 the result is stored at first bit of the nanowires, the result can be
 readout in one cycle; on the contrary if the result is kept at the
 very last bit of the nanowires, it will take tens of cycles to shift
 first before the result is output. Therefore, the choice between
 serial output and parallel output is the tradeoff between access
 latency and design complexity.
 IV. CASE STUDY OF BIG-DATA MATRIX MULTIPLICATION
 Matrix multiplication is one of the essential functions in
 big-data applications like data mining and web searching.
 For instance, singular value decomposition (SVD), which can
 be used for deep learning in neural networks [15], involves
 iterations of matrix multiplication. Google PageRank, which
 intends to provide relative importance of billions of web pages
 according to searching query, also involves large amount of
 matrix multiplication operations [16]. In the following, we will
 show how matrix multiplication can be computed in parallel,
 and how it can be mapped to the proposed platform.
 A. MapReduce-based matrix multiplication
 MapReduce [17] is a parallel programming model to ef-
 ficiently handle large volume of data. The idea behinds
 MapReduce is to break down large task into multiple sub-
 tasks, and each sub-task can be independently processed by
 different Mapper computing units, where intermediate results
 are emitted. The intermediate results are then merged together
 to form the global results of the original task by the Reducer
 computing units.
 The problem to solve is x = M?v. Suppose M is an n?n
 matrix, with element in row i and column j denoted by mij ,
 and v is a vector with length of n. Hence, the product vector
 x also has the length of n, and can be calculated by
 xi =
 n∑
 j=1
 mijvj
 The pseudo-code of matrix multiplication in MapReduce
 form is demonstrated in Algorithm 1. Matrix M is partitioned
 into many blocks, and each Mapper function will take the
 entire vector v and one block of matrix M. For each matrix
 element mij it emits the key-value pair (i, mijvj). The sum
 of all the values with same key will make up the matrix-vector
 product x. A reducer function simply has to sum all the values
 associated with a given key i. The summation process can also
 be executed concurrently by iteratively partial summing and
 emitting until one key-value pair left for each key, namely the
 (i, xi).
Algorithm 1 Matrix multiplication in MapReduce form
 function MAPPER(partitioned matrix p ? M , v)
 for all elements mij ? p do
 emit(i, mijvj) to list li
 end for
 end function
 function REDUCER(key q, s ? lq)
 sum ? 0
 for all values rk ? s do
 sum ? sum+ rk
 end for
 emit(q, sum)
 end function
 B. Workload mapping
 5 2 6 2
 8 0 3 1
 7 1 4 3
 Ç ? Ç ?È Ù È ÙÈ Ù È ÙÈ Ù È ÙÉ Ú É Ú
 Task queue
 compile
 (0x00, 3, 0x10)
 (0x03, 3, 0x10)
 (0x06, 3, 0x10)
 0x00
 0x10
 M 
v
 …...
 stored
 …...
 Data segment
 Step 3: Map process
 DW-LUT
 DW-logic
 Step 2: fetch data
 [5,2,6][2,1,3]
 [8,0,3][2,1,3]
 [7,1,4][2,1,3]
 Step 1: fetch task
 (1, 10)
 (key, value) pairs
 (1, 2)
 (1, 18)
 controllers
 DW-LUT
 DW-logic
 DW-LUT
 DW-logic
 Dispatch 
atom task
 (2, 16)
 (2,0)
 (2, 9)
 (3, 1)
 (3, 12)
 (3, 14)
 DW-LUT
 DW-logic
 DW-LUT
 DW-logic
 DW-LUT
 DW-logic
 Mappers Reducers
 Dispatch 
atom task
 (1, 30)
 (2, 25)
 (3, 27)
 Step 4: Reduce process
 [30,25,27]T
 Step 5: 
Write back 
result
 Fig. 5. Matrix multiplication mapping to proposed domain-wall nanowire
 based computing platform
 Figure 5 shows how the MapReduce based matrix multi-
 plication is mapped into the proposed non-volatile memory
 based computing platform. Before execution, the DW-LUTs
 are configured to execute integer multiplication. In addition,
 the matrix multiplication workload needs to be compiled into
 a task queue and stored into predefined region in the memory.
 The purpose of compilation is to break down the workload
 into smaller tasks that can be executed concurrently. In this
 example, the matrix M is partitioned in the unit of rows,
 so each task only requires dot product of two vectors. Each
 task instruction includes information like the entry address of
 specific row of matrix M and vector v, as well as length of
 row n.
 When the computation of the matrix multiplication starts,
 the controllers, also achieved by reconfigurable LUTs, will
 fetch the tasks from the task queue. For each task in the
 queue, the controller will fetch its required data according
 to the address, and then dispatch the multiplication tasks to
 mappers based on the availability. The controllers will keep
 fetching tasks, interpreting tasks, and dispatched tasks until the
 queue is empty. The DW-LUT in each mapper will compute
 the dispatched multiplication tasks, and emit the <key, value>
 pairs as intermediate results. The <key, value> pairs are
 further combined by the reducers. Specifically, each reducer
 will take two pairs with same key from the intermediate results
 pool, and combine the value by addition, and emit a new pair
 to the intermediate results pool. The core operation in reduce
 process is addition, which is mainly achieved by DW-XOR
 based adder. The reduce process works in an iterative manner,
 combining two pairs to one pair until the intermediate results
 can not be further combined. As such, the final results are
 obtained, and the results write back signifies the end of whole
 process.
 V. NUMERICAL EXPERIMENT RESULTS
 A. DW-SL performance
 TABLE I
 PERFORMANCE FOR DOMAIN-WALL NANOWIRE DEVICE AND SPECIAL
 LOGICS
 domain-wall nanowire device
 Operation speed (cycles) energy (pJ)
 read 1 1
 write 1 0.3
 shift 1 1
 domain-wall nanowire logic
 logic speed (cycles) energy (pJ)
 32-bit XOR 5 110
 32-bit adder 7 218
 32-bit shifter 1?31 50?100
 Table I shows the energy cost and speed of domain-wall
 nanowire basic operations, as well as the performance of DW-
 SL. For the domain-wall nanowire, the MTJ read and write
 operation evaluation is obtained from NVM-spice [10], [11],
 a SPICE simulator for NVM developed based on simulation
 theory in [6], [12], [13]. The current density of 7e10A/m2
 is utilized for magnetization switching, and 2.4K? and 1K?
 resistance values are assumed for anti-parallel state and paral-
 lel state for MTJ. The sub-pJ results agree with the reported
 measurement data in [18]. The current induced shift data is
 extracted from reported measurement data [2].
 The performance evaluation of DW-SL is obtained by
 decomposing the logic into basic operations and summing up
 accordingly. For example, as elaborated in section III-B, the
 DW-XOR performs by two write operations, two shift opera-
 tions, and one read operation. For shift logic, the indeterminate
 range is due to the variable number of bits to shift. Different
 from CMOS based logic, DW-SL needs multiple cycles to
 operate, thus the latency is expected to be much longer
 than its CMOS counterpart. However, computation throughput
 outweighs latency in the context of big-data applications where
the data parallelism is high; benefited from the leakage free
 property, more DW-SL resources are able to be allocated to
 significantly increase the system throughput.
 B. DW-LUT performance
 700800 )
 parallel"output serial"output leakage"power
 200
 300
 400
 500
 600
 200
 300
 400
 500
 600
 700
 ge
 "po
 w
 e
 r"(n
 W
 )
 e
 n
 e
 rg
 y"(
 pJ
 )
 0
 100
 0
 100
 le
 a
 ka
 ge
 LUT size"
 Fig. 6. Power characterization for DW-LUT in different sizes
 Figure 6 shows the power characterization of DW-LUT in
 different array sizes. To obtain the area, power and speed
 of DW-LUT, the memory modeling tool CACTI [19] has
 been extended with domain-wall nanowire model in [14]. In
 terms of dynamic energy per look-up operation, the parallel
 output scenario is much more power efficient than serial output
 scenario, and the gap enlarges when array size increases. This
 is because more cycles are required to output results in serial
 than in parallel, therefore more access operations are involved.
 However, the serial scenario is able to avoid the variable
 access latency issue, which reduces the design complexity of
 the controller. For leakage power, the non-volatility leads to
 extremely low leakage power in nW scale, which is negligible
 compared with its dynamic power. For volatile SRAM and
 DRAM, the leakage power may consume as large as half the
 total power especially in advanced technology node [19].
 Once the domain, range, and precision of function are
 decided, the DW-LUT size can be determined accordingly.
 Therefore, the power characterization can be used as a quick
 reference to estimate the power profile of specific function
 to perform system level design exploration and performance
 evaluation.
 C. MapReduce-based matrix-multiplication performance
 Table II shows the power, area, and performance com-
 parison between proposed platform and conventional multi-
 core platform. The workload is matrix multiplication in the
 scale of million by million, and all matrix elements are 8 bit
 integer numbers. In addition, serial output scenario is adopted,
 and each nanowire is composed of 32 bits, out of which 16
 bits are used for storing results and the other 16 bits are
 reserved for shift operation, leading to an LUT array size of
 64K for one multiplication operation. The comparison focuses
 on the computation resources and is exclusive of memory
 components. The 32nm technology node is assumed for both
 scenarios.
 TABLE II
 PLATFORM COMPARISON
 General settings
 Platform multi-core proposed
 Technology 32nm
 Workload 1M?1M matrix multiplication
 Clock rate 3.4GHz 500MHz
 Under 100W power budget
 Platform multi-core proposed
 # of computing resources 8 cores 58716 LUTs+5963 adders
 Performance 24.48 GOPS 917.44 GOPS
 Area 145 mm2 1292 mm2
 Under 145 mm2 silicon area budget
 Platform multi-core proposed
 # of computing resources 8 cores 6591 LUTs+669 adders
 Performance 24.48 GOPS 102.98 GOPS
 Power 100W 11.23W
 The multi-core based scenario consists of 8 Xeon cores
 where the MapReduce based matrix multiplication is exe-
 cuted. The evaluation flow is in two steps. Firstly, gem5
 [20] simulator is employed to take MapReduce based matrix
 multiplication from Phoenix benchmark suites [21] and to
 generate the runtime utilization rate of core components. Next,
 the generated statistics is taken by McPAT [22], which is able
 to provide core area, power and performance results. For the
 domain-wall nanowire based computing platform, the matrix
 multiplication is translated into the task list form in section
 IV-B. The DW-LUT and DW-SL evaluation is gained from
 previous sections, and combined with the operation count, the
 performance of proposed platform can be estimated.
 The results indicate that, when power constraint is assumed
 for both systems, the proposed memory based platform ex-
 hibits 37x higher throughput, but at the cost of 9x larger
 silicon area. This is because the non-volatile memory based
 computing platform has very high power efficiency, thus
 more computation resources can be afforded to gain better
 performance. In the case if area constraint is adopted, proposed
 system shows 4.2x better performance and 88.77% less power
 consumption.
 VI. CONCLUSION
 In this work, a domain-wall nanowire based computing plat-
 form is proposed for big-data applications. The non-volatile
 memory domain-wall nanowire device is intensively utilized
 to implement low power memory, special logic, and also
 reconfigurable logic. Enabled by the low power dissipation
 with large density, the proposed computing platform exhibits
 very low power dissipation and high scalability, which can
 provide abundant computing resources to exploit the high
 data parallelism. In the case study of matrix multiplication
 by MapReduce, experiment results show that when compared
 to the conventional CMOS based multi-core platform, the
 proposed computing platform shows 37x higher performance
 at the cost of 9x silicon area under the same power budget; and
4.2x better performance and 88.77% less power consumption
 under the same area constraint.
 ACKNOWLEDGMENT
 This work is sponsored by Singapore MOE TIER-2 fund
 MOE2010-T2-2-037 (ARC 5/11) and NRF-CRP fund NRF-
 CRP9-2011-01.
 REFERENCES
 [1] Q. Xia, W. Robinett, M. W. Cumbie, N. Banerjee, T. J. Cardinali, J. J.
 Yang, W. Wu, X. Li, W. M. Tong, D. B. Strukov et al., “Memristor-
 cmos hybrid integrated circuits for reconfigurable logic,” Nano letters,
 vol. 9, no. 10, pp. 3640–3645, 2009.
 [2] D. Chiba, G. Yamada, T. Koyama, K. Ueda, H. Tanigawa, S. Fukami,
 T. Suzuki, N. Ohshima, N. Ishiwata, Y. Nakatani et al., “Control of
 multiple magnetic domain walls by current in a co/ni nano-wire,”
 Applied Physics Express, vol. 3, no. 7, p. 3004, 2010.
 [3] R. Venkatesan, V. Kozhikkottu, C. Augustine, A. Raychowdhury, K. Roy,
 and A. Raghunathan, “Tapecache: a high density, energy efficient cache
 based on domain wall memory,” in Proceedings of the 2012 ACM/IEEE
 international symposium on Low power electronics and design. ACM,
 2012, pp. 185–190.
 [4] S. S. Parkin, M. Hayashi, and L. Thomas, “Magnetic domain-wall
 racetrack memory,” Science, vol. 320, no. 5873, pp. 190–194, 2008.
 [5] L. Thomas, S.-H. Yang, K.-S. Ryu, B. Hughes, C. Rettner, D.-S. Wang,
 C.-H. Tsai, K.-H. Shen, and S. S. Parkin, “Racetrack memory: a high-
 performance, low-cost, non-volatile memory based on magnetic domain
 walls,” in Electron Devices Meeting (IEDM), 2011 IEEE International.
 IEEE, 2011, pp. 24–2.
 [6] Y. Shang, W. Fei, and H. Yu, “Fast simulation of hybrid cmos and stt-mtj
 circuits with identified internal state variables,” in Design Automation
 Conference (ASP-DAC), 2012 17th Asia and South Pacific. IEEE, 2012,
 pp. 529–534.
 [7] Y. Wang and H. Yu, “Design exploration of ultra-low power non-volatile
 memory based on topological insulator,” in Nanoscale Architectures
 (NANOARCH), 2012 IEEE/ACM International Symposium on, 2012.
 [8] Y. Wang, C. Zhang, H. Yu, and W. Zhang, “Design of low power
 3d hybrid memory by non-volatile cbram-crossbar with block-level
 data-retention,” in Proceedings of the 2012 ACM/IEEE international
 symposium on Low power electronics and design. ACM, 2012, pp.
 197–202.
 [9] X. Huang, C. Zhang, H. Yu, and W. Zhang, “A nanoelectromechanical-
 switch-based thermal management for 3-d integrated many-core
 memory-processor system,” Nanotechnology, IEEE Transactions on,
 vol. 11, no. 3, pp. 588–600, 2012.
 [10] Y. Wang, Y. Shang, and H. Yu, “Design of single saw-tooth-pulse based
 stt-ram readout circuit by nvm spice,” in IEEE Non-Volatile Memory
 Technology Symposium (NVMTS). IEEE, 2012.
 [11] Y. Wang, W. Fei, and H. Yu, “Spice simulator for hybrid cmos mem-
 ristor circuit and system,” in Cellular Nanoscale Networks and Their
 Applications (CNNA), 2012 13th International Workshop on. IEEE,
 2012, pp. 1–6.
 [12] W. Fei, H. Yu, W. Zhang, and K. S. Yeo, “Design exploration of hybrid
 cmos and memristor circuit by new modified nodal analysis,” Very Large
 Scale Integration (VLSI) Systems, IEEE Transactions on, vol. 20, no. 6,
 pp. 1012–1025, 2012.
 [13] Y. Shang, W. Fei, and H. Yu, “Analysis and modeling of internal state
 variables for dynamic effects of nonvolatile memory devices,” Circuits
 and Systems I: Regular Papers, IEEE Transactions on, vol. 59, no. 9,
 pp. 1906–1918, 2012.
 [14] Y. Wang and H. Yu, “Ultralow-power memory-based big-data computing
 platform by nonvolatile domain-wall nanowire devices,” in Proceedings
 of the 2013 ACM/IEEE international symposium on Low power elec-
 tronics and design. ACM, 2013.
 [15] S.-J. Lee and C.-S. Ouyang, “A neuro-fuzzy system modeling with
 self-constructing rule generationand hybrid svd-based learning,” Fuzzy
 Systems, IEEE Transactions on, vol. 11, no. 3, pp. 341–353, 2003.
 [16] J. Lin and C. Dyer, “Data-intensive text processing with mapreduce,”
 Synthesis Lectures on Human Language Technologies, vol. 3, no. 1, pp.
 1–177, 2010.
 [17] J. Dean and S. Ghemawat, “Mapreduce: simplified data processing on
 large clusters,” Communications of the ACM, vol. 51, no. 1, pp. 107–113,
 2008.
 [18] H. Zhao, B. Glass, P. K. Amiri, A. Lyle, Y. Zhang, Y.-J. Chen,
 G. Rowlands, P. Upadhyaya, Z. Zeng, J. Katine et al., “Sub-200 ps
 spin transfer torque switching in in-plane magnetic tunnel junctions
 with interface perpendicular anisotropy,” Journal of Physics D: Applied
 Physics, vol. 45, no. 2, p. 025001, 2012.
 [19] Cacti: An integrated cache and memory access time, cycle time,
 area, leakage, and dynamic power model. [Online]. Available:
 http://www.hpl.hp.com/research/cacti/
 [20] The gem5 simulator. [Online]. Available: http://www.m5sim.org
 [21] C. Ranger, R. Raghuraman, A. Penmetsa, G. Bradski, and C. Kozyrakis,
 “Evaluating mapreduce for multi-core and multiprocessor systems,” in
 High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th
 International Symposium on. IEEE, 2007, pp. 13–24.
 [22] S. Li, J. H. Ahn, R. D. Strong, J. B. Brockman, D. M. Tullsen, and N. P.
 Jouppi, “Mcpat: an integrated power, area, and timing modeling frame-
 work for multicore and manycore architectures,” in Microarchitecture,
 2009. MICRO-42. 42nd Annual IEEE/ACM International Symposium on.
 IEEE, 2009, pp. 469–480.

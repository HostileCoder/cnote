Vehicle Tracking with Non-overlapping Views  
for Multi-camera Surveillance System 
 
Wenbin Jiang, Chang Xiao, Hai Jin, Shuo Zhu, Zhiwei Lu 
Services Computing Technology and System Lab 
Cluster and Grid Computing Lab 
School of Computer Science and Technology 
Huazhong University of Science and Technology, Wuhan, 430074, China 
E-mail: wenbinjiang@hust.edu.cn 
 
Abstract—With the rapid development of intelligent video 
surveillance system for transportation, traditional single-
 camera-based video analysis has become insufficient. Many 
researches have focused on the non-overlapping multi-camera 
target tracking. However, the tracking precision and the 
computing overhead are still big obstacles. This paper proposes 
a novel vehicle tracking approach for non-overlapping multi-
 camera targets with data fusion by using minimum cost and 
maximum flow method. Structured information of moving 
targets is extracted and associated with other information such 
as motion time, the topology of camera network to solve 
targeted vehicle tracks. Besides, to improve the performance of 
the process, a parallelization algorithm for camera network 
topology partitioning is presented, which makes it possible for 
each sub-graph to track target independently in parallel. The 
experiment results show that the presented approach is able to 
perform target vehicle tracking analysis with high efficiency 
and accuracy. 
Keywords-vehicle tracking; non-overlapping; camera view; 
data fusion; minimum cost and maximum flow 
I. INTRODUCTION 
Vehicle has become one of essential part of our daily lives. 
Howover, it also has become an impotant cause resulting in 
transportation accidents, criminality, etc. So traffic video 
surveillance system has become a crucial measure to monitor 
the transportation situation while protect our lives and 
properties. It is not only able to make real-time monitoring, 
recording for monitored situations, but also able to perform 
analysis and give warnings for some special or dangerous 
events, such as law violations or crimes. This would be 
helpful for security staffs to deal with emergency and ensure 
the public safety.  
The foundation and core technologies of intelligent video 
surveillance system lie in the detection and tracking of 
vehicle objectives, because only after the images of targets 
are extracted we can do further analysis on their 
characteristics and behaviors. The means of detection and 
tracking of targets can be divided into two categories: the 
single-camera way and the multi-camera way. Benefiting 
from long-term studying, the research on single-camera 
detection and tracking of targets is relatively mature. The 
common detection approaches are frame subtraction and 
background subtraction. The common tracking approaches 
are the ones based on model, characteristic and mean shift. 
However, with the numbers of cameras inside many 
intelligent video monitoring systems increasing rapidly, the 
way of single-camera-based analysis become insufficient to 
meet the practical requirements. Therefore, the multi-camera-
 based analysis technologies have attracted more and more 
attentions of researchers. 
Nowadays, the traditional deployment model of 
surveillance systems for urban roads traffic usually is: 
monitoring and recording in front-end by cameras, 
compressing and transmitting videos in middle-end, and 
storing, processing and analyzing the content of videos in 
surveillance center in back-end. This kind of model has two 
disadvantages. First, it is expensive and ineffective to analyze 
huge amounts of recorded videos manually, especially in 
multi-camera conjoint analysis; second, huge data of video 
files causes great pressure on transmission networks and 
storage devices.  
In order to achieve the goal of tracking vehicle targets 
which pass through multiple cameras, we need to associate 
the targets captured by different cameras. This kind of 
association, which leverages the vehicle-related characteristic 
information and spatial-temporal constraint information, turns 
out to be a data fusion problem. With the structured 
information extracted from traffic surveillance videos, 
combining with the topology structure of camera networks, 
we build a data fusion model based on minimum cost and 
maximum flow, and implement a non-overlapping multi-
 camera target tracking strategy. In order to crack the hard nut 
of the overhead of processing and storage, this paper puts 
forward a network division algorithm based on camera 
network topology. It can improve the stability and the 
performance of the tracking system with massive data, which 
implements distributed target tracking, breaks the 
computational bottleneck of centralized processing and 
significantly reduces the cost in terms of computation time 
and network transmission costs. 
The rest of the paper is structured as follows. Section II 
gives the background knowledge related to this work, 
including an overview of related work about identification 
and re-identification of targets across multiple cameras and 
the major methods used in vehicle tracking in multi-camera 
surveillance systems. Section III presents the features and key 
technologies used in the approach presented, including the 
methods of extracting the target associated features, the 
topology estimation way of camera networks method and the 
target association algorithm. In section IV, the measurements 
of the non-overlapping camera in the multi-camera 
surveillance system both in functionality and performance are 
2013 IEEE International Conference on High Performance Computing and Communications & 2013 IEEE International Conference
 on Embedded and Ubiquitous Computing
 978-0-7695-5088-6/13 $26.00 © 2013 IEEE
 DOI 10.1109/HPCC.and.EUC.2013.172
 1213978-0-7695-5088-6/13 $31.00 © 2013 IEE
discussed, while a traditional Bayesian fusion method is used 
as a contrast. Finally, section V draws conclusions and gives 
some directions for future work. 
II. RELATED WORK 
Tracking targets in a single camera has been widely 
studied. Targets are often modeled using various features 
such as color, motion, size, and time.  However, with the 
rapid development of intelligent video surveillance system for 
transportation, traditional single-camera-based video analysis 
has become insufficient. Many researches have focused on 
the non-overlapping multi-camera target tracking approaches. 
This kind of tracking is often modeled as a global data 
association issue which is formulated as a maximum a 
posteriori probability (MAP) problem. Berclaz et al. [1] 
tracked multiple targets using dynamic programming to solve 
a global data association problem with overlapping multiple 
cameras. Wu and Nevatia [2] detected people using an 
Edgelet-based Adaboost classifier that models target by its 
histogram, size and time. The trajectory of a target is also 
estimated by applying a MAP formulation. 
Object tracking with partially overlapping camera views 
has also been researched extensively in the last decade [3, 4]. 
However, in wide-area tracking and wide-area surveillance 
applications, it is always unrealistic to assume that all the 
cameras in the system have overlapping fields of views. 
Tracking across disjoint camera views is a more challenging 
problem than that with overlapping cameras due to lack of 
spatial continuity, which resulting in blind regions [5]. 
Identification and re-identification of targets across 
multiple cameras are challenging because the same target 
may have significant variations in shape and appearance 
across cameras. Targets may enter and exit a scene randomly 
with highly non-linear motions. The process of identification 
and re-identification is normally performed by finding the 
similarities of the features of objects, and/or the probabilities 
of object transitions. 
Appearance is an obvious feature to associate objects in 
different camera views. However, due to different camera 
angles, illumination variations and different camera 
parameters, some appearances cannot offer help clearly. 
Thus, people prefer to choose features which are influenced 
by environment less. Color is one of the most commonly used 
appearance features. Color information is often represented 
by color histograms in the RGB (red, green, blue) or HSV 
(hue, saturation, value) color spaces. HSV is more robust to 
illumination changes due to its inherent properties. Javed et 
al. [6] proposed a subspace-based brightness transfer function 
(BTF) by using probabilistic principal component analysis 
(PCA) to calculate the subspace of BTFs for a set of known 
correspondences. This method relies on large amount of 
training data with a good range of clothing colors to give an 
accurate mean BTF (MBTF). Prosser et al. [7] proposed to 
use cumulative BTF (CBTF) instead of MBTF, which made 
use of available color information from a very sparse training 
set. A comparison of these two different BTFs [8] 
demonstrated similar behaviors of the two methods when the 
simple association problem needed to be solved. Their 
experiments also showed that appearance matching relying on 
color exclusively is not reliable when the scenario became 
more complicated.  
Fusing more visual features could help improve the 
effectiveness of the appearance matching and reduce the 
influence of environment changes. Wang et al. [9] employed 
multiple features such as color histogram, height, moving 
detection, travel time and speed to match objects across non-
 overlapping views.  
Spatial-temporal information is another important 
evidence to be considered for object re-identification. One 
way of using spatial-temporal constraints is predicting the 
objects’ positions when they are in the blind region. With the 
assumption of linear motion model, a Kalman filter or a 
similar mechanism was employed [10, 11]. The positions of 
the objects could also be inferred based on a common ground 
assumption, which allowed the warping between the cameras’ 
views by using a homography matrix [12]. In [13], an 
expanded triangulation with motion constrains, which 
assumed linear motion of the objects, was employed to infer 
the positions of the objects. 
Another category of research also used spatial-temporal 
information, but focused on the recovery of camera network 
topology, instead of object tracking. To make use of the 
spatial features, some methods have been introduced to 
predict the motions of objects by assuming a common ground 
plane and small gap between two cameras [10, 14]. Kang et 
al. [14] used a spatial-temporal joint probability data 
association filter (JPDAF) to formulate a joint probability 
model for encoding the appearances and motions of objects. 
Chilgunde et al. [10] used a Kalman filter to obtain the 
target’s track in the blind region between cameras. For multi-
 camera correspondence matching, the Gaussian distributions 
are applied for computing the tracking parameters across 
cameras for the target motion and position in the ground 
plane view. However, this approach is not competent to some 
complex situations such as obvious crossroads, big turns. 
Rahimi et al. [15] obtained the calibration parameters of the 
cameras and the trajectories of targets by using MAP 
estimation. Monari et al. [4] intended to track objects in both 
overlapping and non-overlapping camera networks. They 
applied 3D positions combined with color space features to 
perform object association. The 3D positions in the blind 
region are predicted by a Particle filter, which also was used 
in [16]. 
For data fusion algorithm of this kind of global data 
association issue, Huang and Russell [17] adopted multiple 
features for vehicle matching in a Bayesian formulation. An 
association matrix is employed for finding the best 
assignments for multiple objects. Zhang et al. [18] found an 
optimal MAP data association in a single camera by finding 
the max-flow of a network. There are also other fusion 
algorithms. Song and Roy-Chowdhury [19] proposed an 
optimized method to combine short-term feature 
correspondences and long-term feature dependencies across 
multiple cameras. The LP (linear programming) solution 
presented in [20] introduced a term that encouraged relative 
1214
positions of objects to remain constant, however it assumed 
that the number of objects was fixed. 
III. DESIGN AND ALGORITHM 
Vehicle tracking in multi-camera surveillance systems has 
attracted a lot of attention in recent years because of its 
widespread applications in many scenarios such as public 
transportation, campus security, and production process. In 
order to achieve full network tracking, diversified methods 
have been proposed in the last decade. One of the most 
common ones is to associate the same target appeared in 
different cameras by extracting the target appearance 
characteristics and spatial-temporal characteristics, while 
combining with the topology of multiple cameras. 
The key to associate the same targets in different cameras 
is to find the correspondence among various targets. 
Considering the scale of surveillance and the cost of 
installment, most video surveillance systems are non-
 overlapping which makes the observed targets discrete in 
space and time. Blind areas make it complex and difficult to 
track the targets. Now the usual way of doing this is to extract 
the external features and spatial-temporal features of the 
targets, and combine with the topology relationship of 
cameras, then build a spatial-temporal constraint with the 
target shifting among cameras, finally utilize the proper 
association algorithm to match the targets captured by 
different cameras. All of these are done for target tracking 
within the entire network. The solution for this problem 
includes three parts: the extraction of target associated 
features, the topology estimation of camera networks, and 
target association algorithm. 
For large-scale surveillance systems, the number of 
vehicles that appear in the cameras would be very large even 
in a short time, which not only leads to a big rise in 
computational time, but also brings great pressure to the 
system center. In order to achieve load balance, a sub-graph 
network division algorithm is designed and implemented. The 
whole camera network will be divided into multiple sub-
 graph units, the target correlation algorithm can be executed 
independently and parallel within each sub-graph units. 
A. Extraction of Target Associated Features 
The extraction of target associated features is the 
foundation of the realization of targets association. The 
approach presented partitions target associated features into 
two types: descriptive semantic information and characteristic 
quantity-based numerical data information.  
Semantic information refers to the detailed descriptive 
understandable information including the type, color and size 
of a target, when and where the target appears, the speed of 
movement, and its direction and track. The target type means 
the brand and model of the vehicle. Through exacting and 
modeling the header information of the target vehicle, the 
approach presented uses Speeded Up Robust Features (SURF) 
algorithm to compare the information with the standard 
vehicle models in database to obtain the type of the target 
vehicle. In addition, the motion direction and specific track of 
the target are gained by continuously tracking and recording 
the motion trail of the target vehicle within the view of a 
single camera. The target speed is calculated through the 
offset of the movement of the target.  
Characteristic quantity-based numerical data information 
refers to the characteristic value information which can be 
used to identify the level of similarity in the field of image 
recognition. The characteristic value information applied here 
includes vehicle type, Histogram of Oriented Gradients 
(HOG) operators, and Local Binary Pattern (LBP) operators. 
All the associated features are used to judge the relevance of 
two targets, and the semantic information are used for target 
filtering and choosing. 
In order to carry out the associated multi-feature of the 
target with reasonable fusion, a similarity measure function is 
presented here. Similarity measure function combines 
numerical features and set weight for each feature. It is used 
to describe the level of similarity between the targets and 
determine the correlation between the targets. Assume target 
a and target b are two vehicles observed in camera Ci and Cj 
respectively, the similarity measure function S(Oi,a, Oj,b) is 
defined in (1). 
( ) ( ) ( ) ( )
 , , , , , , , ,
 , , , ,i a j b t i a j b l i a j b a i a j bS O O T t t L l l A a a? ? ?= ? ?    (1) 
The above refers to the contrast ratio of the features 
including time denoted by T(ti,a, tj,b), location denoted by L(li,a, 
lj,b), appearance model denoted by A(ai,a, aj,b). We set a 
weight  for each feature to control the reliability. The value 
of  depends on the importance of the feature. For time and 
location, it is relative easy to construct the similarity of the 
specific objects according to object appearing time and the 
topology of the monitoring network.  So, we focus on the 
appearance model, which consists of vehicle type, HOG and 
LBP. 
a) Vehicle type  
The vehicle type means the brand and model of the target. 
Through exacting and modeling the header information of the 
target vehicle, the system uses SURF algorithm to compare 
the information with the standard vehicle models in database 
to obtain the information of the brand and model of the target 
vehicle. It makes the head area of the vehicle compare with 
other samples in the feature library and record the matching 
result.  
b) HOG 
HOG feature descriptor was first proposed by N. Nalal 
and B. Triggs [21] in 2005, its core idea is to represent the 
appearance and outline of the target by gradient or an edge 
direction of density distribution, without setting gradients and 
marginal positions. In the approach presented in this paper, 
two targets’ screenshots are used to do the comparison. First, 
each image is divided into some small connecting areas. 
These areas are often referred as cells. Then generate the 
gradient or edge direction histogram of each pixel within each 
cell. Finally combine these histograms by the statistical 
1215
methods to form the target’s HOG feature descriptor. In order 
to improve the accuracy of the feature, we use overlapping 
local contrast normalization technology. Put the local 
histogram of each cell in a broader interval called block, and 
calculate the density in the interval. According to the density 
of the block, the system normalizes other cells within the 
block. Normalization process makes HOG operator a better 
robustness with light changes and shadows. 
c) LBP 
A LBP feature descriptor is often used to describe the 
local texture features of the image. Since the LBP value is 
corresponding to pixel window, it means that LBP value is 
related with the location of the picture. This makes the 
selection of a suitable comparison window position a critical 
impact on comparative results. In order to reduce the position 
dependence of the LBP operator information, the image is 
divided into several sub-regions. The LBP feature value of 
each pixel is extracted within sub-region. So the LBP feature 
value of this sub-region could be described by the statistic 
histogram of these points’ LBP feature. The multiple sub-
 regional statistic histograms compose the LBP statistic 
histogram of the whole image. 
We use the cosine between feature vectors to represent the 
similarity. HOG and LBP are treated as two separate features 
for object matching. If two HOG descriptors are denoted by 
AH and BH, the cosine similarity score SH of HOG descriptors 
is calculated by (2). The similarity score SL of the LBP 
descriptors is calculated by (3). 
( )cosH H HH
 H H
 A B
 s
 A B
 ?
 ?
 = =                              (2) 
  
( )cosL L LL
 L L
 A B
 s
 A B
 ?
 ?
 = =
                                (3) 
B. Topology Estimation of Camera Networks 
The topology relationship of camera networks refers to 
the temporal and spatial patterns of the target movement 
among cameras. With topology relationship, it is possible to 
build a spatial-temporal constraint of the target shifting 
among cameras. According to the spatial-temporal constraint, 
we can not only properly reduce computing scale, but also 
further improve the accuracy of target association. The 
system builds the spatial topology relationship according to 
installation sites of the cameras and road connection, with the 
temporal topology relationship obtained by statistical 
learning. Within the fixed view of a camera, the appearing 
area and disappearing area of multiple targets can be learned 
based on the position information of the target appearing and 
leaving detected by the camera.  
As Figure 1 shows, for any pair of nodes (pi , pj), piRCm , 
pj RCn , ij, CmCn, there is a variable Li,j = {0, 1}, where 0 
indicates that objects between the two nodes cannot pass 
directly, without going through other nodes; 1 indicates that 
objects can directly pass the two nodes without going through 
other nodes. Since we are not concerned on the connections 
inside the camera, the transfer inside the camera is marked by 
dotted line. In real scenarios, traffic lights and traffic 
conditions may have some influence on the time it takes for 
the target to pass the distance between two cameras. Gaussian 
model is adopted here to indicate the temporal relationship 
between two cameras. The relationship is marked by solid 
edge in the figure. Its parameters (i,j, i,j2) are obtained by 
training and learning. 
 
Figure 1. Topology relationship of camera network 
C. Target Association Algorithm  
The target association algorithm in our approach is a 
minimum cost and maximum flow algorithm based on the 
relationship of the camera network topology. Figure 2 shows 
an example of the cost-flow network built in this paper. The 
following steps are taken to construct a cost-flow network: 
a) Select observed targets in each camera, based on the 
semantic information within a certain period of time, 
mark each of these selected targets as Oi,a; 
b) For each Oi,a, camera Ci has recorded its track 
information, thus we simply add the start point ui,a and 
end point vi,a in the track to graph G as independent 
nodes. At the same time, we also need to add a directed 
edge e(ui,a, vi,a) which has infinite capacity and zero 
cost to graph G; 
c) For any pair Oi,a and Oj,b, if the similar function 
between them is S(Oi,a, Oj,b) > 0, then add a directed 
edge e(vi,a, uj,b) which has the capacity of Cap(Oi,a, Oj,b) 
and the cost of S(Oi,a, Oj,b) to graph G. What’s more, if 
vi,a falls into disappearing area of the camera Ci, uj,b 
falls into the appearing area of the camera Cj and 
Lm,n=1, make Cap(Oi,a, Oj,b)=2, otherwise make 
Cap(Oi,a , Oj,b) = 1; 
d) Append virtual nodes source s and sink t for the graph 
G. For each region node pa in the camera subnet, if 
there are other directed edges from other camera subnet 
point to pa, for each node ui,a that falls into pa, add a 
directed edge e(s, ui,a) which has the capacity of 1 and 
the cost of 0 to graph G. On the contrary, if pa has a 
directed edge point to other camera subnet, for each 
node vi,a that falls into pa , add a directed edge e(vi,a, t) 
that has the capacity of 1 and the cost of 0 to graph G. 
After building cost-flow network, the minimum cost flows 
of the whole network can be solved by minimum cost and 
1216
maximum flow solution algorithm, which is actually to obtain 
the flow with maximum cost by the above algorithm.  
The obtained result is also a series of path of the 
maximum similarity values. The system controls the display 
of the predicted numbers of tracks according to the 
requirements of users, while gives more priority to the track 
with higher prediction probability for display. Besides, the 
system has a modification module, which is capable of self-
 revising according to the correction command submitted by 
users, including modifying the structure of cost-network G, 
recalculating and rebuilding new prediction track. 
Modification module can fix some errors produced by the 
procedure of videos analysis and process, thus improve the 
robustness of the system. 
 
Figure 2. The example of a cost-flow network 
D. Camera Network Sub-graph Partitioning 
A large surveillance system sometimes contains more 
than thousands of cameras which forms a huge camera 
network. If we want to track targets within the whole 
network, the amount of calculation is extremely enormous. 
To overcome this obstacle, in our approach, the camera 
network is divided into several independent sub-network units 
according to the characteristics of its topology relationship. 
The target association algorithm then can be carried on 
independently at the same time.  
The sub-graph partition should not destroy the structure of 
the camera network, so that the trajectory calculated in each 
sub-graph unit can be easily linked to form the total vehicle 
trajectory. In our system, the sub-graph network division 
algorithm should follow several principles. For each sub-
 graph A, A = {pi, qj | i, j = 1, 2, ... , m}, satisfies: 
a) Completeness Principles: For each target, if its 
appeared node qjRA, then the last disappeared node pi 
satisfied piRA. In reverse, if the target’s disappeared 
node pj R A and the target appears again, then the next 
appeared node qk satisfied qk RA. 
b) Minimization Principles: The amount of nodes in each 
unit should be as small as possible on condition of the 
completeness principle. It means that the camera 
network should be divided elaborately to get sub-graph 
units as many as possible. 
The completeness principle makes sure the complete 
independence of each unit, so that all nodes involved are 
within the sub-graph unit when calculating the associate 
relationship of the targets, and do not need exchange 
information with other units. Only each sub-graph remains 
independent, can the trajectory connect into the whole track. 
The minimization principle makes the structure of each sub-
 graph unit as small as possible, cannot be re-split, which 
guarantees the minimum calculation overhead of the 
associated algorithm.  
The method of devising the camera network into sub-
 graph units is as follow: in a field of camera view, if an 
appeared node qi only connected with a disappeared node pj, 
then remove the dotted connection between them and put qi 
and pj into two different sub-graph units. Eventually the entire 
camera network turns into an unconnected graph. Take C3 in 
Figure 1 as an example, the appeared node q1 is only 
connected with the disappeared node p4, then remove the 
edge e'(q1, p4) between them, makes q1 and p4 assigned into 
two different sub-graph unit. The method of dividing the 
whole network into sub-networks provides the distributed 
intelligent video surveillance system with strong support. It is 
possible to build some local process units to process video 
information, and the local process units only transmit key 
information to the central process unit for deeper analysis. 
Our system applies the framework of MapReduce based 
on Hadoop to realize the parallel computing of all sub-graphs. 
First, different tasks from different sub-graphs are mapped to 
different MapReduce nodes for track analysis independently.  
Then, all information will be returned to central server to be 
reduced by the presented approach mentioned in previous 
sub-sections.  
IV. EXPERIENCES AND PERFORMANCE 
To verify the effectiveness of the approach presented in 
this paper, a batch of vehicle monitoring videos from 23 
cameras in the real campus security monitoring system of 
Huazhong University of Science and Technology (HUST) are 
applied and analyzed. All videos are in the resolution of 
1920?1080 with 25 frames per second. 8 non-overlapping 
cameras with 5-minute durations are selected for evaluation. 
These dataset are challenging because: i) the roads monitored 
are two-way with lots of intersections, that makes the tracking 
environment more complicated; ii) due to the shadows of 
many trees along the roads in the campus, the colors and 
illuminations of the vehicle objects tracked change obviously 
across cameras; iii) the traffic are relative heavy. For example, 
in the selected cameras, more than 500 vehicles passed 
through the cameras; iv) high resolution video takes much 
time to process. The map view is shown in Figure 3. 
For evaluation of tracking, the tracking metric in [18] is 
used. Evaluation results across multiple cameras are shown in 
Table I. In the table, real targets indicate the number of real 
vehicles captured by the specified cameras. Track recall is the 
successful tracking rate within a camera. It is hard to get all 
tracks of all objects precisely. If the degree of accuracy of a 
target tracked is more than 90% compared to the 
1217
corresponding real track, it is considered as mostly tracked 
(see Table I). Since the time span of the group video data is 
limited, some vehicle tracks are broken off, which are marked 
as partially tracked. Since some conditions of camera views 
are not perfect enough, when a car disappears in the view 
which goes beyond the detection area, it is marked as mostly 
lost. Mostly lost also includes those cars that disappear in 
blind regions. 
 
Figure 3. Cameras for evaluation on the map 
TABLE I. EVALUATION OF MULTI-CAMERA TRACKING 
Camera 
number 
Real 
targets 
Track 
recall 
Mostly 
tracked 
Partially 
tracked 
Mostly 
lost 
1 15 100% 15 0 0 
2 46 87.0% 37 3 6 
3 43 90.7% 36 3 4 
4 39 97.4% 36 2 1 
5 34 100% 30 4 0 
6 38 92.1% 31 4 3 
7 9 77.8% 5 2 2 
8 9 77.8% 5 2 2 
Average 30.38 90.35% 24.38 2.5 2.25 
A. Location, Time and Appearance Measures 
For the location measure, enter/exit area is defined for 
each camera and their connections across cameras are used as 
spatial features. For the time measure, the mean and variance 
between two corresponding enter/exit areas is learned during 
the training stage. A single Gaussian distribution is used to 
model the object travel time. In this practical traffic 
surveillance system, since all cameras are deployed in a 
campus, the distance between two neighbor cameras is not far. 
Most of them are 50-200 meters, and the speeds of most 
vehicles in the campus are between 10-40km/h. Table II 
shows the parameters for enter/exit areas pairs during the 
training stage. 
TABLE II. LEARNED PARAMETERS OF TIME MEASUREMENT FOR  
ASSOCIATED ENTER/EXIT AREA PAIRS 
Pair of 
Enter/Exit Area 
Arrival time (second) 
Mean Variation 
Camera 1-2 8.5 7.5 
Camera 2-3 6 9 
Camera 3-4 11.5 3.5 
Camera 4-5 7 7 
Camera 5-6 3 3 
Camera 6-7 14 1 
Camera 7-8 5 2 
Additionally, since there are two intersections in the blind 
regions and there is a traffic light in one of the eight camera 
views, the variation of the link between them is set slightly 
looser.  
Location, time, SURF, HOG, and LBP are used for 
appearance measure. Here, the weights for the location, time, 
SURF, HOG and LBP are set to: 1, 1, 0.85, 0.22, and 0.55, 
respectively, based on experiments. 
B. Overall Performance of Multi-camera Tracking 
Since the classification method we used concerns 
primarily with the front area of the vehicle, the identifying 
accuracy is poor when a target shows only the back part in the 
camera views. However, it is a good opportunity to test how 
robustly the system can run in such a serious situation. 
Some vehicle objects of correct tracking across 6 major 
cameras are shown in Figure 4, while some of incorrect 
tracking are shown in Figure 5. 
 
Camera 1 Camera 2 Camera 3 Camera 4 Camera 5 Camera 6
 Figure 4. Objects of correct tracking across six major cameras 
Camera 1 Camera 2 Camera 2 Camera 3 Camera 2 Camera 4
 Figure 5. Objects of incorrect tracking (marked by red crosses)  
C. Computational Time 
Eight serials of monitoring videos with duration of 5 
minutes are processed by the server with 2.26 GHz 2xIntel 
Xeon E5520 CPU. The amount of objects we detected is 233. 
We use two ways to search for the tracks. One is the 
traditional Bayesian method, the other is our minimum cost 
and maximum flow (MCMF) method.  
1218
In accuracy, the contrast ratio between our method and the 
Bayesian method is 1:1.11; in the computation time, our 
method increases about 27.5%. Table III shows the 
comparison of detail computational time taken by two 
methods and Table IV shows the accuracy comparison. 
 
TABLE III. COMPARATION ON COMPUTATIONAL TIME 
Record 
amount 
Computational time 
Bayesian
 ?ms? 
MCMF
 ?ms? 
Performance 
improvement 
3 1534 1013 34% 
6 4112 2763 33% 
10 5873 4279 27% 
12 5627 4628 18% 
15 18811 12461 34% 
17 13195 10567 20% 
Average 8192 5951.83 27.5% 
TABLE IV. COMPARATION ON ACCURACY 
Record 
amount 
Accuracy 
Bayesian MCMF Comparison 
3 100% 100% 1:1 
6 100% 100% 1:1 
10 100% 100% 1:1 
12 50% 57.14% 1:1.14 
15 58.33% 83.33% 1:1.42 
17 53.33% 70% 1:1.31 
Average 76.94% 85.08% 1:1.11 
V. CONCLUSIONS 
In this paper, we present a new vehicle tracking 
surveillance approach with non-overlapping views in multi-
 camera. A framework to perform robust multiple targets 
tracking across multiple cameras is discussed. The multi-
 objects in multi-camera data association problem is formed 
with a minimum cost and maximum flow mode. For robust 
multi-camera tracking, time, location, classification type, and 
appearance of targets are effectively applied as a similarity 
measure. The approach is tested with real surveillance videos 
from the campus of HUST. The experimental results validate 
the robustness and effectiveness of the approach. 
Experimental results also indicate that the method of tracking 
across multiple cameras is feasible after a high quality of 
feature extraction and target detection. For future work, more 
efficient and effective appearance models and feature 
extraction are desired to increase the accuracy of the 
approach. 
ACKNOWLEDGMENT 
This work is supported by National Natural Science 
Foundation of China under grant No.61133008. 
REFERENCES 
[1] J. Berclaz, F. Fleuret, and P. Fua, “Robust people tracking with global 
trajectory optimization”, in Proceedings of IEEE Conference on 
Computer Vision and Pattern Recognition (CVPR’06), 2006, pp.744-
 750. 
[2] B. Wu and R. Nevatia, “Detection and tracking of multiple, partially 
occluded humans by Bayesian combination of Edgelet based part 
detectors”, in International Journal of Computer Vision, Vol.75, No.2, 
November 2007, pp.247-266. 
[3] N. Anjum and A. Cavallaro, “Trajectory association and fusion across 
partially overlapping cameras”, in Proceeding of IEEE International 
Conference on Advanced Video and Signal Based Surveillance 
(AVSS’09), 2009, pp.201-206. 
[4] C. del-Blanco, R. Mohedano, N. Garcia, L. Salgado, and F. 
Jaureguizar, “ Color-based 3D particle filtering for robust tracking in 
heterogeneous environments”, in Proceedings of second ACM/IEEE 
International Conference on Distributed Smart Cameras (ICDSC’08), 
2008, pp.1-10. 
[5] W. Youlu, S. Velipasalar, and C. Gursoy, “Distributed wide-area 
multi-object tracking with non-overlapping camera views”, in 
Multimedia Tools and Applications, 2012, pp.1-33. 
[6] O. Javed, K. Shafique, and M. Shah, “Appearance modeling for 
tracking in multiple nonoverlapping cameras”, in Proceeding of IEEE 
Conference on Computer Vision and Pattern Recognition (CVPR’05), 
vol.2, 2008, pp.26-33. 
[7] B. Prosser, S. Gong, and T. Xiang, “Multi-camera matching using bi-
 directional cumulative brightness transfer functions”, in Proceeding 
of the British machine vision conference (BMVC’08), Vol. 8, 2008, 
pp.1-10. 
[8] T. D’Orazio, P. Mazzeo, and P. Spagnolo, “Color brightness transfer 
function evaluation for non-overlapping multi camera tracking”, in 
Proceeding of ACM/IEEE international Conference on Distributed 
Smart Cameras (ICDSC’09), 2009, pp.1-6. 
[9] W. Youlu, L. He, and S. Velipasalar, “Real-time distributed tracking 
with non-overlapping cameras”, in Proceedings of IEEE Conference 
on Image Processing (ICIP’10), 2010, pp.697-700. 
[10] A. Chilgunde, P. Kumar, S. Ranganath, and W. Huang, “Multi-
 camera target tracking in blind regions of cameras with non-
 overlapping fields of view”, in Proceeding of the British Machine 
Vision Conference (BMVC’04), 2004, pp.1-10.  
[11] E. Monari, J. Maerker, and K. Kroschel, “A robust and efficient 
approach for human tracking in multi-camera systems”, in Proceeding 
of the IEEE International Conference on Advanced Video and Signal 
Based Surveillance (AVSS’09), pp.134–139.  
[12] J. Kang, I. Cohan, and G. Medioni, “Persistent objects tracking across 
multiple non overlapping cameras”, in Proceeding of the IEEE 
workshop on Motion and Video Computing (WACV/MOTIONS’05), 
vol.2, 2005, pp.112-119.   
[13] R. Pflugfelder and H. Bischof, “Tracking across non-overlapping 
views via geometry”, in Proceeding of the International Conference 
on Pattern Recognition (ICPR’08), 2008, pp.1-4.  
[14] Y. Shan, S. Sawhney, and R. Kumar, “Unsupervised Learning of 
Discriminative Edge Measures for Vehicle Matching between Non-
 Overlapping Cameras”, in Proceeding of the International Conference 
on Pattern Recognition (ICPR’05), 2005, pp.894-901. 
[15] A. Rahimi, B. Dunagan, and T. Darrell, “Simultaneous calibration 
and tracking with a network of non-overlapping sensors”, in 
Proceeding of IEEE Computer Society Conference on Computer 
Vision and Pattern Recognition (CVPR’04), 2004, pp.187-194.  
[16] W. Leoputra, T. Tan, and L. Lim, “Non-overlapping Distributed 
Tracking using Particle Filter”, in Proceeding of 18th International 
Conference on Pattern Recognition (ICPR’06), 2006, pp.181-185. 
[17] T. Huang and S. Russell, “Object identification: a Bayesian analysis 
with application to traffic surveillance”, in Artificial Intelligence, 
1998, pp.77–93. 
[18] L. Zhang, Y. Li, and R. Nevatia, “Global Data Association for Multi-
 Object Tracking Using Network Flows”, in Proceeding IEEE 
Conference on Computer Vision and Pattern Recognition (CVPR’08), 
2008, pp.1-8. 
[19] B. Song, and A. K. Roy-Chowdhury, “Stochastic adaptive tracking in 
a camera network”, in Proceeding IEEE Conference on Computer 
Vision and Pattern Recognition (CVPR’08), 2008, pp.1-8. 
1219
[20] H. Jiang, S. Fels, and J. Little, “A linear programming approach for 
multiple object tracking”, in Proceeding IEEE Conference on 
Computer Vision and Pattern Recognition (CVPR’07), 2007, pp.1-8. 
[21] N. Dalal and B. Triggs, “Histograms of oriented gradients for human 
detection”, in Proceeding of IEEE Computer Society Conference on  
Computer Vision and Pattern Recognition (CVPR’05), 2005, Vol.1, 
pp.886-893. 
[22] T. E. Choe, Z. Rasheed, G. Taylor, and N. Haering, “Globally optimal 
target tracking in real time using max-flow network”, in Proceedings 
of IEEE International Conference on Computer Vision Workshops 
(ICCVW’11), 2011, pp.1855-1862. 
1220

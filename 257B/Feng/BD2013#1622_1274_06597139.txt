A Throughput Driven Task Scheduler for Improving MapReduce Performance in
 Job-intensive Environments
 Xite Wang, Derong Shen, Ge Yu, Tiezheng Nie, Yue Kou
 College of Information Science and Engineering Northeastern University, P.R. China
 wangxite@research.neu.edu.cn
 {shenderong, yuge, nietiezheng, kouyue}@ise.neu.edu.cn
 Abstract—MapReduce has been proven to be a highly
 desirable platform for scalable parallel data analysis. The task
 scheduling in MapReduce is very crucial for the job execution
 and has a marked impact on the system performance. To
 the best of our knowledge, the previous scheduling algorithms
 rarely consider the job-intensive environments and are not able
 to provide high system throughput. Hence this paper proposes
 a novel technique for job-intensive scheduling to improve the
 system throughput. Firstly, by making an in-depth analysis
 of job-intensive environments, we sum up 4 major factors
 which affect the system throughput. Secondly, based on the
 factors, an efficient technique, called throughput driven task
 scheduler is proposed, in which, we adopt a series of effective
 measures to improve the throughput of a MapReduce cluster
 system. Finally, plenty of simulation experiments are made and
 the experimental results show that the scheduler can provide
 higher throughput than the previous systems and is able to
 meet the requirements of practical job-intensive applications.
 Keywords-MapReduce; scheduling; throughput;
 I. INTRODUCTION
 Nowadays, daily internet access, business computing and
 scientific research etc. generate a mass of data increasingly.
 The advanced analysis [1], [2] on Big-Data has become a
 very hot topic in many fields. MapReduce [3], a distributed
 data processing framework, has been proven to be a powerful
 technique to process Big-Data analysis. Specifically, the
 MapReduce system uses a master node to manage the cluster
 and some slave nodes to provide computing capacity. While
 the system accepts a job, the task scheduler on the master
 splits the job into some tasks and assigns them to slaves
 automatically. Generally, users need only provide the simple
 map and reduce functions to process terabytes of data.
 In the parallel data analysis, it is indubitably that task
 assignment (or task scheduling) is crucial for the system
 performance. The unbefitting task scheduler usually causes
 a lot of network transmission, and reduces the system
 processing capacity. For the MapReduce platform, there have
 been plenty of scheduling algorithms, which could provide
 suitable approaches of task assignment in many applications.
 However, none of the existing schedulers have considered
 the job-intensive environment which means in a period of
 time, the system receives a batch of jobs, and all the jobs
 must be completed within the stipulated time. Actually,
 the job-intensive environment is quite familiar in many
 IT companies, for example, search engines (e.g. Google)
 would usually choose a period of midnight (e.g. 3:00 am-
 5:00 am) to process some analysis on users’ web logs, and
 shopping sites (e.g. Amazon) prefer tidying the transaction
 information while network access is infrequent (also in
 midnight). Then the submitted jobs must be accomplished
 within the stipulated time, otherwise the daily work will be
 influenced. If the workload gets heavy or the scale of cluster
 is limited, this objective may be difficult to achieve.
 master
 task slot split of f1 split of f2
 s1
 s2 s3
 s4
 (a) Cluster Status
 assign 
task 
request 
task 
data
 transfer 
s2 s3
 s4s1
 master
 (b) Task Assignment for Fairness
 Figure 1. Example of Scheduling
 Whereas the existing schedulers have not considered the
 job-intensive environment, and cannot guarantee the system
 throughput capacity. Figure 1 illustrates a simple example
 to explain the problem. As Figure 1(a) shows, there is a
 distributed system (e.g. Hadoop [4]) which has 4 slave nodes
 and each slave provides 1 task slot for processing, and the
 system adopts the popular Fair task scheduler [5], [6]. In
 the underlying file system there are 2 files f1, f2, which
 are split into several splits. Then the system receives 2 jobs
 j1 (related file is f1), j2 (related file is f2), and both the
 weights of j1, j2 are set to 1, which means either of j1,
 j2 should obtain 2 slots for processing. For the situation
 described in Figure 1(b), the scheduler would allocate a split
 of f2 to the idle slot on s3 to ensure the fair principle and
 lead to more network overhead, obviously it is not benefi-
 cial for the system throughput. Therefore, to provide high
 system throughput, we propose a novel technique, called
 Throughput Driven task (TD) scheduler, for the job-intensive
 MapReduce environment. The scheduler mostly focuses on
 the map phase, which dominates the computational cost of
 the most MapReduce applications. In particular, we make
 2013 IEEE International Congress on Big Data
 978-0-7695-5006-0/13 $26.00 © 2013 IEEE
 DOI 10.1109/BigData.Congress.2013.36
 211
the following contributions.
 1) We describe the scheduling model for job-intensive
 MapReduce environment, and summarize several fac-
 tors which can impact the system throughput.
 2) We propose a novel scheduling approach, TD sched-
 uler, which can guarantee high ratio of local task
 assignments, avoid hotspots and take full advantage
 of the system resources effectively.
 3) We evaluate TD scheduler through experiments with
 real data set. The improvement of system throughput
 is testified by the experimental results.
 The rest of this paper is organized as follows. We state the
 overview of the MapReduce framework and several previous
 approaches of task scheduling as background knowledge in
 Section II. We define the problem model and state the design
 requirements in Section III. We describe the TD scheduler
 in detail in Section IV. We evaluate the performance of TD
 scheduler in Section V. Finally, the conclusion of this paper
 is stated in Section VI.
 II. BACKGROUND KNOWLEDGE
 In this section, we overview the MapReduce framework,
 and present the related work about pre-existing scheduling
 strategies in the MapReduce environment.
 A. MapReduce Overview
 As a successful open-source implement of MapReduce,
 the Hadoop system [4], has arisen for several years. It
 contains a single master node and several slave nodes.
 The master takes charge of the load balancing and task
 scheduling, the slaves are response for the actual data storage
 and each slave has a certain number of task slots for data
 processing. Functionally, Hadoop is constituted by 2 major
 subsystems. a) The underlying file system HDFS [7] stores
 the data in the form of file for further analysis, and due to the
 huge size, files would be split into a number of isometrical
 splits for distributed storage, the master records the location
 of each split in block map. b) The data processing system
 is used to process the MapRedcue jobs. In general, a
 MapReduce job uses a specific file stored on HDFS as its
 input. While a job is submitted, the scheduler will split the
 job into a number of map and reduce tasks and each map
 task processes a specific split of the input file. The tasks
 would be assigned to the task slots on slaves to be executed
 in parallel.
 As Figure 2 shows, while processing the MapReduce jobs,
 each slave periodically sends a heartbeat to the master to
 report the statuses of task slots on it. If a task slot is idle,
 the scheduler would assign a suitable task to the slot.
 B. Related Work
 As the kernel of parallel data analysis, the task scheduling
 technique has been studied for years. The default scheduling
 approach of Hadoop, Job Queue Task scheduler can afford
 task
 slots
 splits
 block map
 task scheduler
 master
 slaves
 he
 a
 rt
 be
 a
 t resp
 o
 n
 setask
 slots
 splits
 task
 slots
 splits
 Figure 2. Hadoop Framework
 a fundamental and effective way to assign tasks. Actually,
 it is a First In First Out (FIFO) strategy, which means all
 the jobs are sorted into a queue according to the submission
 time, and the later jobs will not be scheduled until the former
 jobs are finished.
 The Capacity scheduler [8], which is designed for multi
 users to share one underlying cluster, can allocate resources
 to different users according to the configuration and avoid
 individual user monopolizes the system. It supports multiple
 job queues and each queue has a certain proportion of
 resources. In each queue, jobs are scheduled with the FIFO
 strategy, and the unoccupied resources would be shared with
 the other queues. Fair scheduler [5], [6] is also motivated by
 the multi-user situation. It has several job pools constituted
 by some jobs and each job is assigned a specific weight.
 The difference is that in Fair scheduler each job expects
 to obtain the computing resources according to the weight,
 and the idle resources would be assigned to the job with
 the most vacancies. Thomas Sandholm et al. [9] presented a
 Scheduling strategy which can adjust the allocated capacity
 of users. The scheduler allows uses to apply for more
 computing resources if their submitted jobs are important.
 Besides, there are several further studies on task schedul-
 ing. Jorda Polo et al. [10] presented a method applying to the
 deadline scheduling. Each job is submitted with a specific
 deadline, according to the accomplished tasks, the scheduler
 calculates the end time of the job. If there is a job which
 cannot finish before the deadline with its allocated resources,
 the idle resources will be allotted to it. Kamal Kc et al. [11]
 presented a job execution cost model to predict the finish
 time of a new job. If the expectant time can not meet the
 deadline, the job will be rejected.
 In practice, the performance of each slave node is not
 identical, some older machines may be much slower than
 the others. Therefore, Matei Zaharia et al. [12] presented
 the LATE Scheduler for the heterogeneous environment.
 According to the time cost per task, the scheduler estimates
 the processing ability of each slave and the finish time of the
 tasks in processing, the slower task will be restarted on the
 slave with good processing capacity. YongChul Kwon et al.
 [13] discussed the skew problem in MapReduce applications.
 The technique detects the skew through calculating the finish
 time of tasks, the rest of the slower tasks will be split and
 212
assigned to the other slaves.
 In brief, the previous approaches rarely pay attention to
 the job-intensive environment. In contrast, our technique
 adequately considers the pressure of batch jobs and can
 provide higher throughput.
 III. PRELIMINARIES
 Based on the MapReduce scheduling framework, we will
 define the problem model in job-intensive environment and
 illustrate the design requirements for our TD scheduler.
 A. Problem Definition
 In the job-intensive environment, the job queue Q =
 {j1, j2, j3 . . .} is a set of MapReduce jobs which are sorted
 in ascending order of submission time. Each job in Q is
 related to a specific input file stored on HDFS. For the most
 general situation of MapReduce, we can make the following
 2 assumptions.
 1) Because in a single job all the tasks in the map phase
 execute the same computing, the execution time of the
 map tasks in a single job is almost the same if we do
 not consider the nonlocal processing.
 2) For each job in Q, the map tasks would only read the
 splits of the input file from HDFS and process their
 own computings. Hence we assume that even if job
 j1 uses the same input file that the preceding job j2
 uses, j1 would not get speedup by j2.
 For each job j in Q, we denote the size of j as |j|, which
 is equal to the split number of the input file of j. We also
 denote the number of task slots which are occupied by j
 as j.occupied, the number of remaining untreated tasks of
 j as j.remain. Each job j is attached with a parameter
 j.demand, which means the slot number that j expects to
 occupy. It can reflect both the size and expected completion
 time of the related job. Based on j.demand, we introduce
 a new conception relaxed restriction, which means in job-
 intensive environment, the scheduler can prior consider the
 system throughput and does not guarantee each scheduled
 job j in Q can occupy j.demand slots, but according to
 the uses’ requirements, the scheduler guarantees j.occupied
 will not much smaller than j.demand so that j will not be
 finished much later than expected.
 Given a MapReduce cluster with T task slots and a job
 queue Q with a certain number of jobs, throughput is inverse
 proportional to the completion time of Q. Hence the goal
 of TD scheduler is to finish all the jobs in Q in the shortest
 possible time under the premise that all the jobs meet relaxed
 restriction.
 B. Design Requirements
 In the job-intensive environment, we summarize the fol-
 lowing 4 major factors which can impact the system through-
 put. We design the TD scheduler to satisfy the requirements
 of the 4 factors.
 1. High ratio of the local processing. Nonlocal tasks
 would lead more network transmission and more execution
 delay. In the distributed environment, it is necessary to
 maintain high ratio of the local processing.
 2. Choosing a befitting nonlocal task. In practice, for
 the high parallelism, nonlocal execution cannot be avoided,
 and choosing a better nonlocal task would be beneficial for
 the throughput. For instance, there are jobs j1 (input file
 is f1) and j2 (input file is f2), the split sizes of both f1
 and f2 are 128MB. The map phase of j1 processes some
 computing-intensive tasks (e.g. skyline query), the average
 execution time per task is 40s. In contrast, j2 processes
 IO-intensive tasks (e.g. word count), and the time is 5s.
 Therefore, ideally if we assign a nonlocal task of j1, there
 would be few performance reduction if the data transmission
 speed over the network can reach to 128MB/40s= 3.2MB/s,
 and 25.6MB/s for nonlocal task of j2. However we cannot
 insure the data transmission speed can meet the requirement
 because the reduce phase or the other applications could
 occupy the network resources. As a result, it is better to
 non-locally assign computing-intensive tasks for throughput.
 3. Avoiding hotspots. While processing in parallel, data
 stored on each node may be unbalanced in a certain period
 of time, there will be some nodes from which many nonlocal
 tasks read data, and we call the nodes as hotspots. the
 hotspot can distinctly impact system thoughput because a
 number of nodes read data from it so that the transmission
 speeds become lower, and the performance of hotspot itself
 decreases.
 4. Full use of system resources. Some task schedulers
 [14], [15] allow a small job to occupy the entire cluster,
 and shut down the idle nodes for energy saving. However,
 allowing nodes to be idle is undoubtedly inapposite for
 system throughput. Hence it is fundamental to make full
 use of system resources.
 IV. TD SCHEDULER
 To make TD scheduler more practical, we adopt the
 popular multi-job co-scheduling model which has been used
 for many scheduling strategies (e.g. Fair scheduler), instead
 of scheduling the jobs one by one. We denote the set of the
 co-scheduled jobs as Ssch. Then in order to facilitate the
 description, in Section IV-A, we introduce the scheduling
 approach in the simple case that ssch is stable (correspond-
 ing to Factors 1-3 in Section III-B), which means each job in
 Ssch has abundant tasks so that no job ends and no new job
 comes in. The update of Ssch will be described in Section
 IV-B (corresponding to Factor 4 in Section III-B). Finally,
 we describe the process comprehensively in Section IV-C.
 A. Task Assignment for Stable Co-scheduled Jobs
 For the MapReduce cluster with T task slots and a job
 queue Q = {j1, j2, j3 . . .}, it is very straightforward that
 213
the first n jobs in Q can join Ssch (or be co-scheduled)
 if
 ∑n
 i=1 j.demand ≤ T . We denote
 ∑
 j?Ssch j.demand as
 D. Then, if a slot s on node n becomes idle and requires
 task, what the scheduler actually do is a) to select a suitable
 job j from Ssch and then b) to select a task of j on the
 suitable node to assign. The detailed methods are described
 as follows.
 1) Select the Suitable Job:
 For each job j in Ssch, we denote the average execution
 time per map task as function amt(j), which can be given
 a default value according to the job type in the beginning,
 and estimated through the average completion time per map
 task while j is in processing. Therefore, we sort all the jobs
 in Ssch in ascending order of amt(j), j ? Ssch to generate
 a queue S?sch. Obviously in S?sch, the former jobs are IO-
 intensive and the latter ones are computing-intensive. Hence
 the basic method for job selection is
 1) Considering Factor 1 in Section III-B, while a task slot
 is idle, if there is at least one task belonging to the
 jobs of S?sch on node n, we make a local assignment.
 Otherwise, a nonlocal assignment is unavoidable.
 2) Based to Factor 2 in Section III-B, for the local assign-
 ment in S?sch, the former jobs which have untreated
 local tasks will be prior selected. For nonlocal assign-
 ment, the latter jobs in S?sch will be prior selected.
 Figure 3 shows an example of basic method. As Figure
 3(a) shows, the first 4 jobs in Q can be co-scheduled and
 the corresponding S?sch = {j2, j4, j3, j1} can be calculated
 (Line 4). The job selection is showed in Figure 3(b), where
 a slot on node n1 and a slot on n3 are idle. There are tasks
 of j1 and j2 on n3, hence the former job j2 in S?sch will
 be selected for local assignment. In contrast, there is no
 task belonging to S?sch on node n1, so the later job j1 in
 S?sch will be selected for nonlocal assignment. However if
 we only consider the system throughput, in the early stages
 of processing, most of the nodes could store a few tasks of
 the former jobs (e.g. j2, j4) in S?sch, then the former IO-
 intensive jobs may occupy most of the slots. Whereas the
 later computing-intensive jobs can only get a small number
 of slots for processing (Line 5 in Figure 3(a)), and the
 completion time is not guaranteed, which is against to the
 relaxed restriction. To avoid the problem, we improve the
 basic method by using the threshold technique.
 For the queue S?sch, lower bound L ? (0, 1) means that
 for each job j in S?sch, the scheduler ensures j.occupied ≥
 L ? j.demand, so that all the jobs could be accomplished
 in satisfactory time.
 If only using the lower bound, the scheduling strategy
 is still not perfect enough because there may be sticky
 slots, which means some slots may forcibly execute tasks
 of the same job for a period of time. For example as
 Figure 4 shows, the IO-intensive jobs occupy quite a lot of
 slots, whereas the computing-intensive job j only occupies
 j.demand?L = 70 slots for guarantee. When slot s finishes
 Q j1 j2 j3 j4 j5 ...
 j.demand 200 300 100 350 350 ...
 amt(j) 23 4 10 5 - ...
 sequence number in S?sch 4 1 3 2 - ...
 possible occupied slots 50 480 70 400 - ...
 (a) Jobs in Q (T = 1000)
 m
 n1
 idle
 n2
 busy
 n3
 idle
 . . .
 14 tasks of j1
 17 tasks of j2
 ...
 27 tasks of j5
 ...
 (b) Job Selection
 Figure 3. Example of Basic Method
 a task of j and requests task for processing, a task of j will
 have to be assigned to s because j.occupied = 69 < 70,
 even if there is no local task of j. The sticky slots are
 unsuitable for parallel scheduling and can increase nonlocal
 assignment potentially. Hence, we introduce upper bound to
 reduce the sticky slots.
 master
 done
 task of j demand=100 L=0.7occupied=69
 slot s
 Figure 4. Sticky Slot
 For the queue S?sch, upper bound H (H > 1) means that
 for each job j in S?sch, the scheduler ensures j.occupied ≤
 H ? j.demand. Through limiting the maximum number of
 slots which IO-intensive jobs can occupy, the upper bound
 enables the computing-intensive jobs occupy more slots and
 reduces the probability that sticky slots emerge. Note that the
 upper bound should not be set too high otherwise it will be
 meaningless. Specifically, ?j ? S?sch, T ? j.demand?H >(D ? j.demand) ? L, which means if a job j occupies
 j.demand ? H slots, the other jobs can obtain more than
 (D ? j.demand)? L slots. In other words,
 H < (T ? L?D)/dmax + L (1)
 where dmax = max{j.demand|j ? S?sch}. Given a user
 defined L, the scheduler can compute H according to S?sch.
 As Figure 3(a) shows, if L = 0.7, H < (1000 ? 0.7 ?
 950)/350+ 0.7 = 1.66.
 Consequently by utilizing the combined action of lower
 and upper bounds, the basic scheduling can select suitable
 job for high system throughput. Then we describe the
 method for task selection.
 2) Select the Task on Suitable Node:
 In the previous step, we have get the suitable job j and
 determined the assignment is local or not. The local assign-
 ment is simple so we would not give unnecessary details.
 Whereas the nonlocal assignment could face the hotspot
 214
problem. Hence we will mainly illustrate the method for
 the nonlocal task selection in the following (corresponding
 to Factor 3 in Section III-B).
 In general, a hotspot is the node which occurs data
 transmission with a number of other nodes simultaneously,
 and it will distinctly affect the system performance. To
 further analyze the problem, we firstly divide the hotspots
 into two categories.
 The node n is an actual hotspot if the total number of
 the nodes which occur data transmission with n is larger
 than the threshold ?. In other word, given the network IO
 bandwidth ?, if there is a nonlocal task which reads data
 from n, the performance will not be affected if the data
 transmission speed can reach to ?/?. Besides, node n is a
 potential hotspot if the remaining untreated workload of n is
 larger than the others, in the foreseeable future, a potential
 hotspot may become an actual hotspot.
 To avoid actual hotspot, a data structure hot queue is
 introduced. If there is a new nonlocal task assignment that
 a slot requires data from node n at time t, the hot queue
 appends a tuple (n, t) to the tail. We denote ? as the size of
 each split of input file, and consider the network I/O speed
 at ?/?. Then for current time tcur and the tuple (n, t), the
 tuple is overdue if t+ ?? ?/? ≤ tcur. Periodically, the hot
 queue inspects the tuples from the head and eliminate the
 overdue ones. In addition, there is a corresponding statistical
 table. For each node n appears in the hot queue, the table
 records the total number countn of the tuples which contain
 n. if countn = ?, we forbid more connections to n.
 To avoid potential hotspot, We firstly calculate the
 remaining untreated workload of node n, Rn =∑
 j?S?
 sch
 amt(j)? rsjn, where rsjn is the number of the
 remaining untreated tasks of job j on node n. Then the
 potential hotspot with the largest remaining workload has
 the highest probability to become an actual hotspot, we tend
 to select it and reduce its remaining workload.
 In summarize, while the scheduler has selected j to make
 a nonlocal assignment for the idle slot s, we firstly get
 the node set Nj which still have untreated tasks of j,
 and eliminate the nodes which already have ? connections
 according to the statistical table. Finally, we choose the node
 nj which has the most remaining workload in Nj and assign
 a task of j on nj to s.
 B. Update of Co-scheduled Jobs
 As the processing continues, some jobs in Ssch will
 be finished and the following ones join in. When Ssch
 updates, some idle nodes may obtain no tasks for processing,
 which is not conducive to system throughput. Hence in
 this section, we introduce the strategy for full utilization
 of system resources (corresponding to Factor 4 in Section
 III-B). Firstly the scene which can produce idle nodes will
 be described through an example.
 As Figure 5 shows, job j1, j2, j3, j4 are co-scheduled.
 However in last phase of j1, the remaining untreated
 tasks j1.remain = 0 and 150 tasks of j1 are stil-
 l in processing, so j1 can only occupy 150 slots al-
 though it demands 400 slots. For the other jobs, maximum
 (j2.demand + j3.demand + j4.demand)?H = 780 slots
 can be occupied. Hence 70 slots would acquire no tasks.
 Figure 5. Example of Idle Nodes
 To solve the idle-resource problem, we introduce a con-
 ception job status for the jobs in processing. Specifically,
 the job js is senile if it has been scheduled for some
 time and will be finished soon, js.remain = 0 and
 js.occupied = 0. We denote the jobs in status senile as
 set Ssen, and Osen =
 ∑
 js?Ssen js.occupied. The job ja
 is adult if it is scheduled steadily and ja.remain > 0 &
 ja.demand ? L ≤ ja.occupied ≤ ja.demand ? H . We
 denote the jobs in status adult as set Sadu. The job jt is
 teenaged if it has just been allowed to become adult and
 jt.occupied < jt.demand? L, then the idle nodes will be
 prior assigned to jt to ensure it will become adult in a short
 time. We denote the jobs in status teenaged as set Stee,
 and redefine D =
 ∑
 j?Sadu?Stee j.demand (mentioned in
 Section IV-A). The job ji is infantile if it is scheduled only
 for avoiding idle resources, it is not restricted by L and has
 lower priority than the jobs in Sadu. We denote the jobs in
 status infantile as set Sinf , and note that the size of Sinf is
 no more than 1.
 waiting processing finished
 infantile teenaged adult senile
 PL  1 PL  3 PL  2 
priority level   3>2>1 
Figure 6. Temporal Relations between Statuses
 In addition, job not in processing is either in status waiting
 (the set is Swait) or finished (the set is Sfin). The 6 statuses
 have strict temporal relations. As Figure 6 shows, each
 submitted job is in one of the 6 statuses. The teenaged jobs
 have the highest priority to be selected and the priority of
 infantile jobs is the lowest. The status can be transformed
 only in the order from left to right. Then the first job jw in
 Swait can be transformed to status infantile if Sinf = ? and
 Osen + D < T (2)
 215
The infantile job ji can disaffiliate from Sinf if
 Osen + D + ji.demand ≤ T (3)
 Then ji will become a teenaged job if jt.occupied <
 jt.demand? L, else it can become an adult job directly.
 With the formulas above, we ensure there are no idle
 resources in the data processing. Thus far, our proposed TD
 Scheduler has been introduced. In the next section we will
 describe the process of TD in more detail.
 Algorithm 1: TD Scheduler
 input : an empty slot s on node n; the last task of s belongs to job j
 output: a new task assigned to s
 j.occupied ??;1
 if j ? Sadu ? Stee && j.occupied < j.demand ? L then2
 jass ? j;3
 AssignTask SpecificJob(jass, n, s);4
 else5
 if j ? Ssen then6
 if j.occupied == 0 then7
 add j to Sfin;8
 if Sinf = ? then9
 jinf ? get the infantile job from Sinf ;10
 if Osen + D + jinf .demand ≤ T then11
 remove Jinf from Sinf ;12
 if Jinf .occupied < jinf .demand? L then13
 add jinf to Stee ;14
 else15
 add jinf to Sadu ;16
 else if Osen + D < T then17
 add the first job of Swait to Sinf ;18
 if Stee = ? then19
 jass ? get the first job in Stee ;20
 AssignTask SpecificJob(jass, n, s);21
 else22
 Jass ? AssignTask(n, s) ;23
 jass.occupied ++, jass.remain?? ;24
 if jass ? Stee && jass.occupied ≥ jass.demand? L then25
 remove jass from Stee to Sadu ;26
 if jass.remain == 0 then27
 add jass to Ssen;28
 end;29
 C. Algorithm Description
 In practice, the primary function of TD scheduler is
 assigning a untreated task to a task slot while the slot
 becomes idle until all the submitted jobs are finished. We
 firstly introduce the frame of TD scheduler in Algorithm 1.
 While a slot on node n completes a task of job j and the
 scheduler receives the request, j.occupied ? 1 (line 1). If
 j ? Sadu?Stee and j.occupied < j.demand? L, j becomes
 the selected job jass, and a task of j will be assigned to n
 (lines 2-4) (The function AssignTask SpecificJob(jass, n, s)
 is described in Algorithm 3). Else we will update the job
 status if j is senile (lines 6-18). Specifically, j becomes
 finished if j.occupied == 0 (lines 7, 8). If Sinf = ?, we
 remove the infantile job from Sinf if it conforms to Formula
 3 (lines 10-12) and add it to Stee or Sadu according to the
 slot number it occupies (lines 13-16), if Sinf == ?, we
 consider whether the first job in Swait can become infantile
 according to Formula 2 (lines 17, 18). Then we assign a
 task to s (lines 19-23). If Stee = ?, we assign a task of
 the first job in Stee to s (lines 19-21). Else we choose a
 task of a suitable job to assign (lines 22, 23) (Function
 AssignTask(n, s) is described in Algorithm 2). Finally we
 update the variables of j (line 24). The selected job jass can
 be added into Sadu or Ssen due to the related restrictions
 (lines 25-28). Then the algorithm ends.
 Algorithm 2: AssignTask(node n, slot s)
 input : node n, slot s
 output: the job to which the assigned task belongs
 sort Sadu in ascending order of amt() ;1
 for each job j in Sadu do2
 if n has untreated tasks of j && j.occupied < j.demand?H then3
 assign a task of j on n to s ;4
 return j ;5
 if n has untreated tasks of jinf in Sinf &&6
 jinf .occupied < jinf .demand?H then
 assign a task of jinf on n to s ;7
 return jinf ;8
 for each job j in Sadu by reverse order do9
 if j.occupied < j.demand?H then10
 nass ? GetNonHotspotNode(j);11
 assign a task of j on nass to s ;12
 return j ;13
 nass ? GetNonHotspotNode(jinf );14
 assign a task of jinf on nass to s ;15
 return jinf ;16
 Algorithm 2 describes the details of function Assign-
 Task(node n, slot s). Firstly we sort the jobs in Sadu
 in ascending order of amt() (line 1). we scan the Sadu
 until we get a job j of which n has untreated tasks and
 j.occupied < j.demand?H , then assign a task of j on n
 to s and return (lines 2-5). If we do not get a suitable job,
 we select the job jinf in Sinf if n has untreated tasks of
 jinf and jinf .occupied < jinf .demand?H (lines 6-8). If
 still no suitable job, we scan the Sadu by reverse order until
 we get a job j that j.occupied < j.demand ?H , then we
 use the method described in Section IV-A to get a node nass
 (nass has the most untreated remaining workload and it will
 not become an actual hotspot), and we assign a task of j on
 nass to s and return (lines 9-13). If we still do not get the
 suitable job, we select jinf and get the related non-hotspot
 node nass, then assgin the task and return (lines 14-16).
 Algorithm 3: AssignTask SpecificJob(job j, node n,
 slot s))
 input : job j, node n, slot s
 if n has untreated tasks of j then1
 assign a task of j on n to s;2
 else3
 nass ? GetNonHotspotNode(j);4
 assign a task of j on nass to s;5
 216
Algorithm 3 describes the function Assign-
 Task SpecificJob(job j, node n, slot n). If n has untreated
 tasks of j, assign a task of j on n to s (lines 1, 2). Else get
 a non-hotspot node nass, and assign a task of j on nass to
 s (lines 4, 5).
 V. EXPERIMENTAL EVALUATION
 In this section, we will evaluate the performance of TD
 scheduler through comparing to the Hadoop default FIFO
 scheduler and the popular Fair scheduler. In particular, we
 employ the following 4 kinds of applications: word count,
 inverted index, grep and distributed sort to compose the job
 queues for testing. And we use the real data sets (web pages
 extracted from Wikipedia, ) to build our experimental data
 sets. To imitate different real applications, we also generate
 3 types of job queues. Small type means each job size (or
 the split number of input file) in the queue is small (≤ 100).
 Large type means each job size is large (≥ 250). Mix type
 means the job size can be varied [50?300]. Each type of the
 job queue is composed by the 4 kinds of applications and
 all the split sizes are 64MB. The input file sizes of the job
 queues span [100 - 300] GB (default 200GB). The default
 value of the lower bound L=0.7, upper bound H=1.3.
 We adopt a cluster constituted by 1 master and 50 slaves.
 Each node has Core i7 870 CPU, 500G disk, 8G memory.
 Each slave node has 2 slots for map tasks and 2 for reduce
 tasks, so there are total 100 map task slots. We use Hadoop
 0.20.1 as our software environment. In Section V-A, we
 evaluate the performance of TD through the following 3
 aspects: completion time of a job queue, the ratio of local
 task assignment, the occurrence number of hotspots. We
 evaluate the impact of the threshold in Section V-B.
 A. TD-scheduler Performance
 In Figure 7, we compare the completion time of the 3
 schedulers using different types of job queues. To accurately
 reflect the performance differences, the initialization time
 of the job queue has not been considered. For each type
 of job queue, the completion time of our TD scheduler
 is significantly shorter than the others, in other word, TD
 scheduler can provide better throughput. In particular, by
 comparing the 3 experimental results showed in Figure 7,
 higher ratio of the small jobs in a job queue will markedly
 lead to more completion time of Fair and FIFO. In contrast,
 TD shows better performance.
 We also evaluate the network overhead through calculat-
 ing the ratio of local task assignment, and the results are
 described in Figure 8. For the large type, Fair and FIFO
 can maintain high ratio of local assignment(70%), for small
 type, the ratio is reduced to a very low level(35%). Whereas
 TD scheduler can hold a high ratio of local assignment to
 reduce the network overhead.
 In Table I, we divide the process into 5 stages on average
 and record the the occurrence number of hotspots in each
 stage, then we repeat the same process 3 times and calculate
 the average values as the experimental results. As Table I
 shows, the TD scheduler can avoid majority of the hotspots.
 Note that there are still some hotspots, which can be caused
 by 2 reasons: a) the nonlocal assignment is not allowed to
 be off-rack considering the network overhead. b) In the final
 stage, the distribution of the remaining data may be quite
 unbalanced, hence the hotspots are unavoidable.
 Table I
 OCCURRENCE NUMBER OF HOTSPOTS
 Scheduler Job Type Stage in Processing (percent) Total1-20 21-40 41-60 61-80 81-100
 Fair
 small 25.3 19.7 23 24.3 29 121.3
 mix 17.7 15.3 18.3 15 20 86.3
 large 3 6.3 4 4.7 9 27
 FIFO
 small 23 18.3 19.7 22.7 25 108.7
 mix 14 13.3 15.7 16.3 19.7 79
 large 2 5.7 3.7 4 6.3 21.7
 TD
 small 0 0 0.7 0 2 2.7
 mix 0 1 0 0 1.7 2.7
 large 0 0 0 0 0.7 0.7
 B. Threshold Analysis
 In this section, we firstly introduce the impact of the lower
 bound in TD. As Figure 9(a) shows, high lower bound leads
 to lower throughput, specially if L > 0.7, the throughput
 would reduce signally. Whereas it would has little decrease
 if L < 0.7, and this is because when L < 0.7, the restraint
 of the lower bound is flabby enough, most of the task
 assignments will not be forced. Therefore, we would better
 to set L = 0.7 if there is no strict demand on deadline.
 The upper bound is designed to avoid sticky slots, which
 can impact the system performance. We generate a job queue
 (size = 200GB and H = 1.3), and ?j, j.demand ≤ 35. Then
 for each co-scheduled job set, H < 1.56. Then we evaluate
 the impact of upper bound on throughput in Figure 9(b),
 where H ? [1.1? 1.5]. It is straightforward that high upper
 bound give rise to high throughput, however while H >
 1.3, the throughput becomes lower, because the exorbitant
 upper bound will lead to numerous sticky slots. Hence the
 upper bound H = 1.3 is a suitable choice according to the
 experimental results.
 5
 5.5
 6
 6.5
 7
 7.5
 8
 8.5
 0.5 0.6 0.7 0.8 0.9
 Completion Time 
(?
  100s
 )
 Lower Bound
 small jobs
 mix jobs
 large jobs
 (a) Completion Time vs. Lower
 Bound
 5
 5.5
 6
 6.5
 7
 7.5
 8
 8.5
 1.1 1.2 1.3 1.4 1.5
 Completion Time 
(?
  100s
 )
 Upper Bound
 small jobs
 mix jobs
 large jobs
 (b) Completion Time vs. Upper
 Bound
 Figure 9. The Effect of Bound
 Summary: The experiments above show that the TD
 scheduler could provide higher throughput than the previous
 217
2
 4
 6
 8
 10
 12
 14
 16
 1 1.5 2 2.5 3
 Completion Time 
(?
  100s
 )
 Data Size (? 100GB)
 TD
 FIFO
 Fair
 (a) Small Jobs
 2
 4
 6
 8
 10
 12
 1 1.5 2 2.5 3
 Completion Time 
(?
  100s
 )
 Data Size (? 100GB)
 TD
 FIFO
 Fair
 (b) Mix Jobs
 2
 4
 6
 8
 10
 1 1.5 2 2.5 3
 Completion Time 
(?
  100s
 )
 Data Size (? 100GB)
 TD
 FIFO
 Fair
 (c) Large Jobs
 Figure 7. Completion Time of TD-scheduler
 0
 10
 20
 30
 40
 50
 60
 70
 80
 1 1.5 2 2.5 3Local Assignment Ratio (%
 )
 Data Size (? 100GB)
 TD
 FIFO
 Fair
 (a) Small Jobs
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 1 1.5 2 2.5 3Local Assignment Ratio (%
 )
 Data Size (? 100GB)
 TD
 FIFO
 Fair
 (b) Mix Jobs
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 1 1.5 2 2.5 3Local Assignment Ratio (%
 )
 Data Size (? 100GB)
 TD
 FIFO
 Fair
 (c) Large Jobs
 Figure 8. Ratio of Local Task Assignment
 ones. Besides, we evaluate the impact of the threshold on
 the throughput and give the best threshold setting.
 VI. CONCLUSIONS
 Distributed data analysis on Big Data has become a very
 hot topic and MapReduce is a suitable technique used for
 this field. Indubitably, in the distributed data processing, task
 scheduling is a crucial component and can signally affect the
 system performance. However, the previous scheduling al-
 gorithms have not considered the job-intensive environment
 and can not provide high throughput. To solve this problem,
 we firstly study on the job-intensive problem and summarize
 4 major factors which can impact system throughput. Then,
 we propose a novel approach, TD scheduler to improve
 the system throughput for MapReduce processing. Finally, a
 large number of experiments verify the performance of TD
 scheduler. The results show our method can provide higher
 throughput than others and shows excellent performance if
 the job queue contains a high percentage of small jobs, it is
 very appropriate for practical applications.
 Acknowledgement. This research was supported by the
 National Basic Research 973 Program of China under Grant
 No. 2012CB316201, the National Natural Science Foun-
 dation of China under Grant Nos. 61033007, 61003060,
 the Fundamental Research Funds for the Central Univer-
 sities under Grant No. N100704001, the National Research
 Foundation for the Doctoral Program of Higher Education
 of China Grant No. 20120042110028 and the MOE-Intel
 Special Fund of Information Technology under Grant No.
 MOE-INTEL-2012-06.
 REFERENCES
 [1] B. Kamil, A. Daniel and S. Avi, Efficient processing of
 data warehousing queries in a split execution environment. In
 SIGMOD, Athens, Greece, Jun 12-16, 2011.
 [2] E. Jaliya, P. Shrideep and F. Geoffrey, Mapreduce for data
 intensive scientific analyses. In eScience, Indiana, USA, Dec
 7-9, 2008.
 [3] D. Jeffrey and G. Sanjay, MapReduce: simplified data process-
 ing on large clusters. In OSDI, California, USA, Dec 6-8, 2004.
 [4] Apache Hadoop. http://hadoop.apache.org.
 [5] Fair scheduler. http://hadoop.apache.org/docs/r0.20.2/fair
 scheduler.html.
 [6] Z. Matei, B. Dhruba and S. Joydeep, Delay scheduling: A
 simple technique for achieving locality and fairness in cluster
 scheduling. In EuroSys, Paris, France, Apr 13-16, 2010.
 [7] Hadoop HDFS. http://hadoop.apache.org/docs/r0.20.2/hdfs user
 guide.html.
 [8] Capacity scheduler. http://hadoop.apache.org/docs/r0.20.2/ ca-
 pacity scheduler.html.
 [9] S. Thomas and L. Kevin, Dynamic proportional share schedul-
 ing in hadoop. In JSSPP, Atlanta, USA, Apr 23, 2010.
 [10] P. Jorda`, C. David and B. Yolanda, Performance-driven task
 co-scheduling for mapReduce environments. In NOMS, Osaka,
 Japan, Apr 19-23, 2010.
 [11] K. Kamal and A. Kemafor, Scheduling hadoop jobs to meet
 deadlines. In CloudCom, Indianapolis, USA, Nov 30-Dec 3,
 2010.
 [12] Z. Matei, K. Andy and J. Anthony, Improving MapReduce
 performance in heterogeneous environments. In OSDI, San
 Diego, USA, Dec 8-10, 2008.
 [13] K. YongChul, B. Magdalena and H. Bill, SkewTune: mitigat-
 ing skew in mapReduce applications. In SIGMOD, Scottsdale,
 USA, May 20-24, 2012.
 [14] L. Jacob and K. Christos, On the energy (in) efficiency of
 hadoop clusters. In HotPower, Montana, USA, Oct 11-14, 2009.
 [15] L. Willis and P. Jignesh, Energy management for mapReduce
 clusters. In VLDB, Singapore, Sep 13-17, 2010.
 218

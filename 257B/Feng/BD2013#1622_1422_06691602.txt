Fast Scalable Selection Algorithms for Large Scale Data
 Lee Parnell Thompson
 Department of Computer Science
 University of Texas at Austin
 Austin, TX
 Email: parnell@cs.utexas.edu
 Weijia Xu
 Texas Advanced Computing Center
 University of Texas at Austin
 Austin, TX
 Email: xwj@tacc.utexas.edu
 Daniel P. Miranker
 Department of Computer Science
 University of Texas at Austin
 Austin, TX
 Email: miranker@cs.utexas.edu
 Abstract—Selection finding, and its most common form
 median finding, are used as a measure of central tendency for
 problems in biology, databases, and graphics. These problems
 often require selection finding as a subcomponent where it can
 be called many times, and as such speed is important. The
 Map/Reduce framework has been shown to be an important
 tool for creating scalable applications. There are a number
 of valid implementations of the selection algorithms inside of
 a Map/Reduce framework, certain of which are compared in
 this paper. However, as the volume of data increases, subtle
 theoretical algorithmic implementation differences can lead
 to significant differences in practical application. Therefore,
 an efficient and scalable selection finding method has the
 potential to provide general benefit to a number of applications.
 This paper compares algorithms that have been redesigned or
 created for the Map/Reduce framework for the purpose of
 selection finding, or, finding the k-th ranked element in an
 unordered set. This paper takes the concepts used from two
 existing selection algorithms and translates them into a novel
 method using the Map/Reduce framework with two variations.
 Each approach uses a different methodology to reduce the
 total amount of workload needed for a selection. All the
 algorithms are compared together for scalability and efficiency
 in a computing cluster environment with up to 256 processing
 cores. The results show that the methods proposed in this paper
 outperform several common alternatives in identifying medians
 with Hadoop, including using sorting, Pig, and BinMedian
 methods. Our implementations are also available upon request.
 Keywords-Hadoop; Map Reduce; Selection Algorithms; Me-
 dian Finding
 I. INTRODUCTION
 Finding the kth smallest element of an orderable set is
 a problem that underlies many applications in computer
 science and math and is called selection finding. The most
 common use of selection finding is the computation of a
 median, or the element of rank N2  that divides a set
 into equal numbers. Selection finding is often used as a
 subproblem to more advanced algorithms. For example, the
 median point is used when dividing points equally inside
 a space, and this action is performed recursively when
 building a balanced tree for indexes. For large scale datasets,
 and particularly algorithms that contain the selection as a
 subcomponent, an increase in speed can be very important.
 Within the big-data community the Map/Reduce (MR)
 framework is commonly used for speed and scalability while
 running on large scale data[1]. MR is a framework that
 splits algorithmic tasks into two phases: a mapping and a
 reduction, where each phase is done on a set of computing
 nodes. The mapping phase takes a key/value input pair,
 and produces a set of intermediate key/value pairs. These
 intermediate key/value pairs are then distributed to nodes
 based on the intermediate where the reduce phase merges
 together the values to the final desired result. Hadoop is
 an open source java implementation of the MR framework
 that is widely used. In this paper, our proposed methods
 are implemented and compared using Hadoop framework,
 though they should generalize to all MR frameworks.
 Despite the popularity of Hadoop, little research or analy-
 sis has been done on selection finding within this framework.
 The relative simplicity of the selection algorithm can mis-
 guide users into using a nave approach, a total sort of the
 entire dataset. For example, a search of publicly available
 code revealed that the naive approach, a total sort of the
 entire dataset, was the most common implementation. Even
 the median finding algorithm inside of the O’Reilly book
 MapReduce Design Patterns: Building Effective Algorithms
 and Analytics for Hadoop and Other Systems used a total
 sort to obtain the median. The few statistical libraries
 found, including the most popular Hadoop library Pig, also
 require the data to be completely sorted[2]. Among specific
 implementations, many were incorrect, storing all elements
 within a single main memory datastructure, or inefficient,
 requiring all data to be analyzed at some point by a single
 processor.
 The computational cost of sorting the entire dataset is
 unnecessary for selection algorithms, and can be reduced
 to a much smaller cost by only sorting the small range
 of data that are potential kth candidates. The total sort
 on large datasets can be time consuming even for large
 systems; for example in the 32-node cluster an optimized
 Terasort algorithm running on 1Terabyte of data took over 40
 minutes. Reducing the total sort to a partial sort can provide
 substantial reductions in time when only a kth element is
 needed.
 Selection finding algorithms range in time from O(n2)
 to O(n), the theoretical minimum bound. Most trivial im-
 plementations of the selection problem simply sort the data,
 2013 IEEE International Conference on Big Data
 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 412
Figure 1. Eliminating points by only operating on the bins that might
 contain the k-th element
 yielding a total ordering on the data giving access to all
 kth elements, and a typical running time of O(n log n).
 Since a total ordering is unnecessary when only the kth
 element is needed, quicker algorithms for determining the
 selection can be created. The most well-known is the al-
 gorithm QuickSelect, developed by C.A.R Hoares, which
 has a best case run time O(n), but O(n2) worst case time.
 This worst case time can be improved to a guaranteed
 linear time, O(n), by using the Median of Median(MoM)
 algorithm developed by Blum[3] to smartly select pivot
 points. Even with these algorithmic improvements to the
 lower bound, quick implementations are still necessary and
 desired, especially when using selection as a subcomponent
 of algorithms working on large data sets or when taking
 advantage of parallel environments.
 There are multiple selection algorithms that can be trans-
 ferred to the Hadoop framework. But an efficient redesign
 of these selection algorithms for Hadoop requires under-
 standing the handling of data so that work is not duplicated.
 Notably, many of the selection algorithms considered in this
 paper have a sorting step, but this step may be skipped if
 implemented correctly in Hadoop. Also, even with knowl-
 edge of the framework, the ”best” implementation of any
 algorithm is not obvious.
 In this paper we describe and compare several different
 parallel algorithms for performing the kth selection and the
 process of moving these parallel selection algorithms and
 onto a MR framework. These algorithms are analyzed from
 the standpoint of wallclock, CPU, and network utilization
 to determine the best approach for the distributed systems.
 Two new variations are also introduced, MrS and MrT,
 which use the same selection algorithm but with different
 implementations. These two algorithms were designed to
 explore the inherent properties underlying MR to achieve a
 fast selection that is scalable for large datasets. Both operate
 on the same premise of using a multi-phase MR that to
 perform the sorting on a small range of data.
 The algorithms recursively perform partial sorts of the
 data into interval ranges, find the interval range that con-
 tains the k-th element, and then end with a total sort of
 all elements within an interval range once the number of
 points contained are less than some threshold. All of the
 algorithms described in this paper that avoid a total sort
 of the data run significantly faster, up to 5x times, than
 the optimized TeraSort algorithm. While this paper focuses
 on the Hadoop framework for implementation and results
 the concepts should be generalizable to other MapReduce
 frameworks.
 II. RELATED WORK
 The work in this paper is related to the selection finding
 problem as well as the MapReduce framework. Selection
 finding is an algorithmic problem with a variety of solutions,
 moving from O(n2) solutions to the absolute theoretical
 lower bound of O(n). The naive approach is a simple sort
 of the data, with a running time usually of O(n log n). The
 sorting approach is often acceptable for smaller datasets but
 faster techniques are needed when speed is required or data
 size is large.
 C. A. R Hoare devised a fast selection technique called
 Quickselect[4] in 1961 which was modeled after his Quick-
 sort algorithm[5]. Quickselect runs in expected O(n) time
 and works by utilizing the idea that Quicksort can be used to
 solve the selection problem. The difference between Quicks-
 elect and Quicksort is that at each step during the Quicksort
 only half of the data needs to be analyzed for Quickselect.
 The half that needs to be sorted is based on which subset
 the kth element would be found in. By performing a sort on
 only half of the data at every step, the Quickselect algorithm
 can run in expected O(n) time. Unfortunately, Quickselect
 suffers the same drawback as Quicksort: when bad pivots
 are chosen, the worst case running time degenerates to a
 O(n2) search.
 In 1973, Manuel Blum authored the paper, Time Bounds
 for Selection [3], in which he provides a guaranteed way of
 finding a ”good pivot” in O(n) time. This lowers the worst-
 case run time of Quickselect to the theoretical optimum
 ofO(n). The technique Blum proposed is called the Median
 of Medians (MoM) and works by choosing a pivot that
 will split the data into at most a 30% and 70% split. The
 MoM divides any set into 5 subsets where the algorithm is
 recursively applied on each subset until it contains less than
 10 elements. At this point the middle value is returned from
 the 10 elements and thereafter from each of the 5 children
 until the final middle value is obtained, which becomes the
 pivot. While this is theoretically optimal, in practice the
 additional time spent obtaining the theoretically sound pivot
 causes the algorithm to run longer, in most expected cases,
 than it would if simply using Quickselect with a random
 pivot [6].
 413
A. Map/Reduce and Hadoop
 MapReduce(MR) was developed by Google in a 2004
 paper and since that time has been implemented in many
 languages and is used in numerous applications. The most
 widely used implementation of the MR framework, outside
 of Google, is Hadoop. Hadoop is a java implementation
 that is currently being used in many companies such as
 Facebook, Adobe, Ebay and numerous others. Where the
 MR environment can be customized for a host of applica-
 tions, there are no advanced selection techniques that provide
 optimal speedup on the framework.
 The Hadoop API is very expansive but a few particular
 classes are important for this paper, Mappers, Reducers,
 Partitioners, and Combiners. The Mappers and Reducers are
 classes for the Map and Reduce logic respectively. The Par-
 titioners and Combiners are optimizations that can be used
 for increased performance. Combiners are considered to be
 ”mini-reducers” which run on map nodes and can merge
 together results from mappers before the data is sent to the
 Reducers. The combiners are not guaranteed to be called
 but allow the amount of data traffic across the network to
 be reduced in certain cases. The other important class is the
 Partitioners. Partitioners provide a way of mapping particular
 keys emitted from a Mapper to a particular reducer. This
 can be very useful for load balancing, or as in the case of
 selection algorithms, you would like distinct nearby keys to
 be placed into the same reducer.
 Apache Pig, [2], is a high level language that works on
 top of Hadoop. The language allows concise programs to
 be written for data analysis over a distributed framework.
 Pig compiles scripts written in Pig Latin into a sequence
 of Map/Reduce programs. Pig is often used to speed de-
 velopment of Map/Reduce programs that can be difficult
 to reason or adapt directly to a parallel environments or
 for analysis due to the large number of methods for this
 purpose. It is also extensible and allows programmers to
 create their own libraries, or User Defined Functions. One
 of these libraries, developed by company LinkedIn, is called
 DataFu which has a set of functions for median, deviation,
 and other statistics[7].
 B. Modern Work in Selection Finding
 Modern work has focused on improving the speed of
 the selection algorithms[8], improving the multiple selec-
 tion problem[9], and utilizing parallelization[10], [11], [12],
 [13], [6], [14]. From these algorithms the Binmedian[8]
 was chosen as a base algorithm and is explained below.
 This algorithm was chosen both because of it’s speed and
 more importantly because it uses an easy binning criteria,
 which is easily converted to a MR framework. This binning
 approach can also be seen in Bader’s work Ultrafast[12].
 Many of the other parallel selection algorithms require a
 master/slave approach which are more difficult to convert
 into a framework that relies on node independence.
 Parallelization of the selection has been done for shared
 and distributed systems. Work by M. Cafaro in 2009 sped
 up distributed memory systems [6] and proposed two al-
 gorithms. Both algorithms use similar methods to find
 the selection, one using a single weighted 3-median, and
 the second using two weighted 3-medians. Calculating the
 weighted 3-median finds three intervals where the data can
 be separated based on their most compact clustering. The
 selection finding itself is found through recursive iterations,
 where each iteration discards at least one-third of the data,
 until the median is converged upon.
 Tibshirani’s 2008 paper ”Fast computation of the median
 by successive binning” introduced binmedian . Binmedian
 uses a binning approach as a means to recursively reduce
 the set of possible median points. The algorithm works
 iteratively in four steps. Given a dataset X:
 1) Compute the mean µ and standard deviation ?
 2) Form bins based on [µ? ?, µ+ ?]
 3) Determine the bin containing the median
 4) Recurse on the bin with the median.
 III. ALGORITHMS AND IMPLEMENTATION
 The algorithms described here for the selection problem
 all perform only a partial sort of the data. The central
 concept is to recursively bin the data, eliminate unnecessary
 bins, and then sort the bin that can contain the kth item.
 The two new algorithms presented in this paper are MrS
 and MrT. Both use multi phase mappers/reducers to solve
 the selection finding problem. Both algorithms also use the
 internal structures within Hadoop, to save computational
 costs when possible.
 The BinMedian, MrS, and MrT algorithms all run in O(n)
 total time, see the proof in [8]. This proof holds for MrS
 and MrT as the methodology for choosing the partitions is
 similar, and in some cases smaller.
 A. MrT
 MrT is set up in a two-stage process trying to hold closely
 to the MR idea of using both a Map and Reduce phase. In
 the first Map/Reduce the bins and bincounts are discovered,
 while the second Map/Reduce performs the actual binning.
 The algorithm starts by taking a random sampling of ???
 1 data, where ? is the number of processors and ? is a natural
 constant. These sampled points are passed into a partitioner
 from which P number of bins are created, where P is how
 many reducers are available.
 The first map phase takes data in as a key value pair and
 and uses the partitioner to determine which bin the pair will
 fall into. The default mapper is used as it calls the partitioner
 which will emit the key value to the correct bin location.
 Specifically, the partitioner assigns every element x a key i
 based on P [i] ≤ x ≤ P [i+1]. The reducer, which represents
 a single bin, takes all points and emits a bin count.
 414
Listing 1. MrT: PseudoCode
 1 c l a s s MrT :
 2 de f run ( ) :
 3 whi le ( | S | > t h r e s h o l d ) :
 4 P a r t i t i o n e r p <? f i n d P a r t i t i o n s ( S )
 5 / / Find b i n s and b in coun t s
 6 Defaul tMap . run ( )
 7 Reduce1 . run ( )
 8
 9 / / D i s t r i b u t e Po i n t s
 10 i n t b in <? ge tB in ( )
 11 p <? f i n d P a r t i t i o n s ( b i n )
 12 Defaul tMap . run ( )
 13 S <? Reduce2 . run ( )
 14 S <? t o t a l S o r t ( S )
 15
 16 c l a s s P a r t i t i o n e r :
 17 f i n d P a r t i t i o n ( key , v a l u e ) :
 18 i n t i ndex <? f i n dCo r r e c tR e d u c e r ( key )
 19 va l u e . c l e a r
 20 re turn i ndex
 21
 22 c l a s s Reduce1 :
 23 i n t reducerNumber ; / / S e t t o a p p r o p r i a t e v a l u e
 24 de f r educe ( key , v a l u e s [ ] ) :
 25 c o n t e x t . c o u n t e r s . i n c r emen t ( reducerNumber , v a l u e s . l e n g t h )
 26
 27 c l a s s Reduce2 :
 28 de f r e duc e ( key , v a l u e [ ] ) :
 29 f o r va l u e i n v a l u e s :
 30 emi t ( key , v a l u e )
 The second stage of MrT takes the interval range of the
 bin that will contain the k-th element and creates a new set
 of P bins between this interval. The map phase then iterates
 over every element x and assigns it to a reducer if it belongs
 in the new bins, or emit nothing if it falls outside of the range
 of new bins.
 The algorithm terminates when the amount of remaining
 data is less than some constant, C. Once the number of
 points is less than this threshold a total sort of the data
 is performed from which the selection is trivial. The total
 running time broken into parts is O(N) for counting the
 number of elements inside of the bins, which is then iterated
 O(logN) times. The final sorting step is O(N logN) but on
 a fraction of the original points for a total runtime of O(N).
 See appendix A in the BinMedian for a complete proof of
 runtime for this style of algorithm[8].
 B. MrS
 MrS uses a similar approach to MrT but differs by using
 strictly Mappers in two passes over the same data. The
 idea is to minimize the amount of data needing to be
 transferred between nodes when calculating the correct bin,
 which happens between the map and reduce phase. Once
 the correct bin has been determined the second pass over
 the data distributes the points across the network as usual.
 MrS is set up as a two-stage map, with no reducers, and
 is performed in a recursive manner as follows. The same
 precursor step of choosing the random sampling of ?? ? 1
 data is performed and the Partitioner creates P bins. The
 first map phase though, instead of passing the data to a
 partitioner, performs the bin calculation itself. Hadoop will
 only use partitioners if there are reducers, and in this case
 there are none. The mapper increments the tally of bin counts
 but emits nothing.
 The second stage of MrS takes the interval range of the
 bin that will contain the k-th element and creates a new
 set of P bins between this interval. The second map phase
 then iterates over every element x and will either emit the
 key/value if it belongs in the new bins, or emit nothing if it
 falls outside of the range of new bins.
 Listing 2. MrS: PseudoCode
 1 c l a s s MrS :
 2 de f run ( S ) :
 3 se tNumberOfReducers ( 0 ) ;
 4 whi le ( | S | > t h r e s h o l d ) :
 5 / / Find b in coun t s
 6 Map1 . s e t P a r t i t i o n s ( f i n d P a r t i t i o n s ( S ) )
 7 Map1 . run ( )
 8
 9 / / D i s t r i b u t e p o i n t s
 10 i n t b in <? ge tB in ( )
 11 Map2 . s e t P a r t i t i o n s ( f i n d P a r t i t i o n s ( b . g e t P o i n t s ( ) ) )
 12 S <? Map2 . run ( )
 13 S <? t o t a l S o r t ( S )
 14
 15 c l a s s Map1 :
 16 P a r t i t i o n s [ ] p a r t i t i o n s ;
 17 de f map ( key , v a l u e ) :
 18 p a r t i t i o n <? f i n d P a r t i t i o n ( p a r t i t i o n s , key )
 19 c o n t e x t . c o u n t e r s . i n c r emen t ( p a r t i t i o n , 1 )
 20
 21 c l a s s Map2 :
 22 P a r t i t i o n s [ ] p a r t i t i o n s ;
 23 de f map ( key , v a l u e ) :
 24 / / f i n d P a r t i t i o n w i l l r e t u r n n u l l i f ou t o f range
 25 p a r t i t i o n <? f i n d P a r t i t i o n ( p a r t i t i o n s , key )
 26 i f ( p a r t i t i o n not n u l l )
 27 emi t ( key , v a l u e )
 C. BinMedian
 The BinMedian algorithm is also an iterative algorithm
 that stops when the number of points remaining is less than
 a threshold[8]. The original BinMedian uses the exact mean
 and median as a starting point to calculate the number of
 bins within the range [µ??, µ+?], but to avoid an additional
 Map/Reduce phase this was skipped in favor of a random
 sampling of the distribution. From this random sampling a
 Partitioner was created with the P bins on the range [µ ?
 ?, µ+ ?].
 The mapper in the BinMedian algorithm simply emits the
 key/value pair passed in and lets the Partitioner choose which
 Reducer will be used. To prevent the default sorting by keys
 that occurs inside of Hadoop the normal key was changed to
 the number of the reducer so that every reducer receives only
 one key. The Reducers simply emit the original key/value to
 form a series of P bins with unsorted data. The appropriate
 bin is then chosen and the algorithm continues.
 Listing 3. Binmedian: PseudoCode
 1 c l a s s BinmedianAdapted :
 415
2 de f run ( S ) :
 3 whi le ( | S | > t h r e s h o l d ) :
 4 / / Find b i n s
 5 / / f i n d P a r t i t i o n s w i l l c a l c u l a t e t h e mean and
 6 / / d e v i a t i o n f o r a s u b s e t o f p o i n t s
 7 P a r t i t i o n e r p <? f i n d P a r t i t i o n s ( S )
 8 Map1 . run ( )
 9 Reduce1 . run ( )
 10
 11 / / g e t t h e c o r r e c t b i n
 12 i n t b in <? ge tB in ( )
 13 S <? g e t F i l e ( b i n )
 14 S <? t o t a l S o r t ( S )
 15
 16 c l a s s Map1 :
 17 de f map ( key , v a l u e ) :
 18 p a r t i t i o n <? f i n d P a r t i t i o n ( key )
 19 emi t ( p a r t i t i o n , append ( key , v a l u e ) )
 20
 21 c l a s s Reduce1 :
 22 i n t reducerNumber
 23 de f r educe ( key , v a l u e s [ ] ) :
 24 f o r va l u e i n v a l u e s :
 25 k , v <? s p l i t ( v a l u e )
 26 emi t ( k , v )
 D. Algorithmic Analysis
 The partial-sort stages of MrS and MrT have the following
 expected runtime, assuming ? processors. The map step
 first divides N points to ? processors where each processor
 performs a linear amount of work, ?, for an expected time
 of N?? . During the shuffle sort segment of the map step an
 additional ? log ? work is done for each of the ? processors
 leading to N?? + ?(? log ?) and assuming N >> ? leads to
 O(N? ).
 The final total-sort stage of MrS and MrT is simply a sort
 of the data remaining, O(N? log N? ). In practice the amount
 of data in this stage is small, and theoretically can be shown
 that the entire algorithm including this sort is still O(N), [8].
 E. Test Setup
 All test sets were created using the TeraGen program.
 TeraGen is a program included in the Hadoop distribution
 which creates n data points of 100 bytes each. The generated
 points are a 10 byte key, 2 bytes of space, and an 88 byte
 random value for a total of 100 bytes per point.
 Algorithm speed comparisons were run on one terabyte of
 data. The speed factors considered were total time, measured
 from the start of an algorithm until the result was written
 to HDFS. The time it takes to place the unsorted data
 into HDFS is not included. This means that generating
 the key/value pairs using TeraGen and the corresponding
 write to disk are not part of the total time. Other metrics
 such as Map time and Reduce time are also included when
 appropriate. While these times are accurate it must be noted
 that in most cases Hadoop starts the Reduce phase while
 the Map phase is still running, so there is overlap in the
 recorded times.
 Apache Pig is a commonly used framework for writing
 easy queries on top of Hadoop[2]. The most commonly used
 Figure 2. Algorithm CPU Comparison. Shows the average processor
 utilization at each time step for 1TB of data.
 Figure 3. Algorithm Network Comparison. Shows the average of the bytes
 sent for all processors at each time step for 1TB of data.
 statistical package for Pig is named DataFu which contains
 a method for Median selection which was used in this paper.
 The Pig version used in this paper was Version 0.11.1.
 Scalability results were run on 100 gigabytes of data with
 the cluster size ranging from 16 cores to 256 cores, as each
 node is 8 cores this means 4 through 32 nodes were used.
 All tests were performed on TACC Longhorn. Relevant
 machine specs, each node is a SunBlade x6420 with two
 AMD Opteron Quad-Core 64-bit processors running at 2.3
 GHz. Each node has 48 GB of RAM running at 667 MHz
 DDR2. Nodes are connected with Sun InfiniBand. Full
 machine specs can be found at the TACC website[15].
 Software versions used for this paper, Java 7u27 with
 Hadoop version 0.23.7.
 IV. RESULTS & DISCUSSION
 The performance of the algorithms is based around the
 time required to complete the map/reduce phases, the num-
 ber of iterative steps, and the data transfer. For a parallel
 selection algorithm, the processing steps are as follows: first,
 discovery of the number and choice of bins so that points
 can be distributed amongst the different processors; second,
 comparison of the points and insertion into the appropriate
 416
bins; and finally, either a sort of one or more bins, or a
 re-run of the algorithm on one of these bins. Many of
 these processing steps can be parallelized at the cost of
 transferring the data.
 Pig and Terasort are significantly slower than the selection
 algorithms as they perform a total sort on all the data. Most
 of the Pig results are not shown on the charts as they are
 over 2x slower than Terasort. As can be seen in Figure 4,
 Terasorts demarcation between the map and reduce phase
 is very clear. In the map phase each point is assigned a
 bin and distributed to a reduce node. During the map phase
 the average CPU usage is very high but starts decreasing
 as nodes start finishing their point distribution and become
 idle. Once the Reduce phase starts then the CPUs are again
 mostly utilized until they finish their sort of the data. As can
 be seen, the data distribution during the Map phase is very
 low until nodes start finishing their distribution where they
 need to transfer their points to the Reducers. The Reducer
 phase shows a continuous high data transfer as the results
 are written back to the HDFS.
 The main problems with Terasort are: during the map
 phase node CPU is under utilized as each node begins
 transferring data or finishes processing, and during the
 reduce phase the time spent sorting the data accounts for
 about half of the time of the total algorithm, 4. So we
 can expect that by simply not performing a total sort we
 should be able to create selection algorithms that are twice
 as efficient.
 The algorithms that only do a partial sort of the final data
 are between 2x and 4x times faster than TeraSort on the
 1TB dataset. As can be seen in Figures 5 2, the BinMedian
 CPU curve during the map phase is similar to Terasort
 but less CPU is used, and the phase is much quicker. The
 reduced CPU comes from emitting already sorted keys to
 the reducers, skipping the additional overhead of the default
 Hadoop sort. This leads to both less CPU used and a quicker
 map phase.
 The CPU profiles in Figure 6 and Figure 7 show the two
 stage approach of finding the bin first, then distributing the
 points. First it is important to notice that both approaches
 show significant speedups over Terasort despite performing
 ”redundant” work in the form of repeating the bin cal-
 culations for each point twice. More importantly, despite
 having nearly the same algorithm for selecting the bins and
 distributing the points, they have very different CPU and
 network profiles. Especially noticeable is the CPU utilization
 and network utilization between MrS and MrT. MrS is the
 only algorithm to have consistently high CPU utilization
 that does not decrease as the mapping phase completes. The
 reason for this is that it needs no data transfer in between
 its stages. The network overhead of transferring data inside
 of Hadoop is a bottleneck for CPU use that is eliminated.
 So it appears that performance of the algorithms is tied to
 the data transfer. As the Hadoop framework transfers data
 Figure 8. Scalability Results showing the decrease in the total amount of
 time needed for each algorithm as a function of the number of processors
 used for 100G of data. Number of processors include the Namenode(which
 doesn’t perform work directly)
 from one node to another the CPU utilization is reduced and
 the time increases. This effect is exacerbated on networks
 that utilize a replication factor for redundancy. Since Hadoop
 places the data a priori on the nodes it is more efficient to
 perform calculations on the same data, if possible.
 The MrS algorithm performs multiple passes on the same
 data in successive Map only phases. The first sweep through
 the data merely counts how many items will be in each bin
 and performs no other calculations, so no data is transferred.
 In the second sweep data that belongs to the appropriate bin
 are written out. As each phase is discrete, no reducers will be
 started until all maps are completed, and all network traffic
 is cabined to the end of each Map phase. This approach
 despite more passes on the same data outperforms the others
 because the data transfer is minimized.
 The speedup of the algorithms for the 100G dataset does
 not show a linear improvement, see Figure 8. This shows a
 general trait of Hadoop, that it is built for very large scale
 datasets. As shown in the other results, the data transfer in
 Hadoop can be very costly. With this small size of dataset as
 the nodes increased the ratio of the network overhead versus
 the CPU time becomes a bottleneck for the algorithms. In
 this case algorithms that minimized the amount of traffic
 back and forth showed the best time improvement. Runs
 with larger data sets would be expected to demonstrate the
 linear speedups provided by these algorithms, because the
 transfer time would comprise a smaller portion of total run
 time.
 V. CONCLUSION
 Many algorithms in the fields of indexing, biology, and
 statistics have a subcomponent that utilizes a kth point,
 especially a median, and that is often called recursively.
 As selection finding is often a step where a total order
 is not needed, having fast selection algorithms can greatly
 improve the overall running time of these algorithms. The
 MapReduce model allows for the use of these algorithms to
 scale linearly on large scale datasets. The open source java
 417
Figure 4. TeraSort Profile showing the CPU usage (CPU), Bytes sent across the network (send), Bytes written to disk (writ), and the percentage completed
 of the map/reduce phase.
 Figure 5. BinMedian Profile showing the CPU usage (CPU), Bytes sent across the network (send), Bytes written to disk (writ), and the percentage
 completed of the map/reduce phase.
 implementation of MapReduce, Hadoop, was used as for
 the comparison but the underlying principles of the network
 and CPU utilization should generalize to other MapReduce
 frameworks.
 Many selection algorithms can easily be made using the
 Hadoop framework, but, it is a non-obvious problem as to
 which approach will best take advantage of the Hadoop
 system. This paper shows the implementation of several
 algorithms, but more importantly shows the underlying
 reasons which explain why each algorithm performs the way
 it does. Exposing the underlying CPU, I/O access, and data
 transfer for each of the algorithms gives concrete evidence
 about not only these algorithms, but also ideas on how to
 write future Hadoop programs.
 Several approaches to the selection problem, utilizing a
 variety of algorithms and implementations, were analyzed
 here in an effort to understand what is the best way to use
 Hadoop to solve the selection problem. As with many high
 performance parallel systems, reducing the amount of data
 passed between nodes and ensuring a relative load balance
 among processors is important. Some of the approaches
 that can be used include naively sorting, changing the
 binning criteria from Mappers to Partitioners, moving work
 from the Mapper to the Reducer stage (or visa versa), or
 418
Figure 6. MrT Profile showing the CPU usage (CPU), Bytes sent across the network (send), Bytes written to disk (writ), and the percentage completed
 of the map/reduce phase.
 Figure 7. MrS Profile showing the CPU usage (CPU), Bytes sent across the network (send), Bytes written to disk (writ), and the percentage completed
 of the map/reduce phase.
 even eliminating one of the Map/Reduce phases. A correct
 selection algorithm can be achieved through any of the
 approaches, but given the multitude of options, guidance
 is needed to direct programmers toward more optimal so-
 lutions. In particular it is clear that avoiding data transfer is
 desirable in Hadoop even if it means performing multiple
 stage Map/Reduce algorithms.
 From the results shown here it can be seen that the
 preferable approach is utilizing an algorithm that performs
 multiple Map phases over the same data to eliminate any
 sorting steps or data transfer until the last possible moment.
 Using this approach, selection of a point can be sped up by
 several times over the optimized TeraSort or other selection
 algorithms.
 An efficient and scalable selection finding method has
 the potential to provide general benefit to a number of
 applications. The results show that the methods proposed
 in this paper out perform several common alternatives in
 identifying medians with Hadoop, including using sorting,
 Pig, and BinMedian methods. If incorporated into some of
 the popular statistical packages for Hadoop, all users will
 gain quicker access to the results they need.
 419
ACKNOWLEDGEMENTS
 The authors acknowledge the Texas Advanced Computing
 Center (TACC) at The University of Texas at Austin for pro-
 viding HPC resources that have contributed to the research
 results reported within this paper. URL:http://www.tacc.
 utexas.edu. This research was partially supported by the
 National Institutes of Health (NIH), Grants R01GM085337-
 03
 REFERENCES
 [1] J. Dean and S. Ghemawat, “Mapreduce: simplified data
 processing on large clusters,” Commun. ACM, vol. 51, no. 1,
 pp. 107–113, Jan. 2008.
 [2] C. Olston, B. Reed, U. Srivastava, R. Kumar, and A. Tomkins,
 “Pig latin: a not-so-foreign language for data processing,”
 in Proceedings of the 2008 ACM SIGMOD international
 conference on Management of data, ser. SIGMOD ’08. New
 York, NY, USA: ACM, 2008, pp. 1099–1110.
 [3] M. Blum, R. W. Floyd, V. Pratt, R. L. Rivest, and R. E.
 Tarjan, “Time bounds for selection,” J. Comput. Syst. Sci.,
 vol. 7, no. 4, pp. 448–461, Aug. 1973.
 [4] C. A. R. Hoare, “Algorithm 65: find,” Commun. ACM, vol. 4,
 no. 7, pp. 321–322, Jul. 1961.
 [5] C. A. R. Hoare, “Quicksort,” The Computer Journal, vol. 5,
 no. 1, pp. 10–16, 1962.
 [6] M. Cafaro, V. De Bene, and G. Aloisio, “Deterministic par-
 allel selection algorithms on coarse-grained multicomputers,”
 Concurr. Comput. : Pract. Exper., vol. 21, no. 18, pp. 2336–
 2354, Dec. 2009.
 [7] S. Stadtmu¨ller, S. Speiser, A. Harth, and R. Studer, “Data-
 fu: a language and an interpreter for interaction with read-
 /write linked data,” in Proceedings of the 22nd international
 conference on World Wide Web, ser. WWW ’13. Republic
 and Canton of Geneva, Switzerland: International World Wide
 Web Conferences Steering Committee, 2013, pp. 1225–1236.
 [8] R. J. Tibshirani, “Fast computation of the median by succes-
 sive binning,” Computing Research Repository (CoRR), vol.
 abs/0806.3301, 2008.
 [9] H. Prodinger, “Multiple quickselect&mdash;hoare’s find algo-
 rithm for several elements,” Inf. Process. Lett., vol. 56, no. 3,
 pp. 123–129, Nov. 1995.
 [10] S. G. Akl, “An optimal algorithm for parallel selection,” Inf.
 Process. Lett., vol. 19, no. 1, pp. 47–50, Sep. 1984.
 [11] P. Gupta and G. P. Bhattacharjee, “A parallel selection algo-
 rithm,” BIT, vol. 24, no. 3, pp. 274–287, 1984.
 [12] D. A. Bader, “An improved, randomized algorithm for parallel
 selection with an experimental study,” J. Parallel Distrib.
 Comput., vol. 64, no. 9, pp. 1051–1059, Sep. 2004.
 [13] S. Rajasekaran, “Randomized parallel selection,” in Proceed-
 ings of the tenth conference on Foundations of software
 technology and theoretical computer science, ser. FST and
 TC 10. New York, NY, USA: Springer-Verlag New York,
 Inc., 1990, pp. 215–224.
 [14] L. Monroe, J. Wendelberger, and S. Michalak, “Randomized
 selection on the gpu,” in Proceedings of the ACM SIGGRAPH
 Symposium on High Performance Graphics, ser. HPG ’11.
 New York, NY, USA: ACM, 2011, pp. 89–98.
 [15] Texas Advanced Computing Center, “Texas advanced
 computing center: http://www.tacc.utexas.edu/,” 2006. [On-
 line]. Available: {http://www.tacc.utexas.edu/user-services/
 user-guides/stampede-user-guide}
 420

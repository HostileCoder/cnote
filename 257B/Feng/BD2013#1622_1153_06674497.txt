A New Dimension of Parallelism in Ultra High
 Throughput LDPC Decoding
 Philipp Schla¨fer, Norbert Wehn
 Microelectronic Systems Design Research Group
 University of Kaiserslautern
 67663 Kaiserslautern, Germany
 Email: {schlaefer, wehn}@eit.uni-kl.de
 Matthias Alles, Timo Lehnigk-Emden
 Creonic GmbH
 67663 Kaiserslautern, Germany
 Email: info@creonic.com
 Abstract—In modern communication systems the required data
 rates are continuously increasing. High speed transmissions can
 easily generate throughputs far beyond 1 Tbit/s. To ensure
 error free communication, channel codes like Low-Density Parity
 Check (LDPC) codes are utilized. However state-of-the-art LDPC
 decoders can process only data rates in the range of 10 to
 50 Gbit/s. This results in a gap in decoder performance which has
 to be closed. Therefore we propose a new ultra high speed LDPC
 decoder architecture. We show that our architecture significantly
 reduces the routing congestion which poses a big problem for
 fully parallel, high speed LDPC decoders. The presented 65nm
 ASIC implementation runs at 257 MHz and consumes an area of
 12 mm2. The resulting system throughput is 160 Gbit/s, it is the
 fastest LDPC decoder which has been published up to now. At
 the same time we show that extremely parallel architectures do
 not only increase the maximum throughput but also increase area
 and power efficiency in comparison to state-of-the-art decoders.
 I. INTRODUCTION
 In today’s digitally connected society everybody is ”online”
 all the time. Due to the extensive use of bandwidth-hungry
 multimedia applications like YouTube, HDTV and voice-over-
 IP, the bandwidth demand on wireless and wired commu-
 nication channels is rapidly growing. IEEE has completed
 the ratification of the IEEE 802.3ba standard, setting the
 target Ethernet speed as 100 Gbit/s [1]. For fiber channel
 applications the upper limit is even higher, early results show
 throughputs in the order of 100 Tbit/s [2]. At the same time
 efficient forward error correction is required to ensure error
 free transmission. To approach the channel capacity while
 serving high data rates, LDPC codes are excellent candidates.
 Today LDPC codes are used in a wide range of applications
 like 10 Gigabit Ethernet (10 GBASE-T, IEEE802.3an) [3],
 broadband wireless communication (UWB, WiGig) [4] [5] and
 storage in hard disc drives [6]. Even though LDPC codes
 belong to the best channel coding schemes known today,
 their implementation poses big challenges. This is due to
 the iterative, computationally expensive decoding procedure,
 requiring massive parallel architectures to achieve the targeted
 throughput.
 LDPC codes have been introduced by Gallager in 1962 [7]
 but the high decoding complexity made the application at that
 time impossible. When LDPC codes have been rediscovered
 in the late 90s, the throughput demands have been moderate.
 Serial decoder architectures have been sufficient to fulfill the
  !"  !#  !$  !%
 &!" &!# &!$
 '()
 "("(*(*
 "("("("
 *(*(*("  +, -,
 Fig. 1: H Matrix and Tanner Graph Hardware Mapping
 requirements. As demands on the throughput rose, partially
 parallel architectures became necessary. Today fully parallel
 decoder architectures are required to satisfy applications like
 10 GBASE-T. As future standards emerge, current architec-
 tures will not be able to facilitate the demanded throughputs
 of 100 Gbit/s and more. However these high throughputs can
 only be achieved at the cost of flexibility, e.g. IEEE 802.3an
 defines only one block length and code rate. This makes new
 architectural approaches necessary.
 In this paper we explore a new dimension of parallelism
 for LDPC channel decoders. We propose to unroll the iterative
 decoding loop and instantiate a dedicated hardware for each it-
 eration which is pipelined. This allows for decoders processing
 one block per clock cycle. The novel architecture reduces the
 routing congestion and latency. This allows for a significantly
 higher area and energy efficiency in comparison to state-of-
 the-art architectures which make decoders with ultra high
 throughput of 100 Gbit/s and more feasible.
 II. LDPC DECODING ALGORITHM
 LDPC codes [7] are linear block codes defined by a sparse
 parity check matrix H of dimension M ? N , see Fig. 1a.
 A valid code word x has to satisfy HxT = 0 in modulo-2
 arithmetic. A descriptive graphical representation of the whole
 code is given by a Tanner graph. Each row of the parity check
 matrix is represented by a check node (CN) and corresponds
 to one of the M parity checks. Respectively each column
 corresponds to a variable node (VN) corresponding to one
 of the N code bits. The Tanner graph shown in Fig. 1b is the
 alternative representation for the parity check matrix of Fig. 1a.
 Edges in the Tanner graph reflect the ’1’s in the H matrix.
 There is an edge between VN n and CN m if and only if
 Hmn = 1. LDPC codes can be decoded by the use of different
 algorithms. Which one fits best has to be chosen dependant on
 153
 2013 IEEE Workshop on Signal Processing Systems
 978-1-4673-6238-2/13 $31.00 © 2013 IEEE
the required communications performance. For example the ?-
 min algorithm [8] performs better than the min-sum algorithm
 [9] but has a significantly higher implementation complexity.
 In the following discussion we focus on the min-sum algorithm
 as it is used for our implementation. All algorithms have in
 common, that probabilistic messages are iteratively exchanged
 between variable and check nodes. First the variable nodes
 are initialized with the channel values ?chn . According to the
 connections in the parity check matrix these values are passed
 from each variable node n with n ? {0, ..., N ? 1} to the
 connected check nodes M(n) with
 M(n) = {m|m ? {0, ..., M ? 1} ?Hmn = 0}. (1)
 The set of all variable nodes connected to check node m with
 m ? {0, ..., M ? 1} is defined as
 N (m) = {n|n ? {0, ..., N ? 1} ?Hmn = 0}. (2)
 The check node computation can be split in two parts, the
 actual parity check, represented by the sign sgn() and the
 result’s probability represented by the magnitude ||. These
 extrinsic messages are computed as follows:
 sgn
 (
 (i)m?n
 )
 =
 ∏
 n??N (m)\n
 sgn
 (
 z
 (i)
 n??m
 )
 (3)
 ???(i)m?n
 ??? = ? ? min
 n??N (m)\n
 (
 |z(i)n??m|
 )
 , (4)
 with the extrinsic scaling factor (ESF) ?. Finally the messages
 are returned to the variable nodes which generate new intrinsic
 messages z(i) for iteration i.
 z(i)n?m = ?chn +
 ∑
 m??N (n)\m
 
 (i?1)
 m??n = ?(i?1)n ? (i?1)m?n (5)
 with
 ?(i)n = ?chn +
 ∑
 m??M(n)
 
 (i)
 m??n (6)
 The sign of the a posteriori probability ? can be interpreted as
 the hard decision bit. Message exchange between variable and
 check node continues until either a valid code word is found
 or a maximum number of iterations is exceeded.
 What we described till now reflects a two-phase scheduling.
 This schedule processes all check nodes before the variable
 nodes produce new messages. However it is also possible
 to apply a so called layered scheduling [10] [11] which is
 only applicable to partial parallel decoder architectures. In
 the layered scheduling the check nodes are processed in a
 way, that the variable nodes are intermediately updated. Thus
 the remaining CNs can take results into account which have
 been generated in the same iteration. The layered schedule
 can achieve the same communications performance as the two-
 phase schedule while reducing the needed iterations by almost
 50%.
 Fig. 2: LDPC Decoder Design Space
 III. STATE-OF-THE-ART LDPC DECODER
 ARCHITECTURES
 The LDPC decoder design space comprises a multitude
 of parameters which have to be tuned to the specific re-
 quirements. Each standard has different needs in means of
 error correction performance, number of supported code rates,
 codeword lengths and throughput. There are numerous design
 decisions which have to be made for the hardware to satisfy the
 requirements. We have presented an in depth investigation of
 the design space in [12]. In this paper we focus on the design
 decisions regarding parallelism. The design space extension
 we present is exploring a new dimension of parallelism in
 LDPC channel decoding.
 There are multiple dimensions in which the degree of
 parallelism can be chosen, see Fig. 2. The lowest level of
 parallelism is on the message level. Each message can be
 transmitted in tuples of bits or fully parallel. Today fully
 parallel message transfer can be found in the vast majority of
 architectures. The second degree of parallelism can be found
 on processing node level. The node’s in- and outgoing edges
 can be processed one after an other, partially parallel or fully
 parallel. However the choice of the node’s edge parallelism
 is directly linked to the so called ”hardware mapping”. The
 hardware mapping is the highest level of parallelism which
 is used today. It describes how many check and variable
 nodes are instantiated. When we are talking of a fully parallel
 decoder, an architecture instantiating all processing nodes is
 meant. In contrast partially parallel decoders have a lower
 number of nodes than the Tanner graph. They process the
 parity check matrix in a time multiplex fashion and thus have
 a very high decoding latency. This allows for easy adaption
 of the architecture to new standards but limits the achievable
 throughput. The edge parallelism is chosen to fit the needs of
 the hardware mapping, for example if only one submatrix is
 154
Channel values
 Decoded bits
 Variable Nodes
 Check Nodes
 Chv Reg
 NW-1NW
 ...
 ...
 ...
 ...
 Fig. 3: Fully Parallel LDPC Decoder Architecture
 processed at a time, an edge parallelism of one is sufficient.
 For applications like 10 GBASE-T Ethernet only fully parallel
 architectures can achieve the required throughput. Fig. 3
 depicts the high-level structure of this architecture. However,
 there are some problems with the fully parallel architecture. At
 first we have to consider the two networks between VNs and
 CNs. Dependent on the code length and quantization, each
 of them comprises between several thousands and hundred
 thousands of wires which have to be routed according to
 the parity check matrix. To achieve a good communications
 performance the parity check matrices have long cycles and
 thus no locality, resulting in massive routing congestions. It
 has been shown in earlier publications [13] [14] that the area
 utilization is heavily impaired by this fact and only 50% of the
 chip is used by logic. The second drawback of the fully parallel
 architecture is the high latency which is unavoidable due to the
 iterative nature of the algorithm. The latency issue holds also
 for partially parallel decoders. In these architectures only after
 one block is completed, the next one can be accepted. Thus the
 decoder’s latency directly impacts the achievable throughput.
 Fully parallel, inflexible decoders can still satisfy today’s
 standards requirements in means of throughput. However, for
 future standards the throughput demands will further increase
 but in addition flexibility in means of the supported code rates
 will be required which cannot be implemented by state-of-the-
 art decoders.
 IV. EXPLORING A NEW DIMENSION IN THE LDPC
 DECODER DESIGN SPACE
 In the following section we discuss the possibilities to
 overcome the limitations in LDPC decoder throughput.
 A. Core Duplication
 One solution to achieve the throughputs required by future
 standards is to instantiate several LDPC decoder cores in
 parallel. There are two possible starting points for the core
 duplications. First we can use partially parallel architectures
 which allow for flexibility but suffer from high latencies as
 explained earlier. Furthermore they cannot cope with unstruc-
 tured parity check matrices which gain better communications
 performance. The second option is to instantiate several fully
 parallel decoder cores allowing for reduced latency and high
 throughput. However due to routing congestions they cannot
 achieve a satisfying area efficiency and flexibility. Summarized
  !"##$%&'"%($
 )"*+",%$&-./$0
  !$12&-./$0
 3+4$&5$6-7
    
   
)"*+",%$&-./$0
  !$12&-./$0
 3+4$&5$6-7
    
   
 !"#$!%&'()
  !"#$!%&'(*
 )"*+",%$&-./$0
    
8$1./$/&,+90
  !"#$!%&'(+
    
-7
    
   
   
Fig. 4: Unrolled LDPC Decoder Architecture
 Architecture Flexibility Low Latency Area/Energy Eff.
 Parallel inst., partially
 parallel architecture
 + - o
 Parallel inst., fully
 parallel architecture
 - o -
 Unrolled, fully paral-
 lel architecture
 - + +
 TABLE I: Parallel LDPC Decoder Architectures
 simple decoder duplication is not an efficient architectural
 approach for very high throughput decoders.
 B. Unrolling Iterations
 We propose a new architecture, shown in Fig. 4 to overcome
 the highlighted drawbacks. So far, the iterations were not
 considered as a degree of parallelism for LDPC decoders. In
 [15] and [16] the unrolling of decoding iterations for turbo
 codes has already been considered. However no hardware
 implementation has been published. Thus an in depth inves-
 tigation especially of the influence on routing congestion and
 implementation efficiency is not available.
 We unroll the iterative decoding loop and instantiate an
 unrolled, fully parallel LDPC decoder which is pipelined.
 The number of pipeline stages determines the latency, but
 the throughput is fixed by the cycle duration. Hence this
 architecture has a throughput of one codeword per clock cycle.
 The approach allows for ultra high throughput LDPC decoder
 cores.
 There is an essential change in the resulting data flow.
 Where before data have iteratively been exchanged between
 VNs and CNs, now all data flows in one direction. The result
 is a unidirectional wiring avoiding the overlap of opposed net-
 works. This is a big benefit for the routing. Table I summarizes
 the benefits and drawbacks of the different approaches.
 155
Fig. 5: Bit-Error Rate for IEEE802.11ad, code rate 13/16 and
 64-QAM modulation
 V. THE UNROLLED LDPC DECODER IMPLEMENTATION
 In the following section we present the architectural details
 of the unrolled LDPC decoder. The proposed idea of unrolling
 the iterations can be combined with most other design de-
 cisions. Due to space limitations we can only present one
 point in the huge design space which is spanned by this new
 approach. The implemented decoder is designed for one fix
 code rate of 13/16 complying to the IEEE 802.11ad standard
 with a quantization of four bit. We are applying the min-
 sum algorithm for the decoding process. Fig. 5 shows the
 communications performance for the implemented decoder. In
 comparison to a six bit reference implementation significant
 losses can be observed. The hardware unrolls nine iterations
 as the decoding performance hardly increases after that. The
 focus of this paper is clearly on the presented architecture
 and can easily be adapted to other codes or quantizations if
 required by the target application.
 A. Top Level Architecture
 The top level architecture has already been shown in Fig. 4.
 Each stage consists of all the check and variable nodes defined
 in the parity check matrix and a pipeline register. The decoding
 process is consecutively executed in the pipeline. The number
 of the stages defines the number of decoding iterations. The
 inputs for each stage is the result of the previous stage and
 the channel value. This means the channel value has to be
 available for all iterations and is therefore stored in the pipeline
 register. Due to the fully parallel node instantiation the com-
 putation for one iteration is processed within one clock cycle.
 A new block can thus be accepted in every cycle and when the
 pipeline is filled also a block result is output every cycle. The
 check and variable nodes can be further pipelined to increase
 the maximum achievable frequency. In difference to state-of-
 the-art architectures for the presented decoder the latency has
 no impact on the achievable throughput. Independent of the
 number of pipeline stages a new block can be accepted in
 every clock cycle. For the presented design there are pipeline
 in 0
 in 1
 +
 2C2SM
 2C2SM
 +
 +
 -
 -
 +
 +
 2C2SM
 2C2SM
 +
 +
 -
 -
 in 2
 in 3
 chv
 out 0
 out 1
 out 2
 out 3
 Fig. 6: Fully Parallel Variable Node Architecture
 sort
 sort
 CS4
 sort
 sort
 CS4
 CS4
 sort
 sort
 CS4
 sort
 sort
 CS4
 CS4
 CS4
 esf
 esf
 SM2TC
 SM2TC
 neg.
 Min0
 pos.
 Min0
 neg.
 Min1
 pos.
 Min1
 in 0
 in 1
 in 2
 in 3
 in 4
 in 5
 in 6
 in 7
 in 8
 in 9
 in 10
 in 11
 in 12
 in 13
 in 14
 in 15
 Fig. 7: Fully Parallel Check Node Minimum Search Stage
 registers at the decoder’s in- and outputs, after each check and
 variable node and inside the check nodes. These additional
 registers allow for much higher overall clock frequency and
 thus a higher throughput and area efficiency.
 B. Variable Node
 The variable nodes implement equation 5 which is basically
 an accumulation of all values and a subtraction for each output.
 Fig. 6 shows the according hardware architecture. It processes
 all edges in parallel, for the used code the maximum variable
 node degree is four. We maximize the degree of parallelism
 by an adder tree summing up all extrinsic messages and
 the channel value. The incoming messages have already a
 two’s complement representation which results in very little
 hardware cost for the variable nodes. All outgoing messages
 are computed in parallel and are converted to a sign-magnitude
 representation (2C2SM) which is required for efficient check
 node processing.
 C. Check Node
 The basic operation in the check node is given in equations
 3 and 4. In comparison to the variable node, the check node
 is way more complex. We can separate the check node in
 two main parts. One minimum search stage and the output
 stages. Fig. 7 depicts the minimum search stage, locating the
 two smallest magnitudes of the 16 inputs. In the presented
 architecture this is done with a tree like structure. Each two
 values are sorted and forwarded to compare select (CS) units
 with four inputs. Due to the sorted inputs the CS units can
 neglect some comparisons and save hardware resources. After
 156
sign
 min
 out 0
 neg.
 Min0
 pos.
 Min0
 neg.
 Min1
 pos.
 Min1
 Fig. 8: Fully Parallel Check Node Output Stage
 three CS stages the two smallest magnitudes are found and
 get prepared for the output stages. Therefore the messages
 are scaled with the ESF and converted from sign magni-
 tude representation to the two’s complement number format
 (SM2TC). Fig. 8 shows one of the 16 output stages. Each is
 responsible to generate the message for one outgoing edge. As
 we have pulled all the expensive calculations into the minimum
 search part, the output stages are very simple. They consist
 of three multiplexers selecting one of the four generated
 outputs of the minimum search stage. Two multiplexers select
 the correct sign and the last one chooses between first and
 second minimum. The control signals for the output stages are
 generated by an xor of the message signs and a comparison
 of the minimum with the respective input.
 VI. IMPLEMENTATION RESULTS
 The LDPC decoder is implemented on a 65nm low power
 bulk CMOS library from ST Microelectronics. We consid-
 ered the following PVT parameters: Worst Case (WC, 1.1V,
 125?C), Nominal Case (NOM, 1.2V, 25?C) and Best Case
 (BC, 1.3V, -40?C). Synthesis was performed with Synopsys
 Design Compiler in topographical mode, Placement & Routing
 (P&R) with Synopsys IC Compiler. Synthesis as well as P&R
 were performed with the Worst Case PVT settings of the 65nm
 library.
 The final design reaches a frequency of 257 MHz and thus
 is capable to process 257 ?106 blocks per second. For the
 Nominal Case 420 MHz can be achieved. The architecture
 processes one block per clock cycle, resulting in a maximum
 throughput of 160 Gbit/s for the Worst Case scenario. After
 P&R 12 mm2 are required for the core. An area utilization
 (cell/core area ratio) of 76 % is achieved. We also evaluated
 the power consumption of the physical design using Synopsys
 PrimeTime-PX. The average power consumption at a clock
 frequency of 257 MHz is 5.36 W for the Nominal Case.
 The physical layout of the unrolled LDPC decoder can be
 seen in Fig. 9. Each iteration can be found as one of the
 vertical areas in the corresponding layout. Channel messages
 ripple from left to right through the chip while decoding.
 All routing is very structured and pointing from one iteration
 to the next. This confirms the expectation of a simplified
 routing. Table II compares some recently published high
 throughput LDPC decoder designs. Partially as well as fully
 parallel decoders are presented, supporting different standards
 and algorithms. Obviously a fair comparison is difficult since
 the implementation efficiency must always be linked to the
 Fig. 9: Unrolled fully parallel LDPC decoder layout. Each
 decoder iteration is represented by a different column.
 communications performance. But all the decoders where
 designed for high throughput.
 The reduced routing complexity of the unrolled decoder im-
 pacts the energy consumption. The presented architecture can
 compete in means of energy per decoded bit in comparison to
 other implementations. Regarding further energy optimizations
 the proposed architecture is an excellent candidate for a Near-
 Threshold circuit [21]. E.g. the throughput of 10 Gbit/s can
 already be fulfilled by the presented decoder running at less
 than 20 MHz. Thus aggressive voltage scaling to 0.5-0.6V can
 be applied. This increases the energy efficiency by at least a
 factor of three, allowing for the same or even better energy
 efficiency than [19].
 Even though we have to use a two-phase scheduling we
 can achieve a very good area efficiency, even in comparison
 with layered decoders like [17]. Layered decoders in general
 achieve a higher area efficiency as they need less iterations for
 the same communications performance. However as layered
 scheduling can only be applied to partially parallel decoders
 it is prohibitive for ultra high speed architectures. The increase
 in area efficiency of factor 1.3-8 of our design in comparison
 to other decoders underlines that the unrolling simplifies the
 networks and allows for efficient routing.
 Finally the presented architecture is more than four times
 faster than the fastest comparable decoder. Thus it is to the best
 of our knowledge the first channel decoding system unrolling
 the iterative decoding loop and the fastest LDPC decoder ever
 published.
 VII. CONCLUSION
 Future communication standards will pose big challenges in
 means of throughput requirements and flexibility for decoder
 designs which cannot be solved by today’s architectures. We
 157
Decoder [17] [18] [19] [20] Proposed
 CMOS Technology 65nm LVT 65nm 65nm GP 65nm 65nm SVT
 Supply Voltage [V] 1.2 0.9 1.15 1.3 1.2
 Frequency [MHz] 400 400 540 195 257
 Standard IEEE 802.15.3c IEEE 802.3an IEEE 802.11ad IEEE 802.3an IEEE 802.11ad
 Block Size 672 2048 672 2048 672
 Code Rate 1/2, 5/8, 3/4, 7/8 1723/2048 1/2 1723/2048 13/16
 Level of Parallelism partially parallel partially parallel partially parallel fully parallel fully parallel, unrolled
 Scheduling layered two-phase n/a two-phase two-phase
 Algorithm min-sum min-sum min-sum threshold min-sum min-sum
 Iterations 10 8 10 11 9
 Quantization 6 4 5 5 4
 Decoding Latency [ns] 100 260 148 n/a 105
 Post P&R Area [mm2] 1.30 5.05 1.6 4.84 12.09
 Throughput [Gbit/s] 6.7 8.5 9 36.3 160.8
 Energy Eff. [pJ/bit/It.] 8 11.76 0.9 3.36 3.61
 Area Eff. [Gbit/s/mm2] 5.2 1.7 5.6 7.5 13.6
 TABLE II: State-of-the-Art High Throughput LDPC Decoder Comparison
 explored a new dimension in the LDPC decoder design space
 and implemented the first LDPC decoder unrolling the iterative
 decoding loop. The resulting 65nm ASIC design consumes
 12 mm2, runs at a clock frequency of 257 MHz and generates
 a throughput of 160 Gbit/s. To the best of our knowledge, this
 is the fastest LDPC decoder ever published. Finally it is not
 only the outstanding throughput but also the very high power
 and area efficiency that makes our approach suitable for future
 applications. Compared to the best competitors we increased
 the area efficiency by at least 80%.
 ACKNOWLEDGMENT
 This work has been supported by the Deutsche Forschungs-
 gemeinschaft (DFG) within the project ”Entwicklung effizien-
 ter und flexibler VLSI-Architekturen fu¨r die Kanaldecodierung
 in drahtlosen Multi-Gigabit-Kommunikationssystemen auf der
 Basis von LDPC-Codes”.
 REFERENCES
 [1] IEEE 802.3ba, “Part 3: Carrier Sense Multiple Access with Collision De-
 tection (CSMA/CD) Access Method and Physical Layer Specifications -
 Amendment 4: Media Access Control Parameters, Physical Layers and
 Management Parameters for 40 Gb/s and 100 Gb/s Operation ,” IEEE
 802.3ba-2010, Jun. 2010.
 [2] D. Qian, M.-F. Huang, E. Ip, Y.-K. Huang, Y. Shao, J. Hu, and T. Wang,
 “101.7-Tb/s (370x294-Gb/s) PDM-128QAM-OFDM transmission over
 3x55-km SSMF using pilot-based phase noise mitigation,” in Optical
 Fiber Communication Conference and Exposition (OFC/NFOEC), 2011
 and the National Fiber Optic Engineers Conference, 2011, pp. 1–3.
 [3] IEEE 802.3an-2006, “Part 3: CSMA/CD Access Method and Physical
 Layer Specifications - Amendment: Physical Layer and Management
 Parameters for 10 Gb/s Operation, Type 10GBASE-T ,” IEEE 802.3an-
 2006, 2006.
 [4] WiMedia Alliance, “Multiband OFDM Physical Layer Specification,
 Release Candidate 1.5,” Mar. 2009.
 [5] IEEE 802.11ad, “Part 11: Wireless LAN Medium Access Control (MAC)
 and Physical Layer (PHY) Specifications - Amendment: Enhancements
 for Very High Throughput in the 60 GHz Band,” IEEE 802.11ad-draft,
 May 2010.
 [6] A. Kavcic and A. Patapoutian, “The Read Channel,” Proceedings of the
 IEEE, vol. 96, no. 11, pp. 1761–1774, 2008.
 [7] R. G. Gallager, “Low-Density Parity-Check Codes,” IRE Transactions
 on Information Theory, vol. 8, no. 1, pp. 21–28, Jan. 1962.
 [8] F. Guilloud, E. Boutillon, and J. Danger, “?-Min Decoding Algorithm
 of Regular and Irregular LDPC Codes,” in Proc. 3nd International
 Symposium on Turbo Codes & Related Topics, Brest, France, Sep. 2003,
 pp. 451–454.
 [9] J. Chen, A. Dholakia, E. Eleftheriou, M. P. C. Fossorier, and X.-Y. Hu,
 “Reduced-Complexity Decoding of LDPC Codes,” IEEE Transactions
 on Communications, vol. 53, no. 8, pp. 1288–1299, Aug. 2005.
 [10] J. Zhang and M. Fossorier, “Shuffled belief propagation decoding,” in
 Conference Record of the Thirty-Sixth Asilomar Conference on Signals,
 Systems and Computers, vol. 1, Nov. 2002, pp. 8–15.
 [11] J. Dielissen, A. Hekstra, and V. Berg, “Low cost LDPC decoder for
 DVB-S2,” in Proc. 2006 Design, Automation and Test in Europe (DATE
 ’06), Munich, Germany, Mar. 2006.
 [12] P. Schla¨fer, M. Alles, C. Weis, and N. Wehn, “Design Space of Flexible
 Multi-Gigabit LDPC Decoders,” VLSI Design Journal, vol. 2012, 2012.
 [13] A. Blanksby and C. J. Howland, “A 690-mW 1-Gb/s, Rate-1/2 Low-
 Density Parity-Check Code Decoder,” IEEE Journal of Solid-State
 Circuits, vol. 37, no. 3, pp. 404–412, Mar. 2002.
 [14] N. Onizawa, T. Hanyu, and V. Gaudet, “Design of High-Throughput
 Fully Parallel LDPC Decoders Based on Wire Partitioning,” Very Large
 Scale Integration (VLSI) Systems, IEEE Transactions on, vol. 18, no. 3,
 pp. 482–489, 2010.
 [15] O. Muller, A. Baghdadi, and M. Je´ze´quel, “ASIP-Based Multiprocessor
 SoC Design for Simple and Double Binary Turbo Decoding,” in Proc.
 2006 Design, Automation and Test in Europe (DATE ’06), Munich,
 Germany, Mar. 2006.
 [16] C. Leroux, C. Je´go, P. Adde, D. Gupta, and M. Je´ze´quel, “Turbo
 Product Code Decoder Without Interleaving Resource: From Parallelism
 Exploration to High Efficiency Architecture,” Signal Processing Systems,
 vol. 64, no. 1, pp. 17–29, 2011.
 [17] Z. Chen, X. Peng, X. Zhao, Q. Xie, L. Okamura, D. Zhou, and S. Goto,
 “A macro-layer level fully parallel layered LDPC decoder SOC for IEEE
 802.15.3c application,” in VLSI Design, Automation and Test (VLSI-
 DAT), 2011 International Symposium on, 2011, pp. 1–4.
 [18] Z. Zhang, V. Anantharam, M. Wainwright, and B. Nikolic, “An Efficient
 10GBASE-T Ethernet LDPC Decoder Design With Low Error Floors,”
 Solid-State Circuits, IEEE Journal of, vol. 45, no. 4, pp. 843–855, 2010.
 [19] M. Youn Sung Park, D. Blaauw, D. Sylvester, and Z. Zhang, “A
 1.6-mm2 38-mW 1.5-Gb/s LDPC decoder enabled by refresh-free
 embedded DRAM,” in VLSI Circuits (VLSIC), 2012 Symposium on,
 2012, pp. 114–115. [Online]. Available: http://ieeexplore.ieee.org/stamp/
 stamp.jsp?arnumber=6243816
 [20] T. Mohsenin, D. Truong, and B. Baas, “A Low-Complexity Message-
 Passing Algorithm for Reduced Routing Congestion in LDPC De-
 coders,” IEEE Transactions on Circuits and Systems I: Regular Papers,
 vol. 57, no. 5, pp. 1048–1061, 2010.
 [21] B. Calhoun and D. Brooks, “Can Subthreshold and Near-Threshold
 Circuits Go Mainstream?” Micro, IEEE, vol. 30, no. 4, pp. 80–85, 2010.
 158

Some Aspects of Optimal Human-Computer Symbiosis
 in Multisensor Geospatial Data Fusion
 Eugene Levin
 Surveying Engineering Program
 Michigan Technological University
 Houghton, MI 49931
 906-487-2446
 elevin@mtu.edu
 Aleksandr Sergeyev
 Electrical and Computer Engineering Technology
 Michigan Technological University
 Houghton, MI 49931
 906-487-2258
 avsergue@mtu.edu
 Abstract— Nowadays vast amount of the available geospatial
 data provides additional opportunities for the targeting accu-
 racy increase due to possibility of geospatial data fusion. One
 of the most obvious operations is determining of the targets
 3D shapes and geospatial positions based on overlapped 2D
 imagery and sensor modeling. 3D models allows for the ex-
 traction of such information about targets, which cannot be
 measured directly based on single non-fused imagery. Paper
 describes ongoing research effort at Michigan Tech attempt-
 ing to combine advantages of human analysts and computer
 automated processing for efficient human computer symbiosis
 for geospatial data fusion. Specifically, capabilities provided
 by integration into geospatial targeting interfaces novel human-
 computer interaction method such as eye-tracking and EEG was
 explored. Paper describes research performed and results in
 more details.
 TABLE OF CONTENTS
 1 INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
 2 WHY EYE-TRACKING? STEREOSCOPIC FU-
 SION AND 3D PERCEPTION . . . . . . . . . . . . . . . . . . . . . . 1
 3 EXPERIMENTAL DESIGN . . . . . . . . . . . . . . . . . . . . . . . . 2
 4 EXPERIMENT DESCRIPTION . . . . . . . . . . . . . . . . . . . . 3
 5 GEOMETRICAL ACCURACY ANALYSIS . . . . . . . . . . 3
 6 NEEDED FOR EEG IN PRACTICAL APPLICA-
 TION SCENARIOS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
 7 CONCLUSION AND FUTURE WORK . . . . . . . . . . . . . 6
 REFERENCES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
 BIOGRAPHY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
 1. INTRODUCTION
 Technologies and algorithms aimed at the measurement of
 object’s 3D properties based on imaging analysis were ini-
 tially based on photogrammetry [2] with subsequent im-
 provement by the incorporation of computer vision. Timely
 and accurate extraction of target’s spatial properties in fully
 automated mode is complicated by the fact that multisensor
 geospatial data are burdened by multiple errors. Moreover,
 sometimes difference in sensor models, scale, image quality
 etc. makes algorithmic stereoscopic stereo-restitution not
 feasible. However, trained geospatial analyst may fuse these
 image sources (for example US satellite image on the left
 side and Chinese satellite image on the right side) by means
 of simple mirror stereoscope. This fusion ability is due to
 the human brain’s visual perception efficiency equivalent to
 13, 000 Pentium processors [3]. Therefore geospatial ana-
 978-1-4673-1813-6/13/$31.00 c©2013 IEEE.
 1 IEEEAC Paper #2153, Version 2, Updated 29/11/2012.
 lysts’ involvement in geospatial data refinement and decision
 making is still necessary from practical standpoint. Then
 one of the most interesting research challenges in geospatial
 data fusion research is the of optimal symbiosis between
 automated operations. To meet this challenge we performed
 research experiments attempting to combine advantages of
 human analyst and computer automated processing for effi-
 cient human computer symbiosis for geospatial data fusion.
 Specifically experiments performed were related to analysis
 of potential of incoherent stereo pairs. Incoherent stereo pairs
 were combined of images of obsolete map and actual aerial
 image of the same territory. Anaglyphic product obtained
 after image processing of such stereo pairs was demonstrated
 to human analyst (subject) and stereo perception of such
 a stereo pairs was achieved. The most interesting finding
 of experiment described is the fact that some new objects
 existing on aerial photo only appeared at incoherent stereo
 pairs as 3D. This effect is caused by phenomena of hu-
 man eye-brain system known as a human stereopsis [11]
 and binocular summation [1] which are widely deployed in
 photogrammetry. To obtain the quantative measurements of
 effect, the eye-tracking system has been deployed. Analysis
 of human eye-movements (driven by conscious and subcon-
 scious brain processes) while percepting incoherent dataset
 derives a unique opportunity for human computer symbiosed
 geospatial systems. There are two potential outcomes of such
 approach:
 • interpretative - analysts gaze-fixation zones can help to
 localize the areas where mapping dataset should be updated
 • qunatative - processing of eye fixations geometry dur-
 ing stereo model perception allows transforming virtual
 3D model to geometrical one based on eyegrammetry
 method [5].
 Given rational behind optimal system architecture of the sim-
 biosed human-computer environment we came to conclusion
 that efficiency of eye-tracking deployment can be increased
 by integration of brain encephalogram sensor (EEG), that can
 be trained to help in avoiding false command generated by
 eye-tracking system widely known as a Midas effect [14].
 2. WHY EYE-TRACKING? STEREOSCOPIC
 FUSION AND 3D PERCEPTION
 Described in this paper approach makes use of well-known
 properties of cyclopean vision, the brain’s ability to merge
 separate retinal projections into a single visual representa-
 tion. In normal human vision each eye receives a different
 two-dimensional retinal projection of an object. These two
 retinal projections are offset by the lateral distance between
 the two eyes, a phenomenon known as binocular disparity.
 The visual system calculates the relative depth of an object
 1
Figure 1. Example of bi-temporal stereo-pair
 with respect to the object that the eyes are fixated on. The
 perception of three-dimensional depth from two disparate
 retinal projections is called stereoscopic depth perception,
 or stereopsis [12]. Normal human stereopsis requires that
 projections of viewed object be present in both left and right
 retinal images. When monocular projections to the retina
 are similar or identical, a unified binocular representation is
 formed. This process is called binocular fusion [10]. It is
 also possible for humans to perceive three-dimensional depth
 information about an object when a retinal projection of the
 object is present in one eye only. In our pilot experiments,
 human-analyst viewed two images (a stereopair) through
 anaglyph glasses, which separated the visual information
 accessible to each eye. A target object (for example, a white
 box, shown in Fig. 1) was presented in the view for one eye,
 but absent in the view of the other eye.
 Despite the fact that the target object is present in one
 image only, the cyclopean percept of the stereopair includes
 a three-dimensional representation of the target image. In
 this unified percept, one of the stereopair images appears to
 be semi-transparent and overlaid on top of the other image
 as shown in Figure 1. This percept can be explained by
 the phenomenon of binocular summation [10]. Anaglyph
 glasses present images dichoptically (a separate image shown
 for each eye). When pair of low contrast images is shown
 dichoptically, a dichoptic plaid is created consisting of the
 images overlaid on each other. The perception of a dichoptic
 plaid results from a process of binocular summation. In this
 process, dissimilar monocular images reach consciousness
 at the same time, and their effects are added up, point by
 point. Each image contributes to a unified perception while
 retaining its own distinct properties [10]. In our experiments,
 participants also report a percept of a dichoptic plaid while
 viewing dissimilar stereopairs of bisensory images (images
 representing the same spatial location obtained by different
 sensors). Fig. 2 shows a stereopair of bisensory images
 and a resulting dichoptic plaid. The goal of our research
 is to measure the 3D properties of objects perceived during
 dichoptic viewing of [10] bitemporal and biosensor stere-
 opairs. We used eye-tracking methodology to compute the
 fixation point of each eye as the analyst views a pair of
 dissimilar images through analglyph glasses. Eye-tracking
 is a non-invasive methodology that records the position of the
 fovea during viewing. The fundamental assumptions of eye-
 tracking methodology are that the eye fixates on objects of
 interest during visual attention [4], [7].
 (a)
 (b)
 Figure 2. Difference between (a) stereopsis and ; (b)
 binocular summation image features based points matching
 3. EXPERIMENTAL DESIGN
 Computing depth from binocular disparity is of interest to
 the geospatial information sciences because it can be used to
 automate quick depth calculations. Our research experiment
 uses dichoptic devices (separate images presented to the left
 and right eyes) to study foveal fixation during underlying
 stereopsis A common method of presenting dichoptic images
 is through anaglyph imagery. In this process, the analyst
 views a pair of specially prepared images through spectacles
 with lenses of two different colors, usually red and cyan.
 Each individual anaglyph image is composed of two superim-
 posed images, which represent the same object at two views
 slightly offset relative to each other. Each layer of an anal-
 glyph image is printed in a different color. The analyst views
 the merged image through the colored lens absorbing light
 from one view of offset image, so that images received by the
 analyst’s left and right eyes are offset with binocular disparity.
 Alternative technologies for separating the two views in a di-
 choptic pair are: polarization; temporal separation by shutter-
 glasses: chroma-stereoscopy; and auto stereoscopic displays
 utilizing spatial separation using lenses. Eye-tracking tech-
 nology can compute the depth of a fixation point from the
 binocular disparity and express the disparity numerically.
 While observing a scene, the optical axes of both human eyes
 are naturally directed to the same point on the object. This is
 2
particularly true for visual perception of stereoscopic images
 on a computer screen. Human eyes move very rapidly while
 scanning images and the result of this scan is sent to the
 brain. Our previous research [8], [9] recorded participants’
 eye movements while they observed the virtual stereoscopic
 models and used eye-tracking protocols to identify fixations.
 The fixations are assumed to be feature points of the viewed
 objects and are used to reconstruct 3D geometric models by
 applying classical stereo photogrammetric procedures. This
 eyegrammetric approach is useful for 3D models restoration
 measured by stereopsis. As show in Fig. 2 this method is not
 applicable for binocular summation estimation because one
 of the inhomogeneous stereopairs images does not contain
 the same features. During binocular summation, the human
 brain makes ”guesses” of where the corresponding feature
 should be. Binocular summation reconstruction consists of
 two problems :
 • how to select from the left and right eye fixation cloud
 points (depicted in Fig. 6 the optimal combination of gazes
 from the left and right eyes
 • Determining accuracy of the 3D estimates, expressed in
 eye vergence angles.
 The following approach is proposed to solve the first problem:
 (1) Image feature pixel is selected by of corner detection
 algorithms [6] and applied to the center fixation or cursor
 position if a cursor-click measurement was performed by the
 analyst to measure the point.
 (2) Selecting image feature where exist by corner detection
 algorithm and then search of corresponding eye-gaze vector
 as the most satisfying for classical co-planarity photogram-
 metry equation.
 (3) Selecting corresponding left and right gaze pair by least-
 square optimization.
 (4) 3D coordinates of points are computed from the stereo-
 pair system transformed to 3D geographical coordinates (lat-
 itude, longitude and altitude)
 4. EXPERIMENT DESCRIPTION
 During experiments several datasets composed of geospatial
 images of known geometry were generated. Some examples
 are shown in Fig 3. The challenge was to compare 3D depth
 measured on standard stereopsis and binocular summation
 vision. This comparison gives a proof-of-feasibility of us-
 ing inhomogeneous stereopairs in geospatial data process-
 ing. Specifically, depth measurements can be an indicator
 of the facts how eye-tracking derived binocular summation
 parallaxes are different from respective parallaxes derived for
 the same stereopairs in normal stereopsis mode. As known,
 stereo measurements by human operators were widely de-
 ployed in analog photogrammetry era. Definition of operator
 ability of 3D perception and “personal difference” were
 some biometric procedures which were commonly used at
 those times in photogrammetric production environments.
 We have performed a several research experiments following
 this methodology using Seeing Machines Facelab 4.0 eye-
 tracking system of [18] , shown in Figure 4.
 Experiments were collected with the following sequence:
 • 2D and 3D (wearing anaglyphic glasses) calibration of
 eye-tracking system with subject;
 • recording of eye-movements protocols simultaneously with
 Figure 4. Eye-tracking experimental setup
 3D cursor fixation on virtual 3D model;
 • detection of eye-fixations in eye-movement protocols cor-
 responding to mouse-click events;
 • calculation of relative depth based left and right eye fix-
 ations corresponding to the measurements and calibration
 parameters;
 • comparison of relative depth for normal stereopsis and
 binocular summation cases.
 To perform 3D calibration we used control points in the
 geographical 3D space. Specifically, we used StereoGIS [16]
 workstation provided by SimWright. Fig. 5 depicts anaglyph-
 ically generated test image from StereoGIS and used for the
 measurements on eye-tracker.
 Graphical illustration of eye-movement trajectory and paral-
 laxes for inhomogeneous case is presented on Figure 6
 5. GEOMETRICAL ACCURACY ANALYSIS
 One of the most critical issues in estimation of potential use of
 new image fusion method is way of how accuracy and errors
 are defined. Specifically, for the stereoscopic measurements
 potentially we can use 2 ways of accuracy estimation and
 error definitions:
 • compare 3D coordinates photogrammetrically obtained
 3
Figure 3. Images from research experiments, showing stereoscopic input and cyclopean percepts
 from normal stereo-pairs with respective coordinates ob-
 tained by means of inhomogeneous computation; and 3D
 (wearing anaglyphic glasses) calibration of eye-tracking sys-
 tem with subject;
 • compare dimensions results of inhomogeneous computa-
 tions with respective direct measurements of those dimen-
 sions on the ground.
 In experiments described in this section we applied both
 methods. StereoGIS [16] softcopy photogrammetric envi-
 ronment was deployed in version of experiments by “a”-
 scenario and direct measurements of boxes depicted on Fig. 3
 were used for the accuracy estimation by “b”-scenario. Both
 methods of estimation (direct and indirect) are numeric and
 further can serve as a base for classical statistical errors
 analysis.
 Results of 3D processing recorded images are summarized in
 Table 1. It shows that binocular rival measurements provide
 about 70 percent of 3D depth measurement accuracy com-
 pare to ground dimension, and demonstrates the feasibility
 of the proposed system. Future research will concentrate
 on increasing participant numbers and experimental database
 size using various aerial and satellite geospatial images.
 6. NEEDED FOR EEG IN PRACTICAL
 APPLICATION SCENARIOS
 To avoid well-known Midas-effect associated with eye-
 tracking based control of the manned and unmanned target-
 ing environments the next step is the EEG control sensor
 integration. In teleoperation, complexity or even delayed
 communications can degrade performance and destabilize a
 system [15]. Saving operator time, decreasing complexity, or
 in some cases permitting sliding autonomy as noted in [17],
 may improve robotic interfaces for single and joint robotic
 missions and outcomes. We deployed ubiquitous gaming
 EEG sensors such as Emotiv [19] to combine it with eye-
 tracking HCI. The Emotiv EPOC measures biosignals on the
 head skin surface. Emotiv measure voltage signals at 14
 locations around the scalp, relative to a pair of feedback-
 controlled reference locations. This is a typical measurement
 circuit for microvolt-scale biopotentials with very high input
 impedance, a fixed reference electrode and a secondary driven
 reference electrode which causes the detection system to ride
 on top of common-mode signals, rejecting about 85 dB of
 common mode input and allowing the amplifier reference
 level to follow the background body potential with high
 accuracy. The references are commonly referred to as CMS
 (Common Mode Sensor) and DRL (Driven Right Leg - a
 reference to the attachment of this sensor to the right leg
 of the patient in early electrocardiogram circuits for which
 it was originally developed). Emotiv EPOC input signals
 are AC coupled (0.16 Hz high-pass) and passed to a buffer
 amplifier with extremely high input impedance and a pass-
 band of DC ? 87 Hz. The signals are sampled internally
 using a 16 ? bit ADC at 2048 samples per second per
 channel and then refiltered in the digital domain to remove
 50Hz, 60Hz and to heavily attenuate signals above 64Hz.
 This removes any residual harmonics of the mains signal,
 and other high-frequency noise components (including some
 EMG and very high frequency EEG data). In combination
 with the 50Hz notch filter the effective bandwidth of the
 signal is now 0.16 ? 43 Hz. The signals is downsampled to
 128 samples/sec/channel, packaged into data packets and
 transmitted wirelessly to the receiver. All of this filtering and
 processing removes all high frequency components which
 would otherwise appear as alias components in the 128 Hz
 data stream. The remaining signal has an effective 14 bits
 of skin surface voltage signals, with the LSB resolution of
 about 0.5µV , with undistorted output from 0.16to43 Hz,
 covering the delta, theta, alpha, beta and low gamma bands.
 This voltage trace can then be analyzed in the PC to extract
 these components. Specifically, we are working integrating
 the Emotiv EEG Expressive and Cognitive toolsets. The Ex-
 4
Table 1. Geometrical Accuracy Analysis
 Experiment Measured Stereoscopic Binocular Error Compared Error Compared
 Dimension(m) Stereoscopic Summation to Stereopsis to Ground
 Box 1 (bitemporal) 0.571 0.495 0.386 13.309 -32.399
 Box 2 (bitemporal) 0.68 0.695 0.599 2.205 -11.911
 Building (bisensor) 15.345 16.365 11.842 6.647 -22.828
 Average -22.379
 a
 b
 Figure 5. a)Stereopair processed on StereoGIS and used for
 3D calibration; b) stereoscopic cursor used for control syn-
 chronization of attention eye-fixations with manual control
 measurements procedures.
 pressive toolset enables detecting the following expressions:
 blinking, winking, looking (right or left) raising and furrow-
 ing brows, smiling, clenching and laughing. Using Emotiv
 EEG in sensor fusion and platforms control is efficient for
 processing 2D and 3D datasets. It will accelerate broad-area
 search because many manipulations are made possible using
 thought rather than mouse clicks. We performed pilot-studies
 integrating EEG with softcopy photogrammetric workstation.
 Specifically, EEG was implemented for zooming in and out,
 in addition to panning in the cardinal directions. The pilot
 study user interface is depicted in Fig. 7.
 Figure 6. Left(white) and Right(yellow) eye-movement
 trajectories, corresponding fixations and parallax definition
 principles.
 Fig 7 illustrates screenshot of the normal usage of the appli-
 cation. The user initiates Photomod [13] and and Gyro EGG
 applications separately. Then the user sees a simple window
 with a combo box that contains potential applications that
 the program can interface. The user selects the application
 using from the combo box list. The user then sees the
 association of physical actions (winking, eye brow raise, etc.)
 with application actions (clicking, zoom in/out, etc). To
 use softcopy photogrammetric workstation, the user clicks
 the “Start” button and interacts with the Google Earth using
 the Emotiv EPOC headset. The Gyro EEG application also
 enables control of the mouse using the gyroscope built in
 to the headset. Moving the mouse across the screen is as
 simple as turning one’s head. Looking from side to side,
 up and down causes the mouse pointer to move across the
 screen just as a mouse would. Clicking ”Stop” with either the
 mouse or the headset disables headset control of the mouse
 and the various “Hotkey” actions. Specifically functions of
 our interest included control of stereoscopic 3D cursor which
 makes impression for the subject that it changes its height
 over stereoscopic model. It is achieved by changing of the
 parallax between left and right components of the stereo-
 scopic cursor. Integration of the eye-tracking and EEG HCI
 for the have the following advantages in real-life geospatial
 data fusion scenarios:
 • help avoid visual attention interruptions
 • complement eye-tracking interest and emotion sensitive
 media IES
 5
a
 b
 Figure 7. a)Emotiv gaming EEG system; b) View of
 the application window; Experimental results: EEG based
 control of Softcopy Photogrammetric workstation.
 • help avoid “midas-touch” false commands
 7. CONCLUSION AND FUTURE WORK
 Geospatial data fusion systems will gain the following bene-
 fits of integrating described in this paper approach:
 • Capability of data fusion which is impossible by other
 means
 • No deviation of analyst’s visual attention
 • Increased operators productivity by using combined EEG
 and eye-tracking interaction techniques
 • Applicable for both single display and multi-display sys-
 tems - method can be applied to either worktable displays or
 windows
 • Collaborative work - the Emotiv EEG allows to controlling
 two EEG devices
 • Ease of learning and use
 • Compatibility with currently deployed in industry systems
 and software
 Future developments will be devoted to the integration of the
 developed approach with state-of-the-art geospatial imaging
 interpretation environment. The tasks to initiate stereoscopic
 viewing are eye-tracker calibration, selecting images and
 rough registration to align the two images. In addition, eye-
 tracker calibration is required periodically during the stereo-
 scopic viewing. Consequently, we will use a dual monitor
 system: one monitor devoted to 2D viewing for calibration
 and image selection and another monitor devoted to stereo-
 scopic viewing. The left and right images will be selected
 by database queries and selection from annotated thumbnail
 images. The initial registration will be accomplished by
 selecting corresponding points on the left and right images.
 REFERENCES
 [1] R. Blake, M. Sloane, and R Fox, “Further develop-
 ments in binocular summation”, Perception and Psy-
 chophysics, 3D, 266-276, 1981.
 [2] B. Cyganek, J. Paul Siebert, “Introduction to 3D Com-
 puter Vision Techniques and Algorithms”, Wiley, John
 and Sons, Incor-porated, 2009.
 [3] CVB-Library, http://library.thinkquest.org
 [4] A.T. Duchowski, Eye tracking methodology: Theory
 and practice, New York: Springer, 2003.
 [5] G. Gienko, E. Levin, “Eye-tracking in augmented pho-
 togrammetric technologies”, Proc. of ASPRS Annual
 Conference, Baltimore,MD, 2005
 [6] C. Harris, M. Stephens,“A combined corner and edge
 detector,” Alvey Vision Conf., pp. 147-151, 1988.
 [7] M. Just, P. Carpenter, “Eye fixations and cognitive
 processes”, Cognitive Psychology, 8, 441-480, 1976.
 [8] E Levin, W. Helton, R. Liimakka., G. Gienko, “Eye
 movement analysis in visual inspection of geospatial
 data”, Proc. of ASPRS Annual. Conference, Portland,
 OR, 2008.
 [9] E. Levin, G. Gienko, A. Sergeyev , “Human centric
 approach for geospatial data fusion in homelend defence
 and security application scenrious”, SPIE Defense and
 Security Symposium, Orlando FL , 2008.
 [10] L. Liu, C. Tyler, C. Schor, “Failure of rivalry at low con-
 trast: evidence of a suprathreshold binocular summation
 process”, Vision Res, 32, 1471-1479.
 [11] R. Patterson, W. Martin, Human Stereopsis, Human
 Factors, 34(6), 669-692, 1992.
 [12] Q. Ning, “Binocular disparity and the perception of
 depth”, Neuron, 18, 359-368.
 [13] Photomod softcopy photogrammetric software
 http://www.racurs.ru/
 [14] A. Poole, and L. Ball, “Eye tracking in HCI and us-
 abilityresearch”, In C. Ghaoui (ed.), Encyclopedia of
 humancomputer interaction. Idea Group Inc., Pennsyl-
 vania,2006.
 [15] E. Rodriguez-Seda, and M. Spong “Experimental Com-
 parison Study of Control Architectures for Bilateral
 Teleoperators”, IEEE Transactions on Robotics, 25(6),
 1304-1318, 2009.
 [16] StereoGIS product description
 http://www.simwright.com/stereogis.htm
 [17] M. Springer, B. Dias, B. Kannan, B. Browning, E.
 Jones, B. Argall, M. Dias, M. Zink, “Sliding Autonomy
 for Peer-to-Peer Human-Robot Teams”, Technical Re-
 port CMU-RI-08-16. Robotics Institute. Carnegie Mel-
 lon University. April 2008.
 [18] Seeing Machines, faceLAB;
 http://www.seeingmachines.com
 [19] EPOC Emotiv EEG http://emotiv.com/
 6
BIOGRAPHY[
 Eugene Levin is currently a Program
 Chair of Surveying Engineering and As-
 sistant Professor at School of Technol-
 ogy at Michigan Tech University. Dr.
 Levin also directing Integrated Geospa-
 tial Technology graduate program. He
 received M.S. degree in Astrogeodesy
 from Siberian State Geodetic Academy
 in 1982 and Ph.D in Photogrammetry
 from Moscow State Land Organization
 University in 1989. He has over 25 years of experience in US,
 Israeli and Russian academy and geospatial industry. He held
 research and managing positions with several Russian, Israeli
 and US research, academic institutions and high-tech com-
 panies, including: Research Institute of Applied Geodesy,
 Omsk Agricultural Academy, Rosnitc ”Land”, Ness Tech-
 nologies, Physical Optics Corporation, Digital Map Products,
 American GNC, and Future Concepts. He has served as a
 Principal Investigator and Project Manager in multiple award-
 winning government programs.
 Aleksandr Sergeyev is currently an
 Assistant Professor in the Electrical En-
 gineering Technology program in the
 School of Technology at Michigan Tech-
 nological University. Dr. Aleksandr
 Sergeyev is earned his bachelor de-
 gree in electrical engineering in Moscow
 University of Electronics and Automa-
 tion in 1995. He obtained the Master
 degree in Physics from Michigan Tech-
 nological University in 2004 and the PhD degree in Electri-
 cal Engineering from Michigan Technological University in
 2007. Dr. Aleksandr Sergeyev research interests include high
 energy lasers propagation through the turbulent atmosphere,
 developing advanced control algorithms for wavefront sens-
 ing and mitigating effects of the turbulent atmosphere, dig-
 ital inline holography, digital signal processing, and laser
 spectroscopy. He is also involved in developing new eye-
 tracking experimental techniques for extracting 3-D shape of
 the object from the movement of human eyes.
 7

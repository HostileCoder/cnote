Keynote I 
COM.Geo 2013 
 
OGC Standards and Big Data Analytics 
 
Dr. Carl Reed 
CTo, OGC 
 
 
Abstract 
   This talk will explore the use of Open Geospatial Consortium (OGC) standards in the context of Big 
Data requirements and analytics for accessing and processing geospatially-enabled content. A short 
description of the OGC is provided. This is followed by a discussion of how key OGC standards are or 
can be used for Big Data applications. The discussion of OGC standards and Big Data analytics is in the 
context of geospatial information and sensor fusion. Examples are provided. The talk concludes with a 
discussion of some of the key issues, such as provenance, quality, security and privacy, facing the use of 
geospatial data in Big Data applications. 
 
Bio 
   Dr. Carl Reed is currently the Chief Technology Officer and an Executive Director of the Open 
Geospatial Consortium (OGC). Dr. Reed is responsible for facilitating the OGC standards development 
process, chairing the OGC Architecture Board, and Chairing the OGC Planning Committee. Dr. Reed also 
participates in and collaborates with other standards organizations, including OASIS, NENA, W3C, ISO, 
and the IETF. As a result, Reed has contributed to numerous internet and web standards. During his 
tenure at the OGC, Reed has written numerous book chapters and articles and presented dozens of 
keynotes at geospatial/GIS conferences. Prior to the OGC, Reed was the vice president of geospatial 
marketing at Intergraph and pervious to that President of Genasys Americas. Dr. Reed received his PhD 
in Geography, specializing in systems architectures for GIS technology, from the State University of New 
York at Buffalo in 1980. In 1995 and in 2009, Reed was voted one of the 10 most influential people in the 
GIS industry. For his contributions to the geospatial industry, in 2009 Reed was inducted into the URISA 
GIS Hall of Fame. 
 
  
xiv
Keynote II 
COM.Geo 2013 
 
A Fresh Look at Mobile Location Sensing 
 
Dr. Jie Liu 
Principal Researcher & SERG Manager, Microsoft Research 
 
 
Abstract 
   GPS and WiFi in our smart phones and other mobile devices. However, continuous location sensing 
such as logging, tracking, and geo-fencing, consume too much energy and shorten device battery life. In 
this talk, we take a fresh look at location sensing, in both outdoor and indoor settings. For outdoor 
location, we dive into the principles of GPS receivers and show that by offloading GPS processing to the 
cloud, we can reduce the device side energy consumption by three orders of magnitude. For indoor 
location, we discover that commercial FM signals are good sources of location signatures that work better 
than WiFi signatures by themself, and works even better if combined with WiFi signatures. These low 
energy approaches enable always-there location services without users paying batter life penalty. 
 
Bio 
   Jie Liu is a Principal Researcher at Microsoft Research, Redmond, WA, and the manager of its Sensing 
and Energy Research Group (SERG). He is an ACM Distinguished Scientist. His research interests root 
in understanding and managing the physical properties of computing. Examples include timing, location, 
energy, and the awareness of and impact on the physical world. He has published broadly in areas like 
wireless sensor networks, mobile and embedded systems, ubiquitous computing, and energy efficient 
cloud computing. Dr. Liu is an Associate Editor of ACM Trans. on Sensor Networks, has been an 
Associate Editor of the IEEE Trans. on Mobile Computing, and has chaired a number of top tier 
conferences. Dr. Liu received his Ph.D. degree from Electrical Engineering and Computer Sciences, UC 
Berkeley in 2001. From 2001 to 2004, he was a research scientist at Palo Alto Research Center (formerly 
Xerox PARC). 
    
    
     
  
xv
Keynote III 
COM.Geo 2013 
 
Unexplored 3D Worlds:  
The Futures of Focal Plane GIS 
 
Mr. Mike Liebhold 
Senior Researcher, Distinguished Fellow 
Institute for the Future 
 
 
Abstract 
In this talk we explore some future impacts and challenges of combinatorial innovations in sensing, 
mapping and rendering and computing technologies enabling humans and machines to intimately interact 
with rich 3D geospatial data in a vertical focal plane view. We'll start with a tour through recent 
developments in sensor fusion, machine vision, and liquid data cloud supercomputing and then tour a few 
of  the more  interesting cartographic frontiers  including augmented reality,  robotic SLAM (Simultaneous 
Location and Mapping) and geolocated simulations.    
 
Bio 
  Mike Liebhold is a Senior Researcher and distinguished Fellow at IFTF.org, the Institute for the Future, 
focusing on the mobile and abundant computation, immersive media and geospatial web foundations for 
context-aware and ubiquitous computing. Previously, Mike was a Visiting Researcher, Intel Labs, working 
on a pattern language based on semantic web frameworks for ubiquitous computing. At IFTF MIke leads 
ongoing work in geospatial, and location services for companies like Intel, Nokia, Toyota, Daimler, 
Nissan, and Fujitsu, among others. In 2003 Mike was a producer and program leader for the Technology 
Horizons “New Geography” Conference at the Presidio of San Francisco for technologists and strategic 
planners from top tier companies and the public to better understand the emerging geospatial information 
infrastructure. The event included The Fort Scott Locative Experience, a hands-on field exercise for 
conference attendees exploring a prototype geospatial web combining digital geodata and modern web 
hypermedia - deploying a prototype geo browser to read and write W3C and OGC standard data objects. 
Prior to joining IFTF, Mike contributed to creation of GeoRSS, (the first web standard way to geocode 
web objects). Before that, during the late 1990s Mike worked on GPS enhanced precision agriculture in 
rural and remote regions. Mike is currently active in the AR, Augmented Reality community, launched the 
first ARdev camp, replicated worldwide, and was invited to join the W3C POI (Points of Interest) working 
group as an outside expert. Mike's work in geospatial computing began as an idea for a hypermedia atlas 
in the late ’70s leading to a Lab Director's role at Atari labs working with early MIT augmented reality 
artists, and authors of the Aspen movie map - the pre-eminent model for heads-up geography. Later, from 
1983 -1993 at Apple’s Advanced Technology Group Mike created early hypermedia maps and lead work 
on the Terraform project - an early predecessor to a google earth-like computational framework, and on 
hyper-annotated video and augmented reality. In the ’80s Mike was instrumental in forging a partnership 
between Apple, the National Geographic society, and Lucasfilm to produce new geographic digital media. 
Mike is a frequent speaker, has given keynotes at Where 2.0, Location Intelligence, URISA, NSDGIC and 
the UK Ordnance Survey conferences and has authored a number of papers, including one recently 
published in the Nieman Reports, the Harvard Journalism Review, entitled “Digital Immersion: 
Augmenting Places With Stories And Information” and an earlier co-authored paper published in a special 
edition of the IEEE Journal on Pervasive Computing, “Data Management in the World-Wide Sensor Web.” 
Most recently Mike was profiled in the 12/2011 Ericson Business Review in the cover story entitled 
“Augmented Reality Check.”  
 
        
 
xvi
Keynote IV 
COM.Geo 2013 
 
NASA World Wind Infrastructure for Spatial Data 
 
Patrick Hogan 
NASA World Wind Manager, NASA 
 
Kevin Montgomery, PhD 
CEO, Intelesense Technologies, Inc. 
Sr. Researcher, Stanford University 
 
 
Abstract 
   Spatial information intelligence is a global issue that will increasingly affect our ability to survive as a 
species. Collectively we must better appreciate the complex relationships that make life on Earth 
possible. Providing spatial information in its native context can accelerate our ability to process that 
information. To maximize this ability to process information, three basic elements are required: data 
delivery (server technology), data access (client technology), and data processing (information 
intelligence). NASA World Wind provides open source client and server technologies based on open 
standards. The possibilities for data processing and data sharing are enhanced by this inclusive 
infrastructure for geographic information. It is interesting that this open source and open standards 
approach, unfettered by proprietary constraints, simultaneously provides for entirely proprietary use of 
this same technology.  
 
   Why World Wind? Over ten years ago NASA World Wind began as a single program with specific 
functionality, to deliver NASA content. But as the possibilities for virtual globe technology became more 
apparent, we found that while enabling a new class of information technology, we were also getting in the 
way.  
 
   Researchers, developers and even users expressed their desire for World Wind functionality in ways 
that would service their specific needs. They want to add their own features. They want to manage their 
own data. They told us that only with this kind of flexibility, could their objectives and the potential for this 
technology be truly realized. World Wind is a set of development tools, a software development kit (SDK) 
that allows a software engineer to create applications requiring geographic visualization. 
 
   Modular Componentry. Accelerated evolution of a technology requires that the essential elements of 
that technology be modular components such that each can advance independent of the other elements. 
World Wind therefore changed its mission from providing a single information browser to enabling a 
whole class of 3D geographic applications. Instead of creating a single program, World Wind is a suite of 
components that can be selectively used in any number of programs. 
 
   World Wind technology can be a part of any application. Or it can be extended with additional 
functionalities by application developers. World Wind makes it possible to include virtual globe 
visualization and server technology in support of any objective. As open source, the world community can 
collectively collaborate in advancing this technology, and thereby continually benefit from optimization 
and increased functionality of this open source infrastructure. 
  
   Open Source + Open Standards = Accelerated Solutions. NASA World Wind is NASA Open Source 
software. This means that the source code is fully accessible for anyone to freely use, even in association 
with proprietary technology. 
 
xvii
   Facilitate Solutions. The ability to effectively deliver spatial data is an essential element of the US 
Executive Order 12906 for the National Spatial Data Infrastructure (NSDI). Open standards for data 
format facilitate data access. In the same manner, an open source 'standard' visualization tool facilitates 
the ability for others to generate spatial data solutions, proprietary or other. This open source technology 
for data access and visualization, also improves the ability for information intelligence, the analytical 
results, to be readily and more effectively shared. NASA World Wind open source technology provides 
the foundational tool for spatial data visualization and facilitates the creation and evolution of spatial data 
analysis and information exchange. 
 
Bio 
   Mr. Patrick Hogan currently manages the NASA World Wind development team, a group of world class 
engineers producing open source software that has received National awards and NASA Software of the 
Year for 2009/2010. During his 20 years with NASA, Patrick managed environmental programs and more 
recently the NASA Learning Technologies (NLT) program. NLT was an incubation 'tank' for technologies 
to move NASA content into education. NLT is where World Wind was born. Patrick, a former pilot, deep 
sea diver and high school science teacher, has a Master's in Earth Science and is a Registered Geologist 
in the State of California. 
 
   Dr. Kevin Montgomery is the Chief Executive Officer of Intelesense Technologies. He is also a Senior 
Researcher at the Center for Innovation in Global Health at Stanford University, and formerly the Director 
of the National Biocomputation Center there, where his team developed advanced technologies in 
medicine for NASA, DoD, NIH, and other clients. Dr Montgomery is a veteran of multiple startups, earned 
a Smithsonian Award in telemedicine, serves as a technical advisor to the DoD, and holds a PhD in 
Computer Engineering from the University of California. He has over 25 years of technical experience and 
20 years of management experience leading high-performance teams in academia, government, and 
industry. 
 
  
 
  
xviii
Keynote V 
COM.Geo 2013 
 
To the Edge of the Universe and Back Again:  
The Evolution of the WorldWide Telescope and  
the Ideas That Inspired GeoFlow 
 
Curtis Wong 
Principal Researcher, Microsoft Research 
 
 
Abstract 
  More than five years ago, WorldWide Telescope (WWT) was launched at the TED conference as the 
realization of a dream to build a high performance accurate interactive 3D model of the Universe 
populated by the highest resolution imagery of the heavens from ground and space based telescopes. 
The goal of the project was to build an interactive spatial temporal data visualization environment that  
could empower kids of all ages to explore and understand the Universe. 
 
   Since that time WWT has garnered more than ten million users of all ages on every continent on Earth. 
WWT features narrated interactive guided tours of the Universe produced by a wide spectrum of users 
from a 6 year old talking about the Ring Nebula to educators creating an interactive tour to help 6th 
graders understand the phases of the moon with a 3D simulation of the Moon's movement around the 
Earth, to Astrophysicists telling the story of the research to visualize the large scale structure of the 
Universe. WWT also has the capability to do large scale visualization and is installed in some of the 
biggest planetariums in the US in San Francisco, New York and Chicago. The Adler Planetarium's 80 
megapixel digital dome is showing Cosmic Wonder the first fully interactive planetarium show produced 
and completely using WWT. 
 
   A private internal version of WWT was also used as a prototyping platform to explore challenges of high 
performance interactive geospatial and temporal data visualization. This research work inspired the 
Microsoft Office product group to create Project GeoFlow a geospatial temporal data visualization 
capability for Office Excel 2013 which is now in public beta preview. 
 
    This talk will cover some of the key ideas within WorldWide Telescope and how they are relevant to 
interactive geospatial data visualization and the development of ideas within GeoFlow.  
 
Bio 
   Curtis Wong, Principal Researcher at Microsoft Research, is responsible for basic and applied research 
in media and interaction. He has been granted more than 35 patents with 15 more pending. Recently, 
Curtis has led the effort to enable interactive spatial temporal data visualization as a broad capability for 
everyone to gain insight into the growing tide of data that is being generated from devices and services. 
This work, codenamed Project GeoFlow, will be released as part of Excel 2013 later in the year and is 
Microsoft's first geospatial temporal data visualization application for the broad market.  
 
   Previously, Curtis conceived and developed Project Tuva in collaboration with Bill Gates to make the 
Messenger Series Lectures by acclaimed Nobel Prize winning theoretical physicist Richard P. Feynman 
freely available over the Internet. In 2008 Curtis fulfilled a lifelong goal to create the WorldWide Telescope 
(WWT), which is a free, rich interactive virtual simulation of the entire visible Universe to enable kids of all 
ages to explore and understand the Universe. 
 
 
xix
Keynote VI 
COM.Geo 2013 
 
Big Data Storytelling through Interactive Maps 
 
Dr. Jayant Madhavan 
Tech Lead, Google Fusion Table 
Staff Software Engineer, Structured Data Research group at Google Inc. 
 
 
Abstract 
Google Fusion Tables (GFT) is often used by data journalists to create interactive maps that are then 
embedded in their news articles. These maps offer journalists the ability to overlay large geo-spatial 
datasets on Google Maps, customize their presentation and combine them with complementary datasets, 
without needing any software development skills beyond cut-and-paste. More importantly, they do not 
have to worry about any systems and scalability issues associated with visualizing large datasets nor 
supporting massive user traffic. In this talk, I will highlight the motivation and design underlying GFT, a 
web offering that brings easy-to-use data management in the cloud to data enthusiasts. Such users have 
interesting datasets, but not necessarily the technical expertise to manage their datasets. By relieving our 
users of the need to deal with systems issues, we let them focus on their storytelling and advocacy, tasks 
that better suit their interests and make better use of their expertise. 
 
Bio 
Dr. Jayant Madhavan is a member of the Structured Data Research group at Google Inc. He is broadly 
interested in enabling users make better use of structured data on the Web. He is currently the technical 
lead for Google Fusion Tables, a cloud data management solution. He was the Chief Architect at 
Transformic Inc., a data integration portal that was acquired by Google. He is a co-recipient of the Ten 
Year Best Paper Award at VLDB 2011. He received a Ph.D. from the University of Washington in 2005. 
 
xx

h-MapReduce: A Framework for Workload Balancing in MapReduce
 VenkataSwamy Martha?, Weizhong Zhao?,†, Xiaowei Xu?
 ?University of Arkansas at Little Rock
 Little Rock, AR, USA
 †College of Information Engineering, Xiangtan University,
 Xiangtan, China
 vxmartha@ualr.edu, wxzhao1@ualr.edu, xwxu@ualr.edu
 Abstract—The big data analytics community has accepted
 MapReduce as a programming model for processing mas-
 sive data on distributed systems such as a Hadoop cluster.
 MapReduce has been evolving to improve its performance. We
 identified skewed workload among workers in the MapReduce
 ecosystem. The problem of skewed workload is of serious
 concern for massive data processing. We tackled the workload
 balancing issue by introducing a hierarchical MapReduce, or
 h-MapReduce for short. h-MapReduce identifies a heavy task
 by a properly defined cost function. The heavy task is divided
 into child tasks that are distributed among available workers
 as a new job in MapReduce framework. The invocation of new
 jobs from a task poses several challenges that are addressed
 by h-MapReduce. Our experiments on h-MapReduce proved
 the performance gain over standard MapReduce for data-
 intensive algorithms. More specifically, the increase of the
 performance gain is exponential in terms of the size of the
 networks. In addition to the exponential performance gains,
 our investigations also found a negative effect of deploying h-
 MapReduce due to an inappropriate definition of heavy tasks,
 which provides us a guideline for an effective application of
 h-MapReduce.
 Keywords-MapReduce, workload balancing, hierarchical
 MapReduce;
 I. INTRODUCTION
 Since its introduction in 2004 by Google, MapReduce
 paradigm has become one of the programming model of
 choice for processing large data sets [4]. MapReduce is a
 framework for developing a distributed solution for complex
 problems over huge data sets using a large number of
 computers (nodes), collectively referred to as a cluster or a
 grid. Favor from distributed file systems (such as HDFS) en-
 hanced the framework to improve its efficiency by leveraging
 data locality awareness. Similar to other distributed systems,
 MapReduce also constitutes a master and a set of workers.
 The master is called ‘job tracker’, while the workers are
 called ‘task trackers’. In contrast to traditional distributed
 programming models [1], [3], [2], MapReduce neither pro-
 vides a shared memory nor a message passing interface. To
 compensate for the lack of communication, outputs of some
 set of tasks called map tasks are passed to other set of tasks
 called reduce tasks. A MapReduce algorithm comprises both
 map tasks that project given data set called input records into
 another data set called intermediate records, and a Reduce
 Figure 1. Power law degree distribution of a Twitter user-follower network
 task that combines intermediate records to a desired final
 result. Therefore, the reduce phase does not start until all
 map tasks are completed.
 In general, the Map and Reduce functions divide the data
 that they operate on for load balancing purposes. It is not
 uncommon for data intensive social network analysis that a
 map/reduce task works on a vertex or edge. The workload of
 all map/reduce tasks is not uniform as the degree distribution
 of social networks is skewed. Social networks such as
 Twitter user-follower network and Document-Term network,
 follow power law degree distribution which is skewed. For
 instance, as it is shown in Figure 1, most of the vertices
 have a low degree, whereas only a very small portion of
 the vertices have extreme high degrees for a Twitter user-
 follower network [5]. When such network is distributed
 as a set of vertices among map/reduce tasks for vertex-
 based processing, the workload among the mappers is also
 skewed. The mappers that receive a high degree vertex take
 much longer time to complete than those that receive a low
 degree vertex. However, slow or bogged down machines may
 lead to straggler computations within the system that might
 lead to a longer completion time: the completion time is
 only as fast as the slowest computation. If one straggler
 computation is twice as slow as other computations, then
 the total elapsed time would be twice of that without the
 straggler computation. In such a case, a straggler detection
 and avoidance mechanism becomes necessary.
 In this paper, we identified skewed workload in dis-
 tributing tasks among workers in MapReduce ecosystem.
 The problem of skewed workload is of serious concern
 2013 IEEE 27th International Conference on Advanced Information Networking and Applications
 1550-445X/13 $26.00 © 2013 IEEE
 DOI 10.1109/AINA.2013.48
 637
for massive datasets. We address the issue by introduc-
 ing a hierarchical MapReduce paradigm (h-MapReduce).
 Workload balancing in h-MapReduce framework is achieved
 by splitting the heavy tasks. The solution poses several
 challenges that can be addressed by h-MapReduce. Our
 investigations on h-MapReduce proved the performance ben-
 efits over standard MapReduce for data-intensive algorithms.
 For the demonstration purposes, Hadoop is considered as
 MapReduce ecosystem and Hadoop and MapReduce are
 referred to interchangeably.
 The rest of the paper is organized as follows. We present
 the skewed workload problem in Section II followed by
 our proposed solution h-MapReduce in Section III. An
 experimental evaluation of h-MapReduce is presented in
 Section IV and V. We review related work in Section VI.
 Finally we conclude the paper with future work in Section
 VII.
 II. PROBLEM DESCRIPTION
 MapReduce is intelligent in distributing tasks among task
 trackers by leveraging location aware algorithms. A task can
 be either a map task or a reduce task. Each task tracker com-
 pletes its assigned task. The task assignment is carried out
 by MapReduce’s job tracker using specified configuration.
 A task tracker reports progress of the running task and job
 tracker preempts if it takes longer than configured threshold
 time ( referred as timeout). There is no mechanism to predict
 beforehand that a task is heavy for a task tracker that take
 long time to complete. There are two types of long running
 tasks; 1. Tasks that run forever and never complete 2. Tasks
 that take long time to complete. Type 1 tasks encounter due
 to bugs in algorithm or code. Our work primarily focuses
 on the tasks of type 2. The long running tasks put the job
 on wait for completion. It is a serious concern if there are
 more such long running tasks.
 Example 1: To illustrate the problem, assume a job for
 computing average clustering coefficient (CC) of a network.
 In graph theory, a clustering coefficient is a metric which
 represents how likely network elements tend to come
 together to form clusters. Triangle is primary building
 block of clusters in a network and CC measures the density
 of triangles in a network. A triangle is formed when two
 neighbors of a vertex are themselves neighbors. Let ‘v’ be
 a vertex and N(v) be the neighbor set i.e. adjacency list of
 vertex ‘v’. The number of links among the neighbors i.e.
 the number of triangles with this vertex is
 |triangles(v)| = |{(c, d, v)|c ? N(v), d ? N(v),
 c ? N(d), d ? N(c), and c = d}| (1)
 can also be written as,
 |triangles(v)| =
 ????c?N(v)N(c) ?N(v)
 ??? /2 (2)
 The local clustering coefficient of a vertex in a network is
 proportion of number of triangles to number of possible
 triangles, in other words it is the number of links between
 the vertices within its neighbourhood divided by the number
 of links that could possibly exist between them. Having the
 number of triangles, the local clustering coefficient of the
 vertex ‘v’ in an undirected network is computed as
 CC(v) = (|triangles(v)|)/(|N(v)|?((|N(v)|?1))/2) (3)
 Then on, the average CC of a network is defined as
 average of local clustering coefficients of all vertices in the
 network.
 Consider a job to compute average clustering coefficient
 in MapReduce framework. The input key and values pair for
 map tasks is the vertex and its neighbors in the adjacency
 list. A map task is to compute local clustering coefficient of
 the input vertex, which can be performed by obtaining the
 adjacency lists of its neighbor vertices. MapReduce algorith-
 m to compute average clustering coefficient is presented in
 Algorithm 1.
 Algorithm 1 Algorithm to compute average clustering co-
 efficient CC of a network
 Mapper
 Input: key is a vertex ‘v’and value is adjacency list of ‘v’ i.e.
 ‘N(v)’
 Output: key is a unique term i.e. ‘CC’, value is local CC of vertex
 ‘v’
 1) Let v=vertex;
 2) Let degreeOfv=sizeof(N(v));
 3) Let triangleCount=0;
 4) for each( c in N(v) )
 triangleCount+=CountCommonElements(N(c),N(v));
 5) localCC=triangleCount/(degreeOfv*(degreeOfv-1)/2);
 6) emit(‘CC’,localCC);
 Reducer
 Input: key is unique term ‘CC’, values are local CCs of all
 vertices of the network
 Output: key is unique term ‘CC’, value is average CC of the
 network
 1) Let avgCC=0;
 2) Let ccCount=0;
 3) for each( oneCC in values)
 avgCC+=oneCC;
 ccCount++;
 4) emit( ‘CC’, avgCC/ccCount );
 In the Algorithm 1, the function ‘CountCommonEle-
 ments’ is computationally costly and called for each vertex
 in the neighbor list of a vertex. The more the neighbors, the
 more the number of calls to the function, therefore more
 the time is required to compute local clustering coefficient
 of such high degree vertex. There is no guarantee that the
 degree distribution is uniform. In cases where 99% of the
 vertices are low degree and just 1% of the vertices are high
 degree (compared to low degrees), the maps that run mapper
 638
on lower degree vertices complete very quickly and sitting
 idle to wait for the high degree vertices to complete that
 actually count just 1% of the network. The reduce tasks do
 not start until all the map tasks complete, thus causing long
 time to complete the job. Therefore, if the adjacency list of
 vertices is a skewed distribution like in the case of social
 networks, the task computations are also not balanced. One
 can argue that workload can balanced based on computation
 cost of a task for a vertex which is computed from the size
 of its adjacency list. The cost function cannot balance the
 workload in the case of skewed degree distributions such as
 those of social networks because this cannot change when
 there is one (very less number of) high degree vertex(s). In
 skewed distributions, task trackers who work on high degree
 vertices could have been working while other task trackers
 are sitting idle after completing corresponding tasks. One
 can observe the problem if the tasks cost function is skewed.
 The fundamental risks from the skewed workload distri-
 bution are following:
 • Some task trackers are overloaded with heavy tasks.
 • Heavily loaded tasks run for long, therefore the job
 waits for this task to complete, which in turn increases
 run time of the job.
 • Under-utilization of resources when the heavily loaded
 task tracker is working and other task trackers are
 sitting idle.
 There are many such MapReduce algorithms, besides
 computing average clustering coefficient in a network, that
 encounter the skewed work load distribution issues.
 III. H-MAPREDUCE
 The limitation of MapReduce paradigm prompts us to
 further split heavy tasks of a job. When a task on a record of
 a map reduce job takes long time or needs more resources
 than others, the task on the record is eligible for further
 splitting to make use of idle resources in the infrastructure.
 It is trivial to use the existing MapReduce ecosystem for the
 tasks that split. The new tasks run in the same MapReduce
 framework in the form of a new job and called “Child
 Job”. The tasks that initiated child jobs, refer the tasks
 as parent tasks, wait until the child jobs get completed.
 This way a task of a task tracker is distributed among
 active and available task trackers if the task is predicted
 to run for long time. The decision whether to distribute a
 task needed to be taken beforehand the processing a record
 of the task. The task of a heavily loaded task tracker is
 parallelized to complete the task in reasonable time. The
 parent and child jobs constitute a hierarchy of jobs, thus
 the proposing solution framework is called “h-MapReduce”,
 short for hierarchical MapReduce. Since parent and child
 jobs run on MapReduce ecosystem, both jobs are developed
 using MapReduce programming model. Developers are free
 to use other distributed systems for child jobs, if one exists.
 Parent task sits on hold until its child job gets back with
 Figure 2. Architecture of h-MapReduce
 results. The framework for the hierarchy of parent and child
 jobs does not alter the underlying MapReduce ecosystem
 but leverages it for workload balancing. The architectural
 view of the h-MapReduce framework is presented in Figure
 2. The proposed concept of initiating a job from a task of a
 job is complex and poses several challenges. The challenges
 are addressed in the following discussion.
 A. Challenges in h-MapReduce
 h-MapReduce addresses design and development chal-
 lenges that arise from creating a child job from a task of
 a MapReduce job. The challenges are following.
 1) Definition of heavy task (When to split a task):
 The basis of h-MapReduce is to develop a mechanism to
 draw a line between light-weight tasks and heavy tasks.
 The heavy task definition decides when to split a task. A
 cost function of a task helps to draw such a line. Cost
 functions depend on the algorithm and the problem that the
 algorithm is attempting to address. The cost function of a
 task that computes the clustering coefficient of a vertex in
 an undirected network is the degree of the vertex. A vertex
 having degree more than a threshold implies a heavy task
 and leads to a child job for the vertex from the task.
 2) Algorithm for Child Job (How to distribute a task): As
 we discussed, the existing MapReduce ecosystem is used to
 parallelize the child job. This requires developers to come up
 with a MapReduce-based algorithm for child jobs. To keep
 things simple, the child job needs to be efficiently designed
 for the MapReduce paradigm. The child job works on the
 record that could cause heavy work load in parent job. In our
 example of computing clustering coefficients, the child job is
 implemented as a MapReduce algorithm to count the number
 of triangles associated with the vertex in the parent task.
 With the result from the child job, the parent task computes
 clustering coefficient of the vertex.
 639
3) Deadlock: It is not uncommon that hierarchical job-
 s encounter deadlock situations and h-MapReduce is no
 exception. In the case that all the resources (i.e. all task
 trackers) are occupied by the parent job; child jobs sit
 waiting in queue for available resources. The parent job
 could not vacate the resources until child job complete as
 tasks in the parent job waits for child jobs. This scenario
 resembles deadlock where there is no progress of jobs be-
 cause of child job waiting for resources occupied by parent
 job that waiting for child job to complete. Though there
 are several possibilities to address the deadlock situation, a
 simple approach is presented here that does not disturb the
 MapReduce ecosystem. The simple approach is to reserve
 specified resources (i.e. set of task trackers) for parent
 jobs and the rest for child jobs. This is analogous to a
 division of one MapReduce ecosystem into two MapReduce
 ecosystems. It is not exactly dividing the ecosystem but
 reserving resources for dependent jobs so that they should
 not compete for resources. To demonstrate, reservations are
 accomplished in h-MapReduce through the Hadoop Fair
 Scheduler. Two queues are defined, one for parent jobs and
 aother for child jobs. The Fair Scheduler takes necessary
 steps to prevent competition for resources among two queues
 and consequently prevents competition between parent and
 child jobs.
 4) Configuration conflicts: A configuration is necessary
 to initiate a new MapReduce job. Each task in a job
 obtains the configuration in the MapReduce ecosystem. h-
 MapReduce uses the configuration obtained from the parent
 job to initiate a new job in a task. Since the algorithms
 for parent and child jobs are different, there is a need for
 different configuration. The conflicts encountered from such
 inheritance necessitated attention from developers to reset
 configuration parameters inherited from the parent job. The
 “INPUT SPLIT” parameter in the MapReduce configuration
 set is the best example that demonstrates the importance of
 attention paid to conflicts. The parent job works on massive
 input and needs to set a higher value for INPUT SPLIT
 while child jobs work on a smaller input (e.g. one adjacency
 list) and require a lower value to best utilize MapReduce
 resources.
 By addressing these challenges, h-MapReduce is able
 to successfully initiate and complete child jobs from a
 MapReduce task. We investigated the performance of the h-
 MapReduce with standard apache Hadoop; the experiments
 are presented in Section IV.
 IV. EXPERIMENTS
 Several experiments are conducted to analyse the ef-
 fectiveness of the proposed h-MapReduce in comparison
 to traditional MapReduce framework. We adopted Apache
 Hadoop as the MapReduce framework and implemented h-
 MapReduce on top of it. Two applications are developed on
 both the Hadoop and h-MapReduce. The applications are:
 1. Compute average clustering coefficient of a network and
 2. Sorting terms of each document in a corpus.
 All the experiments are performed on Hadoop cluster
 of 2 masters and 24 slaves each with 7 mappers and 7
 reducers. Each slave has 8 processors and 16GB RAM. The
 experiments include a run using standard Hadoop ecosystem
 and other with proposed h-MapReduce framework. Each
 experiment run twice, and average of the run times is
 considered. The run time for each experiment is noted and
 plotted in charts for the discussion in the following section.
 A. Datasets
 Two types of datasets are used to evaluate the h-
 MapReduce framework for undirected networks. Each serves
 a distinct purpose. One of them is star shaped networks.
 Star networks are constructed in such way that only a single
 vertex is connected to every other vertex in the network
 and there is no other connections. The rationale behind
 choosing this dataset is to find the gain from h-MapReduce
 over traditional MapReduce. In these datasets, we only see
 a heavy task and rest are light weighted ones. Therefore,
 the dataset is good for evaluating the performance in case
 of one heavy task. We generated several star networks of
 various numbers of vertices from 5,000 to 200,000,000.
 The other type of datasets is generated using Benchmark
 network generator [7]. The primary use of the dataset is
 to show the negative effect of the proposed framework
 with wrong definition of heavy task. Benchmark networks
 possess low degree. Other characteristics of the benchmark
 networks include both the degree distribution and the size
 of community structures follows power law distribution.
 Networks with number of vertices ranging from 10,000 to
 160,000 are generated for the purpose.
 B. Calculating Average Clustering Coefficient
 The primary objective of the algorithm is to compute
 average clustering coefficient of a network. MapReduce al-
 gorithm for computing average CC of a network is presented
 in Algorithm 1. Mapper of the algorithm takes a vertex and
 corresponding adjacency list as key and values. The mapper
 emits vertex ‘v’ as key and corresponding local clustering
 coefficient as value. Reducer sums up all the local clustering
 coefficients and emits mean of them as value (key does not
 matter here). An index mechanism is implemented to retrieve
 adjacency list of a given vertex. The index is distributed
 on distributed file system and available for all tasks of a
 MapReduce job. Therefore the mappers leverages the index
 to obtain N(v), N(c) and N(d) to compute CC(v).
 As mentioned in Section II, Algorithm 1 is not ef-
 ficient when the given network has skewed degree dis-
 tribution. Therefore, h-MapReduce strategy is applied to
 address the workload balancing problem. More specifically,
 h-MapReduce splits the task (which is heavy loaded) by
 introducing a new MapReduce job. In our experiments we
 640
define a heavy task if N(v) > 5000 and the heavy task is to
 compute clustering coefficient of a high degree vertex. The
 rationale in choosing 5000 is that our investigations found
 the computation time for vertices withN(v) < 5000 is lesser
 than Hadoop job setup and cleanup times. The h-MapReduce
 paradigm based algorithm for computing average CC is
 summarized in Algorithm 2.
 Algorithm 2 Algorithm to compute average clustering co-
 efficient CC of a network using h-MapReduce (Parent Job)
 Mapper
 Input: key is a vertex ‘v’and value is adjacency list of ‘v’ i.e.
 ‘N(v)’
 Output: key is a unique term i.e. ‘CC’, value is local CC of vertex
 ‘v’
 1) Let v=vertex;
 2) Let degreeOfv=sizeof(N(v));
 3) if(degreeOfv>5000)
 triangleCount = countTrianglesInChildJob(v, N(v));
 4) else
 Let triangleCount=0;
 for each( c in N(v) )
 triangleCount+=CountCommonElements(N(c),N(v));
 5) localCC=triangleCount/(degreeOfv*(degreeOfv-1)/2);
 6) emit(‘CC’,localCC);
 Reducer
 Input: key is unique term ‘CC’, values are local CCs of all
 vertices of the network
 Output: key is unique term ‘CC’, value is average CC of the
 network
 1) Let avgCC=0;
 2) Let ccCount=0;
 3) for each( oneCC in values)
 avgCC+=oneCC;
 ccCount++;
 4) emit( ‘CC’, avgCC/ccCount );
 Expensive operation in the algorithm 1 is counting number
 of triangles associated with a vertex that is reduced in h-
 MapReduce paradigm by introducing a new MapReduce
 job for the purpose. In algorithm 2, a child job is initiated
 if the given vertex has high degree. The function ‘count-
 TrianglesInChildJob’ takes the neighbor list and passes the
 list as input to child job to run the algoirthm 3. The child
 job receives adjacency list as input and counts the number
 of triangles with each vertex in the adjacency list of ‘v’.
 A MapReduce algorithm for child job is developed and
 presented in Algorithm 3.
 A mapper of the child job takes a vertex ‘w’ as input
 value. We define distance of a vertex (say ‘w’) as number
 of hops needed for it to reach the vertex ‘v’ for which the
 child job is invoked. A vertex in adjacency list of ‘v’ is at
 distance 1 and they can reach ‘v’ in one hop. A mapper
 in child job finds neighbors of the vertex ‘w’ and emits its
 Algorithm 3 Algorithm to count number of triangles asso-
 ciated with a vertex ‘v’ (MapReduce Algorithm for Child
 Job)
 Mapper
 Input: key is vertex ‘w’ where w ? N(v)
 Output: key is vertex, value is distance from ‘v’
 1) Let w=vertex;
 2) for each( u in N(w) )
 emit(u,2);
 3) emit(w,1);
 Reducer
 Input: key is vertex, value is distances from ‘v’
 Output: key is vertex, value is number of triangles with ‘v’ and
 this vertex
 1) Let w = vertex,
 nTriangles=0,
 isTriangle = false;
 2) for each( distance in values)
 if( distance = 1)
 isTriangle = true;
 if( distance = 2)
 nTriangles++;
 3) if( isTriangle )
 emit( w, nTriangles );
 distance to reach the vertex ‘v’ i.e. distance of 2 as value.
 Alternately, each vertex in adjacency list of ‘w’ can reach ‘v’
 in 2 hops through ‘w’. The mapper also emits the vertices
 with distance 1 i.e. ‘w’ with distance 1. Reducer of the child
 job receives a vertex as a key and corresponding distances
 as values. For a vertex, if there is a value of ‘1’ then this
 vertex has direct edge with ‘v’ and all the distances of ‘2’
 corresponds to triangles. Therefore the number of triangles
 associated with the vertex ‘v’ and with the vertex ‘w’ is
 number of distances of ‘2’ if there is at least a distance of
 ‘1’. By the end of the reducer, we have a vertex ‘w’ (in
 adjacency list of ‘v’) and corresponding count of triangles.
 The flowchart of the algorithm is presented in Figure IV-B.
 For performance improvement, the reducer emits a record
 for each triangle i.e. for each value of distance ‘2’. The
 number of triangles with vertex ‘v’ in a task from parent job
 is number of output records from child job. The clustering
 coefficient of ‘v’ is computed using the obtained values and
 the Eq. 3. Therefore a task of a mapper is distributed among
 available task trackers in MapReduce ecosystem.
 For star shaped networks, we define a heavy task if a map-
 per gets a vertex of degree over 5,000. The h-MapReduce
 framework computed average clustering coefficient faster
 than traditional MapReduce. A discussion on run time is
 presented in following section. It is obvious that a local
 clustering coefficient of a vertex with only 100 edges can be
 computed by a task for which no child task is necessary. To
 641
Figure 3. Flowchart for computing average Clustering Coefficient
 show the adverse effect of the proposed framework with a
 wrong definition of heavy task, a heavy task is defined with
 the degree over 100. Benchmark datasets are used for the
 purpose of showing negative effect of h-MapReduce. The
 result is presented in Section V.
 C. Sorting terms of each document in a corpus
 Sorting terms of each document in a corpus is another
 case study of the h-MapReduce. Assume there is a bi-partite
 document-term network. One type of vertices in the network
 is documents and other is terms (one or more words). A
 document is connected to several terms which occurred in
 the document. There are many such documents and terms.
 Now the task is to sort the terms in lexicographic order
 in each document. In other words, to sort adjacency list
 of each document vertex in the document-term network.
 Sorting terms can be applied in many real world applications
 such as information retrieval.
 If there is only a document, a fast algorithm can be
 developed on MapReduce ecosystem. The algorithm can
 be extended to sort terms of more than one document in
 a corpus. Each mapper takes a document and sorts its
 terms. Input for a mapper is document identifier as key
 and containing terms as value. Mapper sorts the terms and
 emits document identifier as key and sorted terms as value.
 There is no need for reducer. But if there is a document
 that contains tens of thousands of terms, the mapper that
 processes the document, i.e. sorts its terms, takes a long time
 because the number of terms are more than a mapper can
 handle. h-Mapreduce comes again to rescue when a map task
 is too heavy. Though there are techniques for external sorting
 to deploy in a map task, they do not scale for millions of
 items. MapReduce found its efficacy in sorting of given keys.
 Taking the advantage, h-MapReduce framework distributes
 the task of sorting among MapReduce task trackers as a
 new job. As a default operation MapReduce outputs keys
 in sorted order. The terms of a document is given as input
 keys for a MapReduce job and output obtained from identity
 mapper and identity reducer is sorted list of terms.
 Figure 4. Running time for calculating average CC of star networks
 Investigation on this algorithm is also carried out on star
 network datasets. As discussed, the star networks show fine
 detailed gain of the proposed h-MapReduce over standard
 MapReduce for one heavy task. The analogy of document-
 term layout with star network is that the center vertex is a
 document and other vertices are terms. For this algorithm,
 we define a heavy task that sorts the terms of a document
 with more than 10,000 terms because similar to other
 application, the time to sort terms less than 10,000 is less
 than a Hadoop job setup and cleanup times.
 V. RESULTS AND DISCUSSION
 The results from the experiments evidenced the efficacy
 of h-MapReduce over standard Hadoop. The running time
 from each experiment is recorded for comparison. The size
 of the datasets is increased for repeated experiments.
 A. Runtime analysis in calculating clustering coefficient
 Run time for computing average CC of a network using
 standard MapReduce and the proposed h-MapReduce frame-
 work is recorded. The recorded running time is plotted in
 Figure 4.
 The run time curve that represents standard MapReduce
 grows exponentially, which indicates the effect of lack of
 workload balancing. The plot in Figure 4 also reveals the
 effect of heavy task in Hadoop that slow down the com-
 putation. The speedup measures the gain of h-MapReduce,
 which is defined as follows.
 Speedup =
 Runtime without h-MapReduce
 Runtime with h-MapReduce (4)
 The speedup by using h-MapReduce in comparison with
 standard MapReduce ecosystem increases as the network
 size increases. The plot showing the speedup for each
 network is pictured in Figure 5. The increase in speedup
 is significantly large with increasing size of the networks,
 which further justifies the need of h-MapReduce. The plots
 also demonstrate a significant performance improvement
 over the standard MapReduce ecosystem.
 642
Figure 5. Speedup of h-MapReduce for calculating average CC of star
 networks
 Figure 6. Run time for sorting terms in document-term networks
 Figure 7. Speedup of h-MapReduce for sorting terms in document-term
 networks
 B. Runtime analysis for sorting terms in a document
 Figure 6 shows the plot of the running time for sorting
 terms in each document in a corpus represented as bipartite
 document-term networks. From the plot, it is observed that
 the run time by using h-MapReduce is longer for smaller
 networks, and as the network size grows the run time
 becomes much less in comparison. A longer run time for
 smaller networks suggests that h-MapReduce is not suitable
 for smaller datasets. The speedup of h-MapReduce over
 Hadoop increases as the network grows, which indicates
 that h-MapReduce is more appropriate for data intensive
 tasks. Figure 7 shows the speedup of h-MapReduce over
 the standard MapReduce.
 C. Limitation of h-MapReduce
 h-MapReduce has its limitations with its dependency on
 definition of heavy task. The efficiency of h-MapReduce
 Figure 8. Run time for calculating average CC for benchmark networks
 shows an adverse effect
 highly depends on the decision to split a task. Here negative
 effect of the h-MapReduce is presented, which encountered
 by an inappropriate definition of heavy task. For the purpose,
 we define a heavy task for computing clustering coefficient
 as a task that receives a vertex with more than 100 adjacent
 vertices. Benchmark networks are used for this experiment
 as it contains more vertices of various degrees. We generated
 benchmark networks in such a way that the network degrees
 range is minimal. The run time for computing average CC
 for each benchmark network is plotted in Figure 8.
 For the benchmark networks, using h-MapReduce took
 more time than standard MapReduce ecosystems. The rea-
 sons for the adverse effect is due to the following reasons.
 • A heavy task using a child job takes more time than it
 runs in the parent task.
 • Creating a child job is not free, the child job setup and
 cleanup adds more time.
 • There are many heavy tasks that do not have enough
 resources (task trackers) to run child jobs.
 The adverse effect of h-MapReduce indicates the importance
 of a proper definition of heavy tasks. In practice we should
 only use h-MapReduce for really big data where a skewed
 workload distribution is a great concern.
 VI. RELATED WORK
 In this section, we review existing solutions for optimizing
 Hadoop / MapReduce performance. Zhang and Sterck de-
 veloped a system called “CloudBATCH” to enable Hadoop
 to function as a traditional batch job queuing system with
 enhanced management functionalities for cluster resource
 management [6]. CloudBATCH constitutes of a set of HBase
 [10] tables that are used for storing resource management
 information, such as user credentials, queue and job infor-
 mation, which provide the basis for rich customized man-
 agement functionalities. Elnikety et al. proposed iHadoop, an
 add on for the MapReduce framework optimized for iterative
 applications that exhibit a producer/consumer relation of
 consecutive iterations on the same physical node where the
 cost of inter-iteration data transfer is minimal. [9]. Besides
 iHadoop, Zhang et al. proposed iMapReduce that extracts
 the common features of iterative MapReduce algorithms and
 643
Table I
 EVOLUTION OF HADOOP PERFORMANCE TUNING
 System Level of Scheduling Scheduling Technique
 Hadoop Task Naive
 Speculative
 Hadoop[11], Mantri
 [12] and StarFish[13]
 Task Resource based
 skewTune[14] and
 skewReduce[15]
 Record Overload based
 h-MapReduce instructions in a task
 for a Record
 Cost function based
 provides the built-in support for these features [8]. Specula-
 tive execution is proposed to enhance Hadoop framework in
 heterogeneous infrastructure to reschedule straggling tasks
 on available other workers[11]. In addition there resource
 based schedulers attempt to uniformly distribute the load
 among tasks such as StarFish[13] and Mantri[12].
 In all the MapReduce based frameworks, it is assumed
 that all the map and the reduce tasks are uniformly loaded,
 which may not be true as it is demonstrated for social net-
 works such as Twitter shown in section I. In contrast to the
 traditional way of assuming a task an atomic, there are tech-
 niques to repartition the tasks when found straggling[14].
 SkewTune aggressively repartitions the input data for a
 task so that the task do not run longer than the other
 tasks. Similar to SkewTune, SkewReduce partitions the input
 records to address the skewed workload distribution based
 on user-define cost functions[15]. In contrast, h-MapReduce
 distributes a task for a record among available task trackers
 so to avoid straggling. To summarize h-MapReduce comple-
 ments existing performance tuning techniques. The evolution
 in Hadoop performance tuning techniques at various levels
 are compiled in Table VI.
 VII. CONCLUSION
 MapReduce is a pervasive programming model for big
 data analysis. Despite of its intelligent scheduling tech-
 niques, we identified skewed workload in distributing tasks
 among workers in MapReduce ecosystem. The problem
 of skewed workload is of serious concern for massive
 datasets. h-MapReduce, built on top of MapReduce, is
 proposed in this work to address the lack of workload
 balancing in MapReduce ecosystem. Workload balancing
 in h-MapReduce is achieved by splitting heavy tasks. The
 solution poses several challenges such as deadlocks, in-
 heritance conflicts etc.. h-MapReduce addresses these chal-
 lenges. Our experiments using various networks including
 social networks and document-term networks demonstrated
 the speedup of using h-MapReduce over standard MapRe-
 duce for big datasets where skewed workload is a great
 concern. The investigations also found the negative effect of
 h-MapReduce with a feeble definition of heavy tasks. Our
 future work will explore additional opportunities to further
 improve the performance of h-MapReduce.
 ACKNOWLEDGMENT
 This project was funded by Acxiom Corporation. The
 authors are grateful for invaluable collaboration with Kevin
 Liles and Derek Leonard throughout the project. This work
 was supported in part by the National Science Founda-
 tion under Grant CRI CNS-0855248, EPS-0701890, EPS-
 0918970, MRI CNS-0619069, OISE-0729792, and the Na-
 tional Natural Science Foundation of China (No. 61105052).
 REFERENCES
 [1] Gropp, W.; Lusk, E.; , ”The MPI communication library:
 its design and a portable implementation,” Scalable Parallel
 Libraries Conference, 1993, pp.160-165, 6-8 Oct 1993
 [2] B. Lampson, ”Remote Procedure Calls” LNCS, Vol. 105,
 Springer-Verlag, New York, 1981, pp. 365-370.
 [3] G. R. Andrews and F. B. Schneider, ”Concepts and Notations
 for Concurrent Programming,” ACM Computing Surveys, Vol.
 15, No. 1, Mar. 1983, pp. 3-43.
 [4] Dean, Jeffrey, and Sanjay Ghemawat. MapReduce : Simplified
 Data Processing on Large Clusters. Ed. L Purich Daniel.
 Communications of the ACM 51.1 (2008) : 1-13
 [5] Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
 Moon, What is Twitter, a Social Network or a News Media?
 WWW 2010, April 2630, 2010
 [6] C. Zhang, H. Sterck. CloudBATCH: A Batch Job Queuing
 System on Clouds with Hadoop and HBase. IEEE Conference
 on Cloud Computing Technology and Science, 368-375, 2010.
 [7] Lancichinetti, Andrea, Santo Fortunato, and Filippo Radicchi.
 Benchmark graphs for testing community detection algorithms.
 Physical Review E - Statistical, Nonlinear and Soft Matter
 Physics 78.4 Pt 2 (2008).
 [8] E. Elnikety, T. Elsayed, H. Ramadan. iHadoop: Asynchronous
 Iterations for MapReduce. 3rd IEEE Int’l Conference on Cloud
 Computing Technology and Science, pages, 81-90, 2011.
 [9] Y. Zhang, Q. Gao, L. Gao, C. Wang. iMapReduce: A Distribut-
 ed Computing Framework for Iterative Computation. J. Grid
 Computing, 10:47-68, 2012.
 [10] HBase. http://hadoop.apache.org/hbase/.
 [11] Zaharia, Matei and Konwinski, Andy and Joseph, Anthony
 D. and Katz, Randy and Stoica, Ion; Improving MapReduce
 performance in heterogeneous environments, 8th USENIX
 conference on OSDI, pp.29-42, 2008.
 [12] Ganesh Ananthanarayanan, Srikanth Kandula, Albert Green-
 berg, Ion Stoica, Yi Lu, Bikas Saha, and Edward Harris. 2010.
 Reining in the outliers in map-reduce clusters using Mantri.
 9th USENIX conference on OSDI. pp. 1-16
 [13] Herodotos Herodotou and Harold Lim and Gang Luo and
 Nedyalko Borisov and Liang Dong and Fatma Bilgen Cetin
 and Shivnath Babu; Starfish: A Self-tuning System for Big
 Data Analytics, CIDR, 2011, pp. 261-272
 [14] YongChul Kwon, Magdalena Balazinska, Bill Howe, and
 Jerome Rolia. 2012. SkewTune: mitigating skew in mapreduce
 applications. In Proceedings of the 2012 ACM SIGMOD,
 ACM, New York, NY, USA, pp. 25-36.
 [15] YongChul Kwon, Magdalena Balazinska, Bill Howe, and
 Jerome Rolia. 2010. Skew-resistant parallel processing of
 feature-extracting scientific user-defined functions. 1st ACM
 symposium on Cloud computing (SoCC ’10). pp. 75-86.
 644

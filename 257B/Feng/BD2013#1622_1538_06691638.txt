HIG – An In-memory Database Platform
 Enabling Real-time Analyses of Genome Data
 Matthieu-P. Schapranow, Hasso Plattner
 Hasso Plattner Institute
 Enterprise Platform and Integration Concepts
 August–Bebel–Str. 88
 14482 Potsdam, Germany
 {schapranow|plattner}@hpi.uni-potsdam.de
 Abstract—Costs and time required for sequencing
 of DNA and RNA declined through use of next
 generation sequencing technology, e.g. up to 30-times
 coverage reads are generated in less than two days.
 However, its interpretation and analysis is still a
 time-consuming process potentially taking weeks. In
 this work, we present a completely new architecture
 for processing and analyzing genome data. It builds
 on the in-memory database technology to eliminate
 time-consuming file-based data operations and to
 enable real-time data analysis. We found out that
 the use of in-memory technology as an integral com-
 ponent for genome data processing and its analysis
 significantly reduces time and costs to obtain relevant
 results, e.g. in the course of personalized medicine.
 Keywords-Real-Time Data Analysis; In-Memory
 Database Technology; Genome Data; Personalized
 Medicine; Next-Generation Sequencing
 I. Introduction
 The Human Genome (HG) project officially launched
 in 1990 involved thousands of worldwide research in-
 stitutes and required more than a decade to sequence
 and decode the full HG [1]. Nowadays, Next-Generation
 Sequencing (NGS) devices process whole genomes within
 hours at reduced costs [2]. They are used in research and
 clinical environments to support treatment of specific
 diseases, such as cancer. Personalized medicine aims at
 treating patients specifically based on individual dispo-
 sitions, e.g. genetic or environmental factors [3]. This
 requires tool support to identify relevant data out of the
 huge amount of acquired diagnostic data [4].
 The In-Memory Database (IMDB) technology has
 demonstrated major advantages in analyzing big en-
 terprise data [5], [6]. In the given work, we present
 our findings of applying IMDB technology to enable
 real-time analysis of genome data in course of our
 High-performance In-memory Genome (HIG) research
 project. We developed a specific platform that com-
 bines processing and analyzing of genomic data as a
 holistic process based on the feedback of researchers
 and clinicians. Our HIG architecture is designed to run
 on commodity hardware instead of highly specialized
 H IG  W eb Serv ice
 (R uby on R ails)
 Specific  C loud  
A pp lications
 W orker N ode
 R
 W orker
 (Python)
 In-M em ory 
D atabase Instance
 R eads and
 R eference
 G enom es
 R
 H IG  W eb S erv ice
 (R uby on  R ails)
 Int
 er
 ne
 t
 Int
 ra
 ne
 t
 L  functions,
 S Q L scrip t
 p rocedures
 M aster N ode
 Prim ary In-M em ory
 D atabase Instance
 R
 C lin ic ian  w ith
 M obile  D evice
 Specific  C loud  
A pplication
 R
 R esearcher w ith
 W eb Brow ser
 R
 W ork L is t,
 W orker S ta tus,
 G lobal S tate
 S chedu le r and C oord inator
 Task Scheduler
 (Python)
 Data
  Layer
 Platform
  
Layer
 Application
  
Layer
 R
 R
 W orker
 (P ython)
 R
 3rd  P arty
 Too l
 3rd Party
 Too l
 R
 S pecific  C loud 
A pp lications
 S pecific  M obile  
Applica tion
 R
 R
 Figure 1: The HIG system architecture consist of appli-
 cation, platform, and data layer. Data analysis is directly
 performed within the IMDB eliminating time-intensive
 data transfers from the file system.
 hardware to be a) cost-efficient and to b) make use of
 existing hardware infrastructures. Fig. 1 depicts the sys-
 tem architecture of our system modeled as block diagram
 using the Fundamental Modeling Concepts (FMC) [7].
 The rest of the paper is structured as follows: In
 Sect. II our work is set in context of related work and
 we introduce selected components of our HIG system in
 Sect. III. We share our benchmarks results in Sect. IV
 and discuss our findings in Sect. V. Our work concludes
 with an outlook in Sect. VI.
 2013 IEEE International Conference on Big Data
 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 691
II. Related Work
 Time consumed by sequencing and related costs
 dropped disruptively with the introduction of latest NGS
 technologies around 2007. Meanwhile, algorithms used
 data processing of NGS data date back to early 1970s.
 Related work in the field of genome data processing
 has increased in the recent years, but work focusing on
 implementing end-to-end processes is still rare.
 Wandelt et al. have discussed recent data manage-
 ment challenges for processing NGS data. They observed
 trends towards more and more cloud-based solutions,
 but they identified the efficient mapping of workflow
 tasks onto heterogeneous distributed compute nodes
 and the adjustment of a given workflow to a dynamic
 environment as still open issues [8, Sect. 4.3]. Our
 work contributes by defining a system architecture that
 combines processing and analyzing of genome data. We
 contribute by sharing details about our worker frame-
 work developed in Python in Sect. III, which enables
 integration of compute resources across platform and
 Operating System (OS) borders. Furthermore, we share
 details about our scheduler in Sect. III, which enables
 prioritized processing of multiple tasks in parallel, e.g.
 simultaneous users or multiple departments.
 Pabinger et al. published a survey on variant anal-
 ysis tools and evaluated their functionality [9]. They
 selected 32 of them for evaluation and found out that
 the majority is command-line based, e.g. ANNOVAR,
 AnnTools or snpEff [10], [11], [12]. For web-based tools
 they see a drawback in data preparation before analysis
 can start since "[. . . ] files need to be packed, sorted and
 indexed before they can be used [9]". With our incor-
 porated IMDB technology we address time-consuming
 data transformation steps and replace them by native
 database operations as discussed in Sect. V.
 Furthermore, Pabinger et al. evaluated workflow sys-
 tems and analysis pipeline tools. They realized that these
 tools either miss flexibility or need to have additional
 know-how to install specific tools. Our work contributes
 by introducing a flexible way to execute individual
 pipeline configurations without the need of adapting
 command line scripts. We enable graphical modeling
 of individual pipeline configurations and its execution
 within a single system as presented in Sect. III.
 III. Architecture
 Our platform consists of the following layers: appli-
 cation, platform, and data as depicted in Fig. 1. In the
 following, all layers are described in detail.
 A. Application Layer
 The application layer consists of special purpose ap-
 plications to answer specific medical and research ques-
 tions. Although these applications can only be used
 for limited use-cases, they are highly optimized. For
 example, Fig. 2 depicts our genome browser application,
 which combines patient-specific data and data from
 various annotations databases in a single view.
 All applications in the application layer communicate
 with the web service interface as depicted in Fig. 1.
 We provide a Application Programming Interface (API)
 that can be consumed by various kinds of applications,
 such as web browser applications or iPad and Android
 devices as mobile applications, via asynchronous Ajax
 calls and JavaScript Object Notation (JSON) [13], [14].
 As a result, performing specific analyses is no longer
 limited to a specific location, e.g. the desktop computer
 of a clinician. Instead, all applications can be accessed
 via any internet-connected device, e.g. web browser or
 mobile tablet computer, which enhances the user’s pro-
 ductivity by having access to relevant data at any time.
 B. Platform Layer
 The platform layer holds the complete process logic
 and consists of the IMDB system for enabling real-
 time analysis of genomic data. We developed specific
 extensions for the IMDB system to support genome data
 processing and its analysis. In the following, selected
 extensions and their integration in the IMDB system are
 described in more detail.
 1) Scheduling of Data Processing: We extended the
 IMDB by a worker framework, which executes tasks
 asynchronously, e.g. alignment of chunks of genome data.
 It consists of a task scheduler instance and a number of
 workers controlling dedicated compute resources, e.g. in-
 dividual compute nodes. Workers retrieve tasks and pa-
 rameters by the scheduler instance and perform specific
 tasks, such as workbench preparation, task execution,
 and maintenance of status information. Thus, all workers
 are connected to the IMDB to store status information
 about currently executed tasks.
 The scheduler is responsible for managing the parallel
 execution of jobs on distributed compute resources. It
 performs non-preemptive resp. cooperative scheduling
 operations comparable to a process scheduler known
 from multitasking OS, i.e. assigning tasks to available
 resources without interrupting their operation at defined
 times [15]. We implemented different scheduling poli-
 cies, which can be extended by adding new policies to
 the policies.d folder. The default policy is random
 select, i.e. any free worker select any available job at
 random and starts its execution. In addition, we use a
 first-come first-serve policy, i.e. the job with the oldest
 creation timestamp will be executed first, and a priority
 scheduling policy, which starts the executed of all jobs
 with a higher priority class before starting jobs with a
 lower priority class. The priority scheduling also enables
 a boosting of very long running and compute intensive
 692
Figure 2: Genome Browser: From top to bottom, comparison of reference genome and a selected cell line at a specific
 locus on chromosome 12. A Single Nucleotide Polymorphism (SNP) is depicted on the left. Combined information
 from international research databases about the selected mutation are shown on the right.
 tasks. Therefore, the overall job execution time for a
 specific task is supervised and evaluated corresponding
 to the history of already performed jobs statistics. Once
 a long-running job has been detected, is automatically
 assigned to a higher priority level to a) finish its execu-
 tion and to b) free compute resources for new work as
 soon as possible.
 The scheduler is contacted by the web service, i.e.
 the scheduler is the central coordinator for asynchronous
 job execution. For keeping the global system state of all
 running processes, the scheduler persists assigned tasks
 and resources and status information in the IMDB.
 Furthermore, the scheduler supervised the responsive-
 ness of individual compute resources. For example, it
 performs checks of known worker instances using regular
 heartbeat calls [16]. If a predefined response behavior is
 no longer guaranteed, e.g. due to an overloaded compute
 node or a crashed worker process, workers are marked as
 unresponsive. As a result, work-in-progress tasks of the
 unresponsive worker are reassigned to a new worker to
 guarantee their execution and the unresponsive worker is
 scheduled for a restart or the administrator is informed.
 The scheduler derives the concrete tasks for a selected
 Genome Data Processing Pipeline (GDPP) model in-
 stance. Models of GDPPs are modeled in BPMN in
 a graphical workbench. The GDPP model is stored
 in the eXtended Markup Language Process Definition
 Language (XPDL) data format [17].
 Once a concrete GDPP model is instantiated by as-
 signing specific values for all parameters, it is available
 for genome data processing. The scheduler retrieves the
 selected model instance from the IMDB and creates
 corresponding data structures representing the depen-
 dencies between individual process steps. All ready-to-
 process tasks are added to the work list and assigned
 to available worker instances. The scheduler keeps track
 of there execution and retrieves intermediate results
 comparable to the MapReduce approach [18].
 Tasks that can be executed in parallel, e.g. alignment
 of chunks of DNA data, are split into subtasks and dis-
 patched to individual workers. The scheduler combines
 intermediate results of individual subtasks to form the
 final result set. We focused on high-availability while
 designing important system components, e.g. the coor-
 dinator or the web service. Each worker is equipped with
 the code for becoming a scheduler. Once the first worker
 is started, it checks for existing scheduler instances and
 launched a new one if no scheduler is detected. Thus, in
 case of a crash or unavailability of the task scheduler,
 any worker instance can fulfill its task. After launching,
 it performs cleanup, rollback, and roll-forward tasks to
 migrate the system to a consistent database state.
 2) Updater Framework: We consider the use of latest
 international research results as enabler for evidence-
 693
based therapy decision [19]. The updater framework is
 the basis for combining international research results.
 It periodically checks all registered Internet sources,
 such as public FTP servers or web sites, for updated
 and newly added versions of annotations, e.g. database
 exports as dumps or characteristic file formats, such as
 CSV, TSV, and VCF. If the online version is newer than
 the local available version, the new data are automati-
 cally downloaded and imported in the IMDB to extend
 the knowledge base.
 The import of newly available research databases is
 performed as a background job without affecting the
 system’s operation. We import the new data without
 performing any data transformations [20], [21]. As a
 result, new data are available for real-time analysis
 without any delay.
 For example, the following selected research databases
 are supervised regularly by our updater framework:
 National Center for Biotechnology Information (NCBI),
 Sanger’s catalogue of somatic mutations in cancer, Uni-
 versity of California, Santa Cruz (UCSC). [22], [23], [24].
 C. Data Layer
 The data layer holds all required data for performing
 processing and analyzing of genomic data. The data can
 be distinguished in the two categories: master data, and
 transactional data [25]. For example, human reference
 genomes and annotation data are referred to as master
 data, whereas patient-specific NGS data and Electronic
 Medical Records (EMR) are referred to as transactional
 data [26], [27]. Its analysis is the basis for gathering
 specific insights, e.g. individual genetic dispositions, and
 for leveraging personalized treatment decision in course
 of personalized medicine [3].
 The actual step of analyzing the genetic data requires
 answering very specific questions. Thus, the application
 layer consists of specific applications to answer these
 questions. They make use of the platform layer to ini-
 tialize the data processing.
 IV. Benchmarks
 In the following, we share insights about the gathered
 benchmark results with multiple GDPPs processed by
 our HIG research prototype.
 A. Benchmark Setup
 All benchmarks were performed on a compute cluster
 consisting of 25 identical compute nodes. Each of the
 nodes was equipped with four Intel Xeon E7-4870 Cen-
 tral Processing Units (CPUs) running at 2.40GHz clock
 speed, 30MB Intel Smart cache[28], interconnected by
 6.4GT/s Quick Path Interconnect (QPI), and 1TB of
 main memory capacity. Each CPU consisted of 10 phys-
 ical cores and 20 threads running a 64-bit instruction set.
 Exp. Primary Storage Split Size
 I FS 1
 II IMDB 1
 III FS 25
 IV IMDB 25
 Table II: Comparison of benchmark setups using BWA
 in HIG. Split size describes the number of distributed
 compute nodes (Exp. = Experiment).
 All compute nodes were equipped with Intel 520 series
 Solid State Drives (SSDs) of 480GB capacity combined
 using a hardware raid for local file operations [29]. The
 average throughput rate of the local SSDs was measured
 with 7.6GB/s cached reads and 1.4GB/s buffered disk
 reads. All nodes were interconnected via a network
 file system using dedicated 10Gb/s Ethernet links and
 switches to share data between nodes.
 Instead of using generated test data, we only used
 real NGS data, i.e. FASTQ files, from the 1,000 genome
 project for individual measurements [30]. We used the
 FASTQ file of patient HG00251 for our benchmarks.
 It consumed 160GB of disk space, consists of approx.
 63Gbp, approx. 695M reads with 91 bp individual read
 length forming approx. 20x coverage.
 The aim of all conducted benchmarks was to minimize
 the overall execution time for a single GDPP run, i.e. to
 use the maximum available compute power to achieve
 best parallelization.
 B. Performance Key Indicators
 We investigated the following impact factors for con-
 trolling the overall pipeline execution:
 1) Integration: We implemented GDPPs based on
 existing alignment and variant calling tools in our
 system architecture without any modification of
 the established tools,
 2) Adaption:We adapted existing GDPPs to use the
 IMDB as primary storage for data processing, and
 3) Optimization: We optimized the split size pa-
 rameter controlling the number of distributed com-
 pute nodes.
 C. Results
 For our benchmarks, we integrated Burrows Wheeler
 Aligner (BWA) in our HIG research prototype. We
 designed our benchmarks to compare the impact of the
 incorporated storage and the level of parallelization on
 the overall execution time as outlined in Tab. II. Exp. I
 and III used a FS as primary storage while an IMDB
 was used for Exp. II and IV. Exp. I and II were executed
 on a single compute node, while Exp. III and IV were
 executed on 25 compute nodes to evaluate the impact of
 a fully parallelized execution environment.
 694
Size [Gbp] tpIq [s] tpIIq [s] tpIIIq [s] tpIV q [s] ImppIIq [%] ImppIIIq [%] ImppIV q [%]
 0.5 1,100 808 283 130 27 74 88
 1.0 2,159 1,622 520 245 25 76 89
 2.0 3,900 2,860 943 470 27 76 88
 4.0 7,029 5,259 1,761 893 25 75 87
 7.9 13,626 10,364 3,377 1,733 24 75 87
 15.8 25,147 18,707 6,609 3,387 26 74 87
 Table I: Comparison of execution times in HIG for I) BWA, split size 1, II) the adapted pipeline using the IMDB
 as primary data storage instead of the FS, split size 1, III) split size 25, and IV) for the adapted pipeline using the
 IMDB as primary data storage, split size 25. tpxq = Execution time for Exp. x , Imppxq = Improvement of execution
 time for Exp. x compared to Exp. I calculated as Imppxq “ tpIq´tpxqtpIq (Exp. = Experiment).
  0
  5000
  10000
  15000
  20000
  25000
  30000
  0  2  4  6  8  10  12  14  16
 Pipeline Execution Time [s
 ]
 FASTQ File Size [Gbps]
 Exp. I
 Exp. II
 Exp. III
 Exp. IV
 Figure 3: Exp. II shows improvements of 24-27% on
 single compute node compared to Exp. I. Exp. III and
 IV show improvements up 76% and 89% resp. on 25
 compute nodes compared to Exp. I.
 The overall pipeline execution for varying FASTQ file
 sizes was measured. BWA version 0.6.2 was configured
 to use 80 threads to use the fully available hardware
 resources of our benchmark systems [31].
 Exp. I and II describe the overall pipeline execution
 time on a single compute node as shown in Tab. I. It
 depicts that the use of the IMDB as primary storage
 for intermediate results is for all file sizes beneficial.
 Through this pipeline optimization the runtime can be
 reduced by at least 25 percent in average.
 Exp. III and IV as shown in Tab. I document the
 impact the tuning parameter split size, i.e. the parallel
 execution of individual process steps, on the overall
 pipeline execution time. Parallel execution of selected
 pipeline steps reduces the overall execution time by
 at least 74 percent in average for BWA. Additional
 improvement can be achieved by using the IMDB as
 primary storage. Thus, the overall execution time can
 by reduced by at least 87 percent in average.
 V. Evaluation and Discussion
 Our benchmarks verify two hypothesis: Firstly, the
 use of an IMDB as primary storage improves the overall
 execution time of established alignment algorithms, such
 as BWA, integrated in our HIG system architecture
 as depicted in Fig. 3. Secondly, our system supports
 parallel execution of intermediate process steps across
 multiple compute nodes, which results in an additional
 performance improvement compared to the execution
 on a single compute node. The result incorporating 25
 compute nodes show that the system was not completely
 loaded by our benchmarks since we did not adapt in-
 dividual processing tools, which still implement single
 threaded execution models.
 The best relative improvement shows the adapted
 pipeline using the IMDB as primary storage with at
 least 74 percent on single compute node and up to
 89 percent on 25 compute nodes. It shows that the
 overall pipeline execution time correlates to the number
 of base pairs contained in the FASTQ file in a linear
 way. The scaling factors for the overall execution time
 varies between 1.80 and 1.96 across all experiments and
 file sizes. This indicates a very constant and predictable
 system behavior of our HIG system for varying input file
 size. For example, a doubled number of reads results in
 an overall response slightly below a factor of two. It helps
 to predict execution times and to supervise the correct
 system functionality, e.g. in case of a broken compute
 resource or worker process as outlined in Sect. III.
 Furthermore, our results stress the benefits of an
 IMDB for operating on intermediate results. The
 pipeline optimized for the IMDB replaces individual
 tools operating on files for specific process steps, such
 as sorting, merging, and indexing. In contrast, these
 operations are replaced by the IMDB operations without
 the need to create intermediate files in the File System
 (FS). We integrated existing alignment and variant call-
 ing tools into our HIG architecture without modifying
 their code. Thus, the speed-up documented in our bench-
 marks is mainly achieved by replacing selected file-based
 operations by IMDB operations.
 695
VI. Conclusion and Outlook
 In the given work, we presented our HIG system
 architecture for genome data processing based on an
 IMDB system. Based on our in-memory research ac-
 tivities, we outlined the applicability of this technology
 for processing of genome data to enable its real-time
 analysis. Furthermore, we shared insights in the specific
 architecture layers from an IT perspective and outlined
 details about select IMDB extensions for genome data
 processing, such as scheduling, worker framework or
 updater framework.
 The obtained benchmark results showed that our HIG
 system architecture improves overall pipeline execution
 time by at least 25 percent on a single compute node
 and up to 89 percent involving 25 compute nodes. The
 performance boost is mainly derived from substituting
 intermediate processes, such as sorting, merging, and
 indexing by native IMDB operations.
 In future, we will investigate the impact of optimized
 alignment and variant calling algorithms that directly
 incorporate IMDB technology. We expect to further
 eliminate media breaks and to improve performance due
 to data proximity.
 References
 [1] F. S. Collins et al., “New Goals for the U.S. Human
 Genome Project,” Science, vol. 282, no. 5389, pp. 682–
 689, 1998.
 [2] W. J. Ansorge, “Next-generation DNA Sequencing Tech-
 niques,” New Biotechn, vol. 25, no. 4, pp. 195–203, 2009.
 [3] K. Jain, Textbook of Pers. Medicine. Springer, 2009.
 [4] M.-P. Schapranow et al., “Mobile Real-time Analysis of
 Patient Data for Advanced Decision Support in Person-
 alized Medicine,” in Proceedings of the 5th Int’l Conf. on
 eHealth, Telemedicine, and Social Medicine, 2013.
 [5] M.-P. Schapranow, In-Memory Technology Enables
 History-Based Access Control for RFID-Aided Supply
 Chains. Springer London, 2013, ch. 9, pp. 187–213.
 [6] H. Plattner, A Course in In-Memory Data Manage-
 ment: The Inner Mechanics of In-Memory Databases.
 Springer, 2013.
 [7] A. Knöpfel, B. Grone, and P. Tabeling, Fundamental
 Modeling Concepts: Effective Communication of IT Sys-
 tems. John Wiley & Sons, 2006.
 [8] S. Wandelt et al., “Data Management Challenges
 in Next Generation Sequencing,” Datenbank-Spektrum,
 vol. 12, no. 3, pp. 161–171, 2012.
 [9] S. Pabinger et al., “A Survey of Tools for Variant Analy-
 sis of Next-generation Genome Sequencing Data,” Brief.
 Bioinform, Jan. 2013.
 [10] K. Wang, M. Li, and H. Hakonarson, “ANNOVAR:
 Functional Annotation of Genetic Variants from High-
 throughput Sequencing Data,” Nucleic Acids Res,
 vol. 38, no. 16, 2010.
 [11] V. Makarov, T. O’Grady, G. Cai, J. Lihm, J. D.
 Buxbaum, and S. Yoon, “Anntools,” Bioinformatics,
 vol. 28, no. 5, pp. 724–725, Mar. 2012.
 1All online references were checked on Aug 23, 2013.
 [12] P. Cingolani, A. Platts, M. Coon, T. Nguyen, L. Wang,
 S. Land, X. Lu, and D. Ruden, “A Program for Anno-
 tating and Predicting the Effects of Single Nucleotide
 Polymorphisms,” Fly, vol. 6, no. 2, pp. 80–92, 2012.
 [13] A. T. Holdener, AJAX: The Definitive Guide, 1st ed.
 O’Reilly, 2008.
 [14] D. Crockford, “RFC4627: The application/json Media
 Type for JavaScript Object Notation (JSON),” http://
 www.ietf.org/rfc/rfc4627.txt1, July 2006.
 [15] A. S. Tanenbaum, Modern Operating Systems, 3rd ed.
 Upper Saddle River, NJ, USA: Prentice Hall Press, 2008.
 [16] L. Perkov et al., “High-Availability using Open Source
 Software,” in Proceedings of the 34th Int’l Convention
 MIPRO. IEEE, 2011, pp. 167–170.
 [17] The Workflow Mgmt Coalition, “Process Definition In-
 terface: XML Process Definition Language,” http://
 www.xpdl.org/standards/xpdl-2.2/XPDL6,20131 2012.
 [18] J. Dean and S. Ghemawat, “MapReduce: Simplified
 Data Processing on Large Clusters,” in Proceedings of
 the 6th Symposim on Operating Systems Design and
 Implementation, 2004, pp. 137–150.
 [19] M.-P. Schapranow, H. Plattner, and C. Meinel, “Applied
 In-Memory Technology for High-Throughput Genome
 Data Processing and Real-time Analysis,” in Upgrade
 Devices in Tele Medicine, 2013, pp. 35–42.
 [20] A. Bog, K. Sachs, and H. Plattner, “Interactive Per-
 formance Monitoring of a Composite OLTP and OLAP
 Workload,” in Proceedings of the Int’l Conf on Mgmt of
 Data 2012. ACM, 2012, pp. 645–648.
 [21] F. Färber, S. K. Cha, J. Primsch, C. Bornhövd, S. Sigg,
 and W. Lehner, “SAP HANA Database: Data Man-
 agement for Modern Business Applications,” SIGMOD
 Rec., vol. 40, no. 4, pp. 45–51, Jan. 2012.
 [22] National Center for Biotechnology Information, “All
 resources,” http://www.ncbi.nlm.nih.gov/guide/all/1.
 [23] S. Forbes et al., “The Catalogue of Somatic Mutations in
 Cancer: A Resource to Investigate Acquired Mutations
 in Human Cancer.” Nucleic Acids Res, vol. 38, 2010.
 [24] L. R. Meyer et al., “The UCSC Genome Browser
 Database: Extensions and Updates 2013,” Nucleic Acids
 Res, 2012.
 [25] T. K. Das and M. R. Mishra, “A Study on Challenges
 and Opportunities in Master Data Management,” Int’l
 Journal of Database Mgmt Syst, vol. 3, no. 2, May 2011.
 [26] The Genome Reference Consortium, “Genome Assem-
 blies,” http://www.ncbi.nlm.nih.gov/projects/genome/
 assembly/grc/data.shtml1.
 [27] A. Rector, W. Nolan, and S. Kay, “Foundations for an
 Electronic Medical Record,” Methods of Information in
 Medicine, pp. 179–186, 1991.
 [28] Intel Corporation, “Intel Product Quick Reference
 Matrix,” http://cache-www.intel.com/cd/00/00/47/
 64/476434_476434.pdf1, Apr 2011.
 [29] ——, “Intel Solid-State Drive 520 Series Product
 Specification,” http://www.intel.com/content/dam/
 www/public/us/en/documents/product-specifications/
 ssd-520-specification.pdf1, Feb 2012.
 [30] The 1000 Genomes Project Cons., “A Map of Human
 Genome Variation from Population-scale Sequencing,”
 Nature, vol. 467, no. 7319, pp. 1061—1073, Oct. 2010.
 [31] H. Li and R. Durbin, “Fast and Accurate Short
 Read Alignment with Burrows-Wheeler Transforma-
 tion,” Bioinformatics, vol. 25, pp. 1754–1760, 2009.
 696

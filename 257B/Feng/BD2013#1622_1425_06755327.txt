Managing and Analysing Big Audio Data for Environmental Monitoring 
 
Jinglan Zhang, Kai Huang, Mark Cottman-Fields, Anthony Truskinger,   Paul Roe, Shufei Duan, Xueyan Dong, 
Michael Towsey, Jason Wimmer 
School of Electrical Engineering and Computer Science 
Queensland University of Technology 
Brisbane, Australia 
contact author: jinglan.zhang@qut.edu.au 
 
 
Abstract—Environmental monitoring is becoming critical as 
human activity and climate change place greater pressures on 
biodiversity, leading to an increasing need for data to make 
informed decisions. Acoustic sensors can help collect data 
across large areas for extended periods making them attractive 
in environmental monitoring.  However, managing and 
analysing large volumes of environmental acoustic data is a 
great challenge and is consequently hindering the effective 
utilization of the big dataset collected.  This paper presents an 
overview of our current techniques for collecting, storing and 
analysing large volumes of acoustic data efficiently, accurately, 
and cost-effectively. 
Keywords- acoustic sensing; acoustic data analysis; big data 
management and processing; environmental monitoring; 
visualization; citizen science; eScience. 
I.  INTRODUCTION 
Climate change and human activity are subjecting the 
environment to unprecedented rates of change. Large scale 
environmental monitoring to manage environmental change 
is becoming critical. Acoustic sensing is an effective tool 
for monitoring species that have regular and predictable 
vocalisations such as birds, and offers an enduring record to 
allow for future interpretation of results [1].  
In order to provide insights into ecosystem dynamics, the 
Bioacoustics Research Group collects acoustic data for a 
range of projects, including investigating rare birds, koala 
behaviour, and monitoring the environment of Samford 
Valley's (located 20 kilometres northwest of Brisbane). 
The collection of data is very important.  The ability of 
analyzing and using the data effectively is even more 
important.   The group aims to provide helpful tools for 
ecologists, focusing on automated, semi-automated and 
manual faunal acoustic event detection and identification. 
The tools provide information and computational 
technologies for all aspects of acoustic analysis of data 
collected through environmental monitoring.  
While acoustic sensors are attractive as autonomous data 
collectors, there are significant issues associated with large-
 scale data manipulation and analysis.   
An acoustic sensor can generate several gigabytes of 
compressed data per day. Long term recordings to monitor 
species richness or changes in species composition in large 
areas can generate large volumes of data. These large 
quantities of raw acoustic data must be stored, analysed and 
summarised. However, the raw audio data recorded from 
fields directly is difficult to analyse due to various levels of 
background noise, variation in species vocalisations and 
overlapping vocalisations. Therefore, the analysis of 
acoustic sensor data for environmental monitoring is a 
complex and difficult task. 
This research seeks to address the analysis of large 
volumes of acoustic sensor data by humans or computer 
programs or a combination of both. The main purpose is to 
ensure the reliable, efficient and rapid recognition of 
acoustic events in audio recorded directly from the field. 
This paper presents an overview of our current techniques 
for analysing large volumes of acoustic data. 
There are other research projects around the world which 
are related to automating environmental monitoring using 
acoustic and other sensors.  Representative research 
includes Stuart Gage’s Remote Environmental Assessment 
Laboratory (REAL) 1 .  The REAL project focuses on 
soundscape interpretation and ecosystem dynamics at high 
level over a long period.  The AMIBIO project 2 focuses on 
automatic acoustic biodiversity monitoring and the analysis 
of acquired data.   Other researchers in the area of 
ecological research via bioacoustics include Jerome Seuer 
3  
 who studies animal diversity at both species and 
community levels, and Stuart Parson 4  who focuses on 
animal behaviour. Our research focuses on terrestrial 
bioacoustic monitoring. The other projects described here 
are conducted mainly by ecologists, whereas we focus on 
developing tools to support ecologists to conduct research.  
The remainder of this paper is organised as follows. 
Section 2 outlines the data collection approaches employed 
in our system and their potential applications. Section 3 
discusses the nature of the dataset, strategies for managing 
the large volume of audio data and the framework for data 
management. Section 4 describes requirements, challenges 
and techniques for analysing big environmental data. 
Section 5 presents different data visualization techniques 
employed in the system.  Finally Section 6 concludes the 
paper and discusses future work. 
                                                           
1
  http://www.real.msu.edu/    
2
  http://www.amibio-project.eu/ 
3
  http://sueur.jerome.perso.neuf.fr/ 
4
  http://www.eab.auckland.ac.nz/parsons.html 
2013 IEEE 16th International Conference on Computational Science and Engineering
 978-0-7695-5096-1/13 $31.00 © 2013 IEEE
 DOI 10.1109/CSE.2013.146
 997
II. DATA COLLECTION AND UPLOAD 
Sound is collected by two means: acoustic sensor 
networks and recorders. We are also developing a mobile 
app to allow users to record and upload audio data using 
their mobile phones [2].  The website 
(http://sensor.mquter.qut.edu.au/) provides an interface for 
users to access the acoustic data, annotate sounds of interest, 
and perform various types of analysis. This portal provides 
the following core functionality: 
• Data upload and storage. 
• Data organisation and structuring 
• Audio recording playback and visualisation 
• Data analysis and annotation. 
We describe different data collection approaches below.  
A. Networked Sensors for Near Real-Time Sensing 
These devices provide a near real-time sensing in 
locations with 3G connectivity. Due to bandwidth constraints 
and 3G transmission costs, they are configured to activate at 
regular intervals (typically two to four minutes at half hourly 
intervals), upload data to a central repository and then 
deactivate until the next scheduled recording. These 
networked sensors consist of a 3G smart phone (HTC), an 
electret-style external microphone, pre-amplifier, DC-DC 
converter and a solar panel as an external power supply. The 
system is capable of operating continuously and 
autonomously for months at a time with minimal 
maintenance. Uninterrupted, continuous deployments of 18 
months have been achieved to date. Recordings are sampled 
at 22,050 Hz (or subsequently down-sampled to this value) 
and at a bit rate of 16. 
B. Audio Recorders for Continuous Sensing 
Audio recorders are designed to provide a short term or 
ad hoc high resolution acoustic sensing capability but can be 
configured to record continuously for extended periods. 
Recorded acoustic data is stored internally and the device 
can record continuously for around 2 weeks. These sensors 
are highly portable and powered by batteries for ad hoc 
deployments or externally using a solar power supply for 
fixed deployments. They are self-contained in an all-weather 
container. All recordings so far were obtained from various 
locations around Australia. The recorders returned 
recordings of up to a day in duration.  
C. Handheld Devices for Ad-hoc Sensing  
We are also developing a mobile application to allow 
users to use mobile phones to record audio data of interest 
and upload them into the central repository. These recordings 
are typically short (maximum a few minutes) and people can 
record them whenever they hear interesting things. This 
mobile app can be used by general citizens to collect audio 
data for research purposes.  The users will also be able to 
write brief descriptions about the audio data, view the 
spectrogram or waveform of the audio and mark and 
comment on relevant events. For example, if the audio 
recorded contains a specific bird call, the user can submit the 
audio file to our website along with a description and ask the 
system to suggest the type of bird it is or if there are there 
any similar calls in the system. 
III. STORAGE AND MANAGEMENT 
Acoustic sensing provides ecologists with the capability 
to massively scale ecological observations, both temporally 
and spatially. At long term scales, even scheduled recordings 
(e.g. five minute recordings every 30 minutes) provide 
ecologists with significantly more data than manually 
collected long term survey data. 
A. Characteristics of the dataset 
Sensors provide an effective means to accumulate data at 
large scales and high resolutions. Acoustic sensors have been 
employed in this capacity for some time, both in marine and 
terrestrial environments [4]. They can be used to collect data 
passively, objectively and continuously across large areas for 
extended periods of time. While these factors make acoustic 
sensors attractive as autonomous data collectors, large scale 
data collection presents its own problems: 
• Data volume – acoustic sensors generate vast 
quantities of raw audio data which must be stored, 
analysed and summarized. 
• Data variety – different types of sensors are used in 
different cases.  Different data formats and different 
resolutions have to be accommodated.  Even 
recordings from the same sensor vary a lot depending 
on the distance between the sound source and the 
microphone as well as the environment e.g. an open 
space or a dense bush area.    
• Data velocity – For long term environmental 
monitoring, data will be collected continuously 
therefore the data volume will keep increasing.  The 
speed of increase depends on how many sensors are 
used and the corresponding recording schedule e.g. 
continuous recording or periodical recording.  
As an indication of the volume and velocity of data 
produced by a single acoustic sensor, recording in MP3 
format at 44.1 kHz, 128 kbps for 24 hours generates 1.3GB 
of data [3]. Our repository currently holds over 24TB of 
acoustic data and keeps growing. To monitor larger areas 
with more sensors in the future, the volume produced and 
velocity of production will grow rapidly.  
B. Data storage  
Acoustic data are structured on a hierarchical model of 
Projects, Sites and Recordings. A project can represent any 
logical collection of experiments or studies and specifies 
users’ level of access. Each project consists of a collection of 
Sites. Sites are physical locations with sensors deployed at 
each site. Sensors details are stored to ensure retention of 
experimental design details. Recordings are the raw acoustic 
data files collected from sensor devices in the field and 
uploaded to the website.  
998
Data is stored on a central server: raw sound is stored in a 
file system and sound metadata, tags and other data are 
stored in a database. In this way, large sound files can be 
stored efficiently in a regular file system: currently this is 
hosted by a dedicated file server. The simple file based 
interface lends itself to more sophisticated hosting for 
example by portioning across a set of file servers or even a 
cloud based solution. Long recordings are split into fixed-
 length segments with configurable lengths. For example, a 
continuous 24-hour recording can be divided into 240 six-
 minute segments. This allows the user to start listening at a 
particular time offset without requiring the entire 24-hour 
recording.  On the other hand metadata is stored in a 
relational database, permitting the efficient querying and 
selection of meta-data. This is important to enable fast and 
efficient navigation to data on the web server. It also permits 
complex data sets to be formed through relational joining of 
metadata. 
C. Query support 
The system currently supports four types of queries: 
spatial, temporal, annotation, and project. These query types 
will be expanded to support more flexible and complex 
queries in the future. Spatial queries return all recordings 
gathered from a specific location such as the Samford 
research facility. Temporal queries return all recordings 
gathered between time ranges (e.g. 11:00am  and 3:00pm) or 
date ranges (e.g. 2009-10-01 to 2009-10-14). Annotation 
based queries return all recordings with certain tags or 
frequency and duration bounds. Project-based queries return 
all recordings associated with a project.  We are currently 
developing techniques for Query-by-example, that is, 
content-based audio searching which we call Find Event 
Like This.  
D. Data management framework 
We have designed and partially implemented a multi-
 layer data management framework shown in Fig. 1. This 
system includes four layers: 1) data collection layer for 
collecting and uploading data, 2) data management layer for 
storing and accessing data, 3) event processing layer for 
extracting useful events, and 4) data mining layer for 
correlation analysis and knowledge discovery.  
There is a user interface for each layer.  In the data 
collection layer, uses can upload data from recorders e.g. 
Song Meters or mobile sensors either manually or 
programmatically.  In the data management layer, users can 
access the raw audio data directly, listen to them, or search 
them based on some meta data such as time or location.  In 
the event processing layer, users can detect events using 
some automated tools or manually marquee events on the 
audio display directly or annotate some events with the help 
of machine recommendations. In the data mining layer, users 
can use customized tools or commercial software to analyse 
the events together with related meta data and conduct 
statistical analysis, correlation analysis, trend analysis etc.   
 
 
Figure 1.  Multi-layer data management for environmental monitoring via 
Acoustics   
IV. ENVIRONMENTAL AUDIO DATA ANALYSIS 
Detecting and identifying individual species’ 
vocalisations against a background of general noise and other 
vocalisations is a complex and challenging task. 
First, we work with raw environmental audio, 
contaminated by noise and artefacts, and containing calls that 
vary greatly in volume and clarity depending on the animal’s 
proximity to the microphone and the surrounding 
environment. Second, initial experimentation suggested that 
no single recogniser could deal with the enormous variety of 
calls. Therefore, we developed a toolbox [5] of generic 
recognisers to extract invariant features for each call type. 
The automated recognition of animal calls has not yet 
reached a level of reliability that allows ecologists to use the 
methods without careful verification of results. It makes 
sense to offer semi-automated tools which allow an 
adjustable degree of user interaction with the data. The 
identification of animal calls in arbitrary recordings of the 
environment remains a difficult task. We believe that it is 
most unlikely that automated generic recognition of animal 
vocalisations with high accuracy and consistency will be 
achieved in the near future. Consequently semi-automated 
methods involving algorithms and people will be required 
for accurate and consistent analysis of environmental 
acoustic data for the foreseeable future. Therefore, our group 
is researching approaches to audio analysis with both human 
computation and automatic detection.  
This research has led to the development of species 
specific recognisers that have influenced a number of real-
 world applications, which include invasive (cane toad and 
asian house gecko) and endangered (Koala, Lewin's Rail) 
species monitoring. In addition, species richness studies 
based on random sampling of acoustic sensor data have been 
demonstrated to be comparable to traditional survey 
techniques, such as bird point count surveys [4]. 
999
A. Environmental Data Analysis Requirements and 
Challenges 
Although we have used sensors to a large collection of 
data, analysing the huge dataset has posed a great challenge.  
 
Data Analysis Requirements for environmental 
monitoring  
 
Any application that offers analysis tools to ecologists 
must necessarily offer graded levels of utility from fully 
manual to fully automated.  Traditionally acoustics is used 
by scientists to study a particular species, especially birds, 
frogs, insects, and some mammals [1]. Bioacoustic 
monitoring and powerful analysis of big data set have 
enormous potential to help ecologists scale fauna surveys 
and enable new form of research. Typical ecological 
questions to be answered using the results of sound analysis 
include the followings: 
• Species presence/absence e.g. are there any koalas? 
• Species abundance e.g. how many koalas there? 
• Species richness/diversity e.g. how many different 
species of birds there? 
• Behavioural studies e.g. how is a pest species spreading? 
What is the relationship between behaviour and 
environment?  
• Habitat disturbance e.g. how is land use change affecting 
the habitat of koalas? 
• Environmental health measurement e.g. what value does 
a particular environmental bank have?  
Apart from the general short computation time and high 
accuracy requirements, one more constraint on the data 
analysis tools is that they must be easy to use and flexible to 
accommodate scientists’ needs.  
 
Challenges in environmental audio data analysis 
 
Two main issues with environmental data are noise and 
variability. Other issues include unpredictable, dynamic, and 
overlapping etc.  
• Low Signal-to-Noise-Ratio (SNR). Most approaches on 
this study are based on analysis of data collected in the 
lab, which means they can depress the noise from other 
sources so that their data has high SNR. Different from 
their data, our real world data tend to comprise of 
mixture of sounds from various sources, which has quite 
low SNR. Environmental acoustics contain a wide 
variety of non biological noises such as traffic noise 
having a great range of intensities and a variety of 
animal sounds that have nothing to do with the task at 
hand.  
• Large variation of calls between different bird species, 
regions and environments. There are three cases causing 
variations. The first is distance variation. Birds may fly 
far away from or close to recorders, resulting in different 
appearance of bird calls in the spectrogram. The second 
is Local Call Variations. The same type of birds can 
have a broad range of vocalisations and these 
vocalisations may have significant regional variation.  
Even in some special case, some birds mimic sounds of 
other birds so as to bring misinformation. The third one 
is environmental variations. The environmental noise 
including songs from wind, rain and human activity can 
have great changes over time. Also environmental 
factors such as wind, rain, vegetation and topography 
can attenuate, muffle and distort vocalisations 
considerably. 
• Dynamic and unpredictable. In many cases, there is no 
certain sequential pattern in environmental sound as the 
presence/absence of certain birds or insects etc can be 
random. For example, in urban environments traffic 
noises, air-conditioners, human speech, dogs barking 
and airplanes can interfere. This brings a challenge in 
detecting target calls from background noise in an 
automated way. 
• Overlapping. A large number of acoustic events overlap 
with each other at dawn chorus making it very difficult 
to recognise individual bird calls.   
To sum up, environmental sounds are noisy, unstructured, 
dynamic and unpredictable.  It is very difficult to build 
models for environmental sounds, causing problems for 
environmental sound analysis. 
B. Manual Processing Approach (by Experts and Citizen 
Scientists) 
The simplest approach of analysing the audio data 
collected by sensors is to make use of citizen scientists. 
These keen enthusiasts are often interested in listening 
through the audio data to catalogue the species heard within. 
However, this process, is time consuming and inefficient. 
The manual approach involves an audio player and some 
way to take notes. 
Additionally, many of the citizen volunteers have 
considerable expertise in identifying fauna by their 
vocalisations. However, not all of the participants that 
contribute are experts and many need help. Even for our 
expert volunteers, they are often only experts for one 
ecological region – outside their geographic area of 
expertise, their identification capabilities diminish. 
C. Automated Approach 
Automated animal call recognition has the potential to 
dramatically improve the ability of ecologists to monitor the 
environment on large spatial and temporal scales [11, 12]. 
There are mainly two types of recognisers and they both use 
lots of parameters: species specific recognisers (e.g. Koala 
recogniser), and generalised recognisers (e.g. “Find Event 
Like This” search in databases). The approach taken by most 
call recognition methods to date is to construct a detailed 
representation of each call of interest. This of course requires 
detailed prior knowledge based on the collection of a large 
number of representative calls. A general recogniser is much 
1000
harder to develop as it is very difficult to define a general 
feature set that can be used to recognise all bird calls.  
The majority of our automated analysis tools are based 
on the analysis the Time-frequency spectrogram, which is a 
popular way of representing sound.  This representation suits 
well for both manual labelling of events on the spectrum and 
automated analysis.  Chang-Hsing Lee also proposed a new 
feature descriptor [15] that uses image shape features to 
identify bird species, which is based on the recognition of 
gray-level birdsong spectrograms. Another typical 
representation of sound is waveform, which is in the time 
domain.   
The raw data we collected (sound recordings in 
waveforms) is first processed by noise reduction using signal 
processing techniques, which is followed by transforming 
the waveform into a spectrogram prepared using the Short-
 Time Fourier Transform (STFT). 
The majority of our call recognition algorithms employ 
features derived from spectrograms [5, 7]. The signal is 
framed using a window of 512 samples (23.2ms) which 
offers a reasonable compromise between time and frequency 
resolution. All the automated, semi-automated recognizing 
and binary template matching are based on the time-
 frequency spectrum. 
Representative examples of recognition techniques we 
have implemented include [13, 14]: 
MFCC features + HMMs: We have found this technique 
to be suitable only for high quality single-syllable calls. We 
used the Hidden Markov Model Tool Kit (HTK) [24] and 
applied it to the recognition of Pied Currawong (Strepera 
graculina) calls. 
Acoustic Event Detection (AED): We developed AED to 
extract information about the distribution of acoustic energy 
in a spectrogram using the minimum of prior knowledge. 
AED processes the spectrogram like an image and returns 
rectangular marquees around isolated acoustic events. 
Oscillation Detection (OD): We used a Discrete Cosine 
Transform to find repeating or oscillating elements of calls 
within a user specified bandwidth. 
Event Pattern Recognition (EPR): This technique extracts 
acoustic events from the spectrogram, and then detects a 2D 
pattern of events whose distribution matches a template. 
Acoustic Component Detection: We define generic 
acoustic components in the spectrogram such as lines 
(usually with angles), oscillations, stacked harmonics, and 
others. A set of acoustic components which are distinguished 
from each other are built. Feature vectors of different 
acoustic components are extracted or defined in order to 
develop detectors for each component.  
Syntactic Pattern Recognition (SPR): this technique 
models a call as a symbol sequence, each symbol selected 
from a finite alphabet representing ‘primitive’ elements of 
the composite pattern. Seven broad types of ‘primitive’ 
elements have been defined to construct various animal calls 
[7].  
Similarity Search (SS): In the process of SS, an arbitrary 
acoustic event can be provided, and our algorithm will 
automatically scan through the continuous recording to find 
the similar events. In this research, we have identified a new 
type of features which can be used for similar search in 
environmental audio dataset.  The initial experimental result 
with a small dataset is very promising. We are trying to do 
more experiment on a relatively larger dataset.   
The techniques of automated recognition of animal calls 
are improving over the time.  However, the average 
recognition accuracy is still very low.  These techniques have 
not yet reached a level of reliability that allows ecologists to 
use the methods without careful verification of results.  
D. Semi-Automated Approach 
Manual analysis utilises the sophisticated recognition 
capabilities of a human user, but cannot be efficiently scaled 
to process the volumes of data collected in long-term sensor 
deployments. Automated techniques are effective for 
identifying some targeted species in large volumes of data, 
but they require a high degree of skill to develop and are 
currently not able to cope with the variability found in 
animal calls. 
Combining manual and automated approaches provides 
users with the ability to analyse large volumes of acoustic 
data interactively and systematically. Semi-automated 
analysis takes advantage of the fact that human analysis 
capabilities are currently superior to that of automated 
computational analysis tools. 
To deal with these challenges, our research has identified 
the need to provide participants with flexibility when 
analysing acoustic data. Using technology to assist a manual 
process helps increase the efficiency of our citizen scientists. 
As more concepts are added, we automate more of the 
manual process – this is the spectrum of semi-automated 
analysis. This spectrum of semi-automated annotation 
techniques include any form of digital assistance, be it with 
tools, user interfaces, or collaboration techniques. The 
automation components may be small initially, but as we add 
more and more techniques, the system will be less manual 
and more auto. The semi-automated analysis is a spectrum 
and heading towards more and more automated. 
We continue to investigate the role of citizen scientists in 
analysing large volumes of environmental acoustic data in 
order to identify bird species. Specifically, we investigate 
ways in which the efficiency of a user can be improved 
through the use of species identification suggestion tools, 
guided virtual bird walks, the addition of contextual 
information, and the use of reputation models to predict the 
accuracy of users with unknown skill levels. The aim is to 
combine these tools together to allow large volumes of data 
to be processed effectively by participants with expertise in 
varying environments and species [8-10]. 
 Further to this goal, we have researched the following 
forms of assistance that are designed to help our participants 
when they are analysing acoustic data:  
Online collaboration [10]: enables users to scale manual 
analysis by allowing multiple users to collaborate on 
identification and annotation issues. Collaboration, through 
discussion, is also used as a verification mechanism for 
uncertain identifications. 
Online species identification library: To reduce the time 
taken to identify vocalisations and to improve the 
1001
consistency of novice users, the online species identification 
library can enable comparisons of a call in a spectrogram 
with spectrograms of previously identified species 
vocalisation exemplars. 
Suggestion Tool: An extension the species identification 
library is the suggestion tool. Deigned to work while a 
participant is actively annotating, this tool acts like an auto-
 complete-box would; it offers real-time suggestions of what 
acoustic event a participant may be actively annotating [8].  
Rapid spectrogram scanning: When analysing acoustic 
data there are two problems: identification and classification. 
Some vocalisations have a unique and characteristic 
appearance that people can recognise amongst other 
vocalisations even in complex acoustic environments. Since 
we use human participants that are shown spectrograms, it is 
possible take advantage of the advanced recognition 
capabilities of the human visual cortex. For vocalisations that 
have a unique or pronounced appearance in a spectrogram, it 
is possible to significantly increase the speed at which our 
human participants can process these spectrograms. This is 
achieved by disabling audio playback, and flashing 
successive spectrograms past participants at high speeds. We 
have achieved as much as 12x speedup for time taken to 
identify a species in a section of audio [7]. 
Tag validation, correction, and linking: We choose to use 
a folksonomy tagging system to label our annotations. The 
tags are descriptive and free form, however, as is the case 
with most systems that have human components, errors are 
generated. We have done work to automatically correct, 
clean, and link our folksonomic tags to taxonomic data 
structures [6]. Ensuring the tags on our annotations stay 
clean ensure our core data output for ecologists is not 
affected by human errors. 
Another vein of research includes moving our participant 
analysis tools to mobile phone. Given that our interactive 
annotation editor is already web-based and the increasing 
capability of browsers on mobile devices, this area looks 
promising. 
E. Data Mining for Knowledge Discovery 
By using human computation approach,  ecologists and 
birders have helped us annotate the audio data and get the 
initial set of event tags, which are textual descriptions of 
some sound events e.g. a bird call. However, the knowledge 
embedded in these annotations has not been fully discovered 
yet. We have attempted to use data mining techniques to 
analyse these annotated environmental sounds considering 
species, location and time and try to discover knowledge 
embedded in the dataset.  For the initial experiment, we have 
used a commercial data mining software called SAS.  
By analysing the large amount of human generated tags 
and correlate them with time and location, we have found the 
distribution of animal calls and some useful temporal and 
spatial relationships in the dataset collected.  Fig. 2 shows 
the relationship between the number of calls made (Y-axis) 
and different bird species (X-axis).  It demonstrates that the 
majority of bird calls recorded are made by a small amount 
of bird species and some species rarely call (vocally cryptic).  
This finding is consistent with the existing ecological 
knowledge.  This longtail phenomenon also inspires further 
research into finding cryptic calls in order to conduct 
biodiversity analysis through audio analysis. Other findings 
such as most birds do not call during heavy rain are also 
consistent with common knowledge.   
  
 
Figure 2.  Distribution of bird calls and species  
 
V. DATA VISUALISATION  
Data visualisation is the visual representation of data 
while the main goal is to communicate information clearly 
and effectively through graphical means. Data visualisation 
provides insights into a rather complex data set by 
communicating its key-aspects in a more intuitive way. 
To convey ideas effectively, we use different data 
visualisation approaches at different processing stages. 
A. Time-frequency spectrogram 
The raw data we collected (sound recordings in 
waveforms) is first processed by noise reduction using signal 
processing techniques, which is followed by transforming 
the waveform into a spectrogram prepared using the Short-
 Time Fourier Transform (STFT) (Fig 3.). 
 
Figure 3.  An example spectrogram with annotations shown 
The spectrogram is very useful for manual labelling and 
fast scanning of acoustic events. With the display of a 
spectrogram, while listening to the sound the user can 
marquee the call of interest in a noise reduced spectrogram 
and enters any textual comments about the marked event and 
corresponding sound. This forms the basis of our manual 
acoustic analysis approach. Fig.3 shows an example grey-
 level spectrogram with annotations of some events. The 
spectrograms can also have true colours which represent the 
power of the sound. The spectrogram is very intuitive to 
show some unique acoustic events such as rain because the 
sound of rain looks like rain on the spectrogram (many 
vertical lines) and many bird, insect or mammal calls have 
their own unique visual structures in the spectrogram.  
1002
B. Tag linking 
When presenting species information e.g. bird types to 
users, it is ideal to link together the specific audio file, tags 
which are normally common names of birds appearing in the 
audio, scientific names, pictures of the birds, and other useful 
information.  Fig. 4 shows the concise representation of such 
linked information in our system.  
The initial tags are generated by experienced bird 
watchers.  With the tags normalized, cleaned, and mapped to 
species names, the data is ready to be mapped to a formal 
taxonomic structure. 
The Atlas of Living Australia (ALA) is an online 
resource that provides information on faunal species in 
Australia. This information can be accessed through their 
provided web API (Application Programming Interface). We 
choose to link to the ALA because the majority of our data 
involves birds (for which the ALA has a wealth of 
information) and their API is accessible and well 
documented. 
The goal is to use the species name tag on each 
annotation to search for the associated Life Science Identifier 
(LSID). Each item in the species name map is searched for 
using the ALA API. The query is returned containing a list 
of search results. The taxonomic data returned is then used to 
provide additional relevant information to a user. 
 
Figure 4.  Example of tag linking 
C. 3-D display 
GeoFlow is a three dimensional (3-D) data visualisation 
tool that is developed by Microsoft and integrated with 
Microsoft Excel 2013. It provides a powerful method for 
people to look at information in new ways. 
As an Excel add-in, GeoFlow provides interactive, 3-D 
geospatial and temporal data visualisations. It enables users 
to discover and share new insights from data through rich, 3-
 D data on a globe. It could help us understand the large-scale 
geospatial and temporal trends behind the tedious rows and 
columns. 3-D display is based on Microsoft Bing Map, so it 
is compulsory to be connected to the internet to use 
GeoFlow. 
We use GeoFlow to show the species distribution 
according to static geographical locations or dynamic 
temporal data flow. This can provide us and ecologists a 
quick view of the collected and recognized data, it is also a 
preliminary step prior to a complex data mining algorithm. 
Below is a snapshot of temporal display created by GeoFlow. 
 
Figure 5.  Snapshot of GeoFlow temporal display 
D. Other general visualization tools 
Our system has also implemented many other popular 
visualization tools. Examples include: a map to show all 
sensor distribution, Pie and Bar charts to demonstrate the 
status of sensors or users, and word cloud to show the 
distribution of textual tags.  
VI. CONCLUSIONS AND FUTURE WORK 
The ability to collect data, store and manage them is very 
important. The ability to analyse these data, extract useful 
information to help understanding the data and make 
informed decisions are even more important.  This paper 
presents an overview of various approaches of collecting, 
managing, and analysing large volumes of audio data for 
environmental monitoring. These tools combined together 
will allow large volumes of data to be processed efficiently. 
Currently our research focuses on terrestrial bioacoustic 
event analysis in audio data. We have attempted to analyse 
the large amount of data using three complimentary 
approaches: manual analysis with the help of participatory 
citizen scientists, semi-automated analysis which leverages 
on both human and machine intelligence, and automated 
analysis based on machine learning and pattern recognition.  
All three approaches are aided by data visualization 
techniques wherever possible.  The research findings so far 
are promising but there are still many challenging research 
questions to be addressed.  In the future, we plan to conduct 
more research in the following areas:  
• Engaging citizen scientists to participate in the detection 
and identification of animal vocalisations. The current 
work is focusing on the identification of birds by expert 
birders. 
1003
• The development of interfaces for automated tools that 
are usable by participants. Currently, our automated 
analysis are very technical, their parameters are full of 
jargon, and they take long amounts of time to run. If 
possible, it would be much more efficient to integrate 
these automatic tools into our interactive annotation 
environment, whilst somehow hiding their complexity 
and allowing our participant scientists to make the most 
of them. 
• Mobile support: We will research ways to allow mobile 
users to interact with the data more effectively.   For 
example, we are building mobile applications to allow 
search and retrieve acoustic data from the central 
database by providing sample audios.   
• Data mining. Although ecologists and birders have 
helped generate a large amount of tags and we have 
experimented mining some knowledge based on the 
textual tags, more extensive correlation analysis 
between bird calls, bird behaviors, environment, time, 
location needs to be conducted.  
• Flexible user query support.  We will build more flexible 
user interfaces such as supporting queries by words e.g. 
return all afternoon recordings containing a specific bird 
call.  
All the efforts mentioned will allow our system to be 
more powerful and easier to use thus enabling ecologists, 
citizens and policy makers to utilize the big dataset to gather 
useful information more conveniently, in order to better 
understand the environment and make informed decisions.  
ACKNOWLEDGMENT 
This research is part of an on-going project undertaken in 
the QUT Bioacoustics Research Group which is funded by 
the Queensland State Government under a Smart State 
Innovation Fund (National and International Research 
Alliances Program), Microsoft Research and Queensland 
University of Technology. This research was conducted with 
the support of the QUT Samford Ecological Research 
Facility. 
 
REFERENCES 
 
[1] Mason, Richard and Roe, Paul and Towsey, Michael W. and 
Zhang, Jinglan and Gibson, Jennifer and Gage, Stuart (2008) 
Towards an acoustic environmental observatory. In: 
Proceedings of the IEEE Fourth International Conference on 
eScience, 7-12 December 2008, University Place Conference 
Center & Hotel, Indianapolis, Indiana. 
[2] Jinglan Zhang, Paul Roe, Binh Pham, Richard Mason, 
Michael Towsey,  Jiro Sumitomo,  Archiving Nature's 
Heartbeat Using Smartphones, In book:  Movement-Aware 
Applications for Sustainable Mobility: Technologies and 
Approaches, Author(s)/Editor(s): Monica Wachowicz 
(Wageningen University of Research Centre for Geo-
 Information, The Netherlands), Pages 121-139 (2010), 
ISBN13: 9781615207695, DOI: 10.4018/978-1-61520-769-5  
[3] Wimmer, Jason, Towsey, Michael, Planitz, Birgit, 
Williamson, Ian, & Roe, Paul (2012) Analysing 
environmental acoustic data through collaboration and 
automation. Future Generation Computer Systems. 
[4] Wimmer, Jason, Towsey, Michael W., Planitz, Birgit, Roe, 
Paul, &  Williamson, Ian (2011) Scaling acoustic data 
analysis through collaboration and automation. In 
Proceedings of IEEE Sixth International Conference on e-
 Science, IEEE, Brisbane, QLD. 
[5] Towsey, Michael W., Planitz, Birgit, Nantes, Alfredo, 
Wimmer, Jason, & Roe, Paul (2012) A toolbox for animal call 
recognition. Bioacoustics : The International Journal of 
Animal Sound and its Recording, 21(2), pp. 107-125. 
[6] Truskinger, A., Newmarch, I., Cottman-Fields, M., Wimmer, 
J., Towsey, M., Zhang, J., & Roe, P. (2013). Reconciling 
Folksonomic Tagging with Taxa for Bioacoustic Annotations. 
To be presented at the 14th International Conference on Web 
Information System Engineering, Nanjing, China. 
[7] Truskinger, A., Cottman-Fields, M., Johnson, D, Roe, P. 
(2013). Rapid Scanning of Spectrograms for Efficient 
Identification of Bioacoustic Events in Big Data. To be 
presented at the IEEE 9th International Conference on e-
 Science, Beijing, China. 
[8] Truskinger, Anthony Masters, Yang, Haofan, Wimmer, Jason, 
Zhang, Jinglan, Williamson, Ian, & Roe, Paul (2011) Large 
scale participatory acoustic sensor data analysis : tools and 
reputation models to enhance effectiveness. In Werner, Bob 
(Ed.) 7th IEEE International Conference on eScience, IEEE 
Computer Society, Stockholm City Conference Centre/Norra 
Latin, Stockholm. 
[9] Yang, Haofan, Zhang, Jinglan, & Roe, Paul (2012) 
Reputation modelling in Citizen Science for environmental 
acoustic data analysis. Social Network Analysis and Mining. 
[10] Cottman-Fields, M., Brereton, M., & Roe, P. (2013). Virtual 
birding: extending an environmental pastime into the virtual 
world for citizen science. Paper presented at the Proceedings 
of the 2013 ACM annual conference on Human factors in 
computing systems, Paris, France. 
[11] T. Mitchell Aide, Carlos Corrada-Bravo, Marconi Campos-
 Cerqueira, Carlos Milan1, Giovany Vega, Rafael 
Alvarez.(2013) Real-time bioacoustics monitoring and 
automated species identification. PeerJ 1:e103 
http://dx.doi.org/10.7717/peerj.103 
[12] Lopes, M.T., Lameiras Koerich, A., Nascimento Silla, C., 
Alves Kaestner, C.A.(2011) Feature set comparison for 
automatic bird species identification. IEEE International 
Conference on Systems, Man, and Cybernetics(SMC), pp:965 
- 970 
[13] Duan, Shufei, Towsey, Michael W., Zhang, Jinglan, 
Truskinger, Anthony Masters, Wimmer, Jason, & Roe, Paul 
(2012) Acoustic component detection for automatic species 
recognition in environmental monitoring. In 7th International 
Conference on Intelligent Sensors, Sensor Networks and 
Information Processing(ISSNIP 2011), 6 - 9 December 2011, 
Hilton Hotel, Adelaide, SA. 
[14] Shufei Duan, Jinglan Zhang, Paul Roe, Michael Towsey 
(2012), A survey of tagging techniques for music, speech and 
environmental sound. Artificial Intelligence Review, 1-25. 
[15] Chang-Hsing, Lee, Sheng-Bin, Hsu, Jau-Ling, Shih, Chih-
 Hsun, Chou (2013) Continuous Birdsong Recognition Using 
Gaussian Mixture Modeling of Image Shape Features. IEEE 
Transactions on Multimedia, 15(2), pp:454 – 464 
 
 
   
1004

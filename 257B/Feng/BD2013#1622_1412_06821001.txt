OCPA: An Algorithm for Fast and Effective Virtual Machine Placement and
 Assignment in Large Scale Cloud Environments
 Zhenyun Zhuang
 Salesforce.com, inc.
 The Landmark @ One Market, Suite 300
 San Francisco, CA 94105, USA
 zhenyun@gmail.com
 Chun Guo ?
 IeWare Tech, Di Wang Apartment
 Shun Hing Square Commercial Center
 5002 Shen Nan Dong Road, Shenzhen, China 518008
 † Corresponding author
 Abstract—Cloud Computing is increasingly being deployed
 as a fast and economic solution to various requests that require
 instantiating computing resources to serve customers’ need.
 Despite the distinctions among the three commonly accepted
 service models (i.e., IaaS, PaaS, and SaaS), any large scale cloud
 computing deployment requires the instantiation of multiple
 VMs (Virtual Machines) in a coordinated manner. These VMs
 may further need to be placed in multiple geographically
 distributed data centers, and users that are closer to a VM
 enjoy the benefits of smaller response time, higher bandwidth,
 hence better performance. Thus, how to appropriately place
 VMs and assign them to cloud users can have significant impact
 on the latter’s performance.
 In this work, we consider the problems of VM placement
 and assignment. Based on a set of unique design principles, we
 propose an effective and efficient solution which is referred
 to as OCPA (Opportunity Cost based VM Placement and
 Assignment). OCPA can achieve much better performance and
 the running time complexity is kept linear.
 Keywords-Cloud Computing; Virtual Machine Placement;
 Opportunity Cost
 I. INTRODUCTION
 Cloud Computing model, which involves multiple com-
 puters connected by networks, is being increasingly de-
 ployed as a fast and economic solution to various requests
 that require instantiating computing resources to serve cus-
 tomers’ need. Despite the distinctions among the three
 commonly accepted service models (i.e., IaaS, PaaS, and
 SaaS), any large scale cloud computing environment requires
 the instantiation of multiple VMs (Virtual Machines) in a
 coordinated manner. For instance, a company providing a
 cloud computing service (e.g., real time stock market data)
 which is globally accessed by thousands or even millions of
 clients, may need to scale to multiple VMs if a single VM is
 incapable of handling all requests. These VMs may further
 need to be placed in geographically distributed data centers
 to allow clients to access close-by VMs based on their
 respective locations. Accessing geographically closer VM
 has the benefits of smaller response time, higher network
 bandwidth, hence better performance. In practical deploy-
 ments, once a VM is placed and assigned, users can be
 easily directed to the assigned VM via server redirection
 approaches such as DNS.
 When multiple VMs need to be placed in different data
 centers, there is an immediate question of which VMs are
 allocated in which data centers, a problem we referred to
 as VM Placement problem. As the users (i.e., clients that
 access VMs for cloud services) have different geographical
 locations, placing VMs appropriately in data centers leads
 to better performance experienced by the users. Take the
 following example. Assuming there are multiple data centers
 residing in major cities including Beijing and San Francisco,
 and totally three VMs are needed to serve all users. If two
 thirds of the users are located in San Francisco, while the
 other one third of the users are located in Beijing, then the
 best strategy of VM placement is to place 2 VMs in San
 Francisco, and 1 in Beijing. There is a further question of
 VM Assignment, that is, for a particular user, which VM
 should be assigned to the user 1. This question seems trivial,
 as the straightforward way of VM Assignment is to simply
 assign the VM that is nearest (i.e., with shortest geographical
 distance) to the user. However, as we will demonstrate in
 Section II, such a straightforward solution often leads to
 sub-optimal VM Assignment and worse performance.
 In this work, we consider the above two problems of
 VM Placement and Assignment. Specifically, we consider
 a scenario where a fixed set of users are served by multiple
 VMs. We propose an efficient and effective way of placing
 VMs in multiple available data centers and assigning VMs
 to the users. To this end, we first consider two simple
 solutions that can be easily proposed. Though an optimal
 solution is deemed being NP-hard, we propose a solution
 that has close-to optimal performance, while runs much
 faster. Referred to as OCPA (Opportunity Cost based VM
 Placement and Assignment), the solution can achieve much
 better user performance (i.e., effective), and the running time
 complexity is kept linear.
 1In this work, the two expressions of “Users assigned to VMs” and “VMs
 assigned to users” are used interchangeably.
 2013 International Conference on Cloud Computing and Big Data
 978-1-4799-2829-3/13 $26.00 © 2013 IEEE
 DOI 10.1109/CLOUDCOM-ASIA.2013.81
 254978-1-4799-2830-9/14 $31.00 © 2014 IEE
An alternative solution to the VM placement and as-
 signment problem is to enable VM migration, that is, mi-
 grating running VMs across data centers to achieve better
 performance when current VM placement and assignment
 is suboptimal. Such solutions have attracted intensive re-
 search and discussions recently. However, VM migration
 is associated with certain challenges including implemen-
 tation complexities, network cost, service disruption, and
 computing resource wastage. Note that these VM migration
 solutions essentially work in an orthogonal dimension, and it
 is straightforward to develop an integrated solution that com-
 bines both VM migration and VM placement/assignement.
 Thus in this work, we do not delve deeper into the discus-
 sions of comparing and integrating VM migration.
 For the remainder of the paper, we first provide moti-
 vations for the problem in Section II. We then present the
 detailed algorithm in Section III. We perform performance
 evaluation and show the results in Section IV. We also
 present related works in Section V. And finally in Section
 VI we conclude the work and list our future activities.
 II. MOTIVATION
 We first define the problem we study, and then provide a
 cloud deployment scenario to motivate our design.
 A. Problem Definition
 The problem context that we will study is as follows. The
 assumption is that VM placement and assignment works in a
 particular cloud computing deployment. Such a deployment
 scenario includes the following components, as shown in
 Figure 1: (i) One or more data centers whose geographical
 locations are known; (ii) One or more cloud users that
 comprise the fixed user set, and their geographical locations
 are known. (iii) One or more VMs that are placed in the
 data centers, and each of the VM has an upper bound of the
 number of users it can serve. The following assumptions are
 also made to simplify the problem statement and solutions:
 (i) All VMs are identical in the sense any VM can be placed
 in any data centers; (ii) Data centers have enough capacity
 to place VMs; (iii) Other than geographical locations, data
 centers have identical relevant properties (e.g., price, net-
 work bandwidth, etc.); (iv) User experiences (e.g., response
 time, available network bandwidth) are dominated by the
 geographical distances; (v) The distances between an user
 and a data center can be obtained (e.g., through Geo-IP [1]).
 With the above assumption and statement, we now for-
 mally define the problem as follows. The available data
 cent set is denoted by {DC}, user set by {User} and
 VM set by {VM}. Each VMi is associated with a capacity
 value Capi. We also use Dist{VMi,User j} to denote the
 distance between VMi and User j. Note that though for easy
 description we use geographical distance as the performance
 metric, it can be adapted to use network distance or any
 other meaningful metric. The objective is to place the
 required {VM} in {DC}, such that the total (or average)
 Dist{VMi,User j} is minimized.
 Please note that the proposed solution can easily be
 adapted to accommodate scenarios where some of the as-
 sumptions do not hold. For instance, if some data centers
 have limited capacity in terms of number of VMs placed,
 then these data centers can be removed from current consid-
 erations when capacities reach the limit. We do not explicitly
 discuss these adaptations here due to page limit. Also note
 that though we only consider the problem of VM placement
 and assignment, other relevant problems/solutions such as
 VM migration can be easily integrated into a complete
 solution.
 B. Motivating Scenario
 Consider a cloud deployment scenario where a global
 company would like to utilize a public IaaS provider’s
 platform to deploy 2 VMs serving 4 users. For ease of
 illustration, we assume each VM can only serve up to
 2 users. The IaaS provider has 3 data centers located in
 different continents. The locations of data centers and users
 are shown in Figure 1(a). The company is free to choose any
 data centers to place their VMs, but apparently the nearer
 the user to the assigned VM, the better performance.
 Firstly, let’s consider two simple strategies to the VM
 placement and assignment. In today’s practice, these two
 strategies are widely employed by many cloud providers. In
 Figure 1(b), we show the simplest strategy which randomly
 places VMs in data centers and also randomly assign VMs
 to users, which is referred to as RPRA (Random Placement,
 Random Assignment). At a glance, we see that user experi-
 ence is quite suboptimal due the fact that many VMs are not
 assigned to closer-by users. Particularly, if the users assigned
 to VM1 and VM2 were switched, users would have better
 average performance. So an improved strategy is to assign
 VM to nearest users, subjecting to VMs’ capacity limit.
 We refer to such a strategy as RPCA (Random Placement,
 Closest Assignment). The result with this strategy is shown
 in Figure 1(c). We see that VM2 are assigned to the two
 closest users of U3 and U4. As VM2 reaches its capacity,
 U1 and U2 are assigned to VM1. As we show in Table I,
 this strategy reduces the VM-User distance by about 20%.
 Aiming to obtain even shorter VM-user distances, we
 proposed a few design components that can help improve
 Strategy, VM-
 User Distance
 U1 U2 U3 U4 Aggregate
 RPRA 101.4 51.1 84.1 72.1 308.7
 RPCA 58.0 106.5 37.6 45.0 247.1
 Pop.-aware 101.4 36.4 34.4 45.0 217.2
 OCPA 48.8 36.4 37.6 45.0 167.8
 Table I
 RESULTS OF DIFFERENT STRATEGIES
 255

 
 
  
 
 
 (a) Locations of DCs and users
 
 
 
  
 
 	

 	

 (b) Random Place., Random Assignment
 
 
 
  
 
 	

 	

 (c) Random Place., Closest Assignment
 Figure 1. Motivating scenario and two naive solutions
 on the aforementioned two strategies. These components can
 be combined into a single solution. First, the properties of
 users’ geographical distributions need to be considered when
 placing VMs. We define a term of user population for data
 centers. Specifically, each user is “associated” to the nearest
 data center, and a data center treats the assigned users as
 its population. This design component places as many as
 VMs in data centers subject to the associated population. We
 refer to this component as population aware. The resulting
 assignment of this component is shown in Figure 2(a). Note
 that for this assignment, if there are more users than a
 VM can handle, the users with shortest distances to the
 particular data center are assigned to the VM. We observe
 that this strategy results in another 12% reduction on VM-
 User distance than RPCA, as shown in Table I.
 Second, we observe that the assignment shown in Figure
 2(a) can be further improved by an assignment shown
 in Figure 2(b). The key observation leading to the new
 assignment is that assigning closest users to a VM may result
 in suboptimal performance. In other words, simply assigning
 closest users to a VM may unnecessarily leave other users
 being “punished” harshly. To understand this, let’s look at
 the assignments shown in Figure 2(b). Comparing to the
 assignment of Figure 2(a), we see that the assignments of
 U1 and U3 are switched. Though U3 is assigned to VM1
 in Figure 2(a) ( as U2 and U3 are nearest users to VM1),
 in Figure 2(b), U3 is assigned to VM2 instead. The benefit
 of such assignment change is that U1 now can have a much
 shorter distance to its VM (i.e., VM1 in stead of VM2).
 From the perspective of U1, the distance to VM2 is the
 second largest distance to all VMs, which is thus defined
 as the “opportunity cost” [2] of 2(a). The opportunity cost
 is a price Figure 2(a) actually has paid, because VM1 has
 exceeded its capacity after assigningU2 andU3, thus unable
 to take U1. Thus, rather than assigning VMs to nearest
 users, assigning VMs to users that have largest opportunity
 costs actually can help reduce the average VM-user distance.
 Third, we also notice that in order for the above two design
 components to work effectively, the VM placement and
 VM assignment need to work in tandem, as opposed to
 working independently in RPRA and RPCA. The reason for
 this is that population-aware components works best when
 data centers’ populations are dynamically updated, reflecting
 previous VM assignments. For instance, if the data center
 with largest population has placed and assigned a VM, it
 may not be the data center with largest population anymore,
 thus the next VM placement may need to start from another
 data center.
 Motivated by the above observations, we propose OCPA
 (Opportunity Cost based Placement and Assignment). OCPA
 consists of 3 major design components of Population-aware,
 Coupled placement and assignment, and Opportunity cost
 aware, which are presented in Section III. For the particular
 sample scenario, applying all three components can achieve
 another substantial 25% reduction on VM-User distances
 over a single component of population-aware, as noted in
 Table I.
 C. Investigations and solutions
 In the aforementioned VM placement and assignment
 problem in cloud computing deployment environment, to
 this end, we propose an effective and efficient algorithm.
 Referred to as OCPA, the algorithm places VMs in data
 centers based on user population, and assign VMs to users
 based on a novel metric of opportunity cost. Rather than
 always assigning VMs to the closest users, OCPA assigns
 VMs to users that have larger opportunity cost. For compar-
 ison, in Section IV we not only evaluate the performance of
 OCPA, but also consider other types of solutions.
 III. DESIGN DETAILS
 A. Overview
 With the problem defined in Section II, we now describe
 the design of OCPA. Compared to the straightforward strate-
 gies (e.g., RPRA and RPCA) that randomly place VMs in
 256

 
 
  
 
 	

 
 	

 (a)Population-aware Placement
 
 
 
  
 
 	

 
 	

 (b)OCPA
 Figure 2. Advanced solutions
 data centers, OCPA has the following three major design
 principles: (i) Population-aware. The VM placement is
 based on the “perceived population” of users with regard
 to data centers. The perceived population of a data center is
 defined as the set of users that have the particular data center
 being the nearest one. A population-aware VM placement
 strategy determines the number of VMs placed in a data cen-
 ter based on the latter’s perceived population. In other words,
 the more closer-by users, the more VMs will be placed on
 the data center. (ii) Coupled placement and assignment. The
 VM placement is coupled with VM assignment. Though
 separating VM placement and VM assignment simplifies the
 strategy design, as shown in Sections II and IV, considering
 them in tandem brings certain benefits. (iii) Opportunity cost
 aware. The VM assignment strategy is uniquely based on
 the introduction of “opportunity cost”, which denotes the
 distance to a user’s second nearest data center. In other
 words, when assigning users to a VM, by considering both
 the currently being assigned users and those left-over users,
 OCPA can achieve even further benefits.
 B. Detailed Design
 OCPA runs in the following steps, as shown in Figure 3.
 In lines 1-6 of Variables block, we define a few notations we
 use in describing the algorithm. Particularly, the major inputs
 are the sets of {DC} and {User}; while the outputs are the
 set of {VM} with assigned users. For ease of description,
 every VM can handle a fixed number of Cap users. Also,
 each data center of DCi maintains a state of Unassignedi
 user set.
 Now we move on to the Algorithm block in Figure 3 to
 go through the detailed steps. During the initial state, all
 distance pairs of Useri and DCj are computed, based on
 which the Unassigned j set is filled, as described in Lines
 1 and 2. In addition, the “opportunity cost” of each users,
 defined as the distance between the user and the second
 nearest data center, is computed, as shown in Line 3.
 After that the algorithm consists of two parts. The first
 part runs when there are some data centers have more than
 Cap unassigned users, which indicates at least one VM can
 be placed, and the VM can be assigned to users from the
 data center’s perceived population. The second part runs
 when the unassigned users from a single data center can
 not fill a VM. Thus, two or more data centers need to
 combine their unassigned users to form a VM, which is
 then placed in the data center with majority of the assigned
 users. Now moving on to the first part, where a “while” loop
 (Lines 4-10) will make sure all data centers that are able to
 place a “full” VM have the VMs placed, starting from the
 biggest data center (i.e., with largest number of unassigned
 users). In particular, during the above process, these placed
 VMs are also assigned to users based on their opportunity
 cost. Specifically, the assigned users are those with largest
 opportunity cost, rather than those with smallest distances to
 the data center. The idea behind this is simple - if an user is
 not assigned to the nearest data center, then the user will be
 assigned to a further-away data center, the distance to which
 is at least the user’s opportunity cost.
 In the second part of the algorithm, starting with the
 biggest data center (i.e., the one with biggest Unassignedi),
 VMs are placed and assigned one by one. Each time a VM
 is placed, it is always placed in the biggest data center. The
 VM is assigned to all the unassigned users of the data center,
 and also takes over some more unassigned users from other
 data centers to form a full VM. The taking-over process is
 simply based on the Dist{Useri,DCj} distances. This process
 loops until all users are assigned.
 IV. EVALUATION
 A. Evaluation Setup
 We conduct the evaluation with a flat 2D area, where the
 inputs of data centers and users are placed. For repeatability
 and reality, we fix the locations of data centers, while
 257

 
 
 
 
 	


 
 
 	
	 	
 

 (a) Two data centers
 
 
 
 
 
 	


 
 
 	
	 	
 

 (b) Four data centers
 Figure 4. Average user-VM distance of different strategies
 randomizing the locations of the users. Though such a
 setup is a rather simplified one, we feel it is sufficient to
 demonstrate the effectiveness of OCPA. Specifically, we first
 consider a 1000 by 1000 map, then evenly place a fixed
 number of data centers. For instance, if 4 data centers are
 placed, then their coordinates are (250, 250), (750, 250),
 (250, 750) and (750.750). The set of users are uniformly
 placed in the 2D area.
 We consider all the 3 strategies described in Section II:
 RPRA, RPCA, and OCPA. For simplicity, we assume each
 data center can hold as many as VMs, and the VM’s capacity
 is fixed. We vary the following variables: number of data
 centers, number of users, number of VMs. The performance
 metric we choose is the average distance between a user
 and its assigned VM. Since for all 4 strategies the data
 center set and user set are identical, the number of VMs
 placed is also identical. Thus, the average user-VM distance
 is an appropriate indicator of performance to compare. In
 the following, we use performance and user-VM distance
 interchangeably with regard to an user’s experience.
 B. Evaluation Results
 We firstly consider two scenarios with 2 and 4 data
 centers. The number of users is fixed at 20, while Cap
 is 2. Thus, totally 10 VMs need to be placed. The results
 are shown in Figure 4(a) and (b), respectively. We observe
 that RPRA causes the largest average User-VM distance,
 and RPCA works much better than RPRA, thanks to the
 assignment of VMs to closest users. OCPA results in the
 smallest average User-VM distance. For the two scenarios,
 OCPA beats RPRA on the User-VM distance by 68% and
 66%, respectively.
 Interested in more closely examining the specific perfor-
 mance of each user, we plot the user-VM distance for all
 the 20 users in Figure 5. For all users, RPCA’s performance
 is not worse than RPRA. This is because RPCA improves
 on the basis of RPRA by assigning a possibly closer VM to
 a user. As far as VM-user distance concerned, we observe
 RPRA >= RPCA >= OCPA, with RPRA being the worst
 and OCPA the best.
 We further study the impact of data center set size
 on performance. Intuitively, with more data centers evenly
 distributed in the area, the performance should be better, as
 VMs can potentially be placed in a data center that is closer
 to each user. We fix the number of users to be 1000, and
 number of VMs 20. We then increase the number of data
 centers from 2 to 12. We measure the average performance
 for each of the four considered strategies. The results are
 shown in Figure 6. We observe that with more data centers,
 RPRA’s performance actually becomes worse. This is due
 to the fact that a random assignment of VMs causes inflated
 user-VM distance. For other 2 strategies, the performance
 becomes better with more data centers. For all scenarios,
 OCPA outperforms other strategies. In particular, with 12
 data centers, OCPA’s distance is 50% shorter than RPCA,
 or a 2X improvement. Comparing to RPRA, the de-facto
 practice by most cloud providers today, OCPA’s distance is
 82% shorter, or a 5X improvement.
 V. RELATED WORK
 VM Migration VM migration, which re-allocates VMs
 across data centers or physical machines, has served as an
 alternative solution to accommodate sub-optimal placements
 of VMs. Major OS and platforms including VMware and
 Solaris typically provide such options (e.g., VMotion and
 LDom [3]). In [4] several design options for migrating OSes
 running services with liveness constraints are considered,
 and a solution of migrating VMs on top of the Xen VNM is
 proposed and evaluated. Similarly, works [5], [6] study var-
 ious aspects of VM designs. Many works [7], [8] measures
 performance of certain VM migration strategies.
 VM Placement The problem of VM placement on phys-
 ical machines has been studied in works [9]–[13]. Most
 studies consider the different characteristics of physical
 machines rather than data centers. The goal of optimizing
 VM placement thus is to save energy or computing powers.
 Our work focuses on the VM placement across multiple
 data centers, while the goal is to improve the VM users’
 performance.
 258
Variables:
 1 {DC}: Available Data Center set of DCi;
 2 {User}: User set for Useri;
 3 {VM}: VM set for VMi;
 4 Cap: The capacity vector of each VM;
 5 Unassignedi: the unassigned users of DCi;
 6 NVmi: the number of VM placed on DCi;
 Algorithm:
 1 For every pair Useri and DCj, compute Dist{Useri,DCi};
 2 For each Useri, get the nearest data center of DCi1,
 add Useri to Unassignedj;
 3 For each Useri, get the second nearest data center
 of DCi2 and Dist{Useri,DCi2},
 Dist{Useri,DCi2} is Useri’s opportunity cost;
 4 While {some [Unassignedi]≥Cap}
 5 Get DCi which has the largest Unassignedi;
 let [Unassignedi] be the size;
 6 Compute the number of VM placed on DCi:
 NVmi =  [Unassignedi]Cap ;
 7 Place NVmi Vms on DCi;
 8 For each of the  [Unassignedi]Capi  VMs VMj;
 9 Assign Cap users from Unassignedi that have
 largest opportunity cost;
 10 End of While
 11 While {Some [Unassignedi] = 0
 12 Get DCi which has the largest Unassignedi;
 let [Unassignedi] be the size;
 13 Place a Vm VMj on DCi ;
 14 Assign all Unassignedi users to VMj;
 15 Assign up to Cap? [Unassignedi] nearest users
 from other Unassigned j;
 16 End of While
 Figure 3. Main Algorithm
 
 
 
 
 
 
                    
 	
	 	
 

 Figure 5. User-VM distance of all users
 VI. CONCLUSION
 In this work, we propose a VM placement and assign-
 ment algorithm of OCPA in large scale cloud deployment.
 OCPA runs quickly and can significantly reduce the network
 distance between VMs and clients, hence improving the
 perceived user performance.
 
 
 
 
 
 
 
 
 	

 

       
 

 	
	 	
 

 Figure 6. Impact of DC set size
 REFERENCES
 [1] “Geo ip tool,” http://www.geoiptool.com/.
 [2] “Opportunity cost,” https://en.wikipedia.org/wiki/Opportunity cost.
 [3] “Oracle virtualization,” http://www.oracle.com/us/technologies/
 virtualization/overview/index.html.
 [4] C. Clark, K. Fraser, S. Hand, J. G. Hansen, E. Jul,
 C. Limpach, I. Pratt, and A. Warfield, “Live migration of
 virtual machines,” in Proceedings of NSDI’05, Berkeley, CA,
 USA, 2005.
 [5] T. Wood, K. K. Ramakrishnan, P. Shenoy, and J. van der
 Merwe, “Cloudnet: dynamic pooling of cloud resources by
 live wan migration of virtual machines,” in Proceedings of
 ACM SIGPLAN/SIGOPS VEE ’11, Newport Beach, Califor-
 nia, USA, 2011.
 [6] E. Elmroth and L. Larsson, “Interfaces for placement, migra-
 tion, and monitoring of virtual machines in federated clouds,”
 in Proceedings of GCC ’09, Lanzhou, China, 2009.
 [7] S. Nathan, P. Kulkarni, and U. Bellur, “Resource availability
 based performance benchmarking of virtual machine migra-
 tions,” in Proceedings of ICPE ’13, Prague, Czech Republic,
 2013.
 [8] W. Voorsluys, J. Broberg, S. Venugopal, and R. Buyya, “Cost
 of virtual machine live migration in clouds: A performance
 evaluation,” in Proceedings of CloudCom ’09, Beijing, China,
 2009.
 [9] K. Tsakalozos, M. Roussopoulos, and A. Delis, “Vm: place-
 ment in non-homogeneous iaas-clouds,” in Proceedings of
 ICSOC’11, Paphos, Cyprus, 2011.
 [10] J. Dong, X. Jin, H. Wang, and Y. Li, “Energy-saving virtual
 machine placement in cloud data centers,” in Proceedings of
 CCGrid’13, Delft, the Netherlands, 2013.
 [11] N. Calcavecchia, O. Biran, E. Hadad, and Y. Moatti, “Energy-
 saving virtual machine placement in cloud data centers,” in
 Proceedings of Cloud’12, Honolulu, HI, 2012.
 [12] K. Le, R. Bianchini, J. Zhang, Y. Jaluria, J. Meng, and
 T. D. Nguyen, “Reducing electricity cost through virtual
 machine placement in high performance computing clouds,”
 in Proceedings of SC ’11, Seattle, Washington, 2011.
 [13] E. Mohammadi, M. Karimi, and S. R. Heikalabad, “A novel
 virtual machine placement in cloud computing,” Australian
 Journal of Basic and Applied Science, pp. 1549–1555, August
 2011.
 259

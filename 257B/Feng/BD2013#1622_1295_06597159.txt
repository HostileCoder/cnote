Data Mining Approaches for Packaging Yield Prediction in the Post-Fabrication
 Process
 Seung Hwan Park, Cheong-Sool Park, Jun Seok Kim,
 Sung-Shick Kim, Jun-Geol Baek* 
School of Industrial Management Engineering
 Korea University, Anam-dong, Sungbuk-gu, Seoul, 136-
 701, Korea
 {udongpang, dumm97, bliths, sungskim, 
jungeol}@korea.ac.kr
 Daewoong An
 SK Hynix Semiconductor
 San 136-1, Ami-ri, Bubal-eup, Icheon, Gyeonggi-do, 
467-701, Korea 
daewoong.an@sk.com
 Abstract—In the post-fabrication process for semiconductors,
 it is critical to predict the yield. This process consists of a series 
of electrical and physical tests following semiconductor 
fabrication, tests that generate a significant volume of 
parametric data. While past research has investigated yield 
prediction using parametric test data, most studies have 
difficulty correctly predicting the low and high yield because of 
the wide range of variables and the large data set. Also, in the 
case of the packaging yield, prediction is inaccurate as this 
yield does not directly correlate with the parametric test data. 
Therefore, this study proposes a framework in which the 
packaging yield is classified using the parametric test data of 
the previous step of the packaging test. This study involves
 three stages. In the first, data preprocessing is conducted due 
to the large data set. To learn a data mining model using much 
more data, parametric test data generated in the die level need 
to be changed into the wafer level. In the second stage, a
 random forest algorithm is used to select significant variables 
affecting the packaging yield. Finally, the third stage uses a
 nonlinear support vector machine (SVM) to classify the low 
and high yield. Through the three stages, this study 
demonstrates that this proposed algorithm has a superior 
performance.
 Keywords-Random Forests; Ensemble Support Vector 
Machine; Packaging Yield Classification; Semiconductor 
Manufacturing Process
 I. INTRODUCTION
 The semiconductor manufacturing process produces core 
components such as memory and central processing units 
(CPU). Because of the high demand for the newest electronic 
devices, such as smart phones and tablets, it is important to 
consistently improve the manufacturing process through 
state-of-the-art equipment or process management 
techniques. In particular, accurate yield prediction is 
imperative in managing the process. Yield prediction is 
conducted by classifying the low and high yield groups in 
the final test step of the post-fabrication process. However, 
this step in semiconductor manufacturing is complicated 
because there are hundreds of manufacturing processes
 which take several months to complete. In addition, to
 accurately manage the yield, semiconductor manufacturing 
companies require a super-clean environment, regular 
equipment maintenance and comprehensive worker training
 [1]. But above all yield management through the analysis of 
data generated during the post-fabrication process needs to 
be conducted. Therefore, yield classification is generally 
performed using parametric test data. In the post-fabrication
 process, the packaging yield is more important than other 
kinds of yield. Fig. 1 represents the sequence of steps and the 
yield in the post-fabrication process.
 Figure 1. Flow chart of the semiconductor manufacturing process
 The semiconductor manufacturing process consists of 
about 300 to 400 units and it takes 3 to 4 months to complete 
a finished chip [1]. First, wafers go through the fabrication
 process for 2 to 3 months before moving on to the post-
 fabrication stages. The post-fabrication process consists of a
 wafer test, the assembly, and the final test [2]. In conjunction 
with each of these steps, the fabrication yield, the wafer test 
yield, the assembly yield and the packaging yield are all 
calculated. The fabrication yield refers to the ratio of wafers 
in to wafers out. The wafer test yield is calculated as the ratio 
of whole chips of wafer out and the number of non-defective 
chips as decided by the wafer test. The assembly and 
Fab Wafer Test Assembly FinalTest
 Prediction 
Model
 Parametric data
 Classifying 
high/low yieldWafer
 Wafer test 
yield
 Packaging 
yield
 Assembly 
yield
 2013 IEEE International Congress on Big Data
 978-0-7695-5006-0/13 $26.00 © 2013 IEEE
 DOI 10.1109/BigData.Congress.2013.55
 363
packaging yields are the ratio of the input chips after 
assembly and the input chips after the final test respectively. 
This study focuses primarily on the packaging yield, in 
particular the classification of the low and high yield based 
on the probe test data. 
In order to best predict the yield in the semiconductor 
manufacturing process, a great deal of research has been 
performed. However, most research has focused on the wafer
 test yield prediction, rather than the packaging yield 
prediction. The yield prediction that uses a markov chain is 
conducted by calculating the probability that a chip is 
acceptable given  defects [3]. However, this prediction is 
limited due to the univariate analysis on which it is based. So, 
as the number of variables from the probe test is enormous, 
multivariate analysis is essential. A classic method for 
multivariate data analysis is multiple discriminant analysis. 
However, this method of analysis is limited by the 
assumption that variables are independent, identical and 
normally distributed. For more advanced multivariate 
analysis, the hybrid machine learning method is applied in
 the semiconductor manufacturing process, using an inductive 
decision tree, a neural network with a back-propagation 
algorithm, and a self-organizing map (SOM) [4]. Also, the 
yield prediction is conducted using a fuzzy-based neural 
network from the multivariate parametric test data [5].
 However, this prior research has not focused on the 
packaging yield. To predict the packaging yield, a stepwise 
support vector machine (S-SVM) algorithm is applied to the 
wafer test data [1]. S-SVM classifies the low and high yield 
of the final test by screening for potential defects in the final 
test process. However, problems with parameter selection, 
step selection, and classification accuracy exist in this 
algorithm. Therefore, this paper proposes a data mining 
approach that selects significant parameters and improves 
classification accuracy.
 The rest of this paper is organized as follows. Section II
 and Section III introduce the basic concept of the random 
forest algorithm and the ensemble support vector machine
 (E-SVM) algorithm respectively. Section IV describes the 
proposed algorithm based on Sections II and III. In Section 
V, the experimental results are explained after the proposed 
algorithm is applied to actual industrial data. Finally, a 
conclusion and further studies are described in Section VI. 
II. RANDOM FORESTS ALGORITHM
 Widely known and efficient, the random forests 
algorithm is used for both classification and regression. 
Similar to the ensemble method, this algorithm is based on 
bagging (bootstrap aggregating) and is introduced by 
Breiman [6]. Bagging is a type of ensemble algorithm and is 
generally applied in the decision tree method [7]. The 
advantage of random forests is that there is no need for 
pruning trees and it automatically generates the importance 
and accuracy of variables. Also, this algorithm is not 
sensitive to outliers in training data. 
The basic principle of random forests is to combine many 
binary decision trees built by using several bootstrap samples
 and choosing a subset of predictors randomly at each node 
[8]. The basic flow of random forests is represented by using 
the following algorithm [9]. 
 Step 1: Generate  trees from the bootstrap sample 
data. The number of trees means the training cases; 
let the number of variables for classification be . 
 Step 2: Input  variables to be used to find a 
random split point at a node of the tree unlike an
 optimal split point; m should be far smaller than . 
 Step 3: Determine a training subset for this tree by 
choosing  times with replacements from all 
 available training cases. To estimate the error of the 
tree, the remaining cases are used by predicting their 
classes.
  Step 4: For each node of the tree, randomly choose 
 variables on which to base the decision at that 
node. 
 Step 5: Find an optimal split point based on 
 variables in the training set.
  Step 6: Grow each tree completely; do not prune 
them
 The random forests algorithm is used for the selection of 
variables as well as classification and regression. Many 
selection procedures are based on the importance of the 
variable to ranking and model estimation so that a family of
 models can be generated, evaluated and compared [8].
 III. SUPPORT VECTOR MACHINE
 The SVM is a supervised learning method that classifies 
data by using the hyperplane maximizing margin and is
 introduced by Vapnik [10]. The SVM performs 
exceptionally in document classification, image recognition 
and text recognition [11]. The training data set for the SVM 
can be represented as (1) and it consists of predictors and a
 class label. 
 = (	, 
	)|	  ,   
	   {1, +1}	             (1)
 The training data set  includes  elements, pairs of -
 dimensional vectors  known as a predictor and class 
  
corresponding to   . The hyperplane is defined as    
  = 0 and it is known as the SVM classifier. As shown in 
Fig. 2, the margin 2/ should be maximized for more 
accurate classification. The objective function that 
maximizes 2/ is reformulated to minimize  .
 However, as shown in Fig. 2, a hyperplane that can totally 
classify two class data does not exist. Therefore, a slack 
variable () is used for inseparable cases. The slack variable 
adjusts the degree of misclassification and is adopted for the 
entire training data set. 
364
Figure 2. The concept of the SVM classifier
 In (2), which represents a modified objective function, 
the multiplication of the penalty cost 
 and summation of all 
() are added to the existing objective function. Based on the 
objective function, the optimization problem is formulated 
by adding constraints as shown in (2).
     
 
 
 +   ! 		                                        (2)
 "#$
% %&     
	(  	  ) ' 1  	,   1 *  *       
Also, this problem can be extended to the dual problem as 
shown in (3). 
-   ! .		   ! .	.3
	
3	3 4	536 (3)
 "#$
% %&    0 * .	 *  (i=1,…,n),   ! .	
	 = 0	
 The function of the SVM linear classifier is formulated as 
(4), where "7() represents the function that returns +1
 when  is less than 0, otherwise 1 [12]. 
    8() = "7(5 + ) = "7(! .	
	4	536 + )	      (4) 
Also, the kernel trick should be applied for the SVM non-
 linear classifier. Equation (5) is transformed by replacing
 variable  in (3) with the kernel function 9. Finally, in (6), 
the non-linear classifier using the kernel trick is calculated. 
-    ! .		   ! .	.3
	
3	3 94	536          (5)
 "#$
% %&    0 * .	 *  (i=1,…,n),   ! .	
	 = 0	
                  8() = "7(! .	
	94	536 + )	                (6)
 IV. THE PROPOSED ALGORITHM
 This section describes the proposed algorithm, which 
consists of three parts. First, data preprocessing is performed 
to cover large amounts of data. Second, the random forests 
algorithm is applied to the selection of variables and third,
 the E-SVM is conducted in order to classify low and high 
yield in the final test process. The outline of the proposed 
algorithm is shown in Fig. 3. 
Figure 3. Overall framework of the proposed algorithm
 More detailed description with regards to the proposed 
algorithm is as in the following.
 A. Data Preprocessing
 In the semiconductor manufacturing process, it generally 
takes 3 to 4 months to complete a finished product. During 
this time, a large amount of data is generated. In particular, 
data at the chip level incorporates many observations,
 making analysis difficult in establishing relation to the 
packaging yield. As a result, we focus on data at the wafer 
level. Using statistics such as the mean and the standard 
deviation of the parametric test data, we extract a new 
feature representing wafers. Finally, a single wafer 
represented as just one observation unlike including about 
1500 observations in one wafer.
 B. Variable Selection Using Random Forests
 Even after data preprocessing, a large number of 
variables remain. Because the presence of many variables 
reduces the learning performance, it is necessary to select the 
significant variables. Therefore, this study proposes a
 selection method using the random forests algorithm. The 
objective of this algorithm is to choose variables closely
 correlated with the response variable, for the purpose of 
identification using decision trees. To identify the significant 
variables, four measures to assess the importance of the 
margin
 :  	   = 1  	
 	
 DATA PREPROCESSINGStep 1:
 RANDOM FORESTS
 ENSEMBLE SVM
 Or  OTHER ALGORITHMS
 Raw Data
 Wafer 
Summary Data
 Significant 
Variables
 Classification of High and Low Yield
 Step 2:
 Step 3:
 365
variables are calculated [13]. Four measures are as in the 
following: (1) Measure 1 is calculated by comparing total 
classification error rate with the new classification error rate 
using ; -th variable. (2) Measure 2 is defined as the 
proportion of correct classifications exceeds the proportion 
of the most voted misclassifications.(3) Measure 3 is 
calculated by the difference between the number of lowered 
and raised margins as described framework in Measure 2. (4) 
Measure 4 is calculated through the Gini index or the 
Shannon entropy [13]. These measures are considered as 
points in a four-dimensional space. Fig. 4 shows the process
 of variable selection.
 Figure 4. Procedure for variable selection
 Consequently, the input variables of the classification 
model are selected using the procedure above. 
C. Yield Classification Using E-SVM
 E-SVM refers to ensemble learning using a SVM. The basic 
concept of ensemble learning is to employ multiple learners 
and combine their predictions [14]. The SVM is a widely 
known classifier on the diverse domain. However, there are 
limitations in learning the algorithm due to the non-linearity 
and complexity of the data. Therefore, this study proposes 
an ensemble SVM to increase the classification rate. Fig. 5 
outlines the proposed ensemble SVM. In Step 1, the given 
data set is separated into a training set A and a test set < in
 the ratio of 4 to 1. Training set A is used to build an 
ensemble learning model using the SVM. Step 2 selects 
random samples for learning recursively. Generally, 
bagging, boosting, disjunct partitioning and fold partitioning 
are considered when composing a sample data set [15]. In 
this study, disjunct partitioning that randomly splits the 
training data is used. Following the selection of random 
samples, Step 3 is performed: the building and classification 
of the model. The SVM model 	( = 1, … , )using the 
random sample 	 is constructed and the prediction of the 
test set < is performed using the model 	 . Steps 2 and 3 
are repeated  times and  prediction results are produced.
 The average of the prediction values is then regarded as the 
final predicted class for test set <. 
Figure 5. Flow chart combining the ensemble technique and SVM
 V. EXPERIMENTAL RESULTS
 For the experiment, an actual industrial data set is used. 
The original data set is large, consisting of observations at
 the chip level. Therefore, the data set is reduced to the wafer 
level and each wafer is labeled as a high or low yield using
 specified criteria. In this study, we have labeled the high 
yield in situation where the yield is greater than the average 
yield and the low yield in situation where the yield is less 
than the average yield. Also, 500 wafers are separated into a
 training set and a test set in order to evaluate the
 performance of the model.
 This experiment consists of two parts. The first
 represents the results of variable selection using the random 
forests algorithm and the second evaluates the performance
 of the E-SVM. The results are compared to those of widely 
used classification algorithms in the data mining domain: K-
 Nearest Neighbor (KNN), Random Forests, Decision Tree, 
Neural Network and SVM.
 To evaluate the performance of the algorithms, we 
compared the low yield classification accuracy (LCA),  the 
high yield classification accuracy (HCA) and total 
classification accuracy (TCA) before and after applying the 
significant variables obtained from the random forests 
algorithm. The LCA represents the ratio of classifying 
correctly the actual low yield class as the low yield class and 
Step 1:
 Get a four-dimensional center vector with 
coordinates using an average of four 
measures.
 Calculate a distance between each point and 
the center vector and sort the distances in 
decreasing order.
 Select the variable whose the distance 
exceeds a given threshold.
 Step 2:
 Step 3:
 Step 1:
 Split a training set     and test set 
Get randomly samples         
from 
Learning of SVM using 
Get the SVM Model
 Prediction of     through  
Step 2:
 Step 3:
 Calculate the average of 
prediction results
 Step 4:
 A B
 A
 ),...1( niSi 
 B iM
 iS ),...,1( niM i 
 Repeat
 times
 n
 n
 366
HCA means the ratio of classifying correctly the actual high 
yield class as the high yield class. TCA is calculated by 
averaging the LCA and the HCA. In addition, as shown in 
Table I, the classification accuracy increment (CAI) is 
calculated to identify any differences before and after the 
application of the selected variables. The low yield 
classification accuracy increment (LCAI), the high yield 
classification accuracy increment (HCAI) and the total 
classification accuracy increment (TCRI) represent 
increments corresponding to LCA, HCA and TCA 
respectively. When the CAI is positive, the selected variables 
are significant. In the opposite case, the selected variables 
are not significant. 
Table I is to compare LCAI, HCAI and TCAI in order to 
identify the performance of variable selection. As a result, 
the SVM algorithm demonstrates a powerful performance 
increment. In the case of the remaining algorithms, LCAI
 produces a positive number because the HCAI is greater than 
the absolute value of the LCAI, in spite of the negative LCAI. 
Therefore, the variables selected by using the random forests
 algorithm are significant.
 TABLE I. VARIABLE SELECTION PERFORMANCE
 Table II shows the performance of the algorithms, 
including our proposed algorithm E-SVM. First, the KNN 
algorithm has a difficulty in learning due to the decision of >
 and its weakness caused by multivariate data. Also, the poor 
performance of the neural network algorithm occurs because 
of the structure of network layers or the sensitivity in setting
 the parameters. The SVM using a kernel trick proves suitable
 for classifying the non-linear data set. However, learning 
performance is reduced due to the small data set. Therefore, 
the ensemble technique is applied to the classification 
algorithms, such as decision tree and SVM. The proposed 
algorithm applying the ensemble technique produces the 
highest LCA, HCA and TCA both before and after the 
application of variable selection. Also, the classification 
accuracy increases after using the proposed variable 
selection algorithm.
 TABLE II. THE PERFORMANCE OF THE ALGORITHMS
 Consequently, the proposed algorithm combining the 
ensemble technique with SVM demonstrates significance in
 variable selection using the random forests algorithm, and 
performs better than the other methods tested.
 VI. CONCLUSION
 The post-fabrication process consists of many test stages
 and each stage generates a large amount of parametric test
 data. Therefore, the variable selection technique proposed in 
this study is applied to real industrial data and its 
performance is verified. Also, the ensemble learning using 
SVM is a highly efficient method for predicting the 
packaging yield. Consequently, this study offers two data-
 Algorithms
 Classification Accuracy Increment
 LCAI HCAI TCAI 
Random Forests -6.38%p 8.21%p 0.91%p 
Decision
 Tree 0.00%p 8.46%p 4.23%p 
KNN -8.51%p 11.15%p 1.32%p 
Nerural
 Network -29.79%p 24.39%p -2.70%p 
SVM 25.53%p 14.22%p 19.87%p 
Average -3.83%p 13.29%p 4.73%p 
Algorithms
 Classification Accuracy
 LCA HCA TCA
 N
 o
  V
 a
 riable
  S
 electio
 n
 Random Forests 68.09% 64.71% 66.40%
 Decision
 Tree 55.32% 68.63% 61.97%
 KNN 65.96% 45.10% 55.53%
 Neural
 Network 78.72% 23.53% 51.13%
 SVM 38.30% 60.78% 49.54%
 Proposed Algorithms
 (E-SVM) 76.60% 78.43% 77.51%
 Average 63.83% 56.86% 60.35%
 V
 a
 riable
  S
 electio
 n
 Random Forests 61.70% 72.92% 67.31%
 Decision
 Tree 55.32% 77.08% 66.20%
 KNN 57.45% 56.25% 56.85%
 Nerural
 Network 48.94% 47.92% 48.43%
 SVM 63.83% 75.00% 69.41%
 Proposed Algorithms
 (E-SVM) 74.47% 83.33% 78.90%
 Average 57.45% 65.83% 61.64%
 367
mining approaches to manage yield efficiently. In 
furtherance of these results, the algorithm integrating the two 
data-mining approaches needs to be revised given that
 variable selection and ensemble learning offer superior 
performance in semiconductor manufacturing. Also, many
 more data sets need to be applied to this proposed algorithm
 to test the robustness of these results.
 ACKNOWLEDGMENTS
 This research was supported by the Basic Science 
Research Program through the National Research 
Foundation of Korea (NRF) funded by the Ministry of 
Education, Science and Technology (2012-0008332). 
This research was supported by the Ministry of 
Knowledge Economy (MKE), Korea, under the IT R&D 
Infrastructure Program supervised by the National IT 
Industry Promotion Agency (NIPA) (NIPA-2012-(B1100-
 1101-0002)).
 REFERENCES
 [1] An, D., Ko, H.H., Gulamba, T., Kim, J., Baek, J.G., and Kim, 
S., “A Semiconductor Yields Prediction Using Stepwise 
Support Vector Machine”, IEEE Ineternational Symposium 
on Assembly and Manufacturing,  December 2009.
 [2] Uzsoy, R., C. Lee, and L.A. Martin-Vega, “A Review of 
Production Planning and Scheduling models in the 
Semiconductor Industry PART I: System Characteristics, 
Performance Evaluation and Production Planning,” IIE 
Transactions, Vol. 24, No. 4, pp. 47-60, 1992.
 [3] Ciciani, B. and G. Jazeolla, "A Markov Chain-Based Yield 
Formula for VLSI Fault-Tolerant Chips," IEEE Transactions 
on Computer-Aided Design, Vol. 10, No.2, pp. 252-259, 1991. 
[4] Kang, B.S., Lee, L.H., Shin, C.K., Yu, S.L., & Park, S.C., 
"Hybrid machine learning system for integrated yield 
management in semiconductor manufacturing," Expert 
Systems with Applications, Vol. 15, No. 2, pp. 123-132,
 August 1998.
 [5] Wu, L., and Zhang, J., “Fuzzy neural network based yield 
prediction model for semiconductor manufacturing system”,
 International Journal of Production Research, Vol. 48, No. 11, 
pp. 3225-3243, June 2010.
 [6] Breiman, L., “Random Forests”, Machine Learning, Vol. 45,
 No. 1, pp. 5–32, October 2001.
 [7] Breiman, L., “Bagging predictors”, Machine Learning, Vol. 
24, No. 2, pp. 123–140, August 1996. 
[8] Genuer, R., Poggi, J.,M., and Tuleau-Malot, C., “Variable 
selection using random forests”, Pattern Recognition Letters, 
Vol. 31, pp. 2225-2236, March 2010.
 [9] Radenkovic, P., “Random Forest”, unpublished.
 [10] Vapnik, V., The Nature of Statistical Learning theory, 
Springer-Verlag, 1995
 [11] Joachims, T., "Text catergorization with support vector 
machines," Proceedings ofthe European Conference on 
Machine Learning, 10th European Conference on Machine 
Learning, pp. 137-142, 1998.
 [12] Park, J., Kwon, I.H., Kim, S., and Baek, J., Vol. 3, No. 1, 
2002.
 [13] Breiman, L., “Manual on setting up, using, and understanding 
Random Forests”, Technical Report, Vol. 3, No. 1, 2002.
 [14] Sewell, M., “Ensemble Learning”, unpublished, August 2008.
 [15] Liaw, A., and Wiener, M., “Classification and Regression by 
randomForest”, R News, Vol. 2, No. 3, pp. 18-22, December 
2002.
 368

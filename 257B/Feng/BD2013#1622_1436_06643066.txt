 
 
Distributed File Management System Based on SSH2 and HDFS 
Junxiong Sun,Yan Chen,Taoying Li,Yingying Yu, Penghui Li 
Transportation Management College 
Dalian Maritime University 
Dalian, China 
E-mail: sunbusiness@foxmail.com 
 
Abstract—As time goes on, the current running 
information system produces massive data. Obviously the 
previous hardware could not fit for managing the much 
bigger data any more. In order to upgrade the system, 
there are three main problems need to consider: enough 
storage space, high reliability, high performance and 
relatively low cost. As an open-source J2EE framework, 
SSH2 is widely used by developers to develop all kinds of 
management information systems. Hadoop, another open-
 source project running in an integrated cluster, is good at 
dealing with the big data. In order to solve the three 
problems, this paper puts forward the way to integrate the 
SSH2 and Hadoop. 
Keywords: HDFS; SSH2; distribution; big data. 
I.  INTRODUCTION 
With the rapid development of information 
technology, the scale of the accumulated data is 
becoming huger. Age of Big Data, as a new buzzword 
word, is coming. It is a great challenge to deal with the 
big data that we have to consider at least four main 
problems: 
1.High Storage Space: There is enough storage 
space to store the big data. Even it could grow with the 
scale of the data. 
2.High Reliability: The stored data should be safe. 
3.High Performance: Much more computing power 
is required to process the big data. 
4.Economic: Expense is always very important. 
Though some high-performance mainframes can be 
qualified for the task, the high cost is difficult to be 
accepted by most enterprises or institutions. Clearly, 
what we need is to process the big data effectively but 
with relatively low cost.   
The research on the storage system based on the 
distributed network has become a hot topic. Now, there 
are many kinds of distributed network storage system, 
such as OceanStore, CFS, PAST, Tangler, Pangaea, 
Granary and so on [1]. Generally, distributed file system 
is multi-user, and to a certain degree it is hard to manage. 
This paper will introduce how to design and 
develop a distributed file management information 
system to provide a easy way to manage the distributed 
files. On the other hand, this paper shows a way to 
change a single computer system into a distributed one 
quickly. 
The reminder of this paper is arranged as follows: 
The core technology is introduced in Section 2. In 
Section 3, we describe the details of system construction 
and conclude the paper in Section 4. 
II.  CORE TECHNOLOGY 
A. HDFS  
HDFS[2-5](Hadoop Distributed File System)is a 
subproject of Hadoop. It is a distributed file system 
running on commodity hardware (cheap, easy to get). 
HDFS lives in a cluster containing many nodes and 
integrates all nodes’ storage resources. So that it has 
large storage capacity. Generally, in a huge cluster, the 
rate of node failure is very high. It is thus clear that 
HDFS must have high tolerance of faults [6]. Compared 
with the traditional way of file storage, HDFS’s 
outstanding character lies in that it can save and back up 
a file on several nodes by splitting it into blocks [7]. 
 
Figure 1.  Hadoop Distribution File System Model 
HDFS uses the master-slave structure as in Figure1. 
There are two kinds of nodes in a HDFS cluster: 
NameNode and DataNode. NameNode is responsible for 
the management and maintenance of the cluster’s 
internal file directory information such as the 
information of the location of the file blocks. DataNode 
which is managed by the NameNode is responsible for 
the storage of the file blocks, and sends back 
information list of the stored file blocks regularly. But 
2013 International Conference on Computational and Information Sciences
 978-0-7695-5004-6/13 $26.00 © 2013 IEEE
 DOI 10.1109/ICCIS.2013.152
 549
 
 
HDFS has its disadvantage. Hadoop is more suitable for 
big data set consisted by large files instead of the small 
files.  
The client gets the file from the cluster as 
following: First it should send a request to the 
NameNode, and then the NameNode will return an 
address list of the file blocks which belong to the 
required file. Finally the client reads the file blocks from 
the “nearest” (Ordered by transmission time) 
DataNodes. Similarily, when the client wants to write a 
file into the cluster. First it needs to send a request to the 
NameNode, then the NameNode will check user’s 
authority and the existence of the file. And then the 
NameNode will add a new record for the file. Finally 
client can write file blocks into the appointed 
DataNodes. 
B. SSH2 
SSH2[6-7], a framework used widely in the 
development of Web system, consists of three parts: 
Struts2, Spring and Hibernate  
Struts2 is the next-generation of Struts1, and it is a 
brand new architecture combining Struts1 with 
WebWork technology. Struts2 provides a series of 
actions to execute and response the users’ requests. 
Spring is famous for its great concept: 
IOC(Inversion Of Control). The relationships among the 
entities are clearly defined in a series of configuration 
files. As a result, every entity or component could be 
seen as liberated that they are low coupled. Spring is 
responsible for the business layer and ensures the 
accuracy of business operation.  
 
Figure 2.  System Architecture 
Hibernate is responsible for interacting with the 
database and almost all kinds of popular databases are 
supported. The core of Hibernate is Object/Relation 
Mapping (ORM) which creates “a virtual object 
database” that can be used within the programming 
language. Hibernate separates the data operation from 
business logic effectively during the system 
development process. It makes developers focus on the 
business logic and reduces the difficulty and cost for 
system migration. 
III.  SYSTEM CONSTRUCTION 
As showed in Figure 2, system implementation is 
similar to a common Web system based on SSH2 
architecture. The most important difference is that this 
system relies on a storage cluster based on HDFS. From 
the user’s aspect, storage cluster is transparent, and they 
don’t know and needn’t care where their files are stored. 
For the developers, the files uploaded by the users won’t 
be stored on one certain machine directly. Instead, they 
may have been split into blocks and distributed on 
several machines, but the developers needn’t care how 
to allocate every block, it will be finished by HDFS.In 
addition, the information associated with the files, such 
as the information of the users, is still stored in the 
relational database. When the user wants to download or 
upload his or her own files, system will first check the 
user’s ID according to the information from the 
database, then the user will be authorized. Finally files 
could be uploaded or downloaded through the 
interaction between the user and the cluster. 
 
Figure 3.  User information management 
The system is designed with two major functions 
as following. 
a) User information management: As the Figure 3 
shows, the system administrator has the right to 
add a new user or remove one. Also a certain 
user’s detail information can be searched though 
some key fields like user name. 
b) File information management: As the Figure 4, the 
user could upload or download the file from the 
system. Of course, if necessary, the chosen files 
can be deleted. 
 
Figure 4. File information management 
During the system development process, we set 
Hadoop pseudo distribution pattern to imitate Hadoop 
cluster through one machine, another machine running 
550
 
 
MySql works as a database server. In this way, it 
becomes much easier to develop and debug .  
The most important function of the system is to make 
sure that user could upload files to the cluster and 
download them. Before uploading the files, system will 
calculate the files’ MD5 code to avoid the duplicate 
files .The key code is shown as the following: 
 
Public String getFileMD5Str(File file) 
{ 
FileInputStream in = new FileInputStream(file); 
FileChannel ch = in.getChannel(); 
MappedByteBuffer Bb = 
ch.map(FileChannel.MapMode.READ_ONLY, 0, 
file.length()); 
messagedigest.update(byteBuffer); 
return bufferToHex(messagedigest.digest()); 
} 
 
Public void Upload() 
{ 
…… 
FileSystem fs = FileSystem.get(URI.create(dest), conf);   
OutputStream out = fs.create(new Path(dest), new 
Progressable() {  publicvoid progress() {   
System.out.println("finish uploading?");   
} });   
…… 
} 
 
Public void DownLoad() 
{ 
…… 
FileSystem fs = FileSystem.get(URI.create(dest), conf);   
FSDataInputStream  hdfsIS = fs.open(new Path(dest));   
OutputStream out = new FileOutputStream(mySrc);   
IOUtils.copyBytes(hdfsIS, out, 4096,true); 
…… 
} 
IV.  CONCLUSIONS 
This paper puts forward the design and shows some 
construction details of a web system to interact with 
users, and manages the distributed files based on SSH2 
and HDFS. As a result, to a certain degree, we could 
solve the four problems we just mentioned in the first 
section. With the help of Hadoop, flexible storage space, 
high reliability and performance are all available. SSH2 
provides a stable and visual plat for the users who could 
share or back up their own files through the Web 
browser. Most important, both SSH2 and Hadoop are 
open-source and for free.  
ACKNOWLEDGMENT 
This research was supported by the National 
Natural Science Foundation of China under Grant 
71271034, Liaoning Provincial Department of 
Education Science Research Project (No.L2012173), 
and the Cultivation Fund of Dalian Maritime University 
for National Excellent Doctoral Dissertations 
(NO.2013YB06). 
REFERENCES 
[1] MaJun, ZhengQuan, YinBaoqun: CDN and P2P-Based 
Distributed Network Storage System. Computer 
Applications and Software, 27(2) , 50--52(2010) 
[2] Fengying Yang, Huichao Liu: Research in HDFS Based 
on the Campus Network Environment. Image Analysis 
and Signal Processing (IASP), 648 -- 652 (2011) 
[3] JiangLiu,LiBing,SongMeina: The Optimization of 
HDFS Based on Small Files. Broadband Network and 
Multimedia Technology (IC-BNMT), 912--915(2010). 
[4] Hongbo Shi, Zhenxin Wu: Research on a distributed 
long term preservation system solution based on 
HDFS.Researches On Library Science, 29--34(2012). 
[5] Weiwei Li, Hang Zhao: Research on massive data 
mining based on MapReduce.Computer Engineer and 
Applications, 1--6(2012). 
[6] DingBo, ChaoAinong: Research on AJAX development 
based on Struts2 framework.Computer Engineering and 
Design, 30(16), 3910--3913(2009) 
[7] WangMeiqin: Study on Method of Integration of the 
J2EE Framework Based on Struts, Spring and Hibernate. 
Computer Knowledge and Technology, 7(24),5911--
 5913(2011). 
551

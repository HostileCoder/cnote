On Mixing High-Speed Updates and In-Memory Queries 
A Big-Data Architecture for Real-time Analytics 
Tao Zhong Kshitij A Doshi  Xi Tang Ting Lou Zhongyan Lu Hong Li 
Software and Services Group, Intel 
{tao.t.zhong, kshitij.a.doshi, xi.tang, ting.lou, zhongyan.lu, hong.li}@intel.com 
Abstract— Up-to-date business intelligence has become a 
critical differentiator for the modern data-driven highly engaged 
enterprise. It requires rapid integration of new information on a 
continuous basis for subsequent analyses. ETL-based and 
traditionally batch-processing oriented methods of absorbing 
changes into a relational database schema take time, and are 
therefore incompatible with very low-latency demands of real-
 time analytics. Instead, in-memory clustered stores that employ 
tunable consistency mechanisms are becoming attractive since 
they dispense with the need to transform and transit data 
between storage layouts and tiers.  
When data is updated infrequently, in-memory approaches 
such as RDD transformations in Spark can suffice, but as 
updates become frequent, such in-memory approaches need to be 
extended to support dynamic datasets. This paper describes a few 
key additional requirements that result from having to support 
in-memory processing of data while updates proceed 
concurrently. The paper describes Real-time Analytics Foundation 
(RAF), an architecture to meet the new requirements. 
Performance of an early implementation of RAF is also described: 
for an unaudited TPC-H derived workload, RAF shows a node-
 to-node scaling ratio of 88% at 8 nodes, and for a query 
equivalent to Q6 in the TPC-H set, RAF is able to show 9x 
improvement over that of Hive-Hadoop. The paper also describes 
two RAF based solutions that are being put together by two 
independent software vendors in China. 
Index Terms— Big Data, Real-time, Low-latency, Analytics, 
Resilient Distributed Datasets, CRUD, Clustering. 
I. INTRODUCTION 
Operational analytics enable a company or an organization 
to analyze data from its day to day operations, including up to 
the minute transactions, so that it may act upon findings 
instantly. It is gaining momentum as institutions and 
individuals become pervasively data driven in all spheres of 
life. In addition to capitalizing upon well-catalogued historical 
knowledge, establishments are turning towards just-in-time 
analysis of information in motion that may just be seconds old, 
and not yet well categorized or linked to other data. A few 
examples of value from operational analytics follow: 
• GPS-based navigation equipped with a static information 
base of road networks is a great modern convenience. Add 
instantaneous analysis of traffic conditions and it can guide 
motorists away or out of traffic logjams, to reduce wasted 
energy, accidents, delays and emergencies.  
• A credit card company may use instantaneous correlation 
between a user’s transactions in order to detect and intercept 
suspicious transactions – such as, a transaction that breaks a 
pattern, or issues from a merchant that is too far away from 
the location of a recent transaction. 
• A metropolitan or regional power grid which processes 
millions of sensor readings every second to modulate power 
generation, perform load-balancing, direct repair actions, 
and take policy enforcement steps. In addition to triggering 
immediate reactive actions, the data may also be used in 
long range capacity planning. 
An essential feature in the above examples is the need to 
integrate new transactions into analysis results within a very 
short time—sometimes as short as a few tens of milliseconds. 
A second feature is the need to complete analysis queries 
quickly. Producing answers swiftly requires parallel execution 
of queries and agile sharing of information across parallel 
tasks. To keep pace with the growth of processing demand and 
that in the volume of content to be analyzed, organizations 
have increasingly embraced scale-out clusters (Hadoop[1], 
HBase[2], MPP databases[9], etc.) where more machines can 
be added readily in order to boost throughput and capacity. 
High latencies can make these clusters less suitable for 
operational analytics: transforming data between block format 
and in-memory formats, paging, and sharing via a distributed 
file system such as HDFS[4] prolong analysis. 
To slash latencies by orders of magnitude, Spark [16], HP 
Vertica[14], and several other initiatives [10],[15],[7] propose 
maintaining data in memory (instead of demand paging it from 
disk), noting that rising capacities and dropping DRAM costs 
have made in-memory solutions practical in recent years. This 
paper identifies additional capabilities that are needed to blend 
in-memory data processing between transactional and analytic 
activities into a seamless whole. The resilient distributed 
datasets (RDDs) of Spark[16] make in-memory solutions less 
failure prone. In an architecture that we call Real-time 
Analytics Foundation (RAF), we propose enhancing the RDD 
approach so that resiliency is blended with a few additional 
characteristics listed next: 
• Efficient allocation and control of memory resources 
• Resilient update of information at much finer resolution 
• Flexible and highly efficient concurrency control 
• Replication and partitioning of data transparent to clients 
Architecturally RAF elevates memory across an entire 
cluster to a first class storage entity and defines high level 
mechanisms by which applications on RAF can orchestrate 
distributed actions upon objects stored in cluster memory. By 
merging distributed memory and distributed computing, RAF 
delivers a platform on which applications designed for 
optimized in-memory execution can be distributed easily.  
Services that expose REST interfaces, Create-Retrieve-
 Update-Delete (CRUD) interfaces, SQL-like interfaces, or 
other object access methods for items which are in their in-
 memory formats can be created readily on top of RAF. To 
promote responsible and transparent use of memory, RAF opts 
to use a programming language such as C, C++, over mixed 
language environments in which garbage allocation is opaque 
as a general rule.  
The paper is organized as follows. Section 2 sets out the 
motivation, explaining the requirements and how current 
solutions compare in delivering low-latency analytics. Section 
3 then describes the overall architecture and implementation 
highlights that are the distinctive aspects of the solution. 
Performance of the current implementation is then brought out 
in section 4. Section 4 also describes two end user scenarios in 
which this approach is applied to meet the unique demands. 
Section 5 describes continuing and future work, and section 6 
concludes. 
II. MOTIVATION AND BACKGROUND 
Data has a lot of value when mined. By analyzing an end-
 user’s recent browsing history, an artificial intelligence system 
2013 IEEE International Conference on Big Data
 102978-1-4799-1293-3/13/$31.00 ©2013  IEEE 
such as GoogleNow can automatically determine viewing 
preferences and push selections at him/her without explicit 
requests being made. A manufacturer or a retailer can extract 
and correlate consumer information streaming in from 
shopping outlets and use it in order to group and price 
products for a mix of high turnover and profitability. Smart 
deployment of police and ambulance services based on an 
analysis of traffic patterns can save lives during emergencies. 
This understanding, that data has value, has led to the 
practice of mining it at the point of generation, for example, 
by subjecting it to streaming analytics. SQLstream[13] in 
effect runs queries on streams rather than on previously 
captured and ETL’ed data. However, as data continues to 
compound at brisk rates, institutions need to grapple with two 
broad demands – (a) accumulating, processing, synopsizing 
and utilizing information in a timely manner, and (b) storing 
the refined data resiliently, and keeping it accessible at high 
speed. Recognizing that large scale parallelism is critical and 
that the storage and computational capacities of single node 
multi-core systems quickly limit a solution, recent years have 
seen the emergence of softer-consistency, non-relational, 
cluster-based approaches for data processing described 
collectively as Big Data. The term Big Data itself is elastic 
and serves well as a description of the scale or volume of these 
solutions, but does not define a constraining principle for 
organizing storage or orchestrating processing. It is therefore 
useful to outline a few requirements for low-latency and high 
throughput analytics on datasets that are also subject to non-
 trivial volumes of updates.  
1. In-memory structures and storage: As the complexity of 
queries increases, it becomes necessary not just to keep 
larger amounts of data in memory for avoiding disk 
latencies, but also to use data processing techniques that 
avoid having to marshal pointer linked and deeply nested 
structures between one software module and another, 
particularly when the modules are sharing data on the same 
node. 
2. Resiliency: At the same time, it is obviously critical that data 
that must survive abrupt restarts of individual nodes is 
replicated across nodes and between storage tiers, in a timely 
fashion. 
3. Sharing data through memory:  Passing data by reference 
should be supported efficiently so that needless copying can 
be avoided. A module that produces data should be able to 
collaborate with a module that consumes data without the 
overhead of data copying, marshaling, and that of 
superfluous allocation or deallocation. Furthermore, it is 
important that modules that can form a chain of processing 
are able to operate on data flowing from a previous stage of 
processing into a next stage of processing while the output 
of the previous stage is in memory. 
4. Uniform interaction with storage: Some systems designed 
for in-memory analytics split transactional object 
management from that for the analytics subsystem. Most 
systems mandate differing data storage and processing 
formats.  Due to the resulting fragmentation of formats, 
developers and end users face the danger of being locked 
into specific approaches that later fail to meet shifting 
demands. At the same time, it is necessary to grapple with 
resilience questions that arise with distribution, and naming 
and storing of data that is not limited to a single “native” 
format. 
In a nutshell, solutions need to be interoperable and scalable, 
across multiple hardware platforms and software 
environments. The resulting impetus for OpenStack[17] has 
identified the need to move away from centralized databases 
and towards distributed object storage with object granular 
capabilities for locating, replicating, and migrating contents 
(see figure 1). To meet these objectives in concert with 
requirement #1, objects stored in memory should be 
interoperable, relocatable, replicable in the same way that 
objects are placed in a block storage medium. 
5. Minimizing memory recycling: Garbage collected language 
runtimes (such as Java) popular in many Big Data 
implementations liberate programmers from having to 
manage memory explicitly. However, the lack of direct 
control over memory deallocation can lead to performance 
overhead of garbage collection and cause problems such as 
long stalls during a stop-the-world garbage collection. More 
explicit memory management such as that in C and C++ is 
desirable from the standpoint of ensuring high performance. 
6. Efficient integration of CRUD: In an operational analytics 
solution, it is important to be able to make small updates on 
a continuous basis, than to have to buffer up many changes 
and risk a long latency and resource intensive merging of 
those changes into the source from time to time. We borrow 
the acronym CRUD (Create/ Retrieve/ Update/ Delete) from 
[3] to refer succinctly to two capabilities: (a) writers perform 
their updates atomically at a granularity consistent with an 
application’s data model, and (b) readers have a stable, 
consistent view of the dataset. To minimize overhead and 
maximize concurrency, it is essential to support efficient 
CRUD operations on memory as well as disk versions of a 
dataset which is simultaneously accessed for analytics 
queries. In effect, without burdening the application logic, 
the datastore should provide a logically versioned view in 
which writers update contents granularity finer than that of a 
full dataset. 
7. Synchronizing efficiently: Having to wait for milliseconds in 
order to achieve serialized execution across a cluster negates 
much of the benefit of in-memory processing. If 
synchronization is lock-based, then it requires distributed 
deadlock detection and guarding against node failures. While 
partitioning of data across nodes can mitigate distributed 
synchronization, local synchronization within a node can 
also be a serious performance inhibitor. Given the need for 
large scale concurrency—within and across nodes, it is 
essential to choose processing approaches that are implicitly 
data parallel. 
8. Searching Efficiently: Many NoSQL solutions, particularly 
key-value stores that are available today sort records for 
 
Figure 1: reproduced from [17] 
103
efficient searches based on record keys. E
 also needs to be available across non-key a
 Many a distributed data processing alterna
 in the past decade to displace some pr
 database based solution. Several in-memo
 systems such as [6][10][16] have similarly 
alternatives or as complementary approach
 relational databases. Let us briefly note the 
areas of improvement, among these four
 solutions that are very popular today— S
 Coherence[6], Redis[10], and Memcached[5
 Spark introduces an innovative distributed 
that is particularly well suited to large scale
 A complex analysis task is decomposed in
 transformations, scheduled such that datasets
 transformation are used in another transform
 need to force the datasets out of memory in
 however dataset oriented in that it is not wel
 on a single record based on a record’s key
 small granularity updates into transformatio
 popular due to its performance and ANSI C
 rich data types and consistency guaran
 distributed framework for assembling c
 computations. A single threaded mode
 aggregation performance. Oracle Coheren
 commercial data grid solution with high 
scalability, rich data processing model, 
transactions. That it is implemented in Ja
 garbage collection inefficiencies. And its q
 constrained to filtering and to computing agg
 MAX, MIN and several others. Memca
 stability and simplicity, is widely used for o
 that simplicity also precludes creating co
 software – which limits the use of Memcach
 backend assist for use in accelerating web 
section discusses how to build upon the stre
 some of the soft spots of the current popular
 to better address the needs outlined in this se
 III. ARCHITECTURAL CONCEPTS AND IMPL
 HIGHLIGHTS 
The framework targets common user sc
 complex queries need to be conducted at v
 Information upon which the queries operate
 on some storage medium or generated dyna
 as a result of ongoing transactional activiti
 we translate the eight requirements articulate
 five design elements: (a) C and C++ based
 efficient sharing of data through memory (b
 of new content, (c) Efficient concurrency
 information in motion, and (e) Fast, general, 
Transactional and analytic applications ru
 end-user clients interact with these services 
that abstract away the details of how objec
 cached, stored, replicated, partitioned, and
 clients.  For these applications, the RAF prov
 computing environment. The distribu
 environment is integrated with a memory-ce
 storage system; that is, one application can p
 to another in order to share data in memor
 without first having to serialize or deserializ
 to file systems or transmitting via network co
 To outline the RAF framework, it is usef
 concepts, Delegate, Filter, RDD, and Transf
 this with respect to figure 2. In figure 2, new
 existing data occur as a result of clients con
 fficient searching 
ttributes. 
tive has emerged 
evious relational 
ry non-relational 
emerged either as 
es to in-memory 
strengths, and the 
 in-memory data 
park [16], Oracle 
].  
computing model 
 batch processing. 
to a sequence of 
 generated by one 
ation with rarely a 
to disks. Spark is 
l-suited to operate 
, or to weave in 
ns. Redis is very 
 implementation, 
tees. It lacks a 
omplex, parallel 
l constrains its 
ce is a mature 
availability, high 
and support for 
va exposes it to 
uery capability is 
regations such as 
ched, due to its 
bject caching; but 
mplex enterprise 
ed primarily as a 
servers. The next 
ngths and address 
 solutions in order 
ction. 
EMENTATION 
enarios in which 
ery low latencies. 
 may be available 
mically on the fly 
es. In this section 
d in section 2 into 
 programming for 
) Resilient storing 
, (d) Processing 
ad-hoc searches. 
n as services, and 
--using interfaces 
ts are distributed, 
 so on, from the 
ides a distributed 
ted computing 
ntric, distributed, 
ass a data handle 
y with the latter, 
e data for storing 
nnections. 
ul to discuss four 
ormations. We do 
 data or updates to 
tacting a Storage 
Service. Typically the updates
 and, sometimes, the updates m
 volatile media. 
• An RDD is an acronym for 
introduced in [16]. RAF uses
 identical to those in [16
 information in memory of o
 with assurance that in cas
 machines, an RDD can be re
 repeating well-formed operat
 • A Transformation is an ope
 RDDs in order to generate a 
be transient.  The concept o
 that in [18]; and RAF 
transformations as join, map, 
• A Filter is a particular type o
 [18] produces, out of an inp
 whose contents satisfy a spec
 • In order to facilitate efficien
 non-volatile storage between
 updates and applications that 
a level of indirection to th
 obtained through a set of w
 stable view of content to 
content mutates.  This is desc
 Thus in figure 2 raw inform
 into in-memory storage from 
by data producing clients, is fi
 of Delegate modules, and then 
RDDs (such as RDD1 and R
 Transformations then produce 
reiterate, data sharing is thr
 opposed to a distributed file 
supplemented by a flexible
 applications interact with oth
 (i.e., using RPC), or asynchr
 High performance analytics a
 coupled (and therefore efficient
 A. Efficient Storage Sharing us
 A shared CRUD[3] data s
 Figure 2: RAF Fram
  will happen first in memory, 
ay become propagated to non-
 a Resilient Distributed Dataset, 
 the RDD construct for reasons 
], viz., resilient storing of 
ne or more machines, together 
e of failure of one or more 
constructed from precursors by 
ions that produced it.  
ration applied to one or more 
new dataset or a result that may 
f transformations is similar to 
transformations include such 
union, etc. 
f transformation. A filter, as in 
ut dataset, a resulting dataset 
ified condition. 
t sharing of both volatile and 
 applications that need to make 
need to read the data, we create 
e storage. This indirection is 
rapper functions that ensure a 
read-only consumers, even as 
ribed further in ?3.1. 
ation – whether it’s preloaded 
disks or dynamically generated 
rst made stable through the use 
filtered in order to create initial 
DD2 in the figure). Chains of 
the desired analytics results. To 
ough memory instead of as 
system such as HDFS. This is 
 execution model in which 
er applications synchronously 
onously via queued messages. 
pplications can remain loosely 
) by using message queues.  
ing DELEGATE 
ource (§2) furnishes different 
 
 
ework and Concepts
 104
information to queries at different point
 introduces an impedance mismatch for softw
 on RDDs since a typical storage interface 
CRUD actions does not have the same brea
 that an RDD possesses. This difficulty is rem
 inserting a bridge module named DELEGAT
 DELEGATE is to create a version of th
 particular time, and present it as a memo
 Beneath the DELEGATE module, CRUD
 proceed directly against the mutable datasto
 operations proceed against the RDD that is 
the view produced by DELEGATE, some
 added or deleted without affecting the analys
 For efficiency, the DELEGATE module e
 write through pointer indirection. DEL
 provides C/C++ wrappers for objects in 
persistent or volatile, and using these wr
 creating operation can share memory effic
 with concurrent transactional operations tha
 composition of a dataset. While embracin
 RDDs from [16], RAF deviates in implem
 from [16] in order to carry out explicit 
sharing of RDDs and facilitating coor
 DELEGATE modules, ensuring that data
 efficiently accessible from CPUs for analysis
 B. Memory-centric Storage Operation 
For driving very low latency analytics app
 maintain data in memory, and share access t
 machines with large memory sizes hav
 common in recent years, accumulating an
 large amounts of data has become practica
 softer and application managed consistency 
ACID semantics in the data layer itself, 
applications to spread data among a cluster
 machines to further expand solution sizes. 
control which data needs to be flushed 
medium and when; and this is achieved in R
 disk or file system plugins. 
3.2.1 Reliability 
It is necessary to address the concern that 
memory) can compromise reliability and ava
 is recoverable by design. Thus it is the data s
 an initial RDD is obtained (via Delegate, as 
that needs to be recoverable even if the 
committing changes to a non-volatile copy o
 as to a disk is non-synchronous. This is achie
 Applications run on a cluster and can qu
 know how many nodes are present in the
 application can write an update to more than
 the cluster size drops to 1, can record change
 medium. A storage structure (such as a fil
 uniformly sized partitions with records d
 partitions through hashing of record key
 typically copied to at least one other back 
storage layer maintains the mapping betwe
 and the additional nodes where that partition
 maintained. 
• When a node N is about to leave a cluster,
 cluster abruptly, the storage layer discove
 and backup partitions are on N. For eac
 partitions, it selects one out of its back
 marks it as primary; and creates a new ba
 yet another node in the cluster.  For any ba
 N, RAF creates a new instance of the ba
 some other node.  
s in time. This 
are that operates 
for implementing 
dth of operations 
oved in RAF by 
E. The purpose of 
e datastore at a 
ry resident RDD. 
 operations can 
re. While analysis 
created on top of 
 entities may be 
is. 
mploys copy-on-
 EGATE module 
storage --whether 
appers, an RDD 
iently but safely 
t may change the 
g the concept of 
entation of RDDs 
instantiation and 
dination through 
 is directly and 
 computations. 
lications on RAF 
o it efficiently. As 
e become more 
d operating upon 
l. With a shift to 
--away from strict 
it is possible for 
 of large memory 
Applications also 
to a non-volatile 
AF by means of 
volatility (of main 
ilability. An RDD 
ource from which 
described in ?3.1) 
primary mode of 
f the source, such 
ved as follows. 
ery the system to 
 cluster. Thus an 
 one node; and if 
s to a non-volatile 
e) is divided into 
istributed among 
s. A partition is 
up node, and the 
en each partition 
’s backup copy is 
 or if it leaves the 
rs which primary 
h of N’s primary 
up partitions and 
ckup partition on 
ckup partition on 
ckup partition on 
• If a new node X joins the
 partitions across all nodes inc
 Updates to a partition are 
partition’s replicas through 
burdening the application log
 control the replica count as w
 write is required to be synchron
 C. Data and Storage Types 
In this subsection we brief
 aspects of data layout and dis
 RAF implements data types,
 describes two concepts that re
 across nodes. 
3.3.1 Structured Data 
Structured data is expressed 
illustrated in Figure 3. On the r
 shows two types: CustomerKe
 in [8] this structure is describe
 order to yield data accessor m
 parsing the necessary fields 
addition, a metadata structure s
 in JSON format defines tuples,
 the names (analogous to names
 the key attributes, (3) non-key 
store type, which we will desc
 subsection. In implementing th
 RAF supports both memor
 organization of structured data
 In particular, the responsibility
 it is in memory, on disk, loca
 applications and absorbed int
 Applications supply the metada
 file, and the RAF framewo
 necessary internal structures a
 transmitting or receiving data a
 While the use of Protobuf as
 among distributed entities is w
 RAF is particularly advantage
 is built on the proposition of
 from CRUD actions as quick
 This is best seen by conside
 alternatives for database reco
 updates would require extensiv
 structures are not well suited fo
 3.3.2 Storage Types: 
RAF provides two attributes
 and partitioned store. A datasto
 and it may be optionally replica
 and is updated rarely but is rea
 Figure 3: Type a
  cluster, then RAF rebalances 
luding X, in background. 
automatically streamed to a 
a network interface, without 
ic. Applications can and do 
ell as whether replication of a 
ous.  
ly describe application visible 
tribution. ?3.3.1 explains how 
 and ?3.3.2 on storage types 
late to how data is distributed 
by a two part definition that is 
ight is a protocol buffer [8] that 
y, and Customer. As described 
d in a file which is compiled in 
ethods that are very efficient in 
from a message stream. In 
hown to the left in figure 3 also 
 by describing four aspects: (1) 
 of relations in a database), (2) 
(or “value”) attributes, and (4) a 
ribe a little further in the next 
is type of extensible structure, 
y and disk based, flexible 
, with efficient serialization [8]. 
 for locating the data – whether 
l, or remote is offloaded from 
o the RAF platform modules. 
ta and protobuf definitions in a 
rk automatically creates the 
nd serialization necessary for 
cross node boundaries. 
 an efficient exchange format 
ell understood, its use in the 
ous because real-time analytics 
 propagating updates resulting 
ly and efficiently as possible. 
ring row- and column- based 
rds: in row-based structures, 
e parsing, while column-based 
r high rates of updates. 
 for datastores: replicated store 
re may be single or partitioned, 
ted.  When a datastore is small, 
d frequently, it is advantageous 
 
nd Store Metadata 
105
to keep it all of it as one single extent (p
 memory) at one home location, but make on
 of it available on other nodes. This is captu
 Figure 4. If on the other hand, it is written 
(case (c) in Figure 4), then it makes sense n
 single extent that hosts it, widely. For high 
one copy of it would be advisable on a remot
 If a datastore is large in size (case (b) in Fi
 divided into multiple extents or partitions; an
 given at least one backup copy on some othe
 its home location. 
When there are backup copies, or if there 
partition, a master copy of any given partitio
 node. That node is the one which storage ope
 operations) contact and the master copy ref
 from the update operation into remote copie
 accordance with application guided consis
 An application can specify to the RAF wheth
 through updates or wants remote copies p
 write-behind approach. Under write-thro
 operation returns control to application once
 all the copies; and under write-behind, the op
 soon as it completes locally on the mast
 changes are reflected to other nodes asynchro
 3.3.3 Storage Service Interface 
For CRUD operations, RAF furnishes both
 interface and a C++ language applicati
 interface. As described in ?3.3.1, protocol bu
 accessing data, which makes it easy
 programming interfaces in other language
 very simple grammar is used to write comma
    
COMMAND :== SVC_NAME OP_NAME PARAM E
 SVC_NAME :== ID 
OP_NAME :== ID  
PARAM :== JavaScript Object Notati
 EXA_PARAM :== ID ‘=’ PARAM 
ID :== [a-zA-Z0-9_-\.]+
    
The following, for example, creates a new 
in a business directory in a storage dataset na
  
Figure 4: Attributes of Stor
 referably also in 
e or more replicas 
red as case (a) in 
frequently enough 
ot to replicate the 
reliability, at least 
e node as backup. 
gure 4), then it is 
d each partition is 
r node other than 
are replicas of a 
n is kept on some 
rations (“CRUD” 
lects the changes 
s. This is done in 
tency philosophy. 
er it wants write-
 erformed using a 
ugh, the update 
 data is updated in 
eration returns as 
er copy, and the 
nously. 
 a command line 
on programming 
ffers are used for 
 to implement 
s. The following 
nd line scripts: 
XA_PARAM* ‘;’
 on (JSON)
 customer “Tom5” 
med “Customer”: 
 
D. Distributed Execution of An
 As described earlier in this s
 of resilient distributed datasets
 capitalize on memory resident
 And with the introduction o
 weaves in CRUD capable store
 RDDs can be derived -- just as
 as HDFS files can be used fo
 stages of transformation descri
 (DAG) is used, as in [16], yiel
 computation plan for analyzing
 to produce query results. By d
 multiple partitions each of w
 allows this approach to scale 
moving from a read-only to rea
 RAF adopts the SEDA [12] m
 composable parallelism. Effic
 dispatching of run-to-complet
 context switching for synchron
 is particularly synergistic since
 of wait-states that might otherw
 data into memory from disk-ba
 monitor over data partitions loc
 would operate on that data part
 core counts driving up the n
 within each node and with 
clusters becoming commonplac
 well positioned to match low
 with non-blocking, data flow fo
 3.4.1 Analytics Tasks Interface 
The storage service inter
 complemented by a service in
 can compose and execute a DA
 analytics tasks. A command ca
 designate each source operand
 and the keyword out_rdd to n
 An RDD may be described as
 and as the out_rdd of some o
 possible for an analytics clie
 operations through which desig
 The syntax of an analytic
 respects from that of a sto
 grammar for an analytics task,
 service operation, is the sam
 except that the JSON content i
 (i.e., by operation). Let us 
Following is a command which
 customer, and then applies a
 subset of customers who are in
 the resulting RDD bj_custom
 The following example further
 specified step by step and (b
 happen to share common int
 instead of regenerating the shar
 the following two independe
 average sales per customer for
 Beijing or Shanghai, (2) For c
  
age 
alytics Tasks 
ection RAF reuses the concept 
 (RDDs) from [16] in order to 
 data for low-latency analytics. 
f Delegate (?3.1), RAF also 
s as source datasets from which 
 [16] shows how datasets such 
r deriving RDDs.   Successive 
bed by a directed acyclic graph 
d a memory-based, low latency 
 data in mutable/CRUD storage 
ividing very large datasets into 
hich is backed-up by a copy 
and remain resilient in spite of 
d-write storage model. 
odel to achieve highly efficient, 
iency results from event-based 
ion methods which minimize 
ization. Memory-based storage 
 it drives down the probability 
ise result from having to page 
sed storage. Each node acts as a 
al to it, and each operation that 
ition is queued. With increasing 
umber of computing elements 
larger and larger node count 
e, this architectural approach is 
-latency in-memory execution 
rm of execution.  
face described in ?3.3.3 is 
terface through which clients 
G of operations to orchestrate 
n use the keyword in_rdd to 
 in a filter/transform operation, 
ame the result of the operation. 
 an in_rdd in one operation, 
ther operation, which makes it 
nt to construct a pipeline of 
nated flows of data occur.  
s command differs in minor 
rage service command. The 
 which we refer to as an RDD 
e as that described in ?3.3.3 
s different for each OP_NAME 
use an example to illustrate. 
 takes as input an RDD named 
 filtering operation to extract a 
 the city of Beijing, and names 
er: 
 shows (a) how DAGs can be 
) how independent tasks that 
ermediate datasets can reuse, 
ed datasets. Let us say we have 
nt queries – (1) : Compute 
 customers that are from either 
ustomers from Beijing, identify 
106
the subset of customers who have purchased
 and then for that subset, calculate the averag
 level depiction is as below: 
Computation Flow X: 
Computation Flow Y: 
Both flows take as input a sales r
 progressively compute the RDDs to arrive
 targets: the first flow is aimed at taking all c
 from either Beijing or Shanghai, and avera
 that slice of customers, while the secon
 calculate the average sales per repeat custom
 In RAF, the above computation could be orga
 Common Precursor Action (across X and Y, a
 Computation Flow X only: 
Computation Flow Y only: 
In order to specify the above comp
 applications would proceed to specify the
 computations to an in-memory RDD service,
   
a 
// ---  extract the dataset “customers from Beijing” --
 rdd.service create { “transform_type”:  ”filter”
 “in_rdd”: [{“rdd_name”: ”customer”}] ,   
 “out_rdd”: {“rdd_name”: ”bj_customer”}}
 transform={“op”: ”EQ”, “sub_exp”: [{“op”: ”
 “param”: ”c_city”},  {“op”: ”CONST”, “param
   
b 
// --- extract the dataset “customers from Shanghai”
 rdd.service create {“transform_type”:”filter”, 
“in_rdd”:[{“rdd_name”:”customer”}], 
“out_rdd”:{“rdd_name”: ”sh_customer”}} 
transform={“op”:”EQ”, “sub_exp” : [{“op”:”F
 “param”:”c_city”}, {“op”:”CONST”, “param”
   
c 
 //---  Union Beijing and Shanghai Customers --- 
rdd.service create {“transform_type”:”union”,
 “in_rdd”:[{“rdd_name”:”bj_customer”}, 
{“rdd_name”:”sh_customer}], “out_rdd”: 
{“rdd_name”:”bj_sh_customer”}}   
  
d 
//--- compute avg. sales across Beijing & Shanghai 
rdd.service action {“action_type”:”average”, 
“rdd_name”:”bj_sh_customer” }  action = 
{“expr”:{“op”:”FIELD”, “param”: ”sales_amo
   
e 
// --- compute average sales for repeat customers fr
 rdd.service action {“action_type”:”average”, 
“rdd_name”:”bj_customer”, }  action = 
Sales 
Reports Customers
 Custom
 from 
Beijing 
Shangh
 Customers 
from 
Beijing
 Customers 
from 
Shanghai
 Sales 
Reports Customers
 Repea
 custom
 from 
Beijing
 Customers 
from 
Beijing
 Sales 
Reports Customers
 Customers 
from 
Beijing
 Customers 
from 
Shanghai
 Customers
 from 
Beijing  or 
Shanghai
 Customers 
from 
Beijing
 Customers 
from 
Shanghai
 Repeat 
customers 
from 
Beijing
 Customers 
from 
Beijing
  more than once, 
e sales. The high 
eports file, and 
 at two different 
ustomers who are 
ging the sales in 
d flow aims to 
er in Beijing. 
nized as follows: 
bove): 
 
 
 
ositions flexibly, 
 following three 
 as follows: 
- 
 ,   
 
FIELD”, 
”: ”Beijing”}]}
  --- 
  
IELD”, 
:”Shanghai”}]}
  
customers --
 unt”}} 
om Beijing --
 {“expr”:{“op”:”FIELD”, “para
 All the above script comm
 ?3.4.1.The name for the analy
 Keywords create and action
 the above example rdd.servic
 RDD (tasks a, b, and c) or co
 operating on one or more exis
 one RDD to operate on in the 
e), it is permissible to drop th
 ?3.4.1. RDDs to operate on 
rdd_name in both types of op
 keyword to name an RDD ex
 application to avoid creating r
 suppose one process A exe
 computation flow X; that 
bj_customer and sh_custome
 executes the actions in comp
 repeating the computation of b
 the RDDs explicitly identified 
can avoid recreating bj_custo
 creating it and vice versa. 
We can represent the overall
 the DAG in Figure 5. Let us u
 the execution would occur usin
 5, each box is an RDD. 
creation/transformation steps,
 action steps. Thus bj_custome
 by filter transformer on the cu
 is created by a union transfor
 ers 
 or 
ai
 Average 
sales per 
customer
 t 
ers Average sales per 
customer
  Average 
sales per 
customer
 Average 
sales per 
customer
 Figu
 Figu
 Figu
 m”:” sales_amount”}}
 ands follow the grammar in 
tics service is rdd.service. 
 respectively denote whether in 
e is being asked to create an 
mpute a result (tasks d, e) by 
ting RDDs. When there is only 
case of an action (as in tasks d, 
e in_rdd keyword described in 
are identified by the keyword 
erations. The use of rdd_name 
plicitly in this way allows an 
edundant RDDs. For example, 
cutes the actions shown in 
process would create RDDs 
r. Another process B that 
utation flow Y would end up 
j_customer for its query. With 
by rdd_name keyword, flow Y 
mer if A has already begun 
 flow of both computations by 
se this diagram to discuss how 
g the RAF. In the DAG of Fig. 
Solid lines represent RDD 
 and dashed lines represent 
r and sh_customer are created 
stomer dataset, bj_sh_customer 
mer, and bj_customer_2 is the 
 
re 5 
 
re 6 
 
re 7 
107
result of filtering for repeat customers in bj_
 Averaging actions on bj_customer_2 and
 respectively yield the two results from two 
above, RDD bj_customer is used in both
 creation is shared (not repeated) in the execu
 Figures 6 and 7 show how partitions (?3.3
 customers were a very large dataset that is 
three nodes –shown respectively by blue rec
 1,2,3. The number of partitions, incidentally,
 user. Then, as shown in Figure 6, the crea
 RDDs— bj_customer and sh_customer c
 partition by partition; as would the creation 
bj_sh_customer. Thus data decomposition 
would be easily carried over into destina
 allows, as shown in Figure 7, keeping compu
 by maintaining corresponding partitions in
 where the semantics of an operation all
 example where such an optimization may no
 a join that requires cross-node communicati
 is made more efficient in RAF by replicatin
 rarely updated (?3.3.2) but which may be re
 operations such as joins. Finally, reliability i
 is obtained by virtue of RDDs being 
described in [16]. In RAF, the mainta
 partitions further improves availability of RD
 IV. RESULTS 
In this section we share the performance
 using unit tests that demonstrate outstanding
 decision support performance. We do this in
 in section 4.2, we outline two RAF-based 
established commercial software vendors, a
 performance, to portray how straight-forwa
 realistic operational analytics solutions atop R
 A. Unit Testing: 
We measured RAF on two fronts: h
 operations scale, which represents memor
 operations performance, and, how long 
complete a query to show distributed analy
 Table 1 has configuration data for the updat
 operations, we borrow SSB schema fro
 Benchmark [19] and utilize SSB-DBGEN to
 file. Then the update testing parses the data
 the records into RAF. The performance metr
 TPS, which is total record number divided 
time in second.  
Figure 8 shows consistent increase in the 
test as it distributes over multiple nodes; w
 scaling of 7.0x for an 8 node cluster co
 Xeon® E5-2680 processors.  In this test, fo
 (with asynchronous backup copying to 
obtained an average CPU utilization of abou
 being an update only test (and therefore ne
 the percent of time spent in operating syst
 which is reflective of the fast data capture
 storage capability of RAF. 
Server       Intel® Xeon® E5-2680 (8C, 2.7GHz
 1333, 10Gb NIC 
Network 10Gbps Switch 
Op. Sys. Red Hat Enterprise Linux 6.2,  64 bit
 Table 1 Configuration 
The second test uses a query that is log
 query 6 of the industry standard TPC-H ben
 for execution on top of RAF, and for executi
 Both the RAF test and Hive test are perform
 customer dataset. 
 bj_sh_customer 
queries. As noted 
 queries and its 
tion of the DAG.  
.2) work. Suppose 
partitioned across 
tangles numbered 
 is controllable by 
tion of derivative 
an also proceed 
of the union RDD 
in source RDDs 
tion RDDs. This 
tations node local 
 the same node 
ow. One counter 
t be applicable is 
on. However, this 
g RDDs that are 
ad many times in 
n such operations 
recomputable as 
ining of backup 
Ds. 
 readout for RAF, 
 transactional and 
 section 4.1. Then 
applications from 
nd describe their 
rd it is to create 
AF. 
ow well update 
y-centric storage 
does it take to 
tics performance. 
e test. For update 
m Star Schema 
ol to generate data 
 file and insert all 
ic is measured by 
by total execution 
throughput of this 
ith an aggregate 
nsisting of Intel® 
r the 1 node test 
other nodes) we 
t 80%. Despite it 
twork intensive), 
em was only 7%, 
, distribution, and 
), 128GB DDR3-
  
ically identical to 
chmark, rewritten 
on on top of Hive. 
ed on an 8 node 
Intel® Xeon® E5-2680 cluster
 120G. As shown in Figure 9
 performance of Hive on 1st Ite
 and subsequent iterations of th
 RAF explicitly uses memory a
 this query drops to 150ms bu
 manner. These unit testing res
 memory distributed processing
 substantiated further by m
 Currently we are building more
 high concurrency operational a
 B. Solution-level Implementatio
 We describe in sections 4.2.1
 developed by software ve
 performance. They each requir
 data at the same time that the i
 real time. Both sections describ
 the performance obtained by th
 4.2.1 Telecommunications Subs
 RAF is used by an ISV (
 whose customers are telecom
 The customers have hundreds
 “Unified Service Platform” 
conducts subscriber transaction
 service transactions. The typ
 subscriber pushing fresh credi
 account, prepaying or paying o
 of products. A subscriber belon
 a telecommunications service p
 Figure 8: Scalabi
 Figure 9: Latency re
  and the dataset size used is 
, RAF achieves nearly 9X the 
ration. We do not compare 2nd 
is test with Hive because, since 
s storage, its response time for 
t Hive cannot benefit in like 
ults show the advantage of in-
  oriented design of RAF, to be 
ore sophisticated workloads. 
 TPC-H derived queries,  and a 
nalytics workload. 
n and Testing: 
 and 4.2.2 two solutions being 
ndors and initial solution 
e storing a large volume of new 
nformation must be analyzed in 
e the problem being solved and 
e solution in current testing. 
criber Management 
independent software vendor) 
munications service providers. 
 of millions of subscribers. A 
(USP) provided by the ISV 
s, many of which may be self-
 ical scenario is comprises a 
ts into his/her mobile services 
n demand for a broad spectrum 
gs to one province company of 
rovider, but he/she can use pre-
 lity Testing Result
 lative to Hive/HDFS
 108
paid cards issued by province companies that are different 
from the one to which he/she belongs. USP performs 
transactions, routes requests and responses back and forth 
among participating IT systems of province companies, logs 
transaction histories, and carries out desired analytics against 
stored data or in-progress transactions.  
For one specific customer (telecommunications provider), 
there are more that 300 million subscribers and USP 
transactions have a volume of about 10 million per day, with a 
peak volume of 1000 transactions/second. The customer has 
requested the ISV to furnish USP with two types of business 
analytics: drill-downs of different categories of transactions 
according to subscribers and services consumed and real-time 
detection and flagging of suspicious transactions. One 
particular type of suspicious transaction occurs when 
someone’s account is hacked or a phishing attack 
compromises a subscriber’s secret card number; and is 
detected by monitoring transactions for evidence of abnormal 
levels of activity within a short time period. Previously USP 
was implemented on top of a commercial RDBMS, and on a 
40 core Intel® Xeon® server (E7-4870) platform, yielded an 
average transaction time of 3s, which could not meet the 
deployment goal. Now, with RAF, on a 5 node Intel® Xeon® 
E5-2680 cluster (16 cores/node), the data resulting from credit 
card activity is funneled into in-memory data stores for inline 
analysis for the transactions. As a result, transaction times 
have been reduced ten-fold to 300 ms. The new solution has 
also created significant head room for running complex ad-hoc 
queries for obtaining detailed drill-downs and we are in the 
process of building a systematic solution level workload to 
benchmark the ad-hoc query performance. 
4.2.2 Safe City Solution 
The second solution under development targets license plate 
crime in China. To avoid steep expenses like road tax, 
mandatory insurance, etc., unscrupulous drivers can (and do) 
create fake license plates. One ISV we are working with 
addresses this solution by capturing images of vehicles and 
automatically tying license plate numbers with vehicle 
descriptions, and then detecting duplications using offline, 
batch analysis. Since it is hard for the police to be effective 
with long latency results, the ISV is shifting to using RAF to 
create a real-time detection system. Each vehicle captured by a 
distributed sensor feed is tracked, by funneling data into an in-
 memory RAF cluster, which can also perform the necessary 
checks for duplicated license plate numbers for discrepancies 
between registration information and sensor data about its 
color and make. Recent travel history can also reveal whether 
a license plate is suspicious because the vehicle it is on has 
traveled too far too soon. A variety of common searches and 
aggregations also become possible on the data feeds that have 
recently arrived from the sensors. In current testing, a 5 node 
Intel® Xeon® E5-2680 cluster handles data feeds from 600+ 
video cameras in less than a second, exceeding ISV 
requirement. 
V. CONTINUING AND FUTURE WORK 
Motivated by the high degree of familiarity that many 
developers have with database interfaces, we are incrementally 
introducing SQL-92/JDBC/ODBC like interfaces on top of 
RAF. A number of optimizations are also being added; these 
include: (1) application requested indexing, to accelerate 
searches; (2) blending in column-store capabilities where 
appropriate (for example, for rarely-written data), and (3) 
compression, in order to reduce data transported between nodes. 
VI. CONCLUSION 
This paper presents RAF, an architectural approach that 
meshes memory-centric non-relational query processing for 
low latency analytics with memory-centric update processing 
to accommodate high volumes of updates and to surface those 
updates for inclusion into analytics. This blending is kept 
efficient by merging memory management between the two 
spheres of operation by using a transparent versioning 
capability that we term Delegate, which participates as a 
special type of content transformer in a hierarchy of RDD 
transformations. In RAF, protocol buffers are used to obtain 
data abstraction and efficient conveyance among applications, 
providing applications with a high degree of independence in 
location, representation, and transmission of data. The use of 
protocol buffers is particularly valuable as it removes the need 
for producers and consumers to coordinate explicitly in 
partitioning, tiering, caching, or replication of information that 
may be arriving at high velocities. These improvements for 
distributing a memory-based storage service are combined with 
a message queues based execution partitioning mechanism in 
which each node acts as a local monitor over its data partitions. 
A light-weight but expressive interface makes it easy for RAF 
services to map various transformations that need to occur in 
the course of execution of a query into the data flows that need 
to occur among the distributed execution agents through 
message queues, thus hiding all of the plumbing from the 
services that need to use RAF as a single seamless platform for 
modifying as well as querying data in pooled memory and 
aggregate computing capacity of a cluster of machines. Using 
unit tests we show high cluster scaling capability for 
transactions, an order of magnitude latency improvement for 
query processing. The paper also describes two real-world 
usage scenarios in which RAF is being used to create high 
throughput operational analytics solutions. With rapidly 
increasing memory capacities and large, open-stack based 
clusters, RAF provides a collection of architectural techniques 
for rapid integration of real-time analytics solutions. 
REFERENCES 
[1] Apache Hadoop: http://hadoop.apache.org/ 
[2] Apache HBase: http://hbase.apache.org 
[3] H. Kilov, “From semantic to object-oriented data modeling”, 
Proc. 1st Intl. Conf. on Systems Integration,” 1990. 
[4] K. Shvachko, H.Kuang, S. Radia, R. Chansler. “The Hadoop 
Distributed File System”, Proc. MSST2010, May 2010. 
[5] Memcached: http://www.memcached.org/ 
[6] Oracle Coherence: http://www.oracle.com/technetwork/ middle 
ware/ coherence/ 
[7] H. Plattner, A. Zeier, In-Memory Data Management. 
[8] Protobuf: http://code.google.com/p/protobuf/ 
[9] B. Ramesh, T. Kraus, T. Walter, Optimization of SQL queries 
involving aggregate expressions using a plurality of local and 
global aggregation operations: U.S. Patent 5884299. 
[10] Redis: http://www.redis.io/ 
[11] SAP HANA: http://www.saphana.com/ 
[12] SEDA: http://www.eecs.harvard.edu/~mdw/proj/seda/ 
[13] SQLStream:  http://www.sqlstream.com/ 
[14] Vertica: http://www.vertica.com/ 
[15] VoltDB: http://www.voltdb.com 
[16] M. Zaharia, et al., Resilient Distributed Datasets: A Fault-
 Tolerant Abstraction for In-Memory Cluster Computing. NSDI 
2012. 
[17] Piatt, B., “Openstack Overview”, http://ww.omg.org/news /mee 
tings /tc /ca-10 /special-events/pdf /5-3_Piatt.pdf 
[18] Spark Programming Guide:  http://spark-project.org/ docs/ 
latest/scala-programming-guide.html  
[19] StarxSchemaxBenchmark:xwww.cs.umb.edu/~poneil 
/StarSchemaB.PDF 
109

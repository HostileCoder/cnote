GUIdiff – A Regression Testing Tool for Graphical
 User Interfaces
 Sebastian Bauersfeld
 Software Testing and Quality Group
 Universitat Polite`cnica de Vale`ncia
 Valencia, Spain
 Email: sbauersfeld@pros.upv.es
 Abstract—Due to the rise of tablets and smart phones and
 their impact on everyday life, robust and high-quality Graphical
 User Interfaces (GUIs) are becoming more and more important.
 Unfortunately, testing these GUIs still remains a big challenge
 with the current industrial tools, which only cater to manual
 testing practices and provide limited oracle functionalities such as
 screenshot comparison. These tools often result in large amounts
 of manual labor and thus increase cost. We propose a new GUI
 regression testing tool called GUIdiff, which works similar to diff
 tools for text data. It executes two different versions of a System
 Under Test (SUT) side by side, compares the GUI states against
 each other and presents the list of the detected deviations to the
 tester. The tool is semi-automatic in the sense that it finds the
 differences completely automatic and that the tester labels them
 as faults or false positives.
 I. INTRODUCTION
 Graphical User Interfaces (GUIs) represent the main con-
 nection point between a software’s components and its end
 users and can be found in almost all modern applications. This
 makes them attractive for testers, since testing at the GUI level
 means testing from the user’s perspective and is thus the ulti-
 mate way of verifying a program’s correct behavior. However,
 GUIs are often large and difficult to access programmatically,
 which poses great challenges for an application’s testability.
 Capture and Replay (CR) tools promise to automate testing at
 the GUI level. However, they have always been controversial,
 since they require a great amount of manual effort on the
 part of the testers [1], who need to record and maintain input
 sequences. Especially in the context of frequently changing
 GUIs, CR tools generate high maintenance costs, due to fragile
 input sequences.
 In an earlier work [2] we presented GUITest, a Java library
 which allows to write robustness tests for complex graphical
 applications. Although these tests were fully automatic, they
 lacked a fine-grained oracle and consequently only detected
 severe faults, such as crashes. In this work we build upon
 our library, but set the focus on regression testing. This gives
 us more powerful oracles, since we can compare two SUT
 versions against each other in order to detect anomalous
 behavior. The idea is to run the two versions in parallel and
 report the differences in the observed GUI states back to
 the tester. As we described in [2], even complex GUIs can
 be executed fully automatic. The general idea is to capture
 GUI state information (obtained from the operating system’s
 accessibility API) in widget trees (Figure 1) and automatically
 derive possible actions for each GUI state. By iteratively
 selecting and executing particular actions, it is possible to
 automatically walk through the GUI (Figure 2). The idea
 type: TButton
 ...
 rect: [15, 25, 65, 55]
 hasFocus: true
 enabled: false
 yp
 title: "Button"
 Desktop
 Window
 Button Text SliderMenu
 MI MI MI MI
 type: TMenuItem
 ...
 title: "File"
 ABC
 Derive
 Actions
 Fig. 1. GUIdiff uses the operating system’s accessibility API and represents
 the current state of a GUI as a widget tree whose nodes refer to the visible
 controls and are annotated with their property values. It uses the tree to derive
 and execute possible actions (green).
 Desktop Desktop
 Different...
 Button SliderMenu
 File Edit View
 Diff.
 Text
 ?
 Example
 Button Some Text Menu
 File Edit View Help
 Slider
 1 2
 Fig. 3. Detecting differences in the GUI states of two SUT versions.
 now, is to run two versions of the same application side by
 side, execute the same actions (if possible) and observe the
 differences in the widget trees of their states. This way of
 comparing GUI states is more robust than the comparison
 between two screenshots, since, for example, the position of
 controls does not matter. It will also allow us to only compare
 the properties of the same controls against each other and
 ignore newly introduced controls in the more recent version
 of the SUT. We expect that this will reduce false positives.
 2013 IEEE Sixth International Conference on Software Testing, Verification and Validation
 978-0-7695-4968-2/13 $26.00 © 2013 IEEE
 DOI 10.1109/ICST.2013.84
 499
a) b) c) d) e)
 Fig. 2. Automatic sequence generation: In each state we derive possible actions, select one, execute it and get to the next state.
 II. CHALLENGES
 In order to execute the same actions on the two SUT
 versions and in order to detect differences, one needs to first
 relate identical controls in the different states to each other. A
 similar problem is found in diff tools, which are often used by
 programmers to detect changes in source files. The diff tool
 needs to align the text such that equivalent lines correspond to
 each other, a problem which is known as sequence alignment.
 However, in our case we need to align trees in order to find
 which controls are identical. There has been done extensive
 research on tree alignment algorithms [7] which we will make
 use of. Once we are able to align the widget trees of two GUI
 states, we can a) execute the same actions on identical control
 pairs (which allows us to execute the two SUT versions in
 parallel) and b) compare the properties of the controls (title,
 color, numeric values, enabled / disabled, ...) against each other
 in order to detect and report differences. Figure 3 shows the
 initial states of two versions of the same SUT and their widget
 trees. The alignment shows which controls correspond with
 each other. By comparing their property values, we can find
 and mark the differences, so that the tester can spot them easily.
 Tree alignment algorithms can be computationally expen-
 sive, since the ones which respect hierarchy often exhibit
 exponential runtime complexity [7]. Despite the fact that even
 widget trees for complex applications usually do not contain
 much more than a few hundred controls, this could slow down
 the execution of the test. Therefore, we will have to find a
 trade-off between alignment accuracy and performance.
 Another challenge might be the amount of detected differ-
 ences. As shown in Figure 3, we will present the differences
 in a graphical way, so that they are easy to spot. How-
 ever, excessive amounts of false positives (purposely removed
 controls / deliberately changed properties) might cause the
 tester to get tired of labeling. Therefore, we will strive to list
 the most “suspicious” differences first. This could be done
 such that, whenever the tester labels a difference as false
 positive, GUIdiff re-sorts the list of differences to penalize
 similar occurrences. Another option would be to provide a
 query language that allows to express interests (“Show only
 differences in dialog X!”). Query languages such as SQL or
 Prolog might be suitable for this task.
 III. RELATED WORK
 A large part of the available GUI testing tools falls into
 the capture and replay (CR) or scripting categories. Popular
 commercial and open source tools are for example eggPlant,
 monkeyrunner for Android and Selenium. As mentioned ear-
 lier, these tools often induce a lot of manual labor, since broken
 test cases require frequent maintenance.
 There have been interesting approaches to the automatic
 generation of test cases for GUIs, among which the GUITAR
 framework [3]. Their idea is to walk through the GUI and to
 automatically generate an event flow graph, from which they
 derive test cases by applying several coverage criteria. Other
 automatic test generation approaches have been developed by
 Amalfitano et al. [4] who perform crash-testing on Android
 mobile apps, Marchetto and Tonella [6] who generate test
 suites for AJAX applications using metaheuristic algorithms
 and Artzi et al. [5] who use feedback-directed random test
 case generation for JavaScript web applications.
 IV. CONCLUSION
 We presented an approach for regression testing of appli-
 cations with GUIs, which uses side by side testing of two
 SUT versions to spot differences in the GUI. Our approach
 is orthogonal to existing techniques, which focus on test case
 generation. We are interested in the power of the oracle that
 we obtain by comparing two SUT versions. Currently we walk
 through the GUI at random, but our approach might benefit
 from using one of the abovementioned techniques. We are
 currently implementing the GUIdiff tool for MacOSX and
 Windows and are looking forward to present first results soon.
 ACKNOWLEDGMENT
 This work is supported by EU grant ICT-257574 (FITTEST).
 REFERENCES
 [1] A. M. Memon: A Comprehensive Framework for Testing Graphical User
 Interfaces. PhD thesis, University of Pittsburgh, 2001.
 [2] S. Bauersfeld, T. E. J. Vos: GUITest: A Java Library for Fully Automated
 GUI Robustness Testing. In ASE, 2012.
 [3] S. Huang, M. B. Cohen, A. M. Memon: Repairing GUI Test Suites using
 a Genetic Algorithm. In ICST, 2010.
 [4] D. Amalfitano, A.R. Fasolino, P. Tramontana: A GUI Crawling-Based
 Technique for Android Mobile Application Testing. In ICST, 2011.
 [5] S. Artzi, J. Dolby, S.H. Jensen, A. Møller, F. Tip: A Framework for
 Automated Testing of Javascript Web Applications. In ICSE 2011.
 [6] A. Marchetto, P. Tonella: Using search-based algorithms for Ajax event
 sequence generation during testing. Empirical Softw. Engg., 2011
 [7] P. Bille: A survey on tree edit distance and related problems. Theoretical
 Computer Science, Volume 337, Issues 1-3, 9 June 2005, Pages 217-239
 500

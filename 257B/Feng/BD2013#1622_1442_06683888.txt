ADIOS Visualization Schema:  
A First Step towards Improving Interdisciplinary Collaboration in High Performance 
Computing  
Roselyne Tchoua ?, Jong Choi ?, Scott Klasky ?, Qing Liu ?, Jeremy Logan †, Kenneth Moreland ?, Jingqing Mu †,
 Manish Parashar |, Norbert Podhorszki ?, David Pugmire ?, Matthew Wolf ? 
? Oak Ridge National Laboratories, Oak Ridge, Tennessee 
† University of Tennessee, Knoxville, Tennessee 
? Sandia National Laboratories, Albuquerque, New Mexico 
| Rutgers, The State University of New Jersey, Piscataway, NJ  
? Georgia Institute of Technology, Atlanta Georgia 
Abstract—Scientific communities have benefitted from a 
significant increase of available computing and storage resources 
in the last few decades. For science projects that have access to 
leadership scale computing resources, the capacity to produce 
data has been growing exponentially. Teams working on such 
projects must now include, in addition to the traditional 
application scientists, experts in various disciplines including 
applied mathematicians for development of algorithms, 
visualization specialists for large data, and I/O specialists.
 Sharing of knowledge and data is becoming a requirement for 
scientific discovery; providing useful mechanisms to facilitate 
this sharing is a key challenge for e-Science. Our hypothesis is 
that in order to decrease the time to solution for application 
scientists we need to lower the barrier of entry into related 
computing fields. We aim at improving users’ experience when 
interacting with a vast software ecosystem and/or huge amount 
of data, while maintaining focus on their primary research field. 
In this context we present our approach to bridge the gap 
between the application scientists and the visualization experts 
through a visualization schema as a first step and proof of 
concept for a new way to look at interdisciplinary collaboration 
among scientists dealing with big data. The key to our approach 
is recognizing that our users are scientists who mostly work as 
islands. They tend to work in very specialized environment but 
occasionally have to collaborate with other researchers in order 
to take full advantage of computing innovations and get insight 
from big data. We present an example of identifying the 
connecting elements between one of such relationships and offer 
a liaison schema to facilitate their collaboration.
 Keywords—collaboration; schema; visualization; simulation;
 experiments 
I. INTRODUCTION 
Collaboration between scientists in leadership-scale science 
has grown to include a diverse collection of scientists as the 
computing and storage resources have increased, thus allowing 
researchers to tackled larger, more complex physical problems.
 Progress in technology and computing has also increased the 
amount of data acquired and analyzed from sensors, medical 
equipment or larger instruments such as the neutron beams at 
the Spallation Neutron Source at Oak Ridge National 
Laboratory or the Tokamak Fusion Test Reactor at Princeton 
Plasma Physics Laboratory. Our own experience in the field of 
fusion, during the CPES project [1], is an example of such 
complex collaboration between scientists from various fields 
and institutions with the common goal of successfully running, 
analyzing and getting knowledge from fusion simulations. Our 
experience is not uncommon across universities and national 
laboratories. The Scientific Discovery through Advanced 
Computing (SciDAC) Office of Science program’s mission is 
to accelerate progress in scientific computing in order to 
deliver breakthroughs in various scientific topics. It does so by 
partnering with experts from national laboratories and 
universities to leverage a wide range of expertise and ensure 
that results from this effort benefit the wider research 
community. In this context we have developed an end-to-end 
framework to support collaboration among groups of scientists 
working around fusion simulations [2]. We describe our work 
in more detail including lessons learned from our experience 
that lead to a refreshed approach to collaboration around e-
 Science data.  
We start by exploring the related work and the current state 
of scientific collaboration. We then make a case for a 
decoupled collaboration in e-Science by illustrating common 
scenarios in leadership scale computing. Finally, we focus on 
the relationship between the application scientist and the 
visualization expert, presenting the ADIOS visualization 
schema as a facilitator for expertise and data exchange between 
the two. Our goal is to allow scientists to focus on their area of 
training and interest, while minimizing time wasted in 
acquiring skills in related fields and reducing the need for 
collaborators who are experts in these fields.   
II. SCIENTIFIC COLLABORATION
 A. Scientific collaboration  
The most common, and perhaps the most tangible way to 
study collaboration in research is to measure and deduct 
collaborations through co-authorship [3],[4],[5]. Sonnenwald 
presents a comprehensive study of scientific collaboration and 
influencing factors in [6]. The study warns of the challenges of 
collaboration for its own sake.  The scientists’ perspective in 
the similar work of Hara [7] also shows that successful 
2013 IEEE 9th International Conference on e-Science
 978-0-7695-5083-1/13 $25.00 © 2013 IEEE
 DOI 10.1109/eScience.2013.24
 27
collaboration in research is not easily found. Indeed it is a 
complex problem that has to take into account collaboration 
between individuals, groups, institutions, sectors or nations; as 
well as access to required infrastructure and social networks 
and persona. However these studies, along with various 
documented success stories of scientific collaboration in 
domains including education, health care and computer 
science, indicate that collaboration has the potential to solve 
complex scientific problems. It can lead to new discoveries as 
well as new ways to conduct science. The emergence of 
scientific social networks is demonstrating that scientists are 
increasingly leveraging a wider range of tools to tap into this 
potential. The first attempts at establishing new networks have 
been largely focused on publications. Sites like LinkedIn1 and 
ResearchGate2 help researchers make professional connections 
and stay up-to-date on the latest publications and development 
in their fields. While scientists do not actually work (online) 
using these sites, the availability of Web 2.0 technology has 
also heralded the emergence of gateways and virtual research 
environments. These technologies foster the formation of more 
closely knit communities of researchers who work together 
using portals and common tools in their daily work routines. 
Therefore, we note that as a research topic, scientific 
collaboration is a broad subject that can be tackled from 
various angles: statistics, quality, social character, temporal, 
geospatial considerations, impact of technology etc. 
Interdisciplinary collaboration adds a level of complexity as 
participants have different expertise and training. For example, 
different perspectives on the same general science topic often 
result in confusion due to mismatched nomenclature and 
incompatible software. 
 As Hara et al. [7] suggest, getting a further understanding 
of scientific collaboration requires an in-depth understanding 
of researchers’ daily work practices, leading to an 
understanding of how integrative scientific collaboration may 
be facilitated, perhaps through supporting technologies. This 
paper presents an analysis of collaboration in e-Science. We 
build on the example of the High Performance Computing 
community (HPC) where interdisciplinary collaboration is 
increasingly required.  
B. Interdisciplinary collaboration in e-Science 
As computers are allowing for generation and exploration 
of larger datasets, several application scientists (chemists, 
physicists etc.) have two options if they want to stay up-to-date 
in their respective fields: collaborate with experts in various 
sub-fields of computer science, or become knowledgeable in 
these areas themselves. The first one seems more feasible and 
reasonably achievable as the latter takes away from the time 
and energy focused on their main area of interest. Sites and 
software tools like MyExperiment [8] and ManyEyes [9] make 
it easy for scientists to use, share and reuse workflows, 
visualizations and datasets. Nevertheless, researchers dealing 
with large-scale data still face many challenges and often 
require some knowledge in computing and data management in 
order to manipulate that data. The extreme case of High
                                                           
1
  LinkedIn: http://www.linkedin.com/ 
2
  ResearchGate: http://www.researchgate.net/ 
Performance Computing where increase in computing power 
has allowed scientific problems to grow in complexity and 
scale highlights these challenges. For a theoretician, making 
sense of such large amount of data involves writing codes that 
run on supercomputers, managing data output, analyzing that 
data, and visualizing the results. It is becoming impossible for 
one single scientist to correctly and efficiently perform all of 
the necessary tasks without collaborating with other experts.
 Extracting insight from simulations running on supercomputers 
typically involves pen-and-paper scientists, computation 
scientists, applied mathematicians (modeling algorithms, data 
mining algorithm etc.), I/O and performance experts, and 
visualization experts. Concrete examples of this are presented 
in the next section.  
III. THE HIGH PERFORMANCE COMPUTING (HPC) CASE
 In HPC, collaboration has indeed become an essential piece 
of scientists’ daily lives. Utilizing leadership scale computers 
certainly requires more than acquiring programming skills. 
New software like GlobusOnline [10] and Web groups like 
HPCWeb3 indicate that more scientists are getting access to 
large amount of data and organizing themselves for better 
sharing of resources, data and expertise. GlobusOnline helps 
researchers move, sync and share big data through web 
browsers and avoid time consuming, error prone IT tasks. 
HPCWeb is a community group that is focused on making the 
computing and data resources that underlie simulation science,
 scientific computing, and data-centric science easily accessible 
through web browsers. In this section we describe a specific 
fusion project and other similar science collaboration projects 
around universities and national laboratories.
 A. Example in Physics 
During the five year CPES [1] project, the goal of our 
computer science team has been to support fusion scientists as 
they ran and monitored the XGC-1 code [11] on
 supercomputers at Oak Ridge National Laboratory and the 
National Energy Research Scientific Computing Center.
 Application scientists are often unable to take full advantage 
of currently available computational resources. They may lack 
formal training in software engineering and often get their 
training from other scientists in a team. Code development 
methods, software tools and other such considerations are 
made within a group based on previous scientists’ familiarity 
and/or preferences. A study by Basili et al. [12] gives a 
software engineering perspective on HPC common practices 
and discusses some of these issues. Our role in the CPES 
project has been to provide knowledge about the infrastructure 
and related software in order to bridge this knowledge gap.
 Our close interactions with physicists have helped us identify 
ways in which we could not only provide them with solutions 
but also improve their experience. Our resulting end-to-end 
technologies included three main elements: a fast Adaptable 
I/O System (ADIOS) [13], a workflow management system 
and an electronic simulation monitoring portal (eSiMon4) [14].
                                                           
3
  HPCWeb: http://www.w3.org/community/hpcweb/ 
4
  eSiMon: http://www.olcf.ornl.gov/center-projects/esimmon/ 
28
With ADIOS we abstract away many decisions about 
where and when the I/O is done and which resources are used.
 Such decisions are important to attain satisfactory I/O 
performance and while achieving high performance should be 
a concern for end-users, they are generally not trained to make 
performance decisions. ADIOS has been designed to simplify 
this process. First it uses a simple programming API that does 
not express the I/O strategy but simply declares what to 
output, which is the primary concern of the application 
scientist. Second, the XML-based external description of 
output data and selection of I/O strategy allows users to make 
changes to their I/O choices without changing their code as 
they change environment or new I/O methods become 
available. Third, ADIOS offers multiple transport methods 
selectable at runtime. Finally, the self-describing, log-based 
file format combined with buffered writing insures excellent 
write performance [15]. These design decisions allows I/O
 scientists to conduct research and deliver software while 
minimally impacting the application scientist who only wishes 
to output simulation results.  
Similarly, our KEPLER monitoring workflow [16]
 automates a number of mundane and repetitive data 
management tasks for the XGC-1 users. When users launch 
the fusion workflow, they are unaware of the heavy lifting 
done in the back-end on multiple systems; they simply launch 
their preferred web-browser and log in to eSiMon. While 
workflow tasks include moving files from computational 
resources to visualization clusters, pre-processing and 
archiving raw data files, creating images and movie, eSiMon 
is a lightweight portal that can be launched from any machine 
using a web browser to monitor the initial results. Our 
conscious effort to help researchers think of science instead of 
files and directories is embedded in the eSiMon design. Rather 
than focusing on a file browsing interface, the main simulation 
view displays familiar simulation variable names evolving 
through time as shown on Fig. 1. The tree-view on the left 
shows the scientific variables that can be dragged and dropped 
onto the canvas to display the movies. We use a provenance 
system [17] to link graphics to raw data for downloads and 
analysis. 
Fig. 1. eSiMon main simulation view  
B. Similar cases and different approaches  
Our data management team has worked with various 
applications including combustion (S3D [18]), fusion (GTC 
[19], GTS [20], XGC-1 [11]), and supernova (Chimera [21])
 and more. We have observed a variety of different 
collaboration techniques. Depending on the size of the project 
and the size of the group involved, scientists assign more or 
less time, energy and resources to addressing the data 
management issues they encounter while doing their work. 
 Smaller groups may attempt to gain some knowledge in 
related computing fields in order to get work done. They tend 
to focus on a direct solution to the task at hand, and may not 
take performance considerations into account, especially in the 
beginning phase of their project. Researchers are willing to 
live with performance problems, as long as their work does 
not come to a complete halt. This approach is tedious and time 
consuming when done correctly, but when rushed can result in 
fragile code and excessive dependence on the scientists who 
wrote the initial program.  
Medium-sized groups may have a staff dedicated to 
developing software tools and solutions for the entire team. 
For instance Bellerophon [22] has been designed and 
implemented to perform automated verification, visualization 
and management tasks while integrating with other workflow 
systems utilized by the CHIMERA [21] development team at 
ORNL. Similar to our workflow-dashboard system for fusion 
scientists, this approach is tailored to the needs of this specific 
team of astrophysicists. In an attempt to get results, developers 
find themselves delivering code and team specific solutions 
that work but that are not directly transferrable to other 
application teams. In these projects, like in our own, users 
become dependent on computer science experts and their 
knowledge for introducing changes to the workflow, fixing 
bugs or adding new functionality.  
Larger teams with access to large infrastructure and 
financial resources may benefit from groups such as the 
Scientific Computing Group at ORNL where each member 
acts as a “liaison” to projects running on the supercomputers. 
A liaison is an application scientist with notable computational 
training able both to understand the science and also to 
provide valuable input on best practices for using 
supercomputers. Liaisons help scientists with useful software 
and point them to existing tools.  
Despite this assistance, larger teams still need to develop 
their own data management solutions. In the past few decades 
a number of large projects consisting of collaborators from 
both laboratories and universities have turned to gateways. A
 science gateway is usually a community-developed set of 
tools, applications, and data that are integrated via a portal to 
meet the needs of that community. Gateways provide easy-
 access to tools and data; they enable researchers to focus more
 on their scientific goals and less on assembling cyber 
infrastructure. Gateways can also foster collaboration as a core 
team of supercomputing-savvy developers can build and 
deploy applications that become services available to the 
29
community at large. One example is the Earth Science Grid 
Federation 5  (ESGF), which stores and distributes terascale 
data sets from multiple coupled ocean-atmosphere global 
climate model simulations.   
Even at large scale, gateways tend to cater to specific 
communities. Considerable amount of time and effort from 
cyber infrastructure specialists is invested to serve the need of 
specific domain scientists. In [23] Wilkins-Diehr examines the 
lessons learned from the TeraGrid [24]. This work suggests 
that often the greatest benefit to users was easy access to data 
and the tools that can filter and mine it. In other words 
gateways have mainly addressed the need for easy, integrated 
access to commonly used tools and datasets within a particular 
scientific field. The focus had been primarily on providing 
functionality; user-experience and collaboration are still to be 
improved in future generation gateways. Indeed, despite the 
substantial effort required to build gateways end-users 
increasingly expect the same ease of use as commercial social 
networks for examples. 
IV. OUR APPROACH
 From professional website to complex gateways, scientists 
have started to leverage the advances of Web 2.0 technology 
and rapidly growing advances in computing resources. As a 
group, we have also tried to take advantage of these advances. 
We have used our knowledge in computational science and 
our close relationship with application scientists to identify the 
obstacles preventing researchers from focusing on their 
science. Our work has resulted in the development and release 
of eSiMon and ADIOS. eSiMon, our one-stop-shop to fusion 
simulations, hides the complexity of the workflow and its 
multiple data management tasks from the users and presents 
them with scientific content. Similarly ADIOS provides 
sophisticated techniques that take full advantage of current 
research and resources without burdening the users. 
While ADIOS is currently used in a variety of applications, 
the workflow-dashboard system has been more difficult to 
generalize across applications. Building, running, debugging 
and modifying our Kepler workflow remains a complex task 
that only a workflow expert can tackle. In our experience, 
despite training and tutorials, application scientists have not 
easily mastered the workflows and typically rely on the 
assistance of an expert for any modification. This represents a 
single point of failure for the group; it is a dependency that 
can slow down, or completely stop other group members from 
working.  If and when the knowledgeable person leaves, time 
and energy has to be invested in training a new group member 
before usual work can continue its course. This problem 
caused us to reevaluate our solution, and research and 
implement a better way to connect application scientists and 
computer scientists.   
The entire robust monitoring workflow with its mundane 
and tedious tasks cannot be easily simplified.  Workflow tasks 
tackle issues such as user login and credentials, data 
movement across platforms, file archiving, etc. Generalizing 
                                                          
5
  ESGF: http://esg.nersc.gov/esgf-web-fe/ 
the workflow involves a tremendous amount of effort, as a 
diverse set of requirements and constraints has to be taken into 
account for each application. On the other hand, making 
application specific workflows also involves considerable 
knowledge, expertise and effort to adapt to a different group. 
Therefore, instead of erroneously trivializing the workflow 
tasks, we identify points where certain workflow tasks can be 
more easily connected to fit different application. 
We describe this quest for improved connections, a search 
for a decoupled collaboration. Our hypothesis is that good 
simulation outputs (or results from other inter-disciplinary 
collaboration) are the product of a lot of individual 
knowledge, expertise, and work as well as some collaborative 
effort. Our interviews and meetings with application scientists 
suggest to us that it is important to identify where the work of 
these related but different scientists intersect and specifically 
target these areas for collaboration improvement. Our goal is 
to connect expertise where needed in order to allow each 
researcher to focus on their own area of interest. One example 
of such a connection is that of the application scientist and the 
visualization specialist. We asked the question: what pieces of 
the workflow connect the scientist running his or her 
simulation code and the experts and tools that can display the 
large amount of output data. We build on ADIOS and eSiMon 
for I/O and visualization purposes to design and implement 
ADIOS Visualization Schema.  
A. ADIOS Visualization Schema  
In the teams we have experience with, notably the fusion 
scientists and their most recent project EPSi6 (Center for Edge 
Physics Simulation), scientists often write their own analysis 
scripts or utilize a collection of scripts from the community. 
As in computing in general, their improvised visualization 
skills are not always scalable, hence the need for computer 
scientists in the project. Indeed using visualization tools such 
as VisIt7, ParaView8, AVS9, etc. requires a certain level of 
expertise as well as information on the code data structures. 
Visualization experts need to know where and how to access 
the data to be visualized. In other words they need metadata 
about files and physical variables to be visualized. For 
visualization experts, an understanding of the science is not 
essential; rather they want to know about scalars, vectors, 
arrays and meshes for instance. This is information that 
application scientists are familiar with and can point to in their 
code without knowing protocols and methods expected by 
common visualization tools. Therefore there is a need for 
translation between the language of the application scientist 
and the visualization expert. 
Matching code, data and scalable visualization software is 
not a new problem. HPC scientists have long used self-
 describing data formats such as HDF5 10  and NetCDF 11  to 
                                                          
6
  http://www.scidac.gov/fusion/fusion.html 
7
  VisIt: https://wci.llnl.gov/codes/visit/ 
8
  ParaView: http://www.paraview.org/ 
9
  AVS/Express: http://www.avs.com/ 
10
  HDF5: http://www.hdfgroup.org/HDF5/ 
11
  NetCDF: http://www.unidata.ucar.edu/software/netcdf/ 
30
<?xml version="1.0"?>
 <adios-config host-language="Fortran" schema-     
version="1.1">
     <adios-group name="field3D">
        <var name="n_n" type="integer"/>
 <var name="n_t" type="integer"/>
 <var name="values" gwrite="coord" 
path="/coordinates" type="real*8" 
dimensions="nspace,nnodes"/>
 <var name="node_connect_list" gwrite="nodeid" 
path="/cell_set[0]" type="integer" 
dimensions="nodes_per_elem,ncells"/>
 <mesh name="xgc.mesh" type="unstructured" 
file="xgc.mesh.bp" time-varying="no" />
 <points-single-var value="values"/>
 <uniform-cell count="n_t" 
data="node_connect_list" type="triangle"/>
 </mesh>
 <var name="nphi" type="integer"/>
 <var name="iphi" type="integer"/>
 <global-bounds dimensions="nphi,nnode" 
offsets="iphi,0">
 <var name="dpot" type="real*8" 
dimensions="1,nnode" mesh="xgc.mesh"
 hyperslab=": 0,: 1,: nphi"/>
 </global-bounds>
 …
 <global-bounds dimensions="nphi,nnode" 
offsets="iphi,0">
 <var name="iden" type="real*8" 
dimensions="1,nnode" mesh="xgc.mesh"
 hyperslab=": 0,: 1,: nphi"/>
 </global-bounds>
 </adios-group>
 write large data files.  But these formats do not insure standard 
organization, or consistent naming convention of particular 
visualization elements. Therefore, straightforward 
visualization of the data by visualization collaborators often 
requires a closer connection to the science and the scientist 
who wrote the data. Popular tools like VisIt and ParaView 
develop plugins and readers for common self-describing data 
formats. Development of readers for particular codes requires 
investment of time and energy from domain scientists and 
visualization/software experts. This may seem like a 
reasonable approach as long as codes, platforms and tools do 
not evolve, however this is rarely the case in bleeding edge 
research and development.  
The concept of schemas, even visualization schemas, is not 
new. XDMF 12  (eXtensible Data Model and Format) was 
created out of the need for a standardized method to exchange 
data between HPC codes and tools.  
XDMF (eXtensibleData Model and Format) uses XML to 
store Light data and to describe the data Model. HDF5 is used 
to store Heavy data. Similarly VizSchema [25] attempts to 
link common data formats to common visualization tools. 
Both of these schemas are tightly coupled with the HDF5 file 
format. Even though we embed our schema as attributes in the 
ADIOS XML and binary files, the key difference is not the 
vehicle for the schema, but rather our persistent focus on 
collaboration and separation of expertise. Indeed the purpose 
of the schema is not to link the data format, or simply the 
metadata to the data; rather it is a description of the visual 
understanding of the data: content that can be processed by 
scientists’ eyes and brains. 
By focusing on meshes and physical variables, the 
visualization process becomes independent of code and 
developer. If the visual representation of a variable changes, a 
scientist describes that change in the ADIOS XML file. Even 
though the visualization expert may need to edit the reader for 
the resulting binary data, his or her work is thus decoupled 
from that of the application scientist. Instead of having a 
reader for each code, programs such as VisIt and Paraview 
will require only a single reader to visualize any ADIOS data 
file. If and when changes need to be made, and readers have to 
be revisited and adapted, visualization experts are able to do 
so without requiring an understanding of changes in the 
applications or discussions with domain scientists. Time and 
effort invested in changes are now tied to the data and no 
longer associated with specific codes or developers. A
 physicist, in our case, can simply describe and modify the 
mesh and the variables without having to understand the 
corresponding analysis and visualization scripts and software. 
They can share scientific content with colleagues without 
knowledge of their preferred tools and naming conventions.
 Data can be shared and visualized independently of users’ 
expertise, familiarity, personal preference or availability of 
software packages. Furthermore we provide performance and 
flexibility of I/O as a bonus by embedding the schema into 
ADIOS XML file.  
                                                          
12
  XDMF: http://www.xdmf.org/index.php/Main_Page. 
B. Implementation 
XML tags and attributes provide a convenient way to 
organize data in a human readable way. XML is also an 
intrinsic part of ADIOS; it is already the case that if users 
change the way they write data, they can simply edit their 
ADIOS XML file. We now extend this approach by giving the 
scientists a way to describe the mesh structure, variables that 
compose the mesh and related variables. The goal is to require 
minimal information from a domain scientist as possible and 
still be able to visualize the data on popular HPC tools. When 
data representation changes in the code, or new variables are 
added, users do not have to change their code, they simply 
shuffle variables and meshes around in the XML. 
Visualization software including our own reader then 
interprets the changes and accurately display images on 
eSiMon for example.  
Fig. 2 shows a sample of an ADIOS XML file.  
Fig. 2. Sample ADIOS XML file with additions corresponding to the schema 
underlined. 
31
string   /adios_schema/version_major      attr   = "1"
 string   /adios_schema/version_minor     attr   = "1"
 string   /dpot/adios_schema                 attr   = "xgc.mesh"
 string   /iden/adios_schema                   attr   = "xgc.mesh"
 string   /adios_schema/xgc.mesh/type     
            attr   = "unstructured"
 string   /adios_schema/xgc.mesh/time-varying attr   = "no"
 string   /adios_schema/xgc.mesh/mesh-file        
            attr   = "xgc.mesh.bp"
 string   /adios_schema/xgc.mesh/nspace   attr   = "/nspace"
 string   /adios_schema/xgc.mesh/points-single-var     
            attr   = "/coordinates /values"
 double   /adios_schema/xgc.mesh/ncsets attr   = 1 
string   /adios_schema/xgc.mesh/ccount attr   = "n_t"
 string   /adios_schema/xgc.mesh/cdata           
            attr   = " /cell_set[0]/node_connect_list "
 string   /adios_schema/xgc.mesh/ctype     attr   = "triangle"
 In the case of XGC-1, variables on the mesh and variables 
composing the mesh are stored in separate files. Variables are 
also written as compounds of 2D slices of the tokomak. To 
support such representations and that of other common 
representations in HPC, the ADIOS Visualization Schema has 
to be flexible and expandable. Therefore we allow referring to 
external files for mesh variables as well as the concept of 
hyper slabs for super sets or sub sets of variables. For example 
a variable could be a subset of another variable, or as it is in 
the case in our example, a 3D variable could be visualized as a 
set of planes using a 2D mesh.  ADIOS processes the newly 
added users’ tags and attributes to generate an annotated 
binary output file describe thereafter. This file is then used by 
visualization experts and software to locate, associate and 
display the meshes and variables in the output data. Fig. 3
 shows the schema attributes generated in the output BP file 
(ADIOS file format) as a result to the users’ additions in the 
XML file.  
Fig. 3. Example of schema attributes generated from sample ADIOS XML 
C. Evaluation 
Collaboration is often a critical component of research of 
big machines and big data; however it remains difficult to 
measure via usual methods of observation, interviews and 
questionnaires. Studies of co-authorships offer a perspective 
but not a complete and general picture. In inter-disciplinary 
studies, particularly in fields such as HPC where diverse 
disciplines are required to make sense of a tsunami of data, 
more factors need to be taken into account. We argue that 
connectors such as the ADIOS Visualization Schema should 
be components of the complex workflows that process output 
from large simulations. This type of facilitated juncture 
insures that scientists are spending their valuable time and 
energy where they see it fit.  
Before using ADIOS schema, Applications scientists and 
visualization experts would have to discuss and agree on data 
representation, metadata and I/O patterns – establishing 
dependencies on specific persons, codes and visualization 
tools. Prior to using the schema, collaborators sharing data 
within the same field would also often have to impose a single 
tool for convenience within a team – inhibiting some users due 
to limited access to software or varying degrees of experience. 
Without our standardized schema, modifications to software 
such as changes in data representations, changes in 
infrastructure requiring different I/O, additional variables for 
debugging, etc. would require modifications in the code as
 well as changes to visualization readers. This development 
work would have to be well coordinated and repeated for each 
change.  
To further understand the considerable amount of 
development involved with implementing readers and writers 
for various codes and file formats we examined an assortment 
of VTK13 readers. We found 133 classes declaring themselves 
as reader and 84 declaring themselves as a writer. All together, 
this code amounts to about 18% of the VTK code. VisIt and 
ParaView visualization packages are both based on VTK and 
expectedly have extensive lists of readers and writers in their 
documentation14, 15. Table 1 shows a number of simulations 
that have been used by members of our team, and the number 
of lines of codes in the corresponding VisIt readers. As can be 
seen in the table, custom readers typically require thousands of 
lines of additional code to implement.  
TABLE I. CODE-SPECIFIC READERS AND LINES OF CODES IN VISIT
 File Format Lines of code
 S3D 3114
 Chombo 10520
 M3D 4643
 M3DC1 9888
 GTC 3478
 Pixie 8528
 Grand Total 40171
 The amount of collaborative work is not only reflected 
through lines of code. Our group has been directly involved 
with writing VisIt readers for codes such as GTC and Pixie3D
 [26]. Based on our past experience, this work requires 
visualization expertise and application knowledge and would 
be significantly simplified using our standardized schema 
along with a generic reader. Therefore we anticipate that the 
schema will consolidate numerous file formats and simulation 
code readers in HPC visualization tools. This task has the 
potential to notably impact the ratio of time implementing 
readers over the time spent doing application and visualization 
research.  
As a proof of concept, in the EPSi project we have 
implemented our own schema reader and plotting method for 
                                                          
13
  VTK Visualization toolkit: http://www.vtk.org/ 
14
  VisIt list of readers: 
http://visitusers.org/index.php?title=Detailed_list_of_file_for
 mats_VisIt_supports 
15
  ParaView list of readers: 
http://paraview.org/Wiki/ParaView/Users_Guide/List_of_read
 ers 
32
Monitoring 
workflowSimulation code
 ADIOS
 XML
 edit
 launch
 visualize
 ADIOS
 BP file
 write
 Visualization 
Expert
 Physicist
 XGC-1 code users. We find that the automatically generated 
2D diagnostic images allow physicists to directly dig into the 
science. This visualization portion of the monitoring workflow 
replaces a painstaking process of running a series of standard 
analysis routines before focusing on interesting subsets of the 
data. For the application scientists the added value of our 
modified workflow task resides in the flexibility and 
separation of work via the ADIOS XML file. Researchers can 
be enthusiastic about learning new skills outside of their area 
of training; however, this more in-depth section of analysis is 
where they would rather focus. The scripts and methods they 
use to make real discoveries may only be used once and 
cannot be automated. The intangible metrics we target are 
time spent outside of area of expertise/interest, and time to 
solution in primary research area. In other words, how quickly 
and efficiently can we bring scientists to the research aspect of 
their work, the part that cannot be automated and leads to 
scientific breakthrough? In the meantime, I/O scientists and 
visualization scientists are also conducting research 
independently of the application as the collaboration has been 
decoupled. The visualization expert can now focus on research 
issues regarding visualization at scale instead of implementing 
and debugging readers for applications as discussed earlier. 
Each scientist can do what they are trained to do on their own 
and come together only through this intermediate schema we 
have implemented. This scenario example is illustrated in Fig. 
4.
 Fig. 4. Decoupled collaboration between physicist and visualization expert 
This proof of concept is a specialized and partial 
evaluation. For a more in-depth assessment of our work we 
need measurements of the time scientists spend outside of 
areas of expertise before and after integrating our ADIOS 
schema. In addition to interviews, targets for improvements 
will be tailored to each type of scientists for a more complete 
perspective of the changes in their daily routines. For 
example, in the case of visualization experts, the consolidation 
of simulation code readers and writers gives additional support 
to scientists’ self-reported measurements of time spent outside 
of research.  
V. CONCLUSION
 In this paper we have presented our perspective of the HPC 
community and its answers for inter-disciplinary collaboration 
around large infrastructure and big data. We have replaced part 
of our fusion-specific monitoring workflow by a standardized 
visualization schema and interpreting tool to de-couple 
collaboration between two types of scientists: application 
scientists and visualization experts. We present this work as an 
example of a new approach to multi-disciplinary collaboration. 
While scientific collaboration is generally encouraged, it can 
be detrimental to research when time and energy are spent 
outside of one’s area of training and interest. We minimize 
such waste of time by identifying and improving connections 
between two types of experts through our visualization schema. 
We claim that the schema accelerates discovery by separating 
scientists’ work according to their research area. The schema 
decreases repetitive interactions between researchers who are 
connected but do not speak the same scientific language.  More 
tangible measurements are needed for further analysis. This 
work advocates encouraging inter-disciplinary collaboration by 
emphasizing discipline-specific research and making transition 
from one scientific field of another seamless. 
ACKNOWLEDGMENT
 The research and development of the ADIOS Schema has 
been partly supported by the National Center of Computation 
Science (NCCS) and by DOE SCIDAC: Center for Edge 
Physics Simulation (EPSi). We wish to thank our EPSi 
colleagues Choong-Seock Chang and Seung-Hoe Ku.  
REFERENCES
 [1] Center for Plasma Edge Simulation (CPES). 
http://www.scidac.gov/FES/FES_CPES.html. 
[2] Cummings, J.; Lofstead, J.; Schwan, K.; Sim, A.; Shoshani, A.; Docan, 
C.; Parashar, M.; Klasky, S.; Podhorszki, N.; Barreto, R., "EFFIS: An 
End-to-end Framework for Fusion Integrated Simulation," Parallel, 
Distributed and Network-Based Processing (PDP), 2010 18th 
Euromicro International Conference on , vol., no., pp.428,434, 17-19
 Feb. 2010. 
doi: 10.1109/PDP.2010.97 
[3] Newman, Mark EJ., "Scientific collaboration networks. II. Shortest 
paths, weighted networks, and centrality" Physical Review E, Vol. 64, 
No. 1. (2001). doi:10.1103/PhysRevE.64.016132 
[4] Newman, Mark EJ., "The structure of scientific collaboration 
networks." Proceedings of the National Academy of Sciences 98.2 
(2001): 404-409. 
[5] Barabási, Albert-László, et al. "Evolution of the social network of 
scientific collaborations." Physica A: Statistical Mechanics and its 
Applications 311.3 (2002): 590-614. 
[6] Sonnewald, Diane H. “Scientific collaboration.” Annual review of
 information science and technology 41.1 (2007): 643-681. 
[7] Hara, Noriko, et al. "An emerging view of scientific collaboration: 
Scientists' perspectives on collaboration and factors that impact 
collaboration." Journal of the American Society for Information 
Science and Technology 54.10 (2003): 952-965. 
[8] De Roure, David, Carole Goble, and Robert Stevens. "The design and 
realisation of the myExperiment virtual research environment for 
social sharing of workflows." Future Generation Computer Systems 25
 (2009): 561-567. 
[9] Viegas, Fernanda B., et al. "Manyeyes: a site for visualization at 
internet scale." Visualization and Computer Graphics, IEEE 
Transactions on 13.6 (2007): 1121-1128. 
[10] Foster, Ian. "Globus Online: Accelerating and democratizing science 
through cloud-based services." Internet Computing, IEEE 15.3 (2011): 
70-73. 
[11] C. S. Chang, S. Klasky, J. Cummings, R. Samtaney, A. Shoshani, L. 
Sugiyama, D. Keyes, S. Ku, G. Park, S. Parker, N. Podhorszki,H. 
33
Strauss, H. Abbasi, M. Adams, R. Barreto, G. Bateman, K. Bennett, Y. 
Chen, E. D. Azevedo, C. Docan, S. Ethier, E. Feibush, L. Greengard, 
T. Hahm, F. Hinton, C. Jin, A. Khan, A. Kritz, P. Krsti, T. Lao, W. 
Lee, Z. Lin, J. Lofstead, P. Mouallem, M. Nagappan, A. Pankin, M. 
Parashar, M. Pindzola, C. Reinhold, D. Schultz, K. Schwan, D. Silver, 
A. Sim, D. Stotler, M. Vouk, M. Wolf, H. Weitzner, P. Worley, Y. 
Xiao, E. Yoon, and D. Zorin, “Toward a first-principles integrated 
simulation of Tokamak edge plasmas - art. no. 012042,” Scidac 2008: 
Scientific Discovery through Advanced Computing, vol. 125, pp. 12 
042–12 042, 2008. 
[12] Basili, Victor R., et al. "Understanding the High-Performance-
 Computing Community." (2008). 
[13] J. Lofstead, S. Klasky, K. Schwan, N. Podhorszki, and C. Jin, “Flexible 
IO and integration for scientific codes through the Adaptable IO 
System (ADIOS),” in CLADE 2008 at HPDC. Boston, Massachusetts: 
ACM, June 2008. [Online]. Available: 
http://www.adiosapi.org/uploads/clade110-lofstead.pdf 
[14] Barreto, Roselyne, et al. "Collaboration portal for petascale 
simulations." Collaborative Technologies and Systems, 2009. CTS'09. 
International Symposium on. IEEE, 2009. 
[15] Liu, Qing, et al. "Hello ADIOS: The Challenges and Lessons of 
Developing Leadership Class I/O Frameworks," unpublished. 
[16] Ludäscher, Bertram, et al. "Scientific process automation and 
workflow management." Scientific Data Management: Challenges, 
Existing Technology, and Deployment, Computational Science Series 
(2009): 476-508. 
[17] Pierre, Mouallem, et al. Tracking Files Using the Kepler Provenance 
Framework. Oak Ridge National Laboratory (ORNL); Center for 
Computational Sciences, 2009. 
[18] J. H. Chen et al., “Terascale direct numerical simulations of turbulent 
combustion using S3D,” Comp. Sci. & Disc., vol. 2, no. 1, p. 015001 
(31pp), 2009. [Online]. Available: http://stacks.iop.org/1749-
 4699/2/015001 
[19] S. Klasky, S. Ethier, Z. Lin, K. Martins, D. McCune, and R. Samtaney, 
“Grid -based parallel data streaming implemented for the gyrokinetic 
toroidal code,” in SC ’03: Proceedings of the 2003 ACM/IEEE 
conference on Supercomputing, 2003, p. 24. 
[20] W. X. Wang and et al, “Gyro-kinetic simulation of global turbulent 
transport properties in Tokamak experiments,” Physics of Plasmas, vol. 
13, no. 9, p. 092505, 2006. [Online]. Available: 
http://link.aip.org/link/?PHP/13/092505/1 
[21] O. E. B. Messer, S. W. Bruenn, J. M. Blondin, W. R. Hix, A. 
Mezzacappa, and C. J. Dirk, “Petascale Supernova Simulation with 
CHIMERA,” Journal of Physics Conference Series, vol. 78, no. 1, pp.
 012 049–+, Jul. 2007. 
[22] Lingerfelt, Eric J., et al. "A Multitier System for the Verification, 
Visualization and Management of CHIMERA." Procedia Computer 
Science 4 (2011): 2076-2085. 
[23] Wilkins-Diehr, Nancy, et al. "TeraGrid science gateways and their 
impact on science." Computer 41.11 (2008): 32-41. 
[24] Catlett, Charlie. "The philosophy of TeraGrid: building an open, 
extensible, distributed TeraScale facility." Cluster Computing and the 
Grid 2nd IEEE/ACM International Symposium CCGRID2002. 2002. 
[25] S. Shasharina, J. R. Cary, S. Veitzer, P. Hamill, S. Kruger, M. Durant, 
and D. A. Alexander, VizSchema - Visualization Interface for 
Scientific Data, IADIS International Conference, Computer Graphics, 
Visualization, Computer Vision and Image Processing, 2009, p. 49. 
[26] Chacón, Luis. "A non-staggered, conservative, finite-volume scheme 
for 3D implicit extended magnetohydrodynamics in curvilinear 
geometries." Computer Physics Communications 163.3 (2004): 143-
 171. 
34

Differentially Private Two-Dimension Sparse Data 
Publication under Consistency 
Tingting Chen, Siyong Huang, Hong Chen 
College of Mathematics and Computer Science 
Fuzhou University 
Fuzhou 350108, China 
E-mail: titingchen@qq.com 453496787@qq.com  
chenh1989@gmail.com  
Yingjie Wu, Xiaodong Wang 
College of Mathematics and Computer Science 
Fuzhou University 
Fuzhou 350108, China 
E-mail: yjwu@fzu.edu.cn wangxd@fzu.edu.cn
  
Abstract—Recently, differential privacy data publication has 
become an extremely important research topic in data security 
field. However, most of the differential privacy algorithms do not 
take sparse data publishing into consideration. The aim of this 
study is to present an effective differential privacy algorithm for  
two-dimensional sparse data publication, so as to boost the 
accuracy of range queries of the released data. The proposed 
approach in this paper includes two steps: 1) getting the sampling 
set of the original two-dimensional sparse dataset by adopting 
filter sampling algorithm; 2) building an incomplete quadtree 
based on the sampling dataset and adjusting the tree nodes’ noise 
values under consistency. Experimental analysis is designed by 
comparing the proposed algorithm and the traditional 
algorithms on the accuracy of range queries in the released data. 
Experimental results show that the proposed algorithm is 
effective and feasible. 
Keywords—differential privacy; sparse data publication; filter 
sampling; consistency  
I. INTRODUCTION  
With the rapid development of information technology, 
various organizations can gather vast amount of personal 
information easily, such as hospitals, online social networks, 
etc. Analysis on such data may find valuable information, 
while endanger the privacy of the individuals who contribute to 
the data. Privacy preserving becomes an increasingly serious 
issue. The purpose of  privacy protection research is to provide 
adequate privacy guarantees and enhance the utility of the 
released data. 
Traditionally, there are three famous privacy preserving 
models, including k-anonymity[1], l-diversity[2] and t-
 closeness[3]. Though these models can preserve privacy in 
some situations, they are vulnerable to suffer attacks by the 
adversary who has specific background knowledge[4]. To 
overcome these shortages, Dwork proposed a new privacy 
model called  Differential Privacy[5], which can protect against 
the adversary with arbitrary auxiliary information. Differential 
privacy has emerged as a strong and widely accepted privacy 
model. The privacy safety of differential privacy is guaranteed 
by injecting a small random noise into query answer.  
In general, the data owner could offer only a limited 
magnitude of quite accurate answers with differential privacy 
model. Every query needs a low privacy budget, when the 
numbers of queries grow, the privacy budget is exhausted. The 
non-interactive mechanism publishes a “sanitized” version of 
the collected data, so it can solve the problem of query 
numbers. 
In this paper, we study two-dimensional sparse datasets of 
non-interactive mechanism of differential privacy. Such as the 
geo-spatial data, this type of data is very sparse and important. 
This paper aims to balance the requirements of utility and 
achieve sufficient privacy guarantees. First, we adopt a filter 
algorithm to get a sampling dataset, in order to compress the 
dataset. Second, we build an incomplete quadtree over the 
sampling dataset. It is no longer to divide the area that if no 
node in the sampling dataset. After injecting noise data to the 
tree nodes, we exploit unbiased estimation algorithm to adjust 
the inconsistency of parent and children nodes. 
II. RELATED WORK 
Differential privacy is achieved by adding noise to the real 
results and returned to the user, the noise is based on a random 
variable with a zero mean, like Laplace mechanisms[6], 
Geometric mechanism[7], etc. 
In this section, we present a brief summary of the data 
publication of differential privacy. Dwork[5] raised the basic 
method by adding a random noise data to every data, while 
with wide range of query will cause the noise accumulation. 
Xiao[8] proposed an algorithm using wavelet transforms called 
Privelet. Privelet first conducts a wavelet tree on the original 
data and then adds polylogarithmic noise to the tree nodes. 
Xu[9] generated two methods, NoiseFirst and StructureFirst. 
Ace[10] developed two algorithms based on Fourier transform, 
and the other two methods similar to[9]. Hay[11] pointed out 
consistency constraints, as a post-processing technique, is able 
to boost the accuracy of histogram queries. Xiao[12] first 
generated a synthetic histogram by adding noise to each cell 
histogram, and then build a kd-tree based space partitioning 
strategy for releasing a v-optimal histogram. Graham[13] 
studied spatial data, and proposed structure a quadtree or a kd-
 tree over the input data, after adding noise to the tree, return the 
result by arithmetic method. 
  Sparse data is one of the real data. It has characteristic of 
sparse, so this type of data needs more strict privacy protection 
and higher requirements of utility. In this case, Graham[14] 
proposed a framework to summary the sparse data. This 
2013 International Conference on Cloud Computing and Big Data
 978-1-4799-2829-3/13 $26.00 © 2013 IEEE
 DOI 10.1109/CLOUDCOM-ASIA.2013.55
 497978-1-4799-2830-9/14 $31.00 © 2014 IEE
framework reduces the output size without computing the huge 
contingency table. 
  From these above works, we studied that partition and 
summary both can compress the size of the data, ultimately 
achieve the purpose of reducing noise added. In our work, we 
combine these two methods to achieve data protection and 
enhance practicality. 
III. PRELIMINARIES 
A. Differential Privacy 
Definition 1 ( ? -Differential Privacy[5-6]). A randomized 
algorithm   satisfied  ? -Differential Privacy, for all datasets 
1D and 2D differing only in one tuple( denoted 1 2| | 1D D? = ), 
and any subset of outputs ( )H Range K? : 
1 2Pr[ ( ) ] exp( ) Pr[ ( ) ]K D H K D H?= ≤ ? =  
Differential privacy guarantees that no one tuple can 
significantly affect the result. The noise added is a function of 
the privacy parameter-?  , and a property of the queries called 
sensitivity. 
Definition 2 (sensitivity[6]). Let F be a set of functions, 
such function f F? , ( )f D is a query over dataset D , the 
output of ( )f D is a real number. The sensitivity of  
f ,denoted ( )S F ,is the maximum change in f when any 
single tuple  of D changes: 
1 2 1 2
 1 2
 , :| | 1
 ( ) max | ( ) ( ) |
 D D D D
 S F f D f D
 ? =
 = ?
  
An ? -Differential Privacy mechanism for answering query 
is adding a random noise, the noise is relate to ? and 
( )S F .Usually, the following mechanisms have be used to 
achieve differential privacy. 
1) Laplace mechanisms[6]:The noise is drawn from the 
Laplace distribution with parameter( ( ) /S F? ?= ), the 
Laplace distribution function: 
 
1Pr( ) exp( | | / )
 2
 x x ??= ?  
2)  Geometric mechanism[7]:When ( )f D is integer-
 valued, the noise can be drawn from the geometric distribution: 
 
| |1Pr[ ]
 1
 xX x ? ?
 ?
 ?
 = =
 +
  
where / ( )S Fe ?? ?= , x? .? - Geometric mechanism is 
the discrete of Laplace mechanism. 
B. Problem Defined 
A table has attributes 1,... kA A . It can be represented by a 
one-dimensional contingency table, denoted M .Let n be the 
number of non-zero entries in M . The size of the contingency 
table is computed over attributes, then | |i im A= ? .In Fig. 1, 
3n = and 3 2 6m = ? = . We define the density of M to 
be /n m? = . Hence, applying differential privacy will 
generates an output which is about 1 / ? times lager than the 
data size. When the ? is very small, this makes time and space 
consuming. 
 
 
 
 
 
 
(a)Original table                  (b) Contingency table with noise column 
Fig. 1.  Table M’ is the noisy version of M. The density of  is 
/ 3 / 6n m? = = . 
Definition 3 (Consistency constraints[11]). For a set of queries 
as { }1 2, ... kq q q , 2k ≥ . ( )Area q means the query area of 
q , then ( ) ( )i jArea q Area q ?= ,1 i j k≤ ≠ ≤ . Another 
query Q be
 1
 ( ) ( )k i
 i
 Area Q Area q
 =
 =  , so 
1
 k
 i
 i
 Q q
 =
 = . After 
inject noise to the results, lead to
 ~ ~
 1
 k
 i
 i
 Q q
 =
 ≠ . Adjust the values 
make
 1
 k
 i
 i
 Q q
 ? ?
 =
 = fits consistency condition. 
C. Query Model 
We study 2-dimensional data, and propose to build an 
incomplete quadtree depend on the sampling dataset. Fig. 2(a) 
shows the quadtree after consistency adjust,  iN
 ?
 represents 
adjusted noise. So 12 6 7 8 94 2 2N N N N N
 ? ? ? ? ?
 + = + + + + + . Give a 
query Q , shows by Fig. 2(b).The original answer is 4. Using 
the incomplete quadtree, the query overlaps 124 N
 ?
 +  and 10N
 ?
 . 
So estimation answer of Q
 ?
  is 12 104 / 2N N
 ? ?
 + + . 
 
 
 
 
  
 
 
(a) Incomplete quadtree 
tid Educ Status 
1 HS S 
2 HS S
 3 BS M
 4 PhD M
 cid (Educ,Status) cnt ncnt
 1 (HS,S) 2 4
 2 (HS,M) 0 2
 3 (BS,S) 0 -1
 4 (BS,M) 1 -2
 5 (PhD,S) 0 1
 6 (PhD,M) 1 3
 498
             
(b) Query estimation 
Fig. 2.  Example: incomplete quadtree and query estimation 
IV. THE F-BCQT ALGORITHM 
In this section, we will give the description of the F-BCQT 
Algorithm. The algorithm consists of two parts. First, through 
Filter Algorithm to get the sampling dataset. Second, build the 
incomplete quadtree depend on the sampling dataset. If without 
the first step, the incomplete quadtree will expose all true 
location of the original dataset. 
A. Filter Algorithm 
Contingency table contains a large portion of zero entries. 
If n is the number of non-zero in M and m is the domain 
size, then n m . M  can be stored as a list of non-zero 
entries. For example, in Fig.1(b),  M  consists of rows 1, 4 and 
6. For 2-dimension data, we use ( , )M i j  to show the position 
of entry, i represents the row number, j represents the column 
number. 
Let be the cut-off value( 1? ≥ ). If an item for 2-dimension 
data of ( , )i j has value ?? ≥+ |)(),(| GeojiM  
( ( )Geo ? denote Geometric noise), then we include ( , )M i j  
to 'M , else we drop it. We call this a two-side filter. 
After a two-side filter step, we choose some random zero 
entries to 'M , in order to perturb the M . We draw the 
number of entries  k  that pass the filter from the distribution 
( , )Bin m n p?? , then the k  entries join to 'M .We get the 
final sampling dataset 'M . 
Algorithm 1. Filter Algorithm 
Input: M ( original dataset) 
Output: 'M (sampling dataset) 
1 For every non-zero entry ( , )M i j , add geometric noise 
with parameter ? . If ?? ≥+ |)(),(| GeojiM ,add 
( , )M i j to 'M  
2 For zero entries, compute a numeric k from the binomial 
distribution ( , )Bin m n p?? , where 21p
 ?
 ?
 ?
 ?+
 
   
3 Random select k  locations ( , )i j from M , such 
( , ) 0M i j = . Add the k entries to 'M . 
PROOF. We focus on the distribution of entries which are 
zero in M .If they have added ( )Geo ?  noise, the probability 
that it passes the filter is: 
Pr[| ( , ) ( ) | ] Pr[| ( ) | ]M i j Geo Geo? ? ? ?+ ≥ = ≥  
| |
 | | 0
 1 12
 1 1
 x x
 x x
 ?
 ?
 ? ?
 ? ? ?
 ? ?≥ ≥
 ? ?
 = =
 + +
    
1 1 22
 1 1 1
 p
 ?
 ?
 ?
 ? ?
 ?
 ? ? ?
 ?
 = =
 + ? +
 
  
B. F-BCQT Algorithm 
The F-BCQT algorithm compresses the data, and then 
adjust the noise between tree nodes, to meet the definition 3. 
We call algorithm 3 to adjust inconsistencies between father 
and son nodes. Let x be a father node, the son of x  represent 
by ( )son x . 
Algorithm 2. F-BCQT Algorithm(Filter-Build Consistency 
Quadtree) 
Input: M , 'M ,? ,?  
Output: Consistency Quadtree 
1 Make the 2-dimension sparse data satisfy 2 2d d? , 
d +? . The rest default 0. 
2 Recursive decomposition of the region until the area is 
no point in 'M , every tree node is real value. 
3 Every quadtree node add a ( ( ) / )Lap S F ?  noise. 
4 Bottom-up to call Algorithm 3 (BLUE) to adjust the 
value of the tree node. 
5 ( )xMax ?∆ > , return to step 4. 
We use an algorithm 3 to adjust the value of nodes.  
~
 ( )h x  
denotes the has add a ( )Lap ?  noise after step 3 of F-BCQT. 
Adjusted value expressed by ( )h x
 ?
 . ( )son x  means the son 
number of  x . The algorithm is described as following. 
Algorithm 3. BLUE(Best Linear Unbiased Estimator) 
Input: x (the node of the tree), 
~
 ( )h x , ~
 ( )
 ( )
 i son root
 h i
 ?
  
Output: ( )h x
 ?
 ,
 ( )
 ( )
 i son root
 h i
 ?
 ?
  
1 x is a leaf node, return 
~
 ( )h x . 
2 ( )k son x=  
3 
~ ~
 ( )
 ( ) ( )
 1
 i son x
 x
 h x h i
 k
 ?
 ?
 ∆ =
 +
 
  
499
4 
~
 ( ) ( ) xh x h x
 ?
 = ?∆  
5 For each ( )i son x? , 
~
 ( ) ( ) xh i h i
 ?
 = + ∆  
PROOF. BLUE(Best Linear Unbiased Estimator)could 
adjust the value of minimum variance. The node root  has 
sons. Need to meet the least-square formula as follows. 
 
2 2
 ~ ~
 2
 ( )
 min ( ) ( )
 . ( ) ( )
 i son root
 h root h root h h
 s t h root h i
 ? ?
 ? ?
 ?
  
 ? + ?  
 = 
                (1) 
The above constraints into least squares formula can get the 
following formula. 
2 2
 ~ ~
 ( ) ( )
 ( ) ( ) ( ) ( ) ( )
 i son root i son root
 f h h i h root h i h i
 ? ? ?
 ? ?
     
 = ? + ?          
  
Get the partial derivatives by ( )h i
 ?
 , then equals to 0. 
~ ~
 ( )
 2( ( ) ( )) 2( ( ) ( )) 0
 ( ) x son root
 f h x h root h i h i
 h i
 ? ?
 ?
 ?
 ∂
 = ? + ? =
 ∂
 
  
~ ~
 ( )
 ( ) ( ) ( ) ( )
 i son root
 h i h i h root h i
 ? ?
 ?
 ? = ?   
  all of the partial derivatives of  ( )i son x?  .  
~ ~
 ( ) ( ) ( )
 ~ ~
 ( )
 ( )
 ~ ~
 ~ ( )
 ( ) ( ) ( ) ( )
 ( ) ( )
 ( ) ( ) ( 1)
 ( ) ( )
 ( ) ( 1)
 i son root i son root i son root
 i son root
 i son root
 i son root
 h i h i k h root k h i
 k h root h i
 h root h i
 k
 h root h i
 h root
 k
 ? ?
 ? ? ?
 ? ?
 ?
 ?
 ?
 ? = ?
 +
 = =
 +
 ?
 = ?
 +
   
 
 
   
Then, 
~ ~
 ( )
 ( ) ( )
 1
 i son root
 h root h i
 k
 ?
 ?
 ∆ =
 +
 
  
V. EXPERIMENTS 
This section experimentally compares the effectiveness of 
the proposed solution F-BCQT satisfies consistency constraints 
with Boost[11] and the adjust algorithm BLUE. These three 
methods all base on four partition. All methods are 
implemented in C++ and tested on an AMD Athlon(tm)II X4 
645 Processor ,3.10Hz CPU with 4GB RAM running Linux. 
We extract 50 fixed size of range-query to average the value 
from each algorithm runs, every algorithm is executed 50 
times. It means the fixed size of range-query have made 2500 
times average operation. 
For a set of queries 1 2{ , ,..., }mQ q q q= , iq  denotes real 
result of query , iq
 ?
  denotes the adjusted result . We use the 
following error equation to compare the data accuracy. 
2
 1
 ( )
 m
 i i
 i
 q q
 error
 m
 ?
 =
 ?
 =
 
 "
 Two datasets(both real dataset and synthetic) with different 
characteristics are used in our experiment. The population 
distribution of USA is a real dataset that available at 
http://www.census.gov/. The other data generated by the 
random function, its discreteness has more representatives. 
Table 1 shows the dataset statistic information. 
TABLE I.  DATASET  STATISTIC 
Dataset 
Table More Detail 
Size of 2-dimension /n m? = ?  
USA 1024*2048 0.041 1&0.1 
Synthetic 1024*1024 0.051 1&0.1 
We accomplish the experiments with above two sets. The 
experiments use the privacy parameter ?  with value of 1 and 
0.1. We set some different size of queries. In the below 
Figures, range size with one-dimension fixed, the other 
dimension gradually increase. 
 
Fig. 3. The population distribution of USA with 1? =  
For the sake of observing, we draw the figures by the X 
axis representing the size of the rectangle query ( *i x ), we 
have choose two value of i (200 and 800), x values as shown 
in figures. Y axis representing log10 (error).We can learn from 
Fig.3 that Boost method is not suitable for sparse data. The 
BLUE algorithm improves the accuracy of the data greatly. 
500
While based on the context of sparse data, F-BCQT algorithm 
works better than only used BLUE algorithm.  Besides, 
compare Fig.3(a) with Fig.3(b), the error of the three 
algorithms all get increase when the range size of query get 
larger. Therefore, larger  range lead to more noise accumulated. 
 
Fig. 4. The population distribution of USA with 0.1? =  
Fig.4 shows the error of privacy parameter 0.1? = . The 
error is larger than Fig.3. Differential privacy is achieved by 
adding noise.  The added noise affect the utility of publishing 
data. So the ? more smaller,  the  greater the noise. The 
experimental results proved the theory. 
 
Fig. 5. The synthetic dataset with 1? =  
 
Fig. 6. The synthetic dataset with 0.1? =  
The result of synthetic dataset is similar  to USA population. 
Although the size of synthetic data is only half of the real data, 
the error is not half. Construction of incomplete quadtree 
depends on data. Discrete data will lead quadtree node add 
more noise. The synthetic data is more discrete, The results are 
also verified this. 
In summary, our experiments demonstrate that F-BCQT 
algorithm has a better data utility on sparse data than Boost-4 
and BLUE-4, which is consistent as our theory. 
VI. CONCLUSIONS 
Differential privacy is a robust privacy protection 
mechanism that can against the attack of background 
knowledge. We have propose a general framework of a two-
 step process the 2-dimension sparse data. First, by a filter 
algorithm to get the sample dataset, the purpose is to disrupt 
the original location of non-zero data. Second, we construct the 
incomplete quadtree depend on the sampling dataset. The data 
has compressed after this step. Then, we use unbiased 
estimation algorithm to adjust the tree nodes to satisfy the 
consistency constraints. Verified by experiment, we have 
improve the utility of sparse data. In order to reduce the noise 
added to sparse data, how to compress the sparse data is the 
future work. 
REFERENCES 
[1] Sweeney L. k-Anonymity: A Model for Protecting Privacy.[J]. 
International Journal of Uncertainty, Fuzziness and Knowledge-Based 
Systems. 2002, 10."
 [2] Machanavajjhala A, Kifer D, Gehrke J, Venkitasubramaniam M. l-
 diversity: Privacy beyond k-anonymity[J]. ACM Transactions on 
Knowledge Discovery from Data (TKDD). 2007, 1(1): 3."
 [3] Li N, Li T, Venkatasubramanian S. t-closeness: Privacy beyond k-
 anonymity and l-diversity[C]. In: Data Engineering, 2007. ICDE 2007. 
IEEE 23rd International Conference on.IEEE, 2007. 106-115."
 [4] Martin D J, Kifer D, Machanavajjhala A, Gehrke J, Halpern J Y. Worst-
 Case Background Knowledge for Privacy-Preserving Data 
Publishing[C]. In: Data Engineering (ICDE 2007), 2007 IEEE 23rd 
International Conference on; Istanbul,Turkey.Istanbul,Turkey: 2007. 10."
 [5] Dwork C. Differential Privacy[C]. In: Automata,Languages and 
Programming pt.2.Venice: 2006. 12."
 [6] Dwork C, Mcsherry F, Nissim K, Smith A. Calibrating noise to 
sensitivity in private data analysis[M]. Theory of Cryptography, 
Springer, 2006, 265-284."
 [7] Ghosh A, Roughgarden T, Sundararajan M. Universally Utility-
 Maximizing Privacy Mechanisms[C]. In: Annual ACM International 
Symposium on Theory of Computing (STOC'09).Betheda, Maryland, 
USA: 2009. 9."
 [8] Xiao X, Wang G, Gehrke J. Differential Privacy via Wavelet 
Transforms.[J]. IEEE Trans. Knowl. Data Eng. 2011, 23."
 [9] Xu J, Zhang Z, Xiao X, Yang Y, Yu G. Differentially private histogram 
publication[C]. In: Data Engineering (ICDE), 2012 IEEE 28th 
International Conference on.IEEE, 2012. 32-43."
 [10] Acs G, Castelluccia C, Chen R. Differentially Private Histogram 
Publishing through Lossy Compression[C]. In: Data Mining (ICDM), 
2012 IEEE 12th International Conference on.IEEE, 2012. 1-10."
 [11] Hay M, Rastogi V, Miklau G, Suciu D. Boosting the Accuracy of 
Differentially Private Histograms Through Consistency.[J]. PVLDB. 
2010, 3."
 [12] Xiao Y, Xiong L, Fan L, Goryczka S. DPCube: Differentially Private 
Histogram Release through Multidimensional Partitioning[J]. CoRR. 
2012, abs/1202.5."
 [13]  Cormode G, Procopiuc C, Srivastava D, Shen E, Yu T. Differentially 
Private Spatial Decompositions[J]. CoRR. 2011, abs/1103.5."
 [14] Cormode G, Procopiuc C, Srivastava D, Tran T T.L. Differentially 
private summaries for sparse data[C]. In: Proceedings of the 15th 
International Conference on Database Theory.ACM, 2012. 299-311."
  
501

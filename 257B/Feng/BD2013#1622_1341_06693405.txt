Natural Language Processing and Big Data
 An Ontology-Based Approach for Cross-Lingual Information Retrieval
 Johanna Monti
 Dept. of Social and Human Sciences
 University of Sassari
 Sassari, Italy
 jmonti@uniss.it
 Mario Monteleone, Maria Pia di Buono, Federica
 Marano
 Dept. of Political, Social and Communication Sciences
 University of Salerno
 Fisciano (Sa), Italy
 {mmonteleone, mdibuono, fmarano}@unisa.it
 Abstract—Extracting relevant information in multilingual
 context  from  massive  amounts  of  unstructured,
 structured  and  semi-structured  data  is  a  challenging
 task. Various theories have been developed and applied
 to  ease  the  access  to  multicultural  and  multilingual
 resources. This papers describes a methodology for the
 development  of  an  ontology-based  Cross-Language
 Information  Retrieval  (CLIR)  application  and  shows
 how it is possible to achieve the translation of Natural
 Language (NL) queries in any language by means of a
 knowledge-driven  approach  which  allows  to  semi-
 automatically map natural language to formal language,
 simplifying  and  improving  in  this  way  the  human-
 computer interaction and communication. The outlined
 research activities are based on Lexicon-Grammar (LG),
 a  method  devised  for  natural  language  formalization,
 automatic  textual  analysis  and  parsing.  Thanks  to  its
 main  characteristics,  LG  is  independent  from  factors
 which are critical for other approaches, i.e. interaction
 type (voice or keyboard-based), length of sentences and
 propositions,  type  of  vocabulary  used  and  restrictions
 due to users’idiolects. The feasibility of our knowledge-
 based methodological framework, which allows mapping
 both  data  and  metadata,  will  be  tested  for  CLIR  by
 implementing a domain-specific early prototype system.
 Keywords-Cross-Language  Information  Retrieval  (CLIR);
 Lexicon-grammar; Ontology
 I.  INTRODUCTION
 Cross-language  Information  Retrieval  (CLIR)  allows
 users  to  search  and  access  information  in  multilingual
 document collections on the web in languages different from
 their own. Usually, information is searched by means of a
 query expressed in the user’s mother tongue. This query is
 automatically translated in the desired foreign language and
 the results are translated back in the user’s mother tongue.
 This  process  is  based  on  two  different  translation  stages:
 query  translation  and  document  translation.  The  query
 translation  concerns  the  translation  in  the  desired  foreign
 language of the query expressed in the user’s mother tongue,
 whereas the document translation is the back translation in
 the  user’s  language  of  the  relevant  documents  found  by
 means of the translated query. For instance, an Italian user
 expresses a query in his/her native language and wishes to
 look for relevant information in English on the web: first of
 all the CLIR application translates the queries from Italian
 into  English  (query  translation)  and  searches  the  relevant
 information on the web, subsequently it translates again all
 the information and the documents detected in this way into
 the  user's  mother  tongue (document  translation),  i.e.  from
 English into Italian. 
CLIR  success  clearly  depends  on  the  quality  of
 translations,  therefore  faulty  translations  cause  serious
 problems in retrieving the relevant information.  In domain
 specific  contexts,  indeed,  CLIR  applications  present
 significant shortcomings with reference to query translation,
 since  short  queries  do  not  provide  enough  context  for
 disambiguation  in  choosing  proper  translations  of  query
 words,  and  also  because  they  do  not  use  domain-specific
 semantic constraints. Furthermore, in document translation,
 translation inaccuracies  are  mainly  due to  lack of  domain
 adaptation  of  the  MT  systems  underlying  the  translation
 process.
 This  paper  describes  a  knowledge-based  methodology
 aimed at the development of an ontology-based CLIR system
 to  overcome  the  current  limitations  of  the  state-of  the-art
 applications in this field, with a focus on proper processing
 and translation of domain specific queries. 
The methodology has been set up for the Italian/English
 language pair and in the field of Cultural Heritage (CH) but
 can be easily extended to other language pairs and different
 domains.  The  remaining  of  this  paper  is  organized  as
 follows. The next section briefly explains the related work in
 the  area  of  CLIR.  Section  III  describes  the  methodology
 adopted  in  our  research.  Then,  section  IV  is  devoted  to
 system  overview,  and  section  V  describes  the  feasibility
 study presenting the Language Resources (LR), the semantic
 and  translation  model  used  in  the  research  work.  Finally,
 conclusions and future work are reported in section VI.
 II. RELATED WORK
 There are several approaches to CLIR, either based on
 bilingual  or  multilingual  Machine  Readable  Dictionaries
 SocialCom/PASSAT/BigData/EconCom/BioMedCom 2013
 978-0-7695-5137-1/13 $26.00 © 2013 IEEE
 DOI 10.1109/SocialCom.2013.108
 725
(MRD),  Machine  Translation  (MT),  parallel  corpora  and
 finally ontologies [1, 2, 3].
 MRD-based and MT-based CLIR are the most popular
 approaches  but  they  present  several  shortcomings,
 particularly  in  domain-specific  contexts,  because  of  the
 inaccurate  processing  and  translation  of  multi-word  units
 (MWU),  a  very  frequent  and  productive  linguistic
 phenomenon in languages for special purposes (LSP). 
MWUs are lexical elements composed of more than one
 word which have a particular structural and semantic internal
 cohesion, act as single lexical units and belong to different
 lexical categories (to look at, heavy water, arsenic water; as
 soon as possible; in order to are examples of MWUs). They
 are constructions half way between morphology and syntax
 since  they  have  a  very  similar  structure  to  phrases,  but
 present distribution and cohesion characteristics  which are
 very close to words. 
In  domain-specific  contexts,  the  most  frequent  MWU
 typology is represented by terminological compound words,
 such as  doric frieze or  spiral stem in the Cultural Heritage
 domain. Terminological compound words, with a limited or
 no  variability  of  co-occurence  among  constituents,  often
 have unpredictable, non-literal translations and are therefore
 particularly difficult to process and translate.
 Various  techniques  have  been  proposed  to  reduce  the
 errors due to the presence of MWU introduced during query
 translation in CLIR applications.  Among these techniques,
 phrasal  translation,  co-occurrence  analysis,  and  query
 expansion are the most popular ones.
 Concerning phrasal translation, techniques are often used
 to identify multi-word concepts in  the query and translate
 them as phrases [1, 4, 5, 6, 7].
 Co-occurrence  statistics  is  used  to  identify  the  best
 translation(s)  among  all  translation  candidates  using  text
 collections  in  the  target  language  as  a  language  model,
 assuming that correct translations occur more frequently than
 wrong ones [6, 8, 9, 10].
 As for query expansion techniques, [5, 11] assume that
 additional terms that are related to the primary concepts in
 the query are likely to be relevant and that phrases in query
 expansion via local context analysis and local feedback can
 be  used  to  reduce  the  error  associated  with  automatic
 dictionary translation.
 Concerning  MT-based  CLIR,  in  spite  of  the  recent
 positive  developments  in  Machine  Translation,  the
 identification,  interpretation and translation of MWUs still
 represent  open  challenges,  both  from  a  theoretical  and  a
 practical point of view. There is an increasing attention to
 MWU processing in MT, as witnessed by the recent “Multi-
 word  Units  in  Machine  Translation  and  Translation
 Technology” workshop at MT Summit XIV.
 MWU processing and translation in Statistical Machine
 Translation  (SMT),  the  current  leading  approach  in  MT,
 started  being  addressed  only  very  recently  and  different
 solutions have been proposed so far, but basically they are
 considered either as a problem of automatically learning and
 integrating  translations,  word  alignment  and  finally  Word
 Sense Disambiguation (WSD).
 Current approaches to MWU processing move towards
 hybrid  approaches,  where  phrase-based  models  are
 integrated with linguistic knowledge and scholars are starting
 to use linguistic  resources,  either  hand-crafted dictionaries
 and grammars or data-driven ones, in order to identify and
 process  MWUs as  single  units.  [12]  provides  a  thorough
 overview of the different approaches to MWU processing in
 MT.
 Ontologies are also used in CLIR and are considered by
 several  scholars  a  promising research area to  improve the
 effectiveness of CLIR techniques particularly for technical-
 domain queries. [13] use ontologies as interlingua in cross-
 language information  retrieval  in  the  medical  domain  and
 show  that  the  semantic  annotation  outperforms  machine
 translation of the queries, but the best results are achieved by
 combining a similarity  thesaurus with the semantic  codes.
 [14] perform ontology-based query expansion of  the most
 relevant terms exploiting the synonymy relation in WordNet.
 III. METHODOLOGY
 Our  methodology  is  based  on  the  Lexicon-Grammar
 (LG)  theoretical  and  practical  analytical  framework.  LG
 theory was conceived by the French linguist Maurice Gross
 during  the  ‘60s  [15,  16,  17].  Unlike  Chomsky’s
 transformational grammar and its various offspring [18, 19],
 LG  assumes  that  linguistic  formal  descriptions  should  be
 based on the observation of the lexicon and the combinatory
 behaviors  of  its  elements,  encompassing in  this  way both
 syntax and lexicon. It has also reached important results in
 the domain of automatic textual analysis and parsing, with
 the creation of software and lingware fully oriented toward
 NLP, such as NooJ1, and former software packages used in
 the  LG  framework,  such  as  INTEX  and  UNITEX2.
 Nowadays,  LG methodology  is  being  adopted  by  a  wide
 research community for  several  European (French,  Italian,
 Portuguese, Spanish; English,  German, Norwegian; Polish,
 Czech,  Russian,  Bulgarian;  Greek)  and  non-European
 languages (Arabic; Korean; Malagasy; Chinese; Thai...). 
This  methodology  is  particularly  suitable  for  CLIR
 applications due to the fact that the quality of translation is
 guaranteed  by  a  linguistic  approach  aimed  at  the
 development  of  a  coherent  and  formalized  linguistic
 knowledge  basis.  This  knowledge  basis  is  composed  of
 Linguistic  Resources  (LRs),  mainly  electronic  dictionaries
 and grammars (described in Section IV) useful in achieving
 effective  Information  Retrieval  (IR)  Systems  [20] and  in
 overcoming  the  weaknesses  of  the  current  statistical
 approaches used in MT by Google Translate or Bing, where
 the lack of meaning context represents a serious obstacle to
 MWU disambiguation. LG linguistic framework is grounded
 in the analysis of  “simple sentences”3, the minimal linguistic
 1 See http://www.nooj4nlp.net/pages/nooj.html.
 2 More information on the website http://www-igm.univ-mlv.fr/~unitex/.
 3 In LG, a simple sentence is a context formed by a unique predicative
 element (a verb, but also a name or an adjective) and all the necessary
 arguments selected by the predicate in order to obtain an acceptable
 and grammatical sentence.  For more specification on simple sentence
 definition (Gross, 1968)
 726
meaning  contexts  within  which  word  combinatorial
 behaviors can be analyzed. The study of simple sentences is
 achieved by analyzing the so-called rules of co-occurrence
 and  selection  restriction,  i.e.  distributional  and
 transformational rules based on predicate syntactic-semantic
 properties.  Also,  LG  theory  is  prevalently  based  on  the
 concept  of  Operator-Argument  Grammar  [21],  therefore  it
 highlights  transformational  rules  (active/passive,
 positive/interrogative, etc.) and mutual relationships between
 simple sentences as they have been observed by Zellig Harris
 [22],  starting  from the bloomfieldian  notion of  morpheme
 and  the  method  of  commutation  or  equivalence  between
 different morphemes’ available lexical stuffs [23]. 
Thanks to these above-mentioned research studies,  LG
 range  of  analysis  concerns  lexicon,  and  especially  the
 concept  of  MWU  as  “meaning  unit”,  “lexical  unit”  and
 “word  group”,  for  which  LG  identifies  four  different
 combinatorial  behaviors  [24].  These  meaning  units  are
 collected  and  described  inside  LRs  consisting  of  (I)
 morphologically  and  semantically  tagged  electronic
 dictionaries, (ii) local grammars in the form of Finite State
 Transducers/Automata (FSTs/FSA), used to parse texts,  and
 finally (iii) binary tables which describe syntactic-semantic
 properties of lexical entries (refer to V.A and  V.B).
 A. LG Methodology to Assess the Translation Quality
 The  quality  of  translations  is  guaranteed,  from  the
 beginning,  by  the  development  of  highly  formalized  LRs
 according  to  morphological,  syntactical  and  semantic
 criteria.  Using  smart  technologies  for  translation
 technologies  often  implies  the  deterioration of  Translation
 Quality  (TQ).  In  LG  methodology,  instead,  we  take
 advantage of well-formed LRs to to keep a high level of TQ,
 since from the beginning, a supervised approach is carried
 out by expert linguists during the proper setting of resources.
 Assessing  the  quality  of  resources  before  they  are
 translated  prevents  from  too  many  subsequent  checks  on
 translated  resources,  even  if  an  ex  post evaluation  of  TQ
 results is always necessary.
 According to LG a valid evaluation methodology should
 be  based  on  a  hybrid  approach,  that  encompasses  both
 human and automatic evaluation.
 The process is composed of two cycles. The first cycle
 can be outlined as follows (i) a query expressed in a Source
 Language (SL) is the input of the CLIR application, (ii) the
 MT system produces sample queries (i.e. sample texts) in the
 Target Language (TL), (iii) the resulting translated queries
 are  examined  by  humans  (Linguists,  Translators,
 Terminologists/Domain  Experts)  to  evaluate  their  quality.
 The human judgements are based on common criteria of TQ
 – i.e.  adequacy  and  fluency  –  and  are  expressed  using  a
 Likert scale with scores 1-5 (for instance using follow-ing
 judgements:  1.  Strongly  disagree,  2.  Disagree,  3.  Neither
 agree nor disagree,  4. Agree, 5. Strongly agree),  (iv) only
 texts  which  obtained  scores  4-5  become  “validated”  and
 “supervised” texts which represent the gold standard, (v) this
 gold standard is the training set for the Automatic Evaluation
 process, that can be carried out using METEOR4 and GTM5,
 that are the most suitable methods according to our opinion,
 as well as other ones6.
 During the second cycle,  human evaluation is  skipped
 and the SL queries directly become the input for automatic
 evaluation. 
It  is  necessary  to  periodically  repeat  the  first  cycle  in
 order to enrich the training set and to increase the quality
 cycle.
 IV. SYSTEM OVERVIEW
 The proposed architecture aims to map data and metadata
 exploiting  the  morph-syntactic  and  semantic  information
 stored  inside  both  electronic  dictionaries  and  Finite  State
 Automata/Finite State Transducers (FSA/FSTs) (presented in
 V.B).
 Furthermore,  we use our application in order to define
 entities and to model relationships, mapping linguistic tags
 (i.e. POS) and structures (i.e. sentences, MWUs) to domain
 concepts.
 Indeed, in CLIR systems the most frequent error is the
 assignment  of  wrong  Parts  Of  Speech  (POS)  to  lexical
 meaning  units,  as  stated  in  [25]:  “the  complexity  of  the
 grammatical  structures  and  the  quality  of  parsing  are  the
 main cause of the errors”.
 Our  research  framework  could  allow  achievement  of
 major  improvements  in  IR  and  IE,  both  in  recall  and
 precision.
 The system is based on two workflows which are carried
 out simultaneously but independently. As described in Fig. 1,
 before the execution of a query against a knowledge base it
 is necessary to apply the Translation and the Transformation
 routines.
 The  first  step  performed  is  a  linguistic  pre-processing
 phase  which  formalizes  (i.e.  converts)  natural  language
 strings  into  reusable  linguistic  resources.  During  this  first
 phase  we  also  extract  information  from  free-form  user
 queries, and match this information with already available
 ontological domain conceptualizations.
 The advantages in keeping separate transformation and
 translation routines are:
  the  development  of  an  architecture  with  a  central
 multilingual formalization of the lexicon, in which
 there  is  no  specific  target  language,  but  each
 language can be at the same time target and source
 language;
  the  development  of  extraction  ontologies  and
 SPARQL/SERQL adaptation  systems  which  could
 represent  a  standard  not  only  for  our  multilingual
 electronic dictionaries, but also for any lexical and/or
 language data-base for which translation is required. 
With  this  dual-structure  system,  it  is  easier  to
 successfully  achieve the  question  answering (QA) process
 4 http://www.cs.cmu.edu/~alavie/METEOR/.
 5 http://nlp.cs.nyu.edu/GTM/.
 6 BLEU and NIST (based only on precision measure), F-Measure (based
 also on recall).
 727
since the answers are given explicitly in the target language
 chosen by the user and the translation process is separated
 from the matching with the RDF triples.
 V. FEASIBILITY STUDY
 To test the feasibility of our architecture, we are carrying
 out a translation experiment from Italian into English, using
 all  ontological  and  semantic  constraints  defined  for  the
 Italian model.
 We have chosen the Archaeological domain to test the
 applicability  of  our  approach.  This  choice  allows  us  to
 demonstrate that the modularity of our architecture may be
 applied to a domain which is variable by type and properties
 and is semantically interlinked with other domains.
 In  the  next  paragraphs,  we  will  present  the  LRs
 developed for our study, together with the a description of
 the semantic annotation and the translation routines used in
 query translation.
 A. Electronic Dictionaries
 An  electronic  dictionary  is  a  lexical  database
 homogeneously  structured,  in  which  the  morphologic  and
 grammatical  features  (gender,  number  and  inflection)  of
 lexical  entries  are  formalized  by means  of  distinctive and
 non-ambiguous  alphanumeric  tags  [26,  27].  All  electronic
 dictionaries, built according to the LG descriptive method,
 form the DELA7 system, which is also used as a linguistic
 knowledge-base embedded in several NLP applications (text
 mining, IR, QA systems among others) and parsers. DELA
 electronic dictionaries are of two types:
  simple  word  dictionaries,  which  include
 semantically  autonomous  lexical  units  formed  by
 7 Dictionnaire  Électronique  of  LADL  (Laboratoire  d'Automatique
 Documentaire et Linguistique).
 character  sequences  delimited  by  blanks,  such  as
 home and chair;
  compound word dictionaries, which include lexical
 units composed of two or more simple words with a
 non-compositional  meaning,  such as  nursing home
 and rocking chair.
 Terminological  entries,  as  already  stated  the  most
 common obstacle in CLIR applications, are lemmatized in
 compound word electronic dictionaries.
 The following example represents an excerpt extracted
 from the Italian dictionary of Archaeological Artifacts8
 freccia di balestra, N+NPN+FLX=C45+ 
DOM=RA1SUOARAL+ EN=crossbow arrow, 
N+AN+FLX=EC3
 freccia foliata, N+NA+FLX=C556+DOM=RA1SUOIL 
+ EN=leafed arrow, N+AN+FLX=EC3
 fregio con coronamento, N + NPN + FLX=C12 + 
DOM=RA1EDEAES + EN=frieze crown, 
N+AN+FLX=EC3
 fregio dorico, N+NA+FLX=C523 + 
DOM=RA1EDEAES + EN=doric frieze, N+AN+FLX=EC3
 fusto a spirale, N + NPN + FLX = C7 + 
DOM=RA1EDEAES + EN=spiral stem, N+AN+FLX=EC3
 For instance, the compound word  fregio dorico («Doric
 frieze»)  is  marked  with  the  domain  tag
 «DOM=RA1EDEAES»,  which  stands  for  «Archaeological
 Artifacts  – Building – Architectural  Elements  – Structural
 Elements».
 For each entry, a formal and morphological description is
 also given, which includes:
  the internal structure of each compound. This means
 that  in  the  compound  word  fregio  dorico  the  tag
 «NA» indicates that the given compound is formed
 by a Noun, followed by an Adjective. At the same
 time, in the compound word fregio con coronamento
 the tag «NPN», indicates that the given compound is
 formed  by  a  Noun  followed  by  a  Preposition,
 followed by a Noun;
  the  inflectional  class.  This  means  that  the  tag
 «+FLX=C523» indicates the gender and the number
 of  the  compound  fregio  dorico,  together  with  its
 plural  form,  i.e.  it  indicates  that  fregio  dorico is
 8 It’s important to specify that our domain dictionaries, collected in the
 DELAC/DELACF system,  cover about 180 different  semantic  tags.
 The  most  important  dictionaries  are  those  of  Computer  Science
 (54,000  entries  ca.),  Medicine  (46,000  entries  ca.),  Law  (21,000
 entries) and Engineering (19,000 entries ca.). Each dictionary has been
 created and verified under the supervision of domain experts. Subset
 tags  are  also  previewed  for  those  domains  that  include  specific
 subsectors.  This  is  the  case  of  Archaeological  Artefacts  dictionary
 (9,200 entries ca.) , for which a generic tag RA1 is used, while more
 explicit  tags  are  used  for  object  type,  subject,  primary  material,
 method of manufacture, object description. In order to develop Italian-
 English  dictionary  of  Archaeological  Artefacts,  we  relied  on  the
 Thesauri  and  Guidelines  of  the  Italian  Central  Institute  for  the
 Catalogue  and  Documentation  (ICCD)  available  at
 http://www.iccd.beniculturali.it/index.php?it/240/vocabolari.
 Figure 1. System Workflow
 728
masculine  singular,  does  not  have  any  feminine
 correspondent  form,  and  its  plural  form  is  fregi
 dorici.  Each inflectional class is associated to a local
 grammar which produces  the inflected forms of the
 word according to the inflection class associated to
 it.
 On the  other  hand,  local  grammars  formally  describes
 syntactic features of natural languages and are used in NooJ9
 to parse texts. Such grammars are defined “local” because
 each one of  them account  for  one and only one syntactic
 profile of a given predicate. This means that for instance we
 build a local grammar only to describe the syntactic features
 of the verb to give. Local grammars are built and developed
 in the form of FSA/FSTs [28].
 B. Semantic annotation
 As for ontologies, the formal definition we rely upon is
 the one given by the International  Council  of  Museums -
 Conseil  Interational  des  Musees  (ICOM  –  CIDOC)
 Conceptual Reference Model (CRM) [29]. CIDOC CRM is
 composed of two different hierarchies, one composed of 90
 classes  (which  includes  subclasses  and  superclasses)  and
 another  of  148 unique properties  (and subproperties).  The
 object-oriented  semantic  model  and  its  terminology  are
 compatible  with  the  Resource  Description  Framework
 (RDF). Actually, this ontology was already available and is
 constantly developed.  At the same time,  our methodology
 shows  that  a  given  linguistic  knowledge  can  be  reused
 independently from the domain to which it pertains. Domain
 ontologies refer  to  mid-  and upper-level  ones,  which tend
 pragmatically  to  be  standardized.  Logically,  such  process
 indirectly involves also low-level ontologies, and this allows
 the reuse of linguistic resources regardless of the domain in
 which they were developed or to which they pertain.
 LRs are used for analyzing corpora to retrieve recursive
 phrase structures, in which combinatorial behaviours and co-
 occurrence between words identify properties, also denoting
 a  relationship.  Furthermore,  electronic  dictionaries  also
 include all inflected verb forms allowing to process queries
 expressed  also  with  passive  and  more  generally  non-
 declarative sentences.  Consequently we use FSA variables
 for identifying ontological classes and properties for subject,
 object and predicate within RDF graphs.
 This matching of linguistic data to RDF triples and their
 translation  into  SPARQL/SERQL path  expressions  allows
 the use of specific meaning units to process natural language
 queries.
 9 NooJ  is  an  NLP  software  environment  which  uses  electronic
 dictionaries and local grammars we use automatically read and parse
 digitized texts. For more on NooJ, see www.nooj4nlp.net. 
Figure  2  is  a  sample  of  an  automaton  showing  an
 associated RDF graph for the following sentence:
 Il  Partenone (subject)  presenta (predicate)  colonne
 doriche e ioniche (object).
 According to our approach, electronic dictionaries entries
 (simple words and MWUs) are the subject and the object of
 the RDF triple.
 In Figure 3 we develop an FSA with a variable which
 applies to the sentence the following classes and property:
  E19 indicates “Physical Object” class;
  P56 stands for “Bears Feature” property;
  E26 indicates “Physical Feature” class.
 So, the FSA variables transform our sentence into:
 Il  Partenone  (E19) bears  feature colonne  doriche  e
 ioniche (E26).
 The  role  pairs  Physical  Object/name and  Physical
 Feature/type are trigged by the RDF predicate presenta.
 Besides in Fig. 3 we also indicate specific POS for the
 first  noun phrase  Il  Partenone (DETerminer  + Noun),  the
 verb  presenta (V)  and  the  second  noun  phrase  colonne
 doriche e ioniche (Noun+Adjective+Conjuntion+Adjective).
 By applying the automaton in Fig. 3 (built using the high
 variability of lexical class and not of the original form) we
 can recognize all instances included in E19 and E26 classes,
 the property of which is P56.
 C. Query Translation
 In  our  model,  the  Translation  Routines  are  applied
 independently of the mapping process of the pivot language.
 This  allows  us  to  preserve  the  semantic  representation  in
 both languages. Indeed, identifying semantics through FSA
 guarantees the detection of all data and metadata expressed
 in any different language.
 Figure 4 illustrate a FST in which a translation process
 from Italian to English is performed combining information
 coming  from  dictionaries  and  grammars  of  analysis  with
 lexical  transfer.  This  translation  FST,  recognizes  and
 annotates  the  different  linguistic  elements  of  declarative
 sentences such as Il Partenone presenta fregi dorici, I templi
 romani  hanno  fusti  a  spirale,  etc.,  with  their  morpho-
 syntactic and semantic information and performs automatic
 translations on the basis of an LG Italian-English bilingual
 dictionary.  For instance,  if  a grammar variable, say $E26,
 holds  the  value  fusti  a  spirale,  the  output  $E26$EN will
 Figure 3. Sample of the use of the FSA variables for identifying classes for subject, predicate and object.
 Figure 2. Simple FSA with RDF Graph.
 729
produce the correct English translation “spiral stems”, on the
 basis  of  the  value  associated  to  the  +EN  feature  in  the
 bilingual entry “fusto a spirale, N + NPN + FLX = C7 +
 OM=RA1 + EN=spiral  stem,  N+AN+FLX=EC3”  and  the
 morpho-syntactic analysis performed by the graph in Figure
 4,  which  identifies  and  produces  the  plural  form  of  the
 compound noun fusto a spirale.
 VI. CONCLUSIONS AND FUTURE WORK
 The architecture described in the previous pages ensures
 both the coverage of large quantities of knowledge and the
 preservation  of  any  deep  semantic  relations  which  may
 interlink different languages. Our future work aims therefore
 at  further  developing our  system:  we will  implement  new
 Linguistic Resources in order to test the accuracy of cross-
 language  information  retrieval.  Futhermore,  we  will  carry
 out a larger number of experiments in different domains on
 selected corpora.
 NOTE
 Johanna Monti is author of sections I, II and V.C, Mario
 Monteleone is author of sections V.A and VI, Maria Pia di
 Buono is  author  of  sections  IV,  V and V.B and Federica
 Marano is author of section III and III.A.
 REFERENCES
 [1] L.  Ballesteros  & B.  Croft,  “Dictionary Methods for  Cross-Lingual
 Information  Retrieval”,  Proc.  of  the  7th  DEXA  Conference  on
 Database  and  Expert  Systems  Applications,  Zurich,  Switzerland,
 September 1996, pp. 791-801.
 [2] L. Ballesteros and B. Croft, “Phrasal translation and query expansion
 techniques for crosslanguage information retrieval”, Proc. of the 20th
 annual  international  ACM  SIGIR  conference  on  Research  and
 development in information retrieval, 1997.
 [3] L. Ballesteros & B. Croft, “Resolving Ambiguity for Cross-language
 Retrieval”, Proc. Of SIGIR’98, Melbourne, Australia, August 1998,
 pp. 64-71.
 [4] L. Bloomfield, “Language”, Henry Holt, New York, 1993.
 [5] N. Crofts, M. Doerr, T. Gill, S. Stead, M. Stiff (eds.), “Definition of
 the CIDOC Conceptual Reference Model, Version 5.0”,  2008.
 [6] M.  W.  Davis  and  W.  C.  Ogden,  “Free  resources  and  advanced
 alignment for cross-language text retrieval”, Proc. of The Sixth Text
 Retrieval Conference (TREC-6). NIST, Gaithersbury, MD, 1997.
 [7] G.  De  Bueriis,  A.  Elia  (eds.),  “Lessici  elettronici  e  descrizioni
 lessicali, sintattiche, morfologiche ed ortografiche”, Plectica, Salerno,
 2008.
 [8] J. Gao, J. Nie,  E. Xun, J.  Zhang, M. Zhou, C. Huang, “Improving
 Query  Translation  for  Cross-Language Information  Retrieval  using
 Statistical  Models”,  Proc.  of  the  24th  annual  international  ACM
 SIGIR  conference  on  Research  and  development  in  information
 retrieval. ACM, 2001.
 [9] M. Gross, “Grammaire transformationnelle du français. – I – Syntaxe
 du verbe”, Larousse, Paris, 1968.
 [10] M.  Gross,  “Méthodes  en  syntaxe,  régime  des  constructions
 complétives”, Hermann, Paris, 1975.
 [11] M. Gross, “La construction de dictionnaires électroniques”, Annales
 des  Télécommunications,  vol.  44,  n°  1-2:  4-19,  CENT,  Issy-les-
 Moulineaux/Lannion, 1989.
 [12] Z.S.  Harris,  “Co-occurrence  and  transformation  in  linguistic
 structure”, Language 33, 1957, pp. 293-340.
 [13] Z.S. Harris,  “Transformations in Linguistic Structure”,  Proc. of the
 American Philosophical Society 108:5, 1982, pp. 418-422.
 [14] Z.S.  Harris,  “A Grammar  of  English  on Mathematical  Principles”.
 John Wiley and Sons, New York, USA,  1982.
 [15] D.  A.  Hull,  G.  Grefenstette,  “Querying  across  languages:  a
 dictionary-based  approach  to  multilingual  information  retrieval”,
 Proc.  of  the  19th  annual  international  ACM SIGIR conference  on
 Research and development in information retrieval,  1996, pp. 49-57.
 [16] P. Knoth, T. Collins, E. Sklavounouy, Z. Zdrahal, “Facilitating cross-
 language retrieval  and  machine  translation  by  multilingual  domain
 ontologies”, 2010.
 [17] A. Maeda, F. Sadat et  al.,  « Query Term Disambiguation for Web
 Cross-Language  Information  Retrieval  using  a  Search  Engine”,  in
 Proc.  of  the  Fifth  Int’l  Workshop  on  Info.  Retrieval  with  Asian
 Languages, Hong Kong, China, 2000, pp. 173-179.
 [18] F. Marano, “Exploring Formal Models of Linguistic Data Structuring.
 Enhanced Solutions for Knowledge Management Systems Based on
 NLP Applications”, PhD Dissertation,  University  of  Salerno,  Italy,
 2012.
 Figure 4. Sample of the use of the translation FSA with variables for identifying classes for subject, predicate and object.
 730
[19] M.  Monteleone,  “Lessicografia  e  dizionari  elettronici.  Dagli  usi
 linguistici alle basi di dati lessicali”, Fiorentino & New Technology,
 Napoli, 2002.
 [20] J.  Monti,  “Multi-word  unit  processing  in  Machine  Translation:
 developing  and  using  language  resources  for  multi-word  unit
 processing in Machine Translation”, PhD dissertation, University of
 Salerno, Italy, 2013.
 [21] D. W. Oard, “Multilingual Information Access”, in Encyclopedia of
 Library and Information Sciences, 3rd Ed., edited by Marcia J. Bates,
 Editor, and Mary Niles Maack, Associate Editor, Taylor & Francis,
 2009.
 [22] A. Pirkola, “The Effects of Query Structire and Dictionary Setups”, in
 Dictionary-Based Cross-language Information Retrieval. In W. Croft
 et  al.,  21st  Annual  ACM  SIGIR  Conference  on  Research  and
 Development  in  Information  Retrieval  (SIGIR  2008),  Melbourne,
 Australia, August 24-28, 1998, pp.55-63.
 [23] F.  Sadat,  A.  Maeda  et  al.,  “A  Combined  Statistical  Query  Term
 Disambiguation in  Cross-language Information  Retrieval”,  Proc.  of
 the  13th  Int’l  Workshop  on  Database  and  Expert  Systems
 Applications,  Aix-en-Provence,  France,  September  2002,  pp.  251-
 255.
 [24] X.  Saralegi  &  M.  L.  de  Lacalle,  “Dictionary  and  Monolingual
 Corpus-based Query Translation for Basque-English CLIR”, 2010.
 [25] M. Silberztein,  “Dictionnaires électroniques et  analyse automatique
 de textes”, Masson, Paris, 1993.
 [26] I.  Szpektor,  I.  Dagan,  A.  Lavie,  D.  Shacham,  S.  Wintner,  “Cross
 Lingual and Semantic Retrieval for Cultural Heritage Appreciation”,
 Proc. of the ACL Workshop on Language Technology for Cultural
 Heritage Data, Prague, Czech Republic, 2007.
 [27] S.  Vietri,  A.  Elia,  E.  D'Agostino,  Lexicon-grammar,  Electronic
 Dictionaries and Local Grammars in Italian, in Laporte, E., Leclère,
 C.,  Piot,  M.,  Silberztein  M.  (eds.),  Syntaxe,  Lexique  et  Lexique-
 Grammaire.  Volume  dédié  à  Maurice  Gross,  Lingvisticae
 Investigationes  Supplementa  24,  John  Benjamins,
 Amsterdam/Philadelphia, 2004.
 [28] M. Volk, S. Vintar, and P. Buitelaar, “Ontologies in cross-language
 information  retrieval”,  Proc.  of  WOW2003  (Workshop  Ontologie-
 basiertes Wissensmanagement), Luzern, Switzerland, 2003.
 [29] P.  Vossen,  A.  Soroa,  B.  Zapirain,  G.  Rigau, “Cross-lingual  event-
 mining using wordnet as a shared knowledge interface”, Proc. of the
 6th Global Wordnet Conference, C. Fellbaum, P. Vossen (Eds.), Publ.
 Tribun EU, Brno, Matsue, Japan, January 9-13,  2012, pp.382-390.
 [30] M. Yapomo, G. Corpas, and R. Mitkov, “CLIR-and ontology-based
 approach for bilingual extraction of comparable documents”, The 5th
 Workshop on Building and Using Comparable Corpora,  2012.
 731

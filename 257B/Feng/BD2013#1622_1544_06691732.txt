How Data Partitioning Strategies and Subset Size
 Influence the Performance of an Ensemble?
 Majed Farrash
 School of Computing Sciences
 University of East Anglia
 Email: M.Farrash@uea.ac.uk
 Wenjia Wang
 School of Computing Sciences
 University of East Anglia
 Email: Wenjia.Wang@uea.ac.uk
 Abstract—When dealing with big data, “divide and conquer”
 is the most commonly used strategy in practice to partition a
 big dataset into such smaller subsets that each subset can be
 handled by a computer or a node of cluster or cloud computing
 systems. However, among many existing partitioning or sampling
 techniques, it is not clear which one is suitable and how the
 size of subset may affect the performance of further analysis.
 In this paper, after presenting a generic framework of ensemble
 approach for learning from big data, we focus our investigations
 on systematically evaluating the effect of partitioning strategies
 and subset size on ensemble performance. The experimental
 results have demonstrated that three investigated partitioning /
 sampling strategies behaved statistically similar but the subset
 size may affect the performance of the ensemble in very drasti-
 cally different ways, which are grouped into three patterns, rather
 than just one default perception - the bigger the better.
 Keywords—Big data, partitioning, subset size, ensemble learn-
 ing.
 I. INTRODUCTION
 Nowadays, the quantity of data collected through various
 digital media and activities can quickly become so large that
 any existing computers alone cannot load the data into their
 memory all together for analysis. Therefore, a big dataset has
 to be divided into smaller and manageable subsets to overcome
 the limit of memory. However, although there exist many data
 partitioning and sampling strategies, there is no systematic
 study on them examining firstly how these techniques perform
 when dealing with big data, and secondly how the size of
 subset may influence the performance of machine learning
 and data mining methods. In practice, the second aspect is
 more important as machine learning methods may have quite
 different performance in dealing with smaller datasets and big
 datasets. Studies such as, distributed data mining [1], parallel
 data mining [2], incremental data mining [3] and ensemble
 methods [4] have shown the ability to deal with small datasets,
 but most of them do not show the same performance when
 dealing with single massive dataset. A very plausible reason
 is that the learning algorithms employed by these methods
 are developed under the assumption of having the whole data
 loaded into the main memory, and obviously this assumption
 cannot be upheld when dealing with large datasets that cannot
 be loaded into the main memory all together.
 In this paper, we firstly propose a generic ensemble
 framework that consists of three phases for partitioning big
 data, generating models from the partitioned data subsets, and
 combining the generated models to produce a final answer.
 Then we focus our empirical investigations on the effect of
 diffident partitioning methods and subset size on ensemble
 accuracy in dealing with big datasets.
 The rest of the paper is organized as follows. Section
 2 briefly describes some related works. Section 3 presents
 an ensemble framework. The empirical investigation design
 and experimental results are presented in sections 4 and 5
 respectively. Then the discussion is given in section 6 and
 conclusions in Section 7.
 II. RELATED WORK
 As previously mentioned, partitioning a big dataset into
 smaller subsets that are manageable is a basic strategy used
 by different data mining methods. Some researchers, such as
 Oates and Jensen [5] and [6], have shown that increasing the
 size of a training set does not greatly increase classification
 accuracy. Another research proposed by Hall and Chawla [7]
 showed some promising results in relatively small datasets.
 Their proposed method was to build a single decision system
 after learning is done independently on N disjoint subsets of
 data in parallel computing. In addition, Nittaya and Kittisak
 Kerdprasop [3] proposed an algorithm to partition the training
 dataset into manageable and learning effective subsets and
 tested the algorithm using the incremental data mining method.
 They found that the size of a subset should be between
 10% and 50% of the whole data set. Moreover, Tsang et
 al. [4] proposed a SVM ensemble and sampling technique
 that was used to partition the dataset and showed that the
 use of orthogonal constraints in the SVM ensemble leads
 to better performance than bagging. An algorithm proposed
 by Patil and Bichkar [8] using a round robin partitioning
 method to classify large data sets, showed that the proposed
 algorithm achieved equivalent performance to the performance
 on a complete dataset. COMET is the MapReduce algorithm
 [9] for learning from large datasets. COMET builds multiple
 ensembles on distributed blocks of data and merges them into
 a mega ensemble. Their study showed that COMET compares
 favourably to subsampling Random Forests run serially on a
 single block of data. However their study fixed the subset size
 to 100K which was chosen by running IVoting algorithm for
 1000 iteration. It is not clear whether different subset size
 affects the performance of the classification and in what ways
 if it does. So there is still need to investigate the effect of
 different subset sizes on the ensemble accuracy and find out
 how to choose the best subset size when partitioning a big
 dataset.
 42
 2013 IEEE International Conference on Big Data
 978-1-4799-1293-3/13/$31.00 ©2013  IEEE 
Fig. 1. Proposed Ensemble Framework
 III. PROPOSED ENSEMBLE FRAMEWORK
 In this research, an ensemble method framework is pro-
 posed as shown in Figure 1 to deal with very large datasets. It
 consists of three phases partitioning, modelling and combining
 which will be described in detail below. The operations of the
 framework are described in Algorithm 1.
 Algorithm 1: Proposed Ensemble Algorithm
 Inputs : Training Dataset Tr.
 validation dataset V
 Testing dataset Ts.
 Partitioning Method P.
 Learning algorithm L.
 Fusion strategy F .
 The relative size of a subset Rt as a percentage of
 the whole size of the Tr, number of subset N =
 ?
 1
 Rt
 ?
 Outputs: Ensemble ?.
 Ensemble accuracy Acc(?)).
 The mean accuracy of individual models
 (Acc(mi)).
 Ensemble time (T M?) = Partitioning Time
 (T Mp)+ Learning Time (T Mln) + Classification Time
 (T Mc).
 Declare counter i=0
 Call P ( Tr , Rt ) , and receive a set of N training
 subsets SUB= {t0, t1, t2, ..., tN?1}
 while i less than N do
 Call L (ti ) and receive a base classifier mi.
 add mi to the ensemble ?.
 end
 Evaluate ? with V and Ts using F and receive
 Acc(?, V ),Acc(?, T s), (Acc(mi)), (T M?), (T Mp),
 (T Mln) and (T Mc).
 End
 A. Partitioning Phase
 In this phase a given large dataset will be divided into
 smaller subsets. However, there is no study to show which
 partitioning method should be used, and how they may affect
 the results of mining data as a whole.
 In our experiments, three commonly used data partitioning
 methods, Sequential non-overlapping partitioning, Round robin
 partitioning and Sampling without replacement are examined.
 1) Sequential non-overlapping partitioning: This
 method simply, divides the data instances of a
 training dataset (T r) into a given number of subsets
 (N) in sequential manner. The size of each subset is
 equal to
 ? |Tr|
 N
 ?
 . except the last subset which might
 take up to
 ? |Tr|
 N
 ?
 instances, where |T r| is the size
 of T r . After calculating the size of subsets, data
 instances will be read from T r sequentially, so the
 first
 ? |Tr|
 N
 ?
 instances goes to the first subset and
 so on until the end of the file of T r. Algorithm 2
 illustrate the procedure for this partitioning method.
 Algorithm 2: Sequential non-overlapping partitioning
 Input : Training Dataset T r.
 The relative size of a subset Rt as a percentage of
 the whole size of the Tr
 Output: a Set of partitions, SUB
 = {t0, t1, t2, ..., tN?1}
 Start:
 Declare counter i =0, as counter to keep track of
 created file.
 Calculate number of subset N =
 ?
 1
 Rt
 ?
 .
 Calculate number of instances in a subset |t| =
 ? |Tr|
 N
 ?
 ,
 where |T r| is number of instances in Tr.
 while not reach the end of Tr do
 Create a subset ti
 if (i = (N ? 1)) then
 Read the rest of the instances.
 Save them into subset ti
 end
 Read the first |t| unseen instances from Tr.
 save them into ti
 end
 End
 2) Round robin partitioning method: In round robin
 partitioning, the first instance of a training dataset
 goes to the first subset, the second to the second
 subset, and so on until the last subset. If the last
 subset is reached, then the method starts over from
 the first subset [10]. Algorithm 3 shows how to
 round robin partitioning method works.
 3) Sampling without replacement: Sampling is a
 technique used to create subsets by selecting a
 random instance from a dataset. Random selection
 means that all the data instances have the same prob-
 ability of being selected at any time. In sampling
 without replacement, if an instance was chosen to be
 43
Algorithm 3: Round robin partitioning method.
 Input : Training Dataset T r.
 The relative size of a subset Rt as a percentage of
 the whole size of the Tr
 Output: A set of partitions, SUB
 = {t0, t1, t2, ..., tN?1}
 Start:
 Declare counter i =0, as counter to keep track of
 created file.
 Declare counter j =0, as counter to keep track of
 training instances.
 Calculate number of subset N =
 ?
 1
 Rt
 ?
 .
 Calculate noInstances = |T r| , where |T r| is number
 of instances in Tr.
 while j < noInstances do
 reset counter i.
 while i < N do
 xj = Read data instance (j) from Tr.
 Save xj into subset ti.
 end
 end
 End
 a member of a subset, this instance cannot be chosen
 to be a member of any other subset during the
 sampling process. Algorithm 4 describes sampling
 without replacement method.
 B. Modelling Phase
 The modelling phase is the second stage of constructing an
 ensemble. In this phase a learning algorithm is chosen to learn
 from each subset of the data generated in the partitioning phase
 and then to create a data model. In principle, any learning
 algorithm can be used in this phase.
 In this study a core vector machine (CVM) [11] is used as
 the base classifier because it performs very well with large
 datasets and its source code is available for free from the
 authors website for research purposes [12].
 Core Vector Machine (CVM) Algorithm
 Tsang et al. proposed CVM as an enhanced version of the stan-
 dard SVM to accelerate the learning process. A standard sup-
 port vector machine (SVM) has O(|T r|3) time and O(|T r|2)
 space complexities where |T r| is the size of the training
 dataset. While CVM has a linear time complexity in |T r| and
 space complexity independent of |T r|, this enhancement was
 achieved by formulating “a kernel (including the soft-margin
 one-class and two-class SVMs) as the equivalent minimum
 enclosing ball (MEB) problem and then obtain approximately
 optimal solutions efficiently with the use of core sets”[11].
 Tsang and his colleagues have proven experimentally that their
 proposed CVM algorithm achieves a similar performance to
 standard SVM but is much faster especially with large datasets.
 In 2007, BVM with enclosing balls was introduced [13]as
 a faster version of CVM.
 As mentioned earlier, other types of learning algorithms
 can be used, and the base learners choice depends on the
 Algorithm 4: Sampling without replacement
 Input : Training Dataset T r.
 The relative size of a subset Rt as a percentage of
 the whole size of the Tr
 Output: a Set of partitions, SUB
 = {t0, t1, t2, ..., tN?1}
 Start:
 Declare counter i =0, as counter to keep track of
 created file.
 Declare availableSubSets , as array that contains
 pointers to available subsets.
 Calculate number of subset N =
 ?
 1
 Rt
 ?
 .
 Calculate number of instances in a subset |t| =
 ? |Tr|
 N
 ?
 ,
 where |T r| is number of instances in Tr.
 while i < N do
 create subset ti
 size of ti, |ti| = 0
 add ti into availableSubSets
 end
 while not reach the end of Tr do
 x = Read data instance from Tr.
 select a random subset t form availableSubSets
 save x into subset t
 |t| = |t|+ 1
 if |t| = N then
 Remove t from availableSubSets.
 end
 end
 End
 suitability for the data. This choice and other related issues
 in this phase will not be discussed in this paper as they are
 not the focus of the study at this stage.
 C. Combining Phase
 The combining phase is the last phase of the ensemble’s
 operation. The aim of this phase is to produce the final result
 of the ensemble by combining the predictions of individual
 models. Majority voting will be used as a fusion strategy in
 this experiment. Similar to the modelling phase, other types
 of fusion strategies can be used, their effect on the ensemble
 performance will not be discussed in this paper, as the focus
 of this paper is on partitioning phase.
 IV. EXPERIMENTAL DESIGN
 A. The aim of the experiments
 In this paper, the experiments are designed and carried
 out with an aim to examine the effect of three partitioning
 strategies on the ensemble performance when mining large
 datasets using the proposed ensemble method. In addition, the
 influence of the subset’s size on the ensemble performance will
 be examined too.
 B. Experimental Design:
 1) Datasets: Five datasets of different sizes were used to
 evaluate the performance of our ensemble. The main char-
 acteristics of these datasets are listed in Table I. The Cover
 44
type dataset is available from the UCI Machine Learning
 Repository. In our experiments the used adult dataset is a
 preprocessed version of the original adult dataset which is
 available from the UCI Repository. The details of the prepro-
 cessing process for the adult dataset can be found in [14]. The
 Web and IJCNN01 datasets are available on Libsvm website
 1
 . In addition, IJCNN01 is the preprocessed version of the
 original IJCNN01; full information about the preprocessing
 process can be found in [15]. The KDDCUP-99 dataset was
 downloaded from [12], and the description of the dataset is
 available from the KDD website 2.
 All 5 datasets come with separate testing datasets. For
 evaluation purposes, a validation set was created from each
 training set. The size of validation dataset is 30% of the
 training set. Table II shows the characteristics of used datasets
 after the creation of the validation dataset.
 TABLE I. DATASETS USED
 Dataset Training Testing No. of Numeric categorical
 Instances Instances Attributes Attributes Attributes
 Adult 32,561 16,281 123 0 123
 Web 49,749 14,951 300 0 300
 IJCNN01 49,990 91,701 22 2 20
 Cover type 522,910 58,102 54 51 3
 KDDCUP-99 4,898,431 311,029 127 120 7
 TABLE II. DATASETS USED AFTER THE CREATION OF VALIDATION
 DATASETS
 Dataset Training Validation Testing No. of
 Instances Instances Instances Attributes
 Adult 22,793 9,768 16,281 123
 Web 34,825 14,924 14,951 300
 IJCNN01 34,993 14,997 91,701 22
 Cover type 366,037 156,873 58,102 54
 KDDCUP-99 3,428,902 1,469,529 311 127
 2) Experiment procedure and setups: :
 1) Three series of experiments were conducted, one for
 each partitioning method. Figure 2 illustrates how
 each experiment is performed.
 2) For each experiment, five datasets will be used.
 3) Although for each of the five datasets a separate test-
 ing dataset is provided for testing the performance
 of our ensemble system, a validation dataset will
 be created from each training dataset to be used to
 estimate the learning performance of our ensemble
 before testing.
 4) For a given dataset, each experiment will be repeated
 several times by varying the relative size of the
 subset Rt in a systematic manner as shown in the
 following points.
 5) In all the experiments, the relative size of the
 subset should be chosen to create an odd number
 of partitions to avoid a potential tie situation when
 using majority voting as a fusion strategy.
 6) The relative size of a subset Rt, is varied from
 a given minimum Rtmin = 1%, to maximum
 Rtmax = 49% subject to the above requirements
 in the 5th and 6th steps. The value of Rt varied as
 follow Rt = 4%,6%,7%, 8%, 12%, 15%, 16%, 20%,
 21%, 22%, 23%, 24%, 34%, 35%, 36%, 37% 38%,
 1http://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/
 2http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html
 39%, 40%, 41%, 42%, 43,% 44%, 45%, 46%, 47%
 and 48%.
 7) CVM and majority voting will be used as the base
 classifier and the fusion strategy in each experiment
 respectively.
 8) For each experiment, the results of the ensemble and
 individual models in training, validation, and testing
 datasets will be calculated. Also, the ensemble time
 will be recorded.
 9) Statistical significance test will be carried out to
 analyse the results of experiments.
 Fig. 2. The experiment procedure
 3) Evaluation Methods:
 1) Accuracy and Efficiency
 The accuracy is used as a performance measure in
 this study. In addition partitioning time is recorded
 as a measure of efficiency but it is not discussed
 in this stage of the research due to the three used
 partitioning strategies have very similar time.
 2) Statistical significance analysis
 McNemar’s [16] test is used in this experiment to
 compare between the classification errors of two
 classifiers (ensembles) to find out if they are sta-
 tistically different or not. In addition, Friedman test
 [17], will be used later to compare between more
 than two classifiers over multiple datasets.
 V. EXPERIMENTAL RESULTS
 To evaluate the impact of subset size and partitioning
 strategy on the Acc(?), We run our proposed ensemble on each
 dataset for different runs by varying the Rt and the partitioning
 strategy. The results for each dataset are presented by three
 charts, one for each partitioning strategy. For each strategy
 45
we monitored the ensemble accuracy Acc(?), and the mean
 accuracy of individual models of the ensemble ((Acc(mi)))
 on testing and validation datasets. The experimental results
 section is divided into thee subsections. Firstly the effect of
 Rt on Acc(?), then comparison of the accuracy of the three
 partitioning methods and finally statistical analysis section.
 A. The effect of Rt on Acc(?)
 Fig. 3. Adult dataset experimental results, accuracy of ensemble, the mean
 accuracy of individual models within the ensemble on validation and testing
 datasets, against the size of subset
 1) Result of Adult dataset:
 • Relationship between Acc(?) and Rt
 On the adult dataset, the highest Acc(?) was achieved
 in the three partitioning methods when a small Rt
 was used as a size for the subsets. Figure 3 shows
 the Acc(?) on testing and validation datasets for
 adult dataset when the three partitioning methods were
 used. The left chart in Figure 3 illustrates the result
 when the sequential non-overlapping partitioning was
 used, and it shows that the Acc(?) on the testing
 dataset fell from 84.82% to 82.77% when the subset
 size was increased from 4% to 48% respectively. In
 the same manner, the middle graph and the right
 shows that the Acc(?) decreased when the subset size
 increased and round robin partitioning and sampling
 without replacement were used, respectively.
 • Relationship between Acc(?) and (Acc(mi))
 Another important result that can be observed from
 Figure 3 is that the Acc(?) achieved better perfor-
 mance than the mean of the accuracy of individual
 models of the ensemble ((Acc(mi))) in all the exam-
 ined cases.
 Fig. 4. Web dataset experimental results, accuracy of ensemble, the mean
 accuracy of individual models within the ensemble on validation and testing
 datasets, against the size of subset
 2) Results of web dataset:
 • Relationship between Acc(?) and Rt
 The effect of the size of subset on Acc(?) in the web
 dataset was completely different from its effect on
 the adult dataset. Figure 4 illustrates the relationship
 between Acc(?) and Rt for the three used partitioning
 strategies.
 When round robin and sampling without replacement
 were used, the Acc(?) had a proportional relationship
 to Rt. The only difference between the behaviour of
 the two partitioning methods on this dataset is that in
 round robin partitioning, the Acc(?) increased as a
 result of increasing Rt up to Rt = 34%. Then after
 that point, (Rt = 34%) the Acc(?) became steady and
 no improvement was achieved by the growth of Rt.
 When sampling without replacement was used, the
 Acc(?) continued to increase by the increase of Rt.
 The big difference occurred when the sequential non-
 overlapping partitioning was used, which shows that
 Acc(?) was not affect by Rt at all and that the Acc(?)
 was steady when the value of Rt varied from 4% to
 48%.
 • Relationship between Acc(?) and (Acc(mi))
 Figure 4 also shows that when round robin and
 sampling without replacement were used, the Acc(?)
 achieves still better performance than the mean of the
 accuracy of individual models ((Acc(mi))). While in
 sequential non-overlapping partitioning, although the
 Acc(?) was steady and did not improve during the
 experiment, the ((Acc(mi))) has a direct relationship
 to Rt and ((Acc(mi))) where (Acc(mi)) = 95.47%
 when Rt =4% up to 97.62% when Rt=48%. In ad-
 dition the ((Acc(mi))) achieved better performance
 than the Acc(?) between Rt = 21% and Rt=48%.
 Fig. 5. IJCNN dataset experimental results, accuracy of ensemble, the mean
 accuracy of individual models within the ensemble on validation and testing
 datasets, against the size of subset
 3) Results of IJCNN dataset:
 • Relationship between Acc(?) and Rt
 Figure 5 shows that when the sequential non-
 overlapping partitioning was used, the Acc(?) on
 the testing dataset increased from 94% to 98% when
 Rt also increased from 4% to 48%. When round
 robin partitioning was used, Acc(?) on testing and
 validation datasets varied up and down, with no
 obvious pattern or trend, starting from Rt = 4% up
 to 34%; after 34%, the Acc(?) become steady and
 was not affected by the Rt, whatever its size. When
 sampling without replacement was applied, there was
 an improvement to Acc(?) on validation and testing
 datasets by increasing of Rt, as illustrated by the right
 chart in Figure 5.
 • Relationship between Acc(?) and (Acc(mi))
 46
By focusing on the relationship between Acc(?)
 and mean of the accuracy of individual models
 ((Acc(mi))), Figure 8 shows that our ensemble per-
 forms better than ((Acc(mi))) on the sequential non-
 overlapping and sampling methods. When round robin
 was used, the Acc(?) achieved nearly similar per-
 formance to ((Acc(mi))) in almost all cases except
 between Rt = 21% and 34% where ((Acc(mi)))
 showed better performance than Acc(?).
 Fig. 6. Cover type dataset experimental results, accuracy of ensemble, the
 mean accuracy of individual models within the ensemble on validation and
 testing datasets, against the size of subset
 4) Results of Cover type dataset:
 • Relationship between Acc(?) and Rt
 The relationship between Acc(?) and Rt for this
 dataset is very similar to the relationship between
 Acc(?) and Rt on adult dataset. The highest Acc(?)
 was achieved when a small Rt was used. Figure 6
 shows the Acc(?) on testing datasets fell dramatically
 from 78.86% to 62.94% when Rt varied from 4% to
 48% when the sequential non-overlapping partition-
 ing was used. The same relationship pattern appears
 when round robin partitioning and sampling without
 replacement were used.
 • Relationship between Acc(?) and (Acc(mi))
 Figure 6 also illustrates that Acc(?) achieves better
 performance than the mean of the accuracy of indi-
 vidual models ((Acc(mi))) in all the examined cases
 in the three partitioning strategies.
 Fig. 7. KDDCUP-99 intrusion dataset experimental results, accuracy of
 ensemble, the mean accuracy of individual models within the ensemble on
 validation and testing datasets, against the size of subset
 5) Results of KDDCUP-99 intrusion dataset:
 • Relationship between Acc(?) and Rt
 In this dataset, the Acc(?) for the testing and vali-
 dation dataset in all three partitioning strategies has
 not been affected by the relative size of the subset Rt.
 The Acc(?) was steady during the whole experiment
 as Figure 7 shows.
 • Relationship between Acc(?) and (Acc(mi))
 Figure 7 also illustrates that Acc(?) achieved bet-
 ter performance than the mean of the accuracy of
 individual models ((Acc(mi))) in all the examined
 cases in which the round robin and sampling with-
 out replacement were used. While sequential non-
 overlapping partitioning was used on testing and val-
 idation datasets, the Acc(?) achieved better perfor-
 mance than (Acc(mi)) from Rt=4% up to Rt=40%.
 The (Acc(mi)) achieved better performance from
 Rt=42% than the Acc(?).
 B. Comparing the accuracy of the three partitioning methods
 1) Adult dataset: Figure 8 shows that the sequential non-
 overlapping partitioning and round robin partitioning yield
 similar pattern on the testing dataset, while sampling without
 replacement achieves lowest accuracy compared to the previ-
 ous two partitioning strategies.
 2) Web dataset: By comparing the performance of the three
 partitioning methods depending on testing accuracy, it is clear
 from Figure 8 that round robin and sampling without replace-
 ment have very similar performance, whereas the sequential
 non-overlapping partitioning yields lowest performance and
 has no effect on accuracy of ensemble on the test data.
 3) IJCNN dataset: Figure 8 shows that the round robin
 partitioning achieved the worst accuracy in general, compared
 to the other two strategies. Sampling without replacement
 achieved better accuracy in most cases.
 4) Cover type dataset: By comparing the accuracy of the
 three partitioning methods depending on testing accuracy, it
 is clear from Figure 8 that the sequential non-overlapping
 partitioning and sampling without replacement have very sim-
 ilar accuracy, while the round robin partitioning achieves the
 lowest accuracy when a small subset size is used, and the
 highest accuracy when large subsets are used compared to the
 other two partitioning strategies.
 5) KDDCUP-99 intrusion dataset: By comparing the ac-
 curacy of the three partitioning methods depending on testing
 accuracy, it is clear from Figure 8 that round robin and
 sampling without replacement have very similar accuracy and
 much better accuracy than that achieved by the sequential non-
 overlapping partitioning.
 C. Statistical analysis
 We statistically evaluated our ensembles using two non-
 parametric approaches: the McNemars and Friedman tests.
 1) McNemar’s Test: McNemars test [16] is suitable to test
 two classifiers on a single domain. In this set of experiments
 McNemars test was used to determine if the classification
 errors of two ensembles with different Rt are statistically
 different.
 For all datasets and for each partition , we compared the
 ensemble created from the smallest Rt of 4% denoted by
 ensemble4 and the largest Rt of 48% denoted by ensmble48.
 In most of the 15 examined cases the results of McNemars
 test provide strong evidence to reject the null hypothesis with
 47
Fig. 8. Compare the effect of different partitioning methods on
 Adult,Web,IJCNN, Cover Type and KDDCUP-99 datasets
 a P value less than 0.0005, which means that the ensemble
 created from the training datasets with Rt = 4% is significantly
 different than the ensemble created from the training datasets
 with Rt = 48%.
 Four cases did not present enough evidence to reject the
 null hypothesis: the Web dataset when the sequential non-
 overlapping partitioning was used and in the intrusion dataset
 when all the three partitioning methods were used.
 2) Friedman Test: The Friedman test can be used to
 evaluate multiple classifiers for multiple datasets [17]. In this
 set of experiments the Friedman test was used to determine
 if the three partitioning strategies used in this experiment are
 statically different.
 The results show that there is not a statistical difference in
 the performance of the three partitioning methods on the five
 datasets with p=0.819.
 VI. DISCUSSION
 Our results confirm that subset size affects the ensemble
 accuracy Acc(?) in different ways, which can be roughly
 classified into three groups of patterns as shown in Figure9.
 In P1, Acc(?) decreased when increasing the subset size (Rt).
 In P2, the Acc(?) increases with the growth of Rt. In P3, the
 Acc(?) does not improve or decrease with the increase of Rt.
 Fig. 9. Possible conceptual relationship patterns between Acc(?) and Rt
 Figure 10 show the true relationship patterns between the
 relative subset size Rt and Acc(?)in the three partitioning
 Fig. 10. Relationship patterns between Acc(?) and Rt which were observed
 in our experiments
 strategies examined in this experiment for all the used datasets.
 It is clear from Figure 10 that the relationship between Rt
 and Acc(?)in Adult and Cover type datasets is similar to
 pattern P1, while in web and IJCNN datasets their relationship
 is similar to pattern P2 and the relationship in the intrusion
 dataset is similar to P3.
 In this experiment we intended to find out possible relation
 between the pattern group and dataset. It seems that the number
 of instances and number of attributes are not the only factors
 that determine the relationship pattern. Adult and Cover types
 belong to the same pattern, while they are very different in
 terms of number of instances and number of attributes. Also,
 Web and IJCNN belong to the same pattern; although they have
 a similar number of instances, but a huge difference between
 their numbers of attributes. For these reasons, there is a need
 for further investigation to find how to determine the pattern
 of the relationship and how to find the suitable subset size.
 In addition, our results show that sampling without replace-
 ment and round robin partitioning achieves better accuracy in
 80% of the examined datasets. Table III shows the best par-
 titioning strategy for each dataset depending on their Acc(?)
 on testing datasets.
 TABLE III. BEST PARTITIONING STRATEGIES FOR EACH DATASET
 Dataset Best partitioning strategies
 Adult sequential non-overlapping and round robin
 Web round robin and sampling without replacement
 IJCNN01 round robin and sampling without replacement
 Cover type sequential sampling without replacement
 KDDCUP-99 round robin and sampling without replacement
 VII. CONCLUSIONS
 In this paper, we explored the relationship between parti-
 tioning methods and Acc(?). For this task we examined three
 partitioning strategies: sequential non-overlapping partitioning,
 round robin partitioning and sampling without replacement.
 Although sampling without replacement and round robin par-
 titioning achieved better accuracy in 80% of the examined
 datasets, as shown in Table III, the results of the statistical
 test do not present enough evidence to conclude if there is a
 significant difference in the performance of the three partition-
 ing strategies on the five examined datasets. Nevertheless in
 practise, sampling should be considered in the first place if
 there is no specific knowledge about the other two.
 We also investigated the relationship between the relative
 size of the subset (Rt) and the ensemble accuracy (Acc(?)).
 We found that Rt can have effect on Acc(?). As illustrated
 in Figure 9, this effect can be represented by one of three
 48
relationship patterns between Rt and Acc(?). Our results show
 that the well accepted idea that having more training data
 instances generally leads to the best classification performance
 is not always true, as the best accuracy for in Adult and Cover
 Type datasets were achieved by having small subset sizes.
 The relationship patterns of a dataset cannot be detected by
 the number of its instances and attributes because it is found
 they are not the only factors that affect the behaviour of the
 relationship between Acc(?) and Rt for a dataset.
 Further study should include more investigation to discover
 the relationship type between ensemble accuracy (Acc(?)) and
 the relative size of a subset (Rt), as the number of instances
 and number of attributes do not provide an interpretation for
 this relationship. For that reason, it is necessary to repeat
 the experiment on larger datasets and apply some vertical
 petitioning techniques and conduct an in-depth and detailed
 analysis to find out how to determine the best size of a subset
 and what type of partitioning strategies is suitable for a dataset.
 REFERENCES
 [1] S. Totad, R. Geeta, C. Prasanna, N. Santhosh, and P. Reddy, “Scal-
 ing data mining algorithms to large and distributed datasets,” Intl J
 Database Manag Syst, vol. 2, pp. 26–35, 2010.
 [2] N. Amano, J. o Gama, and F. Silva, “Exploiting parallelism in decision
 tree induction,” in Proceedings from the ECML/PKDD Workshop on
 Parallel and Distributed computing for Machine Learning, 2003, pp.
 13–22.
 [3] N. Kerdprasop and K. Kerdprasop, “Data partitioning for incremental
 data mining,” in The 1st International Forum on Information and
 Computer Science. Citeseer, 2003, pp. 114–118.
 [4] I. Tsang, A. Kocsor, and J. Kwok, “Diversified SVM ensembles for
 large data sets,” Machine Learning: ECML 2006, pp. 792–800, 2006.
 [5] T. Oates and D. Jensen, “The effect of training set size on decision tree
 complexity,” in Proceedings of the 14th International Conference on
 Machine Learning, 1997, pp. 254–262.
 [6] Oates, Tim and Jensen, David, “Large datasets lead to overly complex
 models: an explanation and a solution,” in Proceedings of the Fourth
 International Conference on Knowledge Discovery and Data Mining,
 1998, pp. 294–298.
 [7] L. Hall, N. Chawla, K. Bowyer et al., “Combining decision trees learned
 in parallel,” in Working Notes of the KDD-97 Workshop on Distributed
 Data Mining, 1998, pp. 10–15.
 [8] D. Patil and R. Bichkar, “An optimistic data mining approach for
 handling large data set using data partitioning techniques,” International
 Journal of Computer Applications, vol. 24, no. 3, pp. 29–33, 2011.
 [9] J. Basilico, M. Munson, T. Kolda, K. Dixon, and W. Kegelmeyer,
 “COMET: A recipe for learning and using ensembles on massive data,”
 Data Mining (ICDM), 2011 IEEE 11th International Conference on,
 pp. 41–50, 2011.
 [10] C. Ranichandra and J. Vijayashree, “Efficient like commands for
 dynamic data retrieval and report generation from parallel databases.”
 Asian Journal of Computer Science and Information Technology, vol. 2,
 no. 04, 2012.
 [11] I. Tsang, J. Kwok, and P. Cheung, “Core vector machines: Fast SVM
 training on very large data sets,” Journal of Machine Learning Research,
 vol. 6, no. 1, pp. 363–392, 2005.
 [12] I. Tsang, A. Kocsor, and J. Kwok, “Libcvm toolkit,”
 http://c2inet.sce.ntu.edu.sg/ivor/cvm.html, 2011, accessed: 02/02/2012.
 [13] I. W. Tsang, A. Kocsor, and J. T. Kwok, “Simpler core vector machines
 with enclosing balls,” in Proceedings of the 24th international confer-
 ence on Machine learning, ser. ICML ’07. ACM, 2007, pp. 911–918.
 [14] J. Platt et al., “Sequential minimal optimization: A fast algorithm for
 training support vector machines,” 1998.
 [15] C.-c. Chang and C.-J. Lin, “Ijcnn 2001 challenge: Generalization ability
 and text decoding,” in Neural Networks, 2001. Proceedings. IJCNN’01.
 International Joint Conference on, vol. 2. IEEE, 2001, pp. 1031–1036.
 [16] T. G. Dietterich, “Approximate statistical tests for comparing supervised
 classification learning algorithms,” Neural computation, vol. 10, no. 7,
 pp. 1895–1923, 1998.
 [17] J. Demsˇar, “Statistical comparisons of classifiers over multiple data
 sets,” The Journal of Machine Learning Research, vol. 7, pp. 1–30,
 2006.
 49

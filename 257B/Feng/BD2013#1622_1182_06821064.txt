Smart Task Distributor for MapReduce on Cloud 
Computing 
 
Tzu-Chi Huang1 Kuo-Chih Chu2  Jun-Ming Liang3 
Department of Electronic Engineering 
Lunghwa University of Science and Technology 
Taoyuan County, Taiwan123 
tzuchi@mail.lhu.edu.tw1 kcchu@mail.lhu.edu.tw2 aazz77aa@gmail.com 3 
 
 
Abstract—A MapReduce system is widely used to implement the 
large-scale computation on cloud computing. A MapReduce system 
currently defines computation resources in a node as a roughly 
configurable slot number, and distributes tasks over nodes according 
to the slot number. However, a MapReduce system may make 
computation resources of clusters in the underutilization or 
overutilization condition, because a task of different applications 
unlikely uses the same computation resources and because a node 
may have different CPUs with different capabilities. A MapReduce 
system can use Smart Task Distributor (STD) proposed in this paper 
to solve the problem of the computation resource underutilization or 
overutilization in clusters. Technically, a MapReduce system can use 
STD to smartly distribute tasks over nodes in clusters, because STD 
on the one hand gradually assigns tasks to a node in order to fully 
utilize computation resources in the node and on the other hand 
dynamically estimates the remaining computation resources in the 
node for toggling the assignment of tasks on demand without 
overloading it. In experiments, a MapReduce system is proved to get 
better performances with STD than with other ways. 
Keywords—STD; Computation Resources; Overutilization; 
Underutilization; MapReduce; Cloud Computing 
I.  INTRODUCTION 
MapReduce [1] is the key to the success of cloud 
computing today. As a programming model, MapReduce can 
easily distribute over nodes in clusters for an application that is 
programmed according to the model. MapReduce allows 
application developers who are not familiar with distributed 
and paralleled programming to easily develop applications on 
cloud computing. All MapReduce needs is application 
developers to implement their applications with a Map function 
(a.k.a. Mapper) and a Reduce function (a.k.a. Reducer). At 
runtime, MapReduce uses the runtime system to automatically 
distribute Mappers and Reducers over nodes in clusters on 
behalf of application developers. Because MapReduce highly 
relies on the runtime system, the design of the runtime system 
greatly affects the performance of applications on cloud 
computing. 
On cloud computing, the runtime system partitions input 
files into multiple blocks at the initiation time of an application. 
Next, the runtime system finds a node that has available 
computation resources and then distributes a Mapper of the 
application over the node. When running a Mapper on a node, 
the runtime system provides the Mapper with blocks of input 
files, e.g. loading them from the master node or a distributed 
file system such as the Google File System (GFS) [2] and the 
Hadoop Distributed File System (HDFS) [3]. After a Mapper 
ends execution and generates output files (a.k.a. intermediate 
files), the runtime system finds a node that has available 
computation resources to run a Reducer and forwards the 
intermediate files from the Mapper to the Reducer. Finally, the 
runtime system collects, sorts, and merges the output files 
generated by Reducers in all nodes as the result of the 
application. 
However, a runtime system in the existing MapReduce 
prototypes, e.g. Hadoop [3], defines computation resources in a 
node as a roughly configurable slot number. For example, a 
runtime system may define a node to have two slots capable of 
running two Mappers or Reducers if the node has a dual-core 
CPU in it. Currently, a runtime system leaves the configuration 
of slot number in a node to the administrator of the runtime 
system or the designer of the prototype. Usually, a runtime 
system has a default value and no administrator wants to 
change it arbitrarily. Because a Mapper or Reducer of different 
applications unlikely uses the same computation resources and 
because a node may have different CPUs with different 
capabilities, a runtime system may make computation 
resources of clusters in the underutilization or overutilization 
condition if distributing Mappers or Reducers over nodes 
according to the roughly configurable slot number. 
A runtime system maybe incurs the problem of 
computation resource underutilization if occupying slots in a 
node with I/O-bound tasks [4]. For example, a runtime system 
may distribute to a node the tasks that consume very few 
computation resources, so the node almost idles even though its 
slots are all used. Conversely, a runtime system maybe incurs 
the problem of computation resource overutilization if 
occupying slots in a node with CPU-bound tasks [5]. For 
example, a runtime system may distribute to a node the tasks 
that consume many computation resources, so the node is too 
overloaded to give the underlying operating system or the 
runtime system enough computation resources for performing 
certain important routines. Furthermore, when two nodes have 
CPUs with different capabilities but are configured the same 
slot number just according to the core number, a runtime 
system may incur the problem of computation resource 
underutilization in one node but incur the problem of 
computation resource overutilization in the other node. 
2013 International Conference on Cloud Computing and Big Data
 978-1-4799-2829-3/13 $26.00 © 2013 IEEE
 DOI 10.1109/CLOUDCOM-ASIA.2013.110
 6541978-1-4799-2830-9/14 $31.00 © 2014 IEE
In this paper, Smart Task Distributor (STD) is proposed to 
solve the problem of the computation resource underutilization 
or overutilization in clusters on cloud computing. STD is a 
module working on a master node to help the runtime system 
smartly distribute tasks over nodes in clusters. STD is 
heuristically designed to distribute tasks over nodes according 
to the Additive-Increase/Multiplicative-Decrease (AIMD) 
algorithm [6]. On the one hand, STD can gradually assign tasks 
to a node in order to fully utilize computation resources in a 
node. On the other hand, STD can dynamically estimate the 
remaining computation resources in a node and toggle the 
assignment of tasks on demand without overloading the node. 
Accordingly, STD can prevent nodes from working in the 
underutilization or overutilization condition. In experiments, 
STD is tested with several canonical applications in nodes 
having different slot numbers. Besides, STD is compared with 
a General Task Distributor (GTD) that assigns a task to the 
node having the most available slots, and an algorithm 
(referred to as IDLEST) that assigns a task to the node having 
the most computation resources (i.e. the idlest node). 
According to experiment observations, STD outperforms GTD 
and IDLEST which are the two algorithms widely used in the 
existing runtime systems. 
We briefly highlight the contributions of this paper as 
follows. 
1. Although there are several algorithms capable of 
estimating computation resources in a node in literature, e.g. 
history-based [7] or Single Exponential Smoothing (SES) [8] 
algorithms, we are the first one to propose estimating 
computation resources based on the AIMD algorithm [6], a 
well-known algorithm used by TCP to estimate available 
network bandwidth. 
2. We use STD based on the AIMD algorithm to 
simultaneously avoid the problems of computation resource 
underutilization and overutilization in clusters on cloud 
computing. 
3. We implement STD on a general runtime system and test 
it with several canonical applications to verify its practicability. 
4. We compare STD with GTD and IDLEST in 
experiments to confirm that STD outperforms the other 
algorithms. 
This paper is organized as follows. Section 2 reviews 
MapReduce. Section 3 introduces Smart Task Distributor 
(STD). Section 4 presents experiment results. Section 5 
concludes this paper. 
II. MAPREDUCE 
MapReduce [1] is a programming model proposed by 
Google to compute big data in clusters on cloud computing. 
MapReduce attracts application developers because the 
programming model is simple. MapReduce only needs 
application developers to develop their applications with a Map 
function (a.k.a. Mapper) and a Reduce function (a.k.a. 
Reducer). At runtime, MapReduce relies on the runtime system 
to read input data, distribute Mappers and Reducers over nodes 
in clusters, provide Mappers with input data, forward 
intermediate data from Mappers to Reducers, and collect 
outputs of Reducers as the result of the application. Because 
the runtime system handles the issues of distributing 
applications over nodes, managing tasks, and collecting results 
on behalf of application developers, MapReduce gradually 
becomes a de facto standard in programming applications on 
cloud computing. 
Word Count [1] is a typical application often used to 
explain MapReduce and its runtime system. Word Count needs 
application developers to develop a Mapper that processes 
input data tokenized by the runtime system and generates a key 
and value pair for each word in input data. When reading a 
word “good” in input data, for example, Word Count uses a 
Mapper to generate “good 1” as intermediate data stored in 
local disks. In a Reducer, Word Count can receive intermediate 
data forwarded by the runtime system from certain Mappers, 
and sum all identical words as the count of the word. For 
example, Word Count can generate “good 10” as a part of the 
final result if getting ten strings of “good 1” in intermediate 
data. Finally, Word Count can get the counts of all words in 
input data after the runtime system collects outputs from all 
Reducers. 
In MapReduce, the runtime system plays a critical role. The 
runtime system has to read and partition input data into blocks, 
and then tokenizes data in the blocks before giving tokenized 
data to Mappers. During the execution of a Mapper, the 
runtime system collects its outputs, i.e. intermediate data, and 
forwards intermediate data to the corresponding Reducers over 
networks. Finally, the runtime system has to collect, sort, and 
merge outputs from all Reducers as the result of the 
application. The runtime system manages the tasks, i.e. 
instances of Mappers and Reducers, and has a great impact on 
the performance of an application when distributing tasks over 
nodes in clusters. 
In the existing MapReduce prototypes, the runtime system 
uses a master to distribute tasks over nodes according to 
available slot numbers in nodes. If a node is defined to have 
two slots, for example, the runtime system never gives the node 
more than two tasks. However, the runtime system hardly gives 
an application a better performance according to the method of 
quantifying computation resources with slot numbers, because 
different CPUs have different capabilities. Merely relying on 
slot numbers based on the core number in a CPU, the runtime 
system may accidentally make nodes in the condition of 
computation resource overutilization or underutilization, for 
example, by giving four tasks to a node having a low-end quad-
 core CPU but two tasks to a node having a high-end dual-core 
CPU. 
III. SMART
  TASK DISTRIBUTOR (STD) 
A. Overview 
Smart Task Distributor (STD) is a module working in a 
master of the runtime system and distributes tasks over nodes 
according to the Additive-Increase/Multiplicative-Decrease 
(AIMD) [6] algorithm. STD periodically collects CPU 
utilization statistics from nodes and uses the AIMD algorithm 
to estimate future CPU utilization of each node. When having a 
6552
task ready to run, STD gives the task to the node that is 
estimated to have the least CPU utilization in the future. 
We can use Fig. 1 to explain STD and compare it with a 
General Task Distributor (GTD) that assigns a task to the node 
having the most available slots, and an algorithm (referred to as 
IDLEST) that assigns a task to the node having the most 
computation resources (i.e. the idlest node). We know that 
GTD finds the node having the highest slot number because 
GTD uses slot numbers to quantify computation resources in 
nodes. If a node already has 90% CPU utilization, for example, 
we know that GTD still assigns a new task to the node instead 
of other nodes because the node has the most slots among 
nodes. Conversely, we know that IDLEST will assign a new 
task to a node having 65% CPU utilization because the node 
has the lowest CPU utilization. If the node uses a low-end 
CPU, we think that assigning a new task to the node will 
further degrade the performance because the node may easily 
be overloaded. In STD, we probably assign a new task to a 
node having 80% CPU utilization because the node is 
estimated to have the least CPU utilization in the future. 
 
Fig. 1, STD Overview 
B. Theory 
STD uses the Additive-Increase/Multiplicative-Decrease 
(AIMD) [6] algorithm to distribute tasks over nodes in clusters. 
STD assumes that each node has uncertain computation 
resources for tasks of applications because other factors 
probably exist to compete with the tasks for computation 
resources, e.g. threads or processes belonging to the runtime 
system or the operating system. STD uses the AIMD algorithm 
illustrated in Fig. 2 to estimate future computation resources in 
a node. 
Periodically, STD collects current CPU utilization statistics 
from a node and calculates the maximum CPU cost of running 
a task. STD embodies the maximum CPU cost of running a 
task with the maximum increment, i.e., from the CPU 
utilization statistics before running a task to the CPU utilization 
statistics after running a task. STD uses the maximum CPU 
cost observed in the past to dynamically estimate future CPU 
utilization of the node. If current CPU utilization is smaller 
than future CPU utilization of a node, STD increases future 
CPU utilization of the node by the maximum CPU cost of 
running a task and approves of the assignment of a new task to 
the node. If current CPU utilization is not smaller than future 
CPU utilization of a node, STD decreases future CPU 
utilization of the node by a half but does not give the node a 
new task. Finally, STD corrects future CPU utilization of the 
node to avoid overestimating and underestimating it, i.e. the 
statements of limiting it to the value between 1% and 90% in 
Fig. 2. 
 
Fig. 2, AIMD in STD 
IV. EXPERIMENTS 
A. Implementation 
We implement a general runtime system with the PHP 
language [9] to serve MapReduce applications. We program 
the general runtime system to have the basic functions as the 
existing runtime systems, i.e. reading input files from a master, 
running a Mapper on a node, running a Reducer on a node, 
providing a Mapper with input data, forwarding intermediate 
data from Mappers to the corresponding Reducers according to 
a key-length-based hash function, and collecting outputs from 
all Reducers as the result of the application. Accordingly, we 
provide Mappers of an application with input files stored in 
disks of a master at the beginning of the application execution, 
and then collect outputs from all Reducers to save them in 
disks of a master at the end of the application execution. 
We implement three algorithms in the master of the general 
runtime system. We faithfully follow the proposal in this paper 
to implement STD capable of distributing tasks over nodes 
according to the AIMD algorithm. Next, we implement GTD 
capable of finding the node that has the most available slots. 
We implement IDLEST capable of finding the node that has 
the most computation resources (i.e. the idlest node). For 
meeting the requirement of STD and IDLEST, we implement a 
process in each node to periodically broadcast its CPU 
utilization statistics over networks, so a master can receive 
CPU utilization statistics from all nodes in clusters. Currently, 
we configure the broadcast interval of CPU utilization statistics 
to 2 seconds. 
6563
B. Configuration 
We construct a cluster with 9 identical computers that have 
an AMD Phenom II X6 1055T CPU, 4 GB RAM, and a 2 TB 
7200 rpm HD for each of them. We install Apache 2.2.17, PHP 
5.3.8, and Windows 7 on each of the 9 computers and connect 
them to each other with Gigabit Ethernet. Among the 9 
computers, we select a computer to run a master of the runtime 
system, 4 computers to run Mappers, and 4 computers to run 
Reducers. We prepare a 512 MB input file holding random 
data in a disk of the master and upload data to Mappers on 
demand at runtime. We configure each Mapper to process 8 
MB data each time. We observe performances of GTD, 
IDLEST, and STD when configuring different slot numbers in 
the runtime system at the computers responsible for running 
Mappers and Reducers. 
C.
  
Word Count Performance 
First, Word Count [1] is taken to evaluate performances of 
GTD, IDLEST, and STD because it is the typical application 
widely used to test a MapReduce system. Word Count uses a 
Mapper to emit a string “word, 1” for each word found in input 
data and uses a Reducer to merge intermediate data. Finally, 
Word Count collects outputs of Reducers in nodes to get the 
result of the application. Word Count generates much 
intermediate data from Mappers to Reducers over networks. 
 
Fig. 3, Word Count Performance in GTD, IDLEST, and STD 
In Fig. 3, we observe that the three algorithms get results 
similar to each other when the task number per node (a.k.a. slot 
number per node) is configured less than 8. When the task 
number per node is configured larger than 8, we note that 
performances of GTD and IDLEST begin to degrade greatly 
because they make the runtime system dispatch many tasks to 
overload nodes according to the inappropriate configuration. 
Even though IDLEST can find the idlest node among all nodes 
to run a task, we find that its performance degrades too because 
the workload of an extra task may overload the idlest node. 
Conversely, we observe that STD does not continue 
dispatching a task to a node once it predicts that the addition of 
the extra task may overload a node. As a result, we observe that 
the performance of STD does not degrade greatly according to 
the inappropriate configuration of task number per node. 
 
Fig. 4, Word Count Standard Deviation in GTD, IDLEST, and STD 
The standard deviation [10] in GTD, IDLEST, and STD is 
observed because the appearance of various standard 
deviations between Mapper and Reducer numbers implies the 
appearance of strugglers [11] in a MapReduce system and 
because strugglers have negative impacts on performances. In 
Fig. 4, the standard deviation in GTD is not uniform because 
many Reducers are created when the task number per node is 
configured less than 8 and because many Mappers are created 
when the task number per node is configured larger than 8 in 
comparison to IDLEST and STD. Conversely, the standard 
deviation in IDLEST is much less than in GTD according to 
the summation of Mapper and Reducer numbers, although 
Reducers are still much more than Mappers. Nevertheless, the 
standard deviation in STD is the least among the three ways, 
which corresponds to the observations in Fig. 3. 
D. Quick Sort Performance  
 
Fig. 5, Quick Sort Performance in GTD, IDLEST, and STD 
6574
Quick Sort [12] uses a “divide and conquer” policy to sort 
numbers. First, Quick Sort uses a Mapper to put each number 
into the corresponding intermediate file according to its digital 
length. Second, Quick Sort in a Reducer sorts numbers in 
intermediate data belonging to its responsibility with a 
recursion-based algorithm. Finally, Quick Sort merges outputs 
from all Reducers as the result of the application. Quick Sort 
costs much CPU time in Reducers because they are responsible 
for sorting numbers, while Mappers merely classify and put 
numbers into different intermediate files. 
In Fig. 5, we observe that Quick Sort can get the best 
performance with the configuration of 8 task number per node 
for the three algorithms. However, we note that GTD and 
IDLEST both have performances to degrade suddenly when 
task number per node is increased, because GTD and IDLEST 
make the runtime system continue dispatching tasks to nodes 
and overload the nodes. Although IDLEST can find the idlest 
node among all nodes to run a new task, we observe that it still 
can not avoid overloading a node to degrade the performance. 
Conversely, we witness that STD seldom degrades its 
performance even though task number per node is 
inappropriately configured (e.g. 16 tasks per node), because 
STD can predict the workload of a node with the addition of an 
extra task to smartly determine whether the task should be 
dispatched to the node or not. 
 
Fig. 6, Quick Sort Standard Deviation in GTD, IDLEST, and STD 
According to Fig. 6, the standard deviation in GTD not only 
has the highest value among the three algorithms but also has 
the most variances among the three algorithms. Specifically, 
the standard deviation in GTD indicates that the runtime 
system creates more Mappers than Reducers when task number 
per node is larger than 8. Similarly, the standard deviation in 
IDLEST has a high value and shows a great difference between 
the number of Mappers and the number of Reducers when task 
number per node is larger than 8. Conversely, the standard 
deviation in STD has a lower value and makes the runtime 
system create nearly identical numbers of Mappers and 
Reducers in each configuration of task number per node. The 
standard deviation in Fig. 6 clearly shows why STD makes 
Quick Sort have better performances than GTD and IDLEST in 
Fig. 5. 
V. CONCLUSIONS 
In this paper, Smart Task Distributor (STD) is proposed to 
solve the problem of the computation resource underutilization 
or overutilization in clusters on cloud computing. STD is 
designed as a module to work on a master node for helping a 
MapReduce runtime system smartly distribute tasks over nodes 
in clusters. STD can dispatch tasks to nodes in clusters 
according to the Additive-Increase/Multiplicative-Decrease 
(AIMD) algorithm. STD can gradually assign tasks to a node in 
order to fully utilize computation resources in a node. STD can 
dynamically estimate the remaining computation resources in a 
node and toggle the assignment of tasks on demand without 
overloading the node. Eventually, STD can prevent nodes from 
working in the underutilization or overutilization condition. In 
experiments, STD is tested with several canonical applications 
in nodes having different slot numbers. Besides, STD is 
compared with a General Task Distributor (GTD) that assigns a 
task to the node having the most available slots, and an 
algorithm (IDLEST) that assigns a task to the node having the 
most computation resources (i.e. the idlest node). According to 
experiments, STD greatly outperforms GTD and IDLEST 
when slot numbers of nodes are inappropriately configured. 
ACKNOWLEDGMENT 
We thank the National Science Council of Taiwan for their 
support of this project under grant number NSC 102-2221-E-
 262-014. We thank Lunghwa University of Science and 
Technology for kindly supporting certain critical devices. 
Besides, we thank reviewers for their valuable comments and 
suggestions. 
REFERENCES 
[1] J. Dean, S. Ghemawat, "MapReduce: Simplified Data Processing on 
Large Clusters", Communications of the ACM, Volume 51, Issue 1, 
2008, pp. 107-113 
[2] S. Ghemawat, H. Gobioff, S.-T. Leung, "The Google File System", 
ACM SIGOPS Operating Systems Review, Vol. 37, Issue 5, 2003, pp. 
29-43 
[3] K. Shvachko, H. Kuang, S. Radia, R. Chansler, "The Hadoop 
Distributed File System", Proceedings of the 2010 IEEE 26th 
Symposium on Mass Storage Systems and Technologies (MSST), 2010, 
pp. 1-10 
[4] A. Rasmussen, V. T. Lam, M. Conley, G. Porter, R. Kapoor, A. Vahdat, 
"Themis: an I/O-efficient MapReduce", Proceedings of the Third ACM 
Symposium on Cloud Computing, Article No. 13, 2012 
[5] C. Tian, H. Zhou, Y. He, L. Zha, "A Dynamic MapReduce Scheduler for 
Heterogeneous Workloads", Proceedings of Eighth International 
Conference on Grid and Cooperative Computing, 2009, pp. 218-224 
[6] F. Baccelli, D. Hong, "AIMD, fairness and fractal scaling of TCP 
traffic", Proceedings of Twenty-First Annual Joint Conference of the 
IEEE Computer and Communications Societies, 2002, pp. 229-238 
[7] M. Burtscher, B. G. Zorn, "Prediction Outcome History-based 
Confidence Estimation for Load Value Prediction", Journal of 
Instruction-Level Parallelism, Vol. 1, 1999 
6585
[8] E. Ostertagova, O. Ostertag, "Forecasting using simple exponential 
smoothing method", Acta Electrotechnica et Informatica, Vol. 12, No. 3, 
2012, pp. 62-66 
[9] R. Lerdorf, "PHP: Hypertext Preprocessor", http://php.net 
[10] A. E. Sarhan, "Estimation of the Mean and Standard Deviation by Order 
Statistics", the Annals of Mathematical Statistics, Vol. 26, No. 3, 1955, 
pp. 505-511 
[11] M. Zaharia, A. Konwinski, A. D. Joseph, R. Katz, I. Stoica, "Improving 
MapReduce performance in heterogeneous environments", Proceedings 
of the 8th USENIX conference on Operating systems design and 
implementation, 2008, pp. 29-42 
[12] C. A. R. Hoare, "Quicksort", the Computer Journal, Vol. 5, Issue 1, 
1962, pp.10-16 
 
6596

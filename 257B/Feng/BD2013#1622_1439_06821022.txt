An Iterative Algorithm for Differentially Private 
Histogram Publication 
 
Hong Chen 
College of Mathematics and Computer Science 
 Fuzhou University 
Fuzhou 350108, China 
E-mail: chenh1989@gmail.com 
 
Tingting Chen 
College of Mathematics and Computer Science 
 Fuzhou University 
Fuzhou 350108, China 
E-mail: 779476746@qq.com 
 
  
 
Yingjie Wu 
College of Mathematics and Computer Science 
 Fuzhou University 
Fuzhou 350108, China 
E-mail: yjwu@fzu.edu.cn 
 
Xiaodong Wang 
College of Mathematics and Computer Science 
 Fuzhou University 
Fuzhou 350108, China 
E-mail: wangxd@fzu.edu.cn 
 
Abstract—Traditionally, the accuracy of range queries in 
differentially private histogram publication was boosted by 
obtaining the Best Linear Unbiased Estimator (BLUE) of the 
nodes’ noisy values in the Differential Privacy Range Tree 
(DPRT). However, exist works require DPRT be perfect k-ary, 
which means it is not suitable for differentially private histogram 
publication under arbitrary tree structure In this paper, we 
propose an efficient iterative algorithm GBLUE for differential 
privacy histogram publication based on any tree structure. The 
correctness of GBLUE is guaranteed by theoretical analysis and 
experimental demonstration. Experimental results show that 
GBLUE is effective and feasible. 
Keywords—differential privacy; iterative algorithm; histogram 
publication 
I. INTRODUCTION  
In real life, many research institutions or organizations will 
release data for statistics and scientific research. How to ensure 
the utility of published data while not leaking private 
information of individuals has become a very popular topic in 
database research field[1,2]. 
In the past ten years, people have put forward some privacy 
preserving data publishing models. However, most of the 
existing privacy protection models are based on anonymization, 
such as k-anonymity [3], l-diversity [4] and t-closeness [5] and 
so on. These models require special attack assumptions and 
some background knowledge, so it has great limitations. After 
that, Dwork et al proposed a more robust privacy protection 
model-differential privacy model [6,7].The model will provide 
strong privacy guarantee by adding random noise to the 
released data. No matter what background knowledge the 
attacker has, he can not identify whether a record is in the 
original dataset or not. Recently, Many researchers focus on 
data dissemination in differential privacy and do some 
meaningful research work, especially in the field of differential 
privacy histogram publication. However, the differential 
privacy protection level and the availability of publishing 
histogram data is a contradiction. Therefore, how to publish 
privacy security data while assuring high data quality is the 
core issue of differential privacy data publication [8]. 
II. RELATED WORKS 
Histogram publication is one general and widely used 
method for releasing data under differential privacy. The basic 
idea of publishing is to divide the table into several disjoint sets 
based on one or more attributes, and each bar in the histogram 
represent the number of records whose attribute value are in 
the specific range. Histograms can visually represent data with 
information about the data distribution, and support statistical 
queries. In recent years, researchers made a number of valuable 
researches in it. 
Dwork [6,7,8] has worked on differential privacy and 
published series of papers on differential privacy data releasing. 
Most researches focus on the statistical queries on the database, 
and achieve differential privacy by simply adding Laplace 
noise to the original results. We review some major work under 
the framework of differential privacy. Xiao [9] developed a 
technique enforced ?-differential privacy with economical cost 
by wavelet transform. Graham Cormode et al[10] releases a 
compact summary of the noisy sparse data with the same 
privacy guarantee by sampling and filtering, which also works 
for [9]. [11] using the clustering technique proposed an 
alternative optimization interval counting queries, and [12] 
using greedy strategy to achieve clustering to improve the 
efficiency of clustering. 
Hay et al [13] proposed one method based on the best 
linear unbiased estimator of the noisy data in perfect k forks 
differential privacy range tree. It is with high accuracy in range 
count query and consistency. However, the existing differential 
privacy histogram publication algorithm based on best linear 
unbiased estimation only applies to a perfect differential 
privacy range tree with k fork. This paper intends to design 
method based on the best linear unbiased estimation that 
2013 International Conference on Cloud Computing and Big Data
 978-1-4799-2829-3/13 $26.00 © 2013 IEEE
 DOI 10.1109/CLOUDCOM-ASIA.2013.41
 403978-1-4799-2830-9/14 $31.00 © 2014 IEE
applies for arbitrary tree structure for differential privacy range 
tree in differential privacy histogram publication, and analyze 
its feasibility and effectiveness with similar algorithms. 
III. PRELIMINARIES 
In this section, we will give the background of the problem, 
the detail definition of ?-differential privacy and will describe 
Hay’s algorithm [13] and some necessary notions to 
understand this algorithm. We will also point out the defect of 
Hay’s algorithm in this section. 
Assume that there is one relational table T, which has n 
records. Users will perform some range query operations, 
which are as follows: 
Select count(*) from T where T.a?[x,y] 
It is obvious that T can be represented by a histogram M 
with several bars, each bar Xi will represented by the number of 
records within one spicific attribute range.  
DEFINITION 1. (?-differential privacy) [6]A randomized 
algorithm ? satisfied ?-differential privacy, if and only if (i) for 
any two tables T1 and T2 that differ only in one tuple, and (ii) 
for any output O of ?, we have 
 P[?(T1) = O] ≤ e? ? P[?(T2 ) = O]  (1) 
When running the randomized algorithm ? that satisfies ?- 
differential privacy, the probability distribution of the output is 
almost the same between these two tables. Which provide 
strong privacy guarantee on whether Mary is in the database or 
not. By Dwork’s theory [5], we can use the Laplace 
mechanism [5] to release M under differential privacy, which 
by adding noise with Laplace distribution. A Laplace 
distribution has a probability function: 
 ( ) 1
 2
 p e
 ?
 ??
 ?
 ?
 =
  (2) 
By equation (2), we get that the Laplace distribution has an 
expectation 0 and a variance 2?2. After adding noise, the 
releasing version histogram M* is accomplished. If we keep ? 
constant, the magnitude ? depends on the sensitivity of the 
query sequences, which is defined as follows. 
DEFINITION 2. (Sensitivity [5]). Let F be a set of functions, 
such that the output of each function f?F is a real number. The 
sensitivity of F is defined as 
 S(F ) = max
 T1,T 2
 ( f (T1) ? f (T2 )
 f ?F
 ∑ )  (3) 
T1 and T2 are any two tables that differ in only one tuple. 
If F is the function set that maps T to M, F will contain m 
functions in total and each of them output a real number entry 
in M respectively, then we have 
THEOREM 1. ([5]) Let F be a set of functions with a 
sensitivity S(F). Let ? be an algorithm that adds independent 
noise to the output of each function in F, such that the noise 
follows a Laplace distribution with magnitude ?. Then, ? 
satisfies    (S(F)/ ?)-differential privacy.  
A bigger ? will lead to a better privacy preserving but a lower 
quality of releasing data and vice versa. In hay’s algorithm, it 
maps the histogram to one perfect k-range tree. And then 
compute the new value of each node with the noisy ones. Here 
k-range tree is defined as follows 
DEFINITION 3. (k-range tree) For given statistical 
histogram Mk-range tree satisfies: 
(1) k is no less than 2 
(2) Each Tree Node x holds domain interval [L(x),R(x)] 
and the count sum from L(x) to R(x), we will use 
Seg(x)={L(x),L(x)+1…R(x)} to denote [L(x),R(x)]. The node 
with |Seg(x)|=1 is leaf node 
(3) Node x has some child nodes, Son(x) 
if x is leaf node, then Son(x) is empty 
(4)  For one non-leaf node x: 
1) |Seg(x)|  k, Split |Seg(x)| leaf node 
2) Otherwise, split k son node such that 
 
 
 
Here follows one example with 2-range tree when the 
histogram has 5 bars 
Seg( y)
 y?Son( x )
 ? = Seg(x)???
 ?
 ??? Seg( y) Seg(z)?y ,z?Son( x )?y<>z = ?
 ?
 ??
 ?
 ??
 | Seg(x)
 k
 ?
 ??
 ?
 ?? ≤ Seg(x)y?Son( x )
 ≤
 | Seg(x)
 k
 ?
 ??
 ?
 ??
 404
 
Figure 1 2-Range Tree with 5 elements in histogram 
Every node x is associated with one fx takes T as input and 
output the count sum between[L(x), R(x)]. If T1 and T2 are 
differ in one entry, then for some specific leaf node x,      
|fx(T1)-fx(T2)|=1, moreover all its ancestor node y will have
 |fy(T1)-fy(T2)|=1, so S(F) is the farthest distance from one leaf 
node to root in k-range tree. For example, in figure 1, S(F) = 4. 
For histogram, we know that S(F)=1 since adding/deleting 
one record will lead to one bar to change by 1. For the range 
count query Q which may cover O(n) bars, will have the 
variance O(2n?2). In one word, public the histogram directly 
after adding the noise will lead to great error on range count 
query. 
We can construct the k-range tree on the histogram. It is 
obvious that S(F) =O(logn). By theorem 1, adding Laplace 
with parameter S(F)/? to each tree node will guarantee              
?-differential privacy.  The k-range tree after adding Laplace 
noise will be mentioned as k-Differential Privacy Range Tree 
(DPRT). The following figure demonstrate how we process 
with the histogram.  
For one range count query Q=[L,R], query operation on k-
 DPRT is to find such node set A that: 
Figure 2 Process of constructing 2-DPRT with 5 elements 
(1)  
Seg(x)
 x?A
 ? = Q  
(2)  
Seg( y) Seg(z)?
 y ,z?A?y<>z
 = ?  
(3) The size of A is minimized 
In this paper we define that each node in A is covered by Q. 
k-DPRT is with O(logn) level. For any query Q, at most 
O(k) node will be covered in each level, so the variance of the 
answer will be  
2? 2
 x?A
 ∑ = 2 A ? 2 =O(2k logn? 2 ) =O(k? 2 logn)  
 
which could dramatically improve the accuracy for range 
count query. 
However, inconsistency will occur due to the noise. 
Suppose the user have three queries Q(0)=[1,2], Q(1)=[3,4], 
Q(2)=[1,4] and the k-DPRT returns with P(0)=3.1, P(1)=0.5, 
P(2)=23. It is obviously that P(0) + P(1) is not equal to P(2). 
Hay’s work [13] could boosting the accuracy of range 
count query with consistency. However it only apply for the 
histograms that could map to a perfetc k-DPRT, which may 
lead to massive padding and reduce the performance.  
A. Local Best Linear Unbiased Estimator(BLUE) on k-
 DPRT 
Given one k-DPRT with exactly k+1 nodes. The value of 
node root and its son nodes Son(root)={i1,i2…ik} after adding 
Laplace noise will be 
??
 ???
 ?
 +=
 ??
 ???
 ?
 +???
 ?
 ???
 ?
 =
 ?
 ?
 ∑
 ?
 ?
 )()(
 )()(
 )(
 )(
 ~
 )(
 )(
 ~
 FSLapXih
 FSLapXrooth
 iL
 rootSoni
 rootSoni
 iL
  
h
 ?
 = [h
 ?
 (i1),h
 ?
 (i 2 )...h
 ?
 (ik )]T
  
h
 ~
 = [h
 ~
 (i1),h
 ~
 (i 2 )...h
 ~
 (ik )]T
  
indicates the BLUE and original disturbed value of these nodes.  
THEOREM 2. The BLUE for k-DPRT with exactly k+1 nodes 
is 
h
 ?
 (root) = h
 ~
 (root)? ∆root
 h
 ?
 (i)
 i?Son(root )
 = h
 ~
 (i)+ ∆root
  
∆root =
 h
 ~
 (root)? h
 ~
 (i)
 i?Son(root )
 ∑
 k +1
  
 
by [13] , Here follows the algorithm to solve the Local Best 
Linear Unbiased Estimator (LBLUE) 
 
405
Algorithm 1. LBLUE  
Input: k-DPRT node x 
OutputBLUE of all k+1 nodes 
1 if xSon(root), return its value. Otherwise go to step 2 
2  k ? Son(x) ,h
 ?
 (x) = h
 ~
 (x)? ∆root  with 
 
                        ∆ x =
 h
 ~
 (x)? h
 ~
 (i)
 i?Son(x )
 ∑
 k +1
  
3 for i?Son(x) do  
                                     h
 ?
 (i) = h
 ~
 (i)+ ∆root  
B. Iterative algorithm GBLUE based on LBLUE 
We run the LBLUE algorithm from button to up in k-
 DPRT with the following algorithm 
Algorithm 2. LBLUE-ADJUST 
(Local Best Linear Unbiased Estimator ADJUST) algorithm 
Input: k-DPRT node x 
Outputadjusted value of all nodes 
1 if xSon(root), return its value. Otherwise go to step 2 
2 for iSon(x) do LBLUE-ADJUST(i) 
3 LBLUE(x) 
 
 
Algorithm 3. GBLUE 
(Global Best Linear Unbiased Estimator)algorithm 
Input: k-DPRT T, threshold ? 
OutputBLUE of all nodes 
1 if T satisfied consistency within threshold ?, then return.  
Otherwise go to step 2 
2 LBLUE-ADJUST(root), go to step 1 
 
C. Convergence of the GBLUE 
DEFINITION 4 (Incompatible value) After iterating for m 
times, the largest gap between the value of one non-leaf node 
and the sum of its sons’ value is defined as ∆(m)   
 
Gapm (x) = h
 ?
 m (x)? h
 ?
 m (i)
 i?Son(x )
 ∑
 ∆(m) = max
 x?T
 {Gapm (x)}
  
In particular Gapm(y)=0 for leaf node y. h
 ?
 m (x)  is the value of 
node x after m iteration, and in particular  h
 ?
 0 (x) = h
 ~
 (x) . 
DEFINITION 5. (depth of tree) The depth of tree is defined 
as  
Dep(x) = 0, x ? Leaf (root)
 Dep(x) = max
 y?Son(x )
 {Dep(y)}+1,otherwise
 ?
 ??
 ??
  
Leaf(root) indicates the set of root’s son nodes. 
 
LEMMA 1. GBLUE will converge to the BLUE of k-DPRT 
nodes’ value 
Proof. For the value of node in k-DPRT, we have 
??
 ???
 ?
 +???
 ?
 ???
 ?
 = ∑
 ? ?
 )()(
 )(
 )(
 ~ FSLapXyh
 ySoni
 iL
  
all the variable is with Laplace noise, which are with 
expectation zero and are uncorrelated and have equal variances. 
By Gauss–Markov theorem, the BLUE of the value is given by 
the ordinary least squares (OLS) estimator 
min h
 ?
 (i)? h
 ~
 (i)???
 ?
 ??
 2
 i
 ∑  
 h
 ?
 ( j)
 j?Leaf (i)
 ∑ = h? (i)  (4) 
which will derive the following optimization problem 
min h
 ?
 ( j)
 j?Leaf (i)
 ∑ ? h~ (i)????
 ?
 ?
 ??
 2
 i
 ∑  
Is is one convex function, so the partial derivative of h
 ?
 (i)
 i?Leaf (root )
  
should be 0 at the answer 
0)()(2)()(2
 )( )(:
 ~
 )(:
 ~
 )(
 =??
 ???
 ?
 ?=???
 ?
 ???
 ?
 ?=
 ∂
 ∂ ∑∑ ∑
 ?
 ?
 ? ?
 ?
 ?
 jLeafijjLeafij jLeafk
 jhjhjhkh
 ih
 f
  
 
∑∑
 ??
 ?
 =
 )(:
 ~
 )(:
 )()(
 jLeafijjLeafij
 jhjh
  (5) 
If Dep(root)=1, then one single LBLUE-ADJUST will 
find the BLUE. 
We only consider nodes whose depth is larger than 1 in 
the following. 
406
In LBLUE, each node x will be subtracted by ∆ x  on its 
value while every son node of it will be added by ∆ x  on their 
value. So after LBLUE, equation 4 will always hold. 
Now let’s review the LBLUE-ADJUST , here follows one 
example that we adjust the value of node x . 
 
 
 
 
Figure 3 update the value in the m-th iteration 
  
Node x is with Dep(x)≥2. In Figure 3(a) Gapm
 y?Son(x )
 (y) = 0  
 however after adjusting the Gapm
 y?Son(x )
 (y) = ∆ x  and they will 
not change again in the same iteration. So we can have  
}{max)(
 2)( xxDep
 m ∆=∆
 ≥ and ∆ y
 Dep(y)=0
 = 0  for leaf node. Let 
Size(y) = Son(y)  then   
∆ y
 Dep(y)=1
 =
 h
 ?
 m (y)? h
 ?
 m (x)
 x?Son(y)
 ∑
 Size(y)+1 =
 Gapm (y)
 Size(y)+1
 ∆ y
 Dep(y)=1
 ≤
 ∆(m)
 Size(y)+1 < ∆(m)
  
 
Since the adjusting is button-up, we must adjusting every 
node in Son( y)  before adjusting y 
1)(
 )(
 1)(
 )()(
 )()(
 2)( +
 ∆+
 =
 +
 ??
 ???
 ? ∆??
 =∆
 ∑∑
 ??
 ??
 ≥ ySize
 yGap
 ySize
 xhyh
 ySonx
 xm
 ySonx
 xmm
 yDep
 y
  
∆ y
 Dep(y)≥2
 =
 Gapm (y)+ ∆ x
 x?Son(y)
 ∑
 Size(y)+1 ≤
 Gapm (y) + ∆ x
 x?Son(y)
 ∑
 Size(y)+1 ≤
 ∆(m)+ ∆x
 x?Son(y)
 ∑
 Size(y)+1
  
By mathematical induction we could get  
∆ y
 Dep(y)≥1
 < ∆(m)  
)(}{max)1(
 2)(
 mm x
 xDep
 ∆<∆=+∆
 ≥
  
so every single adjusting (iteration) will always guarantee 
equation 4, and since the 
)(}{max)1(
 2)(
 mm x
 xDep
 ∆<∆=+∆
 ≥
  
is descent, equation 5 will hold finally. So after the 
iteration, it will converge on the answer on the OLS, which 
means it will converge on the BLUE of k-DPRT nodes’ value. 
 
IV. EXPERIMENTS 
This section will evaluate the algorithm running efficiency and 
accuracy by experiments. This GBLUE algorithm is compared 
to the previous work of Boost algorithm [13], and the literature 
[14] pointed out that for random range count queries, perfect 
12-ary tree is a more appropriate choice. In this paper, we 
choose traditional two and twelve forks range tree with Boost 
algorithm (hereinafter respectively referred Boost-2 with 
Boost-12) and 12-range tree with GBLUE algorithm 
(hereinafter referred to as GBLUE). For real data set, we run 
GBLUE and Boost-2, Boost-12 algorithm and compare theirs 
time and space efficiency; At last we compare and analyze the 
accuracy on range count query. 
The data from [15] is the access records to Amazon.com 
from 3/1/2005 to 8/31/2010. In this experiment we will use the 
random range count queries with length 2i (i ≥ 0). For any 
range count query length, generating 500 random range queries. 
We will add noise for 50 times and the error is evaluated 
through the mean square error of average valuation. 
The experimental platform is 1.8Ghz Intel Core i5 and 
4GB Ram in MAC OS X 10.8.3. The algorithm is implemented 
by C++ and the figures are plotted  by MATLAB 7.0. Here 
follows the statistic about the dataset. 
Table 1 statistics on the dataset 
Dataset Data size Parted by Size after partition
 Amazon 716064 Day 2010 
 
We run GBLUE, Boost-2 and Boost-12 on the dataset. We will 
run them 50 times and get their average running time under the 
thresh hold ?=10-3 and ?=10-6  
407
Figure 4 Program runtime in Amazon 
 
We could see that GBLUE runs fast on the dataset. 
However with less thresh hold, the running time of GBLUE is 
much longer. The difference in ? do not make big different in 
running time for all three algorithms. 
GBLUE will terminate only when m<?. Fewer threshold 
means more iterations, which lead to longer running time. 
However Boost-2 and Boost-12’s running time is not 
associated with ? and ?.  
In the experiment we chose ?=10-6 and run GBLUE, Boost-
 2 and Boost-12 on the dataset Amazon. For random interval 
query calculates the average standard deviation, the 
experimental results are shown in Figure 5. 
 
 
 Figure 5 Range queries error in Amazon 
 
We could see from the experiment, the accuracy of Boost-2 
is below the GBLUE and Boost-12 in the dataset. On Amazon, 
GBLUE is better than Boost-12. GBLUE has the lowest error 
among the three algorithms demonstrate its effectiveness. 
V. CONCLUSION 
In this paper, we present one iterative algorithm GBLUE in 
publishing histograms under differential privacy condition. We 
strictly prove that GBLUE will converge on the best linear 
unbiased estimator of the noisy data in k-range tree and provide 
high consistency guarantee. We analyze the advantages of it by 
comparing with existing solutions. Finally, the experimental 
results demonstrate that GBLUE has outstanding performance 
at range-count queries on differential privacy histograms. 
GBLUE works for any tree-like data structure. If the 
dataset could be store in such data structure, the GBLUE 
should apply. Therefore we should study how GBLUE could 
be extended to other data type or even to multi-conditioned 
query in the future work. 
 
REFERENCES 
[1] S. Zhou, F. Li, Y.F. Tao, et al. Privacy Preservation in Database 
Applications: A Survey [J]. CHINESE JOURNAL OF COMPUTERS, 
2009, 32(5): 847-858
 [2] B. C. M. Fung, K. Wang, R. Chen, P. S. Yu. Privacy-preserving data 
publishing: A survey on recent developments. ACM Computing 
Surveys[J], 2010, 42(4) 
[3] L. Sweeney. k-anonymity: A model for protecting privacy. International 
Journal on Uncertainty, Fuzziness and Knowledge Based Systems [J], 
2002, 10( 5): 557-570 
[4] A. Machanavajjhala, J. Gehrke, D. Kifer, M. Venkitasubramaniam. l-
 diversity: Privacy beyond k-anonymity. Proceedings of the 22nd 
International Conference on Data Engineering (ICDE)[C]. Atlanta, 
Georgia, USA, 2006: 24-35 
[5] N. Li, T. Li. t-closeness: Privacy beyond k-anonymity and l-diversity. 
Proceedings of the 23rd International Conference on Data Engineering 
(ICDE)[C]. Istanbul, Turkey, 2007: 106-115 
[6] C. Dwork. Differential Privacy. Proceedings of the 33rd International 
Colloquium on Automata, Languages and Programming (ICALP)[C]. 
Venice, Italy, 2006: 1-12 
[7] C. Dwork, F. McSherry F, K. Nissim, A. Smith.  Calibrating Noise to 
Sensitivity in Private Data Analysis. Proceedings of the 3th Theory of 
Cryptography Conference (TCC)[C]. New York, USA, 2006: 363-385 
[8] C. Dwork, A. Smith. Differential privacy for statistics: what we know 
and what we want to learn. Journal of Privacy and Confidentiality[J], 
2009, 1(2):135–154 
[9] X. Xiao, G. Wang, and J. Gehrke. Differential privacy via wavelet 
transforms. TKDE[J], 2011, 23(8):1200–1214. 
[10] G. Cormode, C. M. Procopiuc, D. Srivastava, T. T. L. Tran. 
Differentially private summaries for sparse data. Proceedings of 15th 
International Conference on Database Theory (ICDT)[C], Berlin, 
Germany, 2012: 299-311 
[11] J. Xu, Z. Zhang, X. Xiao, Y. Yang, G. Yu.  Differentially Private 
Histogram Publication. Proceedings of IEEE 28th  International 
Conference on Data Engineering(ICDE)[C], Washington, DC, USA, 
2012:32-43 
[12] G. Acs, R. Chen.  Differentially Private Histogram Publishing through 
Lossy compression. Proceedings of the 11th  IEEE International 
Conference on Data Mining (ICDM)[C], Brussels, Belgium, 2012:84-95 
[13] M. Hay, V. Rastogi, G. Miklau, D. Suciu. Boosting the Accuracy of 
Differentially Private Histograms through Consistency. Proceedings of 
the 36th Conference of Very Large Databases (VLDB)[C], Istanbul, 
Turkey, 2010: 1021-1032 
[14] S. Peng, Y. Yang, Z. Zhang, M. Winslett, Y. Yu. DP-tree: indexing 
multi-dimensional data under differential privacy. Proceedings of the 
ACM SIGMOD International Conference on Management of Data 
(SIGMOD)[C], Scottsdale, AZ, USA, 2012:864 
[15] http://archive.ics.uci.edu/ml/datasets/Amazon+Access+Samples 

 
  
408

A Scalable Algorithm for Mining Maximal Frequent Sequences Using Sampling
Congnan Luo and Soon M. Chung∗
Dept. of Computer Science and Engineering
Wright State University
Dayton, Ohio 45435, USA
Abstract
In this paper, we propose an efﬁcient scalable algorithm
for mining Maximal Sequential Patterns using Sampling
(MSPS). The MSPS algorithm reduces much more search
space than other algorithms because both the subsequence
infrequency based pruning and the supersequence fre-
quency based pruning are applied. InMSPS, sampling tech-
nique is used to identify long frequent sequences earlier, in-
stead of enumerating all their subsequences. We propose
how to adjust the user-speciﬁed minimum support level for
mining a sample of the database to achieve better perfor-
mance. This method makes sampling more efﬁcient when
the minimum support is small. A signature technique is uti-
lized for the subsequence infrequency based pruning when
the seed set of frequent sequences for the candidate gen-
eration is too big to be loaded into memory. A preﬁx tree
structure is developed to count the candidate sequences of
different sizes during the database scanning, and it also fa-
cilitates the customer sequence trimming. Our experiments
showed MSPS has very good performance and better scal-
ability than other algorithms.
1 Introduction
Mining sequential patterns from large databases is an im-
portant problem in data mining. With numerous practical
applications such as consumer market-basket data analy-
sis and web-log analysis, it has become an active research
topic. Since it was introduced in [2], many algorithms have
been proposed, but most of them are to discover the full set
of frequent sequences.
In pure bottom-up, breadth-ﬁrst search algorithms such
as GSP [8] and PSP [6], only subsequence infrequency
based pruning is used to reduce the number of candidate
sequences. So, if a sequence with length l is frequent, all
of its 2l subsequences must be enumerated ﬁrst. Thus, if
∗This research was supported in part by Ohio Board of Regents, Lexis-
Nexis, NCR, and AFRL/Wright Brothers Institute (WBI).
some frequent sequences are long, the overhead of enumer-
ating all of their subsequences is so much that mining the
full set of frequent sequences is impractical. An alternative
approach is mining only the maximal frequent sequences. A
frequent sequence is maximal if none of its supersequence
is frequent. Mining only the the set of maximal frequent se-
quences (MFS) is efﬁcient because the search space can be
reduced a lot by using the supersequence frequency based
pruning. In interactive data mining, after mining the set
of maximal frequent sequences quickly, we can selectively
count the interesting patterns subsumed by this set by scan-
ning the database just once.
For the association rule mining, many efﬁcient algo-
rithms were proposed to mine maximal frequent itemsets
[5]. However, differences between the two kinds of mining
make those algorithms very difﬁcult or impossible to be ap-
plied for the maximal frequent sequence mining. A critical
question for the maximal frequent sequence mining is how
to look ahead for longer or maximal frequent sequences at
a reasonable cost. If the look-ahead is not performed effec-
tively, its cost can offset the gain from the supersequence
frequency based pruning, like the cases of AprioriSome and
DynamicSome algorithms [2].
In this research, we combined the Apriori candidate gen-
eration method [1, 2, 8] and the supersequence frequency
pruning for mining maximal frequent sequences. This is
achieved by using a sampling technique. The main search
strategy of MSPS is bottom-up and breadth-ﬁrst. But after
the pass 2 over the database, we mine a small random sam-
ple database ﬁrst starting with the candidate 3-sequences
(i.e., sequences of 3 items) generated from L2, the set of fre-
quent 2-sequences. The local maximal frequent sequences,
that are found from the the sample database starting with
the global candidate 3-sequences, are veriﬁed in a top-down
fashion against the original database, so that we can efﬁ-
ciently collect the longest frequent sequences covered by
them. Then, the bottom-up search is resumed from the pass
3, and supersequence frequency based pruning is applied at
each pass.
The main contributions of this research are: 1) A new
Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence (ICTAI 2004) 
1082-3409/04 $20.00 © 2004 IEEE
MSPS algorithm and optimization methods were developed
for mining maximal frequent sequences. MSPS outper-
forms GSP considerably, and it also shows better scalability
than SPAM [3] and SPADE [11]. 2) In our research, how
to apply the sampling technique for sequence mining was
studied thoroughly. In association rule mining using sam-
pling, lowering the user-speciﬁed minimum support (de-
noted by minsup) further was proposed for mining a sam-
ple to guarantee no misses (i.e., false negatives). For the
case of sequence mining, where the search space is much
larger, if the user-speciﬁed minsup is small, simply using
this minsup or a lowered one often misleads the mining on
a sample, such that the cost of mining the sample itself and
verifying the sample results could be too high. For MSPS,
we proposed a theoretic method of adjusting the small user-
speciﬁed minsup to avoid this problem.
The rest of the paper is organized as follows. Section
2 introduces the basic concepts of sequence mining. Sec-
tion 3 reviews some related works on sequence mining and
sampling. Section 4 describes the MSPS algorithm. The ex-
perimental results and performance analyses are presented
in Section 5, and Section 6 contains some conclusions.
2 Sequence Mining
Let I = {i1, i2, . . . , in} be a set of items. An k-itemset
i is a set of k items denoted by {im1 , im2 , . . . , imk}, where
1 ≤ m1 < m2 < . . . < mk ≤ n. A sequence s is an
ordered list of itemsets denoted by < s1, s2, . . . , sk >,
where each si, 1 ≤ i ≤ k, is an itemset. A sequence
sa =< a1, a2, . . . , ap > is contained in another sequence
sb =< b1, b2, . . . , bq > if there exist integers 1 ≤ j1 <
j2 < . . . < jp ≤ q such that a1 ⊆ bj1 , a2 ⊆ bj2 , . . . , ap ⊆
bjp . If sa is contained in sb, sa is a subsequence of sb,
and sb is a supersequence of sa. An item may appear at
most once in an itemset, but it may appear multiple times
in different itemsets of a sequence. If there are k items in a
sequence, the length of the sequence is k, and we call it a k-
sequence. For example, a 3-sequence < {A}, {B,C} > is
a subsequence of a 5-sequence < {C}, {A,D}, {B,C} >.
For simplicity, these two sequences can be represented as
A−BC and C −AD −BC.
Given a databaseD of customer transactions, each trans-
action consists of a customer-id, transaction-time and an
itemset which includes all the items purchased by the cus-
tomer in that single transaction. All the transactions of a
customer can be viewed as a customer sequence, where
these transactions are ordered by their transaction times.
We denote a customer sequence t as < T1, T2, . . . , Tm >,
which means the customer has m transactions in the
database and each transaction Ti, 1 ≤ i ≤ m, contains
all the items purchased in that transaction. A customer sup-
ports a sequence if the sequence is contained by the cus-
tomer sequence. The support for a sequence in database
D is deﬁned as the fraction of total customers who support
this sequence. Given a user-speciﬁed minimum support, de-
noted by minsup, a sequence is frequent if its support is
greater than or equal to minsup. The problem of sequence
mining is to ﬁnd all the frequent sequences in the database
with respect to a user-speciﬁed minsup. If a sequence is fre-
quent and none of its supersequences is frequent, then it is
a maximal frequent sequence.
Based on the above deﬁnitions, two properties are often
utilized to speed up the sequence mining: 1) any superse-
quence of an infrequent sequence is not frequent, so it can
be pruned from the set of candidates. This is called sub-
sequence infrequency based pruning. 2) any subsequence
of a frequent sequence is also frequent, so it can be pruned
from the set of candidates. This is called supersequence fre-
quency based pruning.
In [8], the above deﬁnition of sequence mining was gen-
eralized by incorporating time constraints, sliding time win-
dows, and taxonomy. This generalization makes the se-
quence mining more complex. For example, a sequence
A−BC −D −GH is frequent does not necessarily mean
that its subsequence A − BC − GH is also frequent, be-
cause the subsequence may not satisfy the time constraints.
In this research, we consider the nongeneralized sequential
pattern discovery.
3 Related Work
Mining sequential patterns was introduced in [2] with
AprioriAll, AprioriSome and DynamicSome algorithms.
Although AprioriSome and DynamicSome try to generate
and count long candidate sequences before enumerating all
their subsequences, their performance is usually worse than
that of AprioriAll. The reason is too many false candidates
are generated without being pruned by the subsequence in-
frequency based pruning. The performance gain from the
supersequence frequency based pruning is not enough to
offset the cost of counting so many false candidates.
GSP [8] was proposed for generalized sequence mining.
GSP requires multiple passes on the database. At pass k,
the set of candidate k-sequences are counted against the
database and frequent k-sequences are determined. Then,
the candidate (k + 1)-sequences are generated by joining
frequent k-sequences for the next pass. This process will
continue until no candidate is generated. Even though GSP
is much faster than AprioriAll, it has a very high overhead
of enumerating every single frequent subsequence when
there are some long patterns. This is also the main weak-
ness of other Apriori-like algorithms, such as PSP [6]. For
PSP, a preﬁx tree was developed as the internal data struc-
ture to organize and count candidates more efﬁciently.
SPADE [11] works on the databases with a vertical id-
Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence (ICTAI 2004) 
1082-3409/04 $20.00 © 2004 IEEE
list format, where a list of (customer-id, transaction-time)
pairs are associated with each item, and the candidates are
counted by intersecting the id-lists. A lattice-theoretic ap-
proach is used to decompose the search space into small
pieces so that all working id-lists can be loaded into mem-
ory. PreﬁxSpan [7] projects a large sequence database re-
cursively into a set of small postﬁx subsequence databases
based on the currently mined frequent preﬁx subsequences.
Then, the subsequent mining is conﬁned to each small pro-
jected database. A memory-based pseudo-projection tech-
nique is developed to save the computation cost of projec-
tion and the memory space for projected databases. SPAM
[3] uses a vertical bitmap representation of the database for
candidate generation and counting. A bitmap is created for
each item in the database, where each bit corresponds to a
transaction. If transaction j contains item i, then bit j in
the bitmap for item i is set to 1; otherwise, it is set to 0.
SPAM also uses a depth-ﬁrst traversal of the Lexicographic
sequence tree and an Apriori-based pruning of candidates.
These three algorithms (SPADE, PreﬁxSpan, and
SPAM) were reported more efﬁcient than GSP. However,
their performance may not be scalable in certain cases. For
SPADE, if the database is in the horizontal format, where
the transactions form the tuples in the database, transform-
ing it to a vertical one requires extra disk space with roughly
the same size. This may be a problem in practice if the
database is large. Even if the database is in the verti-
cal format, to efﬁciently count 2-sequences, SPADE pro-
poses transforming it back to the horizontal one on the ﬂy.
This usually requires much time and memory for very large
databases and results in performance degradation.
PreﬁxSpan may be challenged when the database has a
large number of customer sequences and items. A large
number of items often produce many combinations at the
early stage of mining, and it requires PreﬁxSpan to con-
struct more projected databases. If the database is very
large, the cost of projection will be high and much more
memory is necessary. SPAM is claimed to be a memory-
based algorithm. According to our tests, its scalability
is much more sensitive to the number of items and the
database size than other algorithms. The comparison be-
tween MSPS, GSP, SPADE and SPAM is presented in detail
in the performance analysis section.
In [10], sampling was evaluated as an efﬁcient way to
mine an approximate set of frequent itemsets. In [4], the
FAST algorithm mines a large sample ﬁrst to accurately es-
timate the support of 1-itemsets, and then progressively re-
ﬁne the initial sample to obtain a small ﬁnal sample. FAST
reports the set of frequent itemsets in the ﬁnal sample as the
results. Our research is more related to [9] because both
try to speed up the mining of the exact ﬁnal result using
sampling. In [9], a lowered minsup is used to mine the sam-
ple, so that the probability a frequent itemset is missed from
the sample result would be small. Then, one more database
scan is needed to ﬁnd the misses and the overestimates. In
[9], the focus was mainly on how to make the sample re-
sult include all frequent itemsets without missing. On the
other hand, our main goal is not the sample result itself, but
obtaining some knowledge that can speed up the mining of
exact ﬁnal result. Thus, the misses are allowed to some ex-
tent. In practice, the cost for avoiding the misses can be a
concern. Therefore, a critical question is how much sam-
pling can speed up the mining of the exact result, and we
explored this question from the point of balancing the cost
and the gain of sampling.
4 MSPS Algorithm
Like GSP, MSPS also uses the candidate generation
then counting approach to perform the mining, but the per-
formance is improved very much by combining the su-
persequence frequency based pruning into its bottom-up,
breadth-ﬁrst search. It has the following original compo-
nents: 1) A signature technique is used to perform a par-
tial subsequence infrequency based pruning when the set of
frequent k-sequences is too big to be loaded into memory
totally for the generation of candidate (k + 1)-sequences.
2) To efﬁciently count candidates of different sizes, a preﬁx
tree structure is developed, and it also facilitates the cus-
tomer sequence trimming. 3) To support supersequence
frequency based pruning, sampling is used to ﬁnd long fre-
quent patterns early. 4) To make the sampling more efﬁcient
and robust in sequence mining, a theoretic method of adjust-
ing the small user-speciﬁed minsup for mining the sample
database was proposed.
In this section, we ﬁrst present an overview of MSPS and
then describe the candidate generation and pruning, candi-
date counting and sampling in detail. The following nota-
tions will be used in our description: DB is the original
database and db is a small random sample of DB. If a se-
quence is frequent in DB, it is called a global frequent se-
quence. LDBk is the set of all global frequent k-sequences
and CDBk is the set of all candidate k-sequences generated
from LDB(k−1). MFS
DB is the set of all global maximal fre-
quent sequences. If a sequence is frequent in the sample db,
we call it a local frequent sequence. Ldbk , Cdbk , and MFSdb
are the sets of all local frequent k-sequences, candidate k-
sequences and maximal frequent sequences in the sample,
respectively.
4.1 Description of MSPS
The basic idea of MSPS is simple: if some long frequent
patterns are found early, they can be used to prune the search
space so that the mining can speed up. To ﬁnd long frequent
patterns, a small sample db is mined ﬁrst. We must balance
Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence (ICTAI 2004) 
1082-3409/04 $20.00 © 2004 IEEE
the gain from the supersequence frequency based pruning
and the cost for mining the sample and then verifying the
sample result. MSPS consists of three phases:
Phase 1: LDB1 and LDB2 are determined. Candidate 3-
sequences are generated from LDB2 . To count can-
didate 2-sequences, a two-dimensional array is used.
The entry at position (i, j) in the upper-triangle of the
array contains the counts of three candidates i − j, ij
and j − i.
Phase 2: A random sample is drawn from DB, then how
much the user-speciﬁed minsup should be adjusted for
mining the sample is determined. We mine the sam-
ple starting with CDB3 in a bottom-up, breadth-ﬁrst
manner. The local maximal frequent sequences are ex-
tracted to construct MFSdb. Then we perform a top-
down search for long global frequent sequences from
MFSdb. All those sequences in MFSdb are consid-
ered as global candidates and counted against DB. If
a k-sequence, (k > 3), is infrequent, all of its (k− 1)-
subsequences are considered as candidates for the next
pass. For a frequent k-sequence, we stop splitting it
and put it into the set LongFSDB if none of its super-
sequences is already in this set. For a newly generated
candidate (k−1)-sequence, if it has any supersequence
in LongFSDB, we remove it from further considera-
tion. We also check if the newly generated candidate
(k−1)-sequence has any subsequence which is already
identiﬁed as infrequent. If yes, this candidate (k − 1)-
sequence must be split again.
Phase 3: The bottom-up search suspended at the end of
Phase 1 is resumed from pass 3. With LongFSDB,
we can apply the supersequence frequency based prun-
ing on the candidates generated at each pass. The can-
didates which appear in LongFSDB or have any su-
persequence in LongFSDB don’t need to be counted.
They are simply considered as frequent and used for
the candidate generation for the next pass. Finally,
MFSDB is extracted from all global frequent se-
quences found.
4.2 Candidate Generation and Pruning
Both in Phases 2 and 3, we have performed the bottom-
up, breadth-ﬁrst search on the sample and the original
database, respectively. At pass k, the candidates are gen-
erated in two steps:
Join Step: we generate local (global) candidate (k + 1)-
sequences by joining Ldbk with Ldbk (LDBk with LDBk )
as in the GSP algorithm. For any two local (global)
frequent k-sequences s1 and s2 in Ldbk (LDBk ), if the
subsequence obtained by dropping the ﬁrst item of s1
is the same as the subsequence obtained by dropping
the last item of s2, a new candidate is generated by
extending s1 with the last item in s2. The added item
starts a new itemset for s1 if it was a separate itemset
in s2. Otherwise, it becomes part of the last itemset in
s1.
Prune Step: In both phases 2 and 3, the subsequence in-
frequency based pruning is applied. The local (global)
candidate (k + 1)-sequences with any subsequence of
length k which is not in Ldbk (LDBk ) are deleted. Espe-
cially in Phase 3, since we have LongFSDB, the su-
persequence frequency based pruning also can be per-
formed. Thus, we remove global candidate (k + 1)-
sequences which are in LongFSDB or have any su-
persequence in it.
A weakness of GSP is the way that a large LDBk is pro-
cessed. When the user-speciﬁed minsup is very small, LDBk
could be too large to be loaded into memory totally. For this
case, GSP proposed using a relational merge-join technique
to generate candidates. But in this manner, subsequence
infrequency based pruning cannot be applied because the
whole LDBk is not available in memory and retrieving the
relevant portions of LDBk from a disk requires too frequent
swaps. Without subsequence infrequency based pruning,
usually the performance of GSP degrades a lot. In MSPS,
we adopted a new method to solve this problem. If some
Ldbk in Phase 2 (LDBk in phase 3) requires too much mem-
ory, we give each local (global) frequent k-sequence an in-
teger signature which is highly correlated to the content of
the sequence. Following is a simple example of the signa-
ture, where t is the number of itemsets in the sequence; mi
is the number of items in ith itemset; Iij is the jth item in
the ith itemset; Ci, 1 ≤ i ≤ t, is the weight imposed on
the ith itemset; and C0 is the weight imposed on the total
number of itemsets.
(C0 ∗ t) +
t∑
i=1
(Ci ∗mi ∗
mi∑
j=1
Iij)
All the signatures are sorted and put into an array.
Compared with the case of loading the whole Ldbk (LDBk )
into memory, the signature array requires much less space.
Thus, we can load working portions of Ldbk (LDBk ) and all
the signatures into memory at the same time. When a new
candidate (k+1)-sequence is generated, the signatures of its
k-subsequences are computed and searched in the signature
array. If any one of them is not in the array, the candidate
should be removed. Since all the signatures are in memory,
subsequence infrequency pruning can still be applied. It is
possible that two or more k-subsequences have the same
signature. However, that probability is very low. Our exper-
iments showed that signatures are much more efﬁcient than
Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence (ICTAI 2004) 
1082-3409/04 $20.00 © 2004 IEEE
hashing. MSPS performs much better than GSP when the
seed set of frequent sequences for generating the candidates
cannot be loaded into memory totally at some passes. If the
memory cannot hold all the candidates generated, they can
be processed part by part.
4.3 Counting Candidate Sequences
During the top-down search for long patterns covered by
MFSdb, to reduce the number of passes, we need to count
candidates of different sizes at each pass over the database.
For that purpose, we developed a new preﬁx tree structure.
Since it is much more efﬁcient than the hash tree, we also
use it to count the candidates of the same size during the
bottom-up search in Phases 2 and 3. We ﬁrst describe our
preﬁx tree and the customer sequence trimming technique,
and then compare it with the preﬁx tree used for PSP [6].
4.3.1 Overview of the Preﬁx Tree and the Customer
Sequence Trimming
The following example shows how the preﬁx tree works.
Suppose we have 10 candidates of length 2 or 3. The preﬁx
tree is constructed as shown in Figure 1. Each node is as-
sociated with a pointer. If the path from the root to a node
represents a candidate, the pointer points to the candidate;
otherwise, it is NULL. A node may have two types of chil-
dren. The “I-extension” child means the item represented
by the child node is in the same itemset with the item rep-
resented by its parent node. The “S-extension” child means
the item represented by the child node starts a new item-
set. All the S-extension (I-extension) children of a node are
linked together, and only the ﬁrst child is linked to their
parent node by a dashed (solid) line. For example, nodes 4
and 5 are the S-extension children of node 1, and the corre-
sponding pathes represent the candidates A−A and A−E,
respectively. Nodes 6 and 7 are I-extension children, and
their pathes represent AC and AD, respectively.
To speed up the counting, a bit vector is associated with
the preﬁx tree to facilitate the customer sequence trim-
ming. In this example, we have 8 items in the database:
A,B,C,D,E, F, and H . Since B, F , and G do not appear
in any candidate, they should be ignored during counting.
Thus, the bit vector is set as (10111001), where 1 at the i-th
bit position means item i appears in the preﬁx tree. All the
bits are initialized to 0, and the corresponding bits are set to
1 as we insert candidates into the preﬁx tree.
Given a customer sequence s = ABCD − ADEFG −
B−DH , we trim it to s′ = ACD−ADE−DH using the
bit vector ﬁrst. Then a recursive method is used to count all
the candidates contained in s′. At the root node, we check
each item in ACD −ADE −DH to see if it is in the root
node’s S-extension children. The ﬁrst item of s′ is A, and
Candidates : 
10) D − A − H 
9)   D − AE 
8)   AD − A 
7)   ADE 
6)   D − A 
5)   CH 
3)   A − A 
2)   AD 
1)   AC 
4)   A − E ACD − ADE − DH 
ABCD − ADEFG − B −DH 
Bit Vector (10111001) 
1 2
CA
3
D
4
6 7 8
9
10
11
12
13
EA
C
H
A
E
E A
D H
 Root
5
DH
ADE − DH E − DH 
DE − DH 
CD − ADE −DH 
ADE − DH 
DH
D − ADE − DH 
ADE − DH 
DH
DH
DE − DH 
Figure 1. Preﬁx Tree of MSPS
it appears as the ﬁrst S-extension child of the root node. So
we recursively call the count function at the root node with
two sequence segments. The segment CD−ADE−DH is
used in the call for node 1’s I-extension link, while ADE−
DH is for its S-extension link. Then, we can locate the
second item of s′, C, at node 2. Since node 2 has no S-
extension child, only one recursive call with the segment
D−ADE−DH is made for its I-extension link. The third
item of s′, D, is the last item of the ﬁrst itemset in s′. Only
one call with segment ADE − DH is made for node 3’s
S-extension link. The fourth item of s′, A, can be located
it at node 1 again, and we make two recursive calls. One
is for the node 1’s I-extension link with DE − DH , and
the other one is for its S-extension link with DH . Then,
we process the remaining items in s′, one by one, in the
same way. Whenever we locate an item at some node, if the
pointer associated with the node is not NULL and the count
of the corresponding candidate is not increased yet (for the
current customer sequence), it should be increased.
The root node is processed differently from other nodes.
At the root node, there is no constraint on which items in
the customer sequence should be checked against the root’s
S-extension link because the ﬁrst item of a candidate can
appear anywhere in the customer sequence. At other nodes,
there are always some constraints. Let’s see how to make
recursive calls at node 1 along its I-extension link. Recall
that we have made two recursive calls at the root node with
segments, CD − ADE − DH and DE − DH , for node
1’s I-extension. Now we process them at node 1. Since the
two segments are speciﬁed for node 1’s I-extension link,
we should check the items in their ﬁrst itemsets, CD and
DE, against node 1’s I-extension link. For CD −ADE −
DH , since C appears at node 6 which has no child, we stop
there by just increasing the count of AC. Another item,
Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence (ICTAI 2004) 
1082-3409/04 $20.00 © 2004 IEEE
D, appears at node 7. We increase the count of AD and
make recursive calls for node 7’s links. Since D is the last
item of the ﬁrst itemset in CD − ADE − DH , only one
recursive call with the segment ADE − DH is made for
node 7’s S-extension link. For another sequence segment
DE −DH at node 1, two items of the ﬁrst itemset, D and
E, are checked. D is located at node 7. Since the count of
AD is already increased before, we should not increase it
again. Two recursive calls are made at node 1 for node 7’s
links. One is with E − DH for node 7’s I-extension link
and the other is with DH for the S-extension link. We can
ignore E because it is not an I-extension child of node 1.
This process will continue until a leaf node is reached or
the sequence segment is empty.
4.3.2 Features of the Preﬁx Tree and the Customer Se-
quence Trimming
There are some major differences between our preﬁx tree
and the PSP’s preﬁx tree: 1) Our preﬁx tree is used to count
candidates of different sizes, whereas PSP’s preﬁx tree is
only used to count the candidates of the same size. 2) To im-
prove the candidate counting, a bit vector is associated with
our preﬁx tree to facilitate the customer sequence trimming.
3) The supersequence frequency based pruning reduces the
size of our preﬁx tree when we count the candidates against
the whole database.
Due to both 2) and 3), the preﬁx tree of MSPS is much
more efﬁcient. In our preﬁx tree structure, the I-extension
children (and S-extension children) of a node are linked to-
gether. During the candidate counting, we frequently need
to locate the items in the customer sequences along these
links.
Obviously, making the tree smaller or reducing the num-
ber of search operations can enhance the counting process.
In MSPS, by performing supersequence frequency based
pruning in Phase 3, only a part of the candidate set needs
to be processed. Thus, our preﬁx tree is usually much
smaller than PSP’s preﬁx tree at each pass. Moreover,
we also reduce the search operations by trimming the cus-
tomer sequences. In PSP, the items not in the preﬁx tree
are not trimmed from the customer sequence. Thus, when
these items are processed, they are searched along the cor-
responding links exhaustively, even though they are not in
those links. This unnecessary search cost is not trivial when
the number of customer sequences is large. MSPS can avoid
this problem. As the mining process makes progress, fewer
and fewer items would remain in the longer candidate pat-
terns, and the customer sequence trimming can save a lot of
time.
4.4 Analysis of the Sampling
Here, we discuss some important issues in sampling. For
both frequent itemset mining and sequence mining, if a pat-
tern is found frequent in db but turns out to be infrequent in
DB, it is an overestimate. On the other hand, if a pattern is
infrequent in db but actually frequent in DB, it is a miss.
Both our research and [9] try to mine the exact result
with the help of sampling. While we focused on how to
maximize the performance improvement, more attention
was given in [9] on how to reduce the probability of misses.
To achieve that goal, two methods were suggested in [9]: 1)
mine a large sample, and 2) lower the user-speciﬁed min-
sup for mining the sample. These two methods can reduce
the misses but also potentially degrade the overall perfor-
mance. Mining a large sample cuts the merit of sampling,
while lowering the user-speciﬁed minsup may generate a
large number of overestimates. Obviously, a complete sam-
ple result without misses does not necessarily mean the best
overall performance. In MSPS, the cost related to sampling
includes all the overhead of mining the sample and veri-
fying the sample results, whereas the performance gain is
from the supersequence frequency based pruning. The ef-
fectiveness of this pruning is determined by how many long
frequent patterns can be found from the sample. As dif-
ferent settings of sample size and the adjusted minsup for
mining the sample are used, the overall performance varies
accordingly. Thus, we pay our attention to the sample size
and the adjusted minsup in the following discussion.
4.4.1 Sample Size
In [9, 10], the minimum sample size that guarantees a small
chance of misses with certain conﬁdence is given by the
Chernoff boundary. Unfortunately, this theoretic guideline
is not quite practical because it is too conservative. In
MSPS, a large sample can improve the quality of sample
results with fewer misses and overestimates. Consequently,
verifying the sample result can be done quickly and the su-
persequence frequency based pruning can be very effective.
But the overhead of mining a large sample is high. On the
other hand, with a small sample, the overhead of mining
sample is low, but MFSdb may be in bad quality. Then, the
cost of verifying the sample result containing many over-
estimates would be high. If the small sample size makes
the minimum support count for mining the sample (i.e.,
minsup ∗ |db| or lowered minsup ∗ |db|) very small, min-
ing the sample itself may take a long time. Thus, a small
sample does not necessarily mean a lower cost. Further-
more, if only few long frequent sequences are found under
the border formed by MFSdb, then the supersequence fre-
quency based pruning will not be effective, either. That’s
why the sample should not be too large or too small.
Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence (ICTAI 2004) 
1082-3409/04 $20.00 © 2004 IEEE
In general, we do not know the distribution characteris-
tic of the database to be mined, so it is hard to determine
the best sample size. MSPS allows users to choose a plausi-
ble sample size empirically. In our experiments, we set the
sample size as 10% of the original database size. By using
a default sample size, how to balance the cost related to the
sampling and the quality of sample result mainly depends
on the adjusted minsup for mining the sample. Even though
the default sample size may not be the best one all the time,
with the method of adjusting the minsup, it works very well
in practice according to our extensive experiments.
4.4.2 Adjusting the User-speciﬁed Minimum Support
for Mining the Sample
In the sample mining result, a certain rate of misses is tol-
erable. Our tests show that, for a missing k-sequence, if
most of its long subsequences, such as subsequences with
length k − 1 or k − 2, are found, then the supersequence
frequency pruning is not affected much. In practice, as long
as the sample size is not too small, the probability that most
of these subsequences are also missed is quite low. Com-
pared to misses, overestimates could be a bigger problem.
Once an infrequent k-sequence is identiﬁed frequent in db at
pass k, then it may be joined with many other k-sequences
to generate a large number of false candidates in mining
the sample. Most importantly, the situation may become
even worse when the minimum support count for mining
the sample is very small. We found this is more serious for
sequence mining than for frequent itemset mining, because
the search space is much bigger. For MSPS, it not only de-
grades the efﬁciency of mining the sample, but also causes
a high cost to identify the overestimates.
In [9], they proposed using the lowered minsup for the
sample, however they did not consider the case that the user-
speciﬁed minsup is very small. In that case, it is dangerous
to lower the minsup further. In this research, we investi-
gated how to avoid the overestimates in the case of small
user-speciﬁed minsup, because such mining task is more
time-consuming.
There are three different cases that can happen when
MSPS is used: 1) If the user-speciﬁed minsup is big, sim-
ply using it or even a lowered one to mine the sample works
ﬁne. Only a small number of misses and overestimates oc-
cur in our tests. This is usually safe because our default
sample size is not very small. 2) If the user-speciﬁed min-
sup is small, the sampling technique is challenged. Using a
lowered minsup or even the original user-speciﬁed minsup
for mining the sample often causes many overestimates be-
cause lowered minsup∗|db| or minsup∗|db| is too small.
Even though increasing the sample size could be a solution
for this case, it limits the merit of sampling. Thus, we con-
sider increasing the minsup a little to mine the sample, hop-
ing it will limit the overestimates to a reasonable level. In
that case, more misses may occur. However, even though
there is a missing pattern, as long as most of its long subse-
quences are still contained in the sample result, the super-
sequence frequency based pruning is not affected much. 3)
In some rare cases, the user-speciﬁed minsup is extremely
small. Then, just increasing the minsup for mining the sam-
ple cannot solve the problem. We must consider increasing
the sample size too. Actually, both 2) and 3) cases raise
the same technical question: when user-speciﬁed minsup is
small, how to increase the minsup for mining the sample of
a certain size? We must keep in mind that if the increase in
the minsup for mining the sample is not enough, the prob-
lem of overestimates cannot be solved. On the other hand, if
it is increased too much, we may not ﬁnd any long patterns
from the sample.
Consider the original database DB and an arbitrary se-
quence X . If the support of X in DB is PX , then the proba-
bility that a customer sequence randomly selected from DB
contains X is also PX . Let’s consider a random sample S
with m customer sequences that are independently drawn
from DB with replacement. The random variable TX ,
which represents the total number of customer sequences
containing X in S, has a binomial distribution of m tri-
als with the probability of success PX . In general, if m is
greater than 30, TX can be approximated by a normal dis-
tribution whose mean is m ∗ PX and the standard deviation
is
√
m ∗ PX ∗ (1− PX).
In MSPS, suppose that we draw a sample S with m cus-
tomer sequences from DB, and then try to use the point
estimator P ′X = TX/m to estimate the support of X in
the population of DB. Then, P ′X is an unbiased estima-
tor with mean m ∗ PX/m = PX and standard deviation√
m ∗ PX ∗ (1− PX)/m =
√
PX ∗ (1 − PX)/m.
If we assume that the support of X in DB, PX , is the
user-speciﬁed minsup that we want to estimate, then P ′X ,
which is observed from a sample S, should be around PX
with a normal distribution as described above. If the ad-
justed minsup is denoted as P ′′X , we can assume P ′′X > PX
because our goal is to ﬁnd out how much we should in-
crease the minsup for mining the sample. If we use P ′′X to
mine the sample, the probability that the sequence X can be
found as a local frequent sequence in db is 1−PZ , where Z
= (P ′′X − PX)/
√
PX ∗ (1− PX)/m, and it is often called
the z-score.
Let’s consider the standard deviation of P ′X ,√
PX ∗ (1− PX)/m. The value of its part PX ∗ (1− PX)
= −(PX − 1/2)2 + 1/4 is increasing in the PX interval of
[0, 1/2]. Since minsup is usually smaller than 50%, we can
assume the value of PX ∗ (1− PX) is increasing in the PX
interval of [0,minsup]; that means, the standard deviation
of P ′X is increasing in this PX interval. If the support of
another sequence Y in DB is lower than the minsup PX ,
Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence (ICTAI 2004) 
1082-3409/04 $20.00 © 2004 IEEE
then both the mean and the standard deviation of observed
P ′Y should be smaller than those of P ′X , respectively.
Therefore, compared with P ′X , the distribution curve of P ′Y
is shifted left and is shaper. Thus, if we set the adjusted
minsup for mining the sample as P ′′X , the probability that an
infrequent sequence Y is identiﬁed as frequent in S should
be lower than 1 − PZ . This guarantees the probability
that any infrequent sequence is identiﬁed as frequent in
the sample is lower than 1 − PZ . In our experiments,
the critical value of Z is set to 1.28, where PZ = 0.90,
such that the probability of the overestimate is at most
10%. From Z = (P ′′X − PX)/
√
PX ∗ (1− PX)/m, we
can drive P ′′X = PX + Z ∗
√
PX(1− PX)/m, where
PX = minsup and m = |db|.
This formula provides a theoretic guideline for adjusting
the user-speciﬁed minsup to mine the sample. Even though
this adjusted minsup value may not be the best one all the
time, it worked well in most of our experiments.
5 Performance Analysis
To compare MSPS with other algorithms, we imple-
mented GSP and obtained the source codes of SPAM and
SPADE from their authors’ web sites. All the experiments
were performed on a SuSE Linux PC with a 2.6 GHz Pen-
tium processor and 1 Gbytes main memory.
MSPS was compared with others on various databases,
and we evaluated the scalability of these algorithms in terms
of the number of items and the number of customer se-
quences. We also investigated how the sample size and
the adjusted minsup for mining the sample affect the per-
formance of MSPS. Since the sampling technique is proba-
bilistic, we ran MSPS 100 times for each test. The average
execution time of the 100 runs was reported as the perfor-
mance result. The default sample size was ﬁxed as 10% of
the test database for all experiments. The databases used in
our experiments are synthetically generated as in [2]. The
database generation parameters are described in Table 1.
For all databases, NS = 5000 and NI = 25, 000; and the
names of these databases reﬂect other parameter values used
to generate them.
5.1 Performance Comparison
We ran MSPS, GSP, SPADE and SPAM on three
databases with medium sizes of about 100 Mbytes. The
number of items in these databases is 10,000. In our tests,
SPAM could not mine these databases, and its run was ter-
minated by the operating system. Our machine is a 32-bit
system, but the user address space is limited to 2 Gbytes. In
all these tests, SPAM always required more than 2 Gbytes
memory, hence caused the termination.
Table 1. Parameters Used in Database Gener-
ation
D Number of customers in the database
C Average number of transactions per customer
T Average number of items per transaction
S Average length of maximal potentially frequent sequences
I Average length of maximal potentially frequent itemsets
N Number of distinct items in the database
NS Number of maximal potentially frequent sequences
NI Number of maximal potentially frequent itemsets
As discussed before, when the user-speciﬁed minsup is
small, simply using it or a lowered one to mine the sample
may cost too much due to so many overestimates. In prac-
tice, we may not know the data distribution characteristics
of the database to be mined. Thus, we conservatively as-
sumed that all user-speciﬁed minsups in our tests are small,
and simply increased them a little bit for mining the sam-
ple. The adjusted minsup for each test is computed using
the formula given before. The probability that an overesti-
mate occurs is set to 10% at most, i.e., Z = 1.28.
The test results are shown in Figure 2. With the op-
timization components integrated, MSPS performs much
better than GSP because it processes fewer candidates in
a much more efﬁcient way. The advantage of SPADE is the
efﬁcient counting of the candidates by intersecting the id-
lists. However, when mining a medium size database with
400,000 customers, the counting for LDB2 in SPADE is in-
efﬁcient and degrades the overall performance very much.
Considering both factors, we can say that if there are not
enough number of candidates to be counted, SPADE can-
not show its efﬁciency. That is why SPADE is even worse
than GSP when the minsup is big, as shown in some of the
ﬁgures.
When the minsup is decreased, more and more candi-
dates appear during the mining. In that case, the overhead
of GSP in candidate generation, pruning, and especially
the counting using a huge hash tree increases drastically.
For MSPS, this situation is considerably improved by us-
ing the supersequence frequency based pruning, the preﬁx
tree structure, and the customer sequence trimming. When
many passes are required for the mining, most candidates
usually appear after pass 2, hence MSPS can outperform
GSP further when the minsup is decreased. This improve-
ment also makes MSPS better than SPADE in most tests on
the medium size databases. Only when the minsup is very
small, SPADE can beat MSPS.
5.2 Scalability Evaluation
Both SPADE and SPAM need to store a huge amount
of intermediate data to save their computation cost. When
Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence (ICTAI 2004) 
1082-3409/04 $20.00 © 2004 IEEE
0200
400
600
800
1000
1200
1400
0.3 0.2 0.1 0.08 0.06
Minimum Support (%)
Ex
ecu
tion
 Ti
me
 (se
c)
GSP
SPADE
MSPS
(a) D400K-C10-T5-S5-I1.25-N10K
0
500
1000
1500
2000
2500
3000
0.3 0.25 0.2 0.15 0.12
Minimum Support (%)
Ex
ecu
tion
 Ti
me
 (se
c)
GSP
SPADE
MSPS
(b) D400K-C20-T2.5-S10-I2.5-N10K
0
1000
2000
3000
4000
5000
0.3 0.25 0.2 0.15 0.12
Minimum Support (%)
Ex
ecu
tion
 Ti
me
 (se
c)
GSP
SPADE
MSPS
(c) D400K-C20-T2.5-S10-I1.25-N10K
Figure 2. Performance Comparison on Medium Size Databases
the memory space requirement is over the memory size
available, CPU utilization drops quickly due to the frequent
swapping. Compared with them, MSPS and GSP process
the customer sequences one by one, hence only a small
memory space is needed to buffer the customer sequences
being processed. MSPS can also handle the situation that
LDBk or C
DB
k can not be totally loaded into memory by
using the signatures as explained in Section 4. Therefore,
MSPS does not require the memory space as much as GSP,
SPADE and SPAM.
Many real-life customer market-basket databases have
tens of thousands of items and millions of customers, so we
evaluated the scalability of the mining algorithms in these
two aspects. First, we started with a very small database
D1K-C10-T5-S10-I2.5 and changed the number of items
from 500 to 10,000. The user-speciﬁed minsup was 0.5%.
To run MSPS on such a small database with only 1000
customers, we selected the whole database as the sample
and keep the user-speciﬁed minsup unchanged to mine it.
Since MSPS does not apply the sampling on such a small
database, supersequence frequency based pruning is not
performed in mining. Thus, in this case, SPADE and SPAM
performed better than MSPS and GSP as long as their mem-
ory requirement is satisﬁed.
As the number of items is increased, SPAM shows its
scalability problem. Theoretically, the memory space re-
quired to store the whole database into bitmaps in SPAM is
D ∗ C ∗ N/8 bytes. For the id-lists in SPADE, it is about
D ∗C ∗ T ∗ 4 bytes. But we found these values are usually
far less from their peak memory space requirement during
the mining, because the amount of intermediate data in both
algorithms is quite huge.
Compared with SPAM, SPADE divides search space into
small pieces so that only the id-lists being processed need
to be loaded into memory. Another advantage of SPADE
is that the id-lists become shorter and shorter with the
progress in mining, whereas the length of the bitmaps does
not change in SPAM. These two differences make SPADE
much more space-efﬁcient than SPAM.
Second, we investigated how they perform on C10-T5-
S10-I2.5-N10K when the user-speciﬁed minsup is 0.18%.
We ﬁxed the number of items as 10,000 and increased the
number of customers from 400,000 to 2,000,000. SPAM
cannot perform the mining due to the memory problem.
For SPADE, we partitioned the test database into multiple
chunks for better performance when its size was increased.
Otherwise, the counting of CDB2 for a large database could
be extremely time-consuming. We made each chunk con-
tain 400,000 customers so that it is only about 100 Mbytes,
which is one tenth of our main memory size. Figure 3 shows
that the scalability of MSPS and GSP are quite linear. As
the database size is increased, MSPS performs much better
than the others.
When database is relatively small with only 400,000 cus-
tomers, SPADE performed the best, about 20% faster than
MSPS. But SPADE cannot maintain a reasonable scalabil-
ity as the database becomes larger, and MSPS starts outper-
forming SPADE. When the database size is increased from
1600K customers to 2000K customers, there is a sharp per-
formance drop in SPADE, such that it is even slower than
GSP. In that case, MSPS is faster than SPADE by a factor
of about 8. As discussed before, counting CDB2 is a per-
formance bottleneck for SPADE, because the transforma-
tion of a large database from the vertical format to the hor-
izontal format takes too much time. When the database is
very large, the transformation also requires a large amount
of memory and frequent swapping, hence the performance
drops drastically. Partitioning the database can relieve this
problem to some extent but does not solve it completely. In
addition, for the database with a large number of items and
customers, SPADE needs more time to intersect more and
longer id-lists.
Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence (ICTAI 2004) 
1082-3409/04 $20.00 © 2004 IEEE
100
1000
10000
100000
400 800 1200 1600 2000
Number of Customers ('000s)
Ex
ecu
tion
 Ti
me
 (se
c)
GSP
SPADE
MSPS
Figure 3. Scalability: Number of Customers
(on C10-T5-S10-I2.5-N10K, minsup=0.18%)
Finally, we mined a large database D2000K-C10-T5-
S10-I2.5-N10K, which takes about 500 Mbytes, for vari-
ous minsups. This database is partitioned into 5 chunks for
SPADE, and the results are shown in Figure 4.
Based on our tests, we found SPADE performs best for
small size databases. For medium size databases, MSPS
performs better for relatively big minsups while SPADE
is faster for small minsups. When database is large,
SPADE’s performance drops drastically and MSPS outper-
forms SPADE very much. If the user-speciﬁed minsup is
big and there are very few long patterns, GSP may perform
as well as, or even better than, others due to its simplicity
and effective subsequence infrequency based pruning.
100
1000
10000
100000
0.33 0.3 0.25 0.2 0.18
Minimum Support (%)
Ex
ec
uti
on
 Ti
me
 (se
c)
GSP
SPADE
MSPS
Figure 4. Performance on a Large Database
D2000K-C10-T5-S10-I2.5-N10K
6 Conclusions
In this paper, we proposed a new algorithm MSPS for
mining maximal frequent sequences using sampling. MSPS
combined the subsequence infrequency based pruning and
the supersequence frequency based pruning together to re-
duce the search space. In MSPS, a sampling technique is
used to identify potential long frequent patterns early. When
the user-speciﬁed minsup is small, we proposed how to ad-
just it to a little bigger value for mining the sample to avoid
many overestimates. This method makes the sampling tech-
nique more efﬁcient in practice for sequence mining. Both
the supersequence frequency based pruning and the cus-
tomer sequence trimming used in MSPS improve the can-
didate counting process on the new preﬁx tree structure de-
veloped. Our extensive experiments proved that MSPS is
a practical and efﬁcient algorithm. Its excellent scalability
makes it a very good candidate for mining customer market-
basket databases which usually have tens of thousands of
items and millions of customer sequences.
References
[1] R. Agrawal and R. Srikant, “Fast Algorithms for Mining As-
sociation Rules,” Proc. of the 20th VLDB Conf., 1994, pp.
487–499.
[2] R. Agrawal and R. Srikant, “Mining Sequential Patterns,”
Proc. of Int’l Conf. on Data Engineering, 1995, pp. 3–14.
[3] J. Ayres, J. Gehrke, T. Yiu, and J. Flannick, “Sequential Pat-
tern Mining Using a Bitmap Representation,” Proc. of ACM
SIGKDD Conf. on Knowledge Discovery and Data Mining,
2002, pp. 429–435.
[4] B. Chen, P. Haas, and P. Scheuermann, “A New Two-
Phase Sampling Based Algorithm for Discovering Associ-
ation Rules,” Proc. of ACM SIGKDD Conf. on Knowledge
Discovery and Data Mining, 2002, pp. 462–468.
[5] S. M. Chung and C. Luo, “Efﬁcient Mining of Maximal Fre-
quent Itemsets from Databases on a Cluster of Workstations,”
to appear in IEEE Transactions on Parallel and Distributed
Systems.
[6] F. Masseglia, F. Cathala, and P. Poncelet, “The PSP Ap-
proach for Mining Sequential Patterns,” Proc. of European
Symp. on Principle of Data Mining and Knowledge Discov-
ery, 1998, pp. 176–184.
[7] J. Pei, J. Han, B. Mortazavi-Asl, H. Pinto, Q. Chen, U.
Dayal, and M. C. Hsu. “PreﬁxSpan: Mining Sequential Pat-
terns Efﬁciently by Preﬁx-Projected Pattern Growth,” Proc.
of Int’l. Conf. on Data Engineering, 2001, pp. 215–224.
[8] R. Srikant and R. Agrawal, “Mining Sequential Patterns:
Generalizations and Performance Improvements,” Proc. of
the 5th Int’l Conf. on Extending Database Technology, 1996,
pp. 3–17.
[9] H. Toivonen, “Sampling Large Databases for Association
Rules,” Proc. of the 22nd VLDB Conf., 1996, pp. 134–145.
[10] M. J. Zaki, S. Parthasarathy, W. Li, and M. Ogihara, “Eval-
uation of Sampling for Data Mining of Association Rules,”
Proc. of the 7th Int’l Workshop on Research Issues in Data
Engineering, 1997.
[11] M. J. Zaki, “SPADE: An Efﬁcient Algorithm for Mining Fre-
quent Sequences,” Machine Learning, 42(1), 2001, pp. 31–
60.
Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence (ICTAI 2004) 
1082-3409/04 $20.00 © 2004 IEEE

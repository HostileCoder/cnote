Proceedings of the Third International Conference on Machine Learning and Cybernetics, Shanghai, 26-29 August 2004 
SUPPORT VECTOR REGRESSION WITH LOCAL E PARAMETERS WITH 
THE SUPPORT VECTORS 
XUNXIAN WANG, YUNFENG WANG, DAVID BROWN 
Intelligent systems and fault hagnostic group, Deprtment of Electroruc and Computer engmeermg, Umversity of 
Portsmouth Portsmouth, Po1 3DJ, UK 
Abstract: 
In support vector machine regression (SVR) a big 
Evalne will give a rough system model WW LMe support 
vectors and a small E value wlll give a accurate system model 
wltb numy support vectors The seleetbn of lhe support 
vectors wlll be effected by a small change of the training data 
To obtain an accurate model whb little support vettors, a 
method Locludes two steps is proposed in this paper, In step 
one, a big E value is used to selett a small number of the 
support vectors; in step two, by gMng these selected support 
vectors a anall value while others a blg one, a accurate system 
model will be obtaloed. The experimental reaults demonstrate 
the eflldeay of the proposed method 
Keywords: 
Mercer kernel; 
1. Introduction 
Machine learning; regredon; support vector machioe; 
The generalization ability of a learning machine can be 
measured by its VC confidence [ 1,2] which tells that in 
order to obtain a good leaming machine by using a given 
trainiig set, both of the empirical risk and the VC- 
dimension should be small. Support vector machine 
classification (SVMC) [ 1-41 uses linear function as the 
classification function. By applying the maximum margin 
principle on the determmation of the function parameters, a 
good classifying machine can be obtained. Expended the 
idea to regression problem, the error band is used in support 
vector machine regression ( S W )  [1,2,5,6]. And linear 
functions are used to do the system regression just as in the 
SVMC. In the situation where linear regression is improper, 
a mapping will be seek by which the training data can be 
mapped into a higb dmensional space where the linear 
regression can be implemented. Due to the difficulties of 
f m h  the exactly mapping, Reproducing Kemel Hilbert 
Space [RKHS] theory [7] is applied and plenty of ideas are 
used to determine the kemel function, kemel parameter [8- 
101 as well as to determine the width of the error band 
0-7803-8403-2/04/$20.00 @2W IEEE 
denoted by E [ 111. In thii paper, we show that even when 
the proper mapping is known, the obtained system model 
will not be in the ideal format. In addition, a small change 
of the training set will result in a different system model. 
To improve the method, two levels of E value are used. 
Whde the big one is used to determine the position of the 
support vectors, the small one is used to refine the weight 
of each support vectors 
2. Basic support vector regression algorithm 
Given N pairs of training set {(xk,yk)}ki where x is 
the m-dimensional input variable, y is the output variable, if 
the Hyper-plane function is used to approximate the set, the 
regression function can be stated as 
Y’=(w.x)+b (2) 
With the above linear function as the regression 
function, the SVMR problem can be represented as the 
following constmint minimization problem 
(w.x,)+b-y, <&+5, 
<; t 0 Subject to for 1 5 i 5 I 
5, < o  
(31 
Through its Lagrange format, use KKT condition, its 
Maxunize 
Dual problem can be represented as 
4289 
Proceedings of the Third International Conference on Machine Learning F d  Cybernetics, Shanghai, 26-29 August 2004 
Subject to 0 5 a,* s C, i = I,.-,/ 
O<a,<C, i=l , . . . , I  (4) 
Due to the convexity of the problem (3), The dual gap 
of the above two problems is zero and so the solution of 
problem (3) can be obhined by s o l ~ n g  problem (4) [I 21. 
SVM is advantaged from the dot product of the input 
value x j ,  x j  ,i, j = 1,. . . , I  in the above epuation If the hyper- 
plane is not suitable for the regression problem, a mapping 
dx)  can be used to map the trained set from its origmal 
space to a high dimensional space (sometimes called 
feature space), where the training data set is expected to be 
represented by a hpr-plane in the new space. After tbis 
mapping dx), the trairdng set becomes K d x k ) , y k ) I k ,  . 
As a resulf the dot product in the above equation becomes 
(~xi ) .~x , ) ) .Noteasy toget th ismap CO($ and 
somehes  the mapplng has too many items to deal with, 
the Mercer kemel is used to replace the above product by 
def- 
1 
K ( x , , x , ) =  ( d X i ) . d X J ) )  
Then the general SVMR problem can be stated as 
i=l 
Subjectto Osa: sC, i = l , . - , I  
Osa, iC, i= l , . . . , I  (5) 
Solving the above optindsation problem, the resulttng 
leaming machine can be represented as 
If the mapping &)is defined as 
function (7) can be converted kom I-D domain space 
p(x)  = (cosx,sinx) = @,q) (8) 
to a linear function in a 2-D domain space as below 
y = cos x cosxo +sinxsinx, + e  
'PPO + W O  + e  
With mapping p , the kemel can be written as 
k(x, Y )  =< dx), d Y )  > 
=c (cos x,sinx), (cos y ,  sin y )  > 
= cos(x- y )  
(9) 
(IO) 
AS an example, if the tmining set (1, , y i ) f l  is given, 
the regression function will have the following format 
where B=(pl ... ~, )dbedeterminedbySVMR 
algorithm. 
1 
f ( x ; B ) =  ZB' cdx-x , )  (11) 
,=I 
choose the training set as 
( ( x j , y 8 ) l y ,  =cos(x8),xi =0.5*i-5)::l . U s e e l ,  
E = 0.1 in SVMR, a system model with 3 suppat vectors 
can be obtained as 
f ( x ;  B) = 0.4520 cos x -0.2260 cos(x + 3.0) 
-0.2260cos(x-3.0) (12) 
The figure is shown in Fig.] .a. From the figure, it can 
be seen that besides the support vector (SV) at x = 0, there 
are other two SV at x = -3 and x = 3 .  But a ideal system 
model should have only one support vector at x = 0 .  
+:model set 
':support vecto 
I I 
Input 
system model; * dots are the suppm veCtoIS 
5 0 '  ' 5 '  10 3. Support veetor regression analysis by a simple 
example Fig.l.a:,Mo&lhgredtwith C=land &=O.I.The 
solid line is the cosine function; dot line with + are the 
(there are total 3 in this situation) 
The trairdng set is given by using the following cosine 
y = cos(x-x,,)+e (7) 
function plus noise e as 
4290 
Proceedings of the Third International Conference on Machine Laming and Cybernetics, Shanghai, 26-29 August 2004 
If a single noise is added by set e(1) = 0.2 , In this 
situation, the SVR gives a system model with 5 suppat 
vectors shown in Fig.1.b. The system model has the 
following f o m t  
to the training sef E = 0.1,O.Z and 0.5. There are 5,3 and 3 
support vectors used in the system models. 
Alternatively, when keep E = 0.1 and change C=1,0.1 
f ( x ;  p) = cos(x+ 4.5)- 0.2324 cos(x + 4) 
-0.0616cos(x+ 1) + 0.1708cos(x+0.5)-co~x- 2.5) 
In this ssituatio~ h e  SV in x = 0 doesn't exist any 
and 0.01 respectively, 5,12 and 19 support vectors are 
needed in the system models relatively. The result is shown 
inFig2.b. 
(13) 
more. 
-:training set 
+:model set 
0 6  ":support vecto r 
5 0 3 10 
Input 
Fig.1.b: Modelhgresultwhen e(l)= 0.2isaddedto 
the trairdng set (with C = I ,  E = 0.1). There are total 5 
support vectors in this situation 
The above result shows that a slngle noise will change the 
establishedmodel a lot 
Keep using the noise model, when use C = 1 and 
E = 0.1,0.2 and 0.5 respectively, three models are produced 
in Fig.2.a. There are 5,3 and 3 support vectors used in the 
system models. 
I I 
-5 0 '  ' 5 " 1 0 .  
Input 
Fig.2.a: Example inpuVoutput and the system model 
p r h c e d b y  SVRwith C =1 and when eo) = 0.2 is added 
I J 
5 0 " 5 10 
Input 
Fig.2.b: Example inputloutput and the system model 
produced by SVR with E = 0. I , C = 5,1,0.1 and 0.05 
respectively. e(l)= 0.2. There are 12,5,12 and 19 support 
vectors used in the system models. 
4 Support vector regression with special E on the 
support vectors 
Suppressing the innuence of the noise as small as passible, 
keeping the system model as accurate and sunple as 
possible, the following method is proposed There are two 
steps: First: determination of the support vectors: be ware 
that a big E value can help the system model avoichg the 
innuence of the noise better, a big E is used to obtain a 
system model with small nmber of support vectors; 
Second, based on the support vectors (SVs) obtained in step 
I,  change the E value on these SVs to a small value others 
unchanged do the SVMR again, this time, a new system 
model with much smaller empirical risk will be obtained 
and the SVs n u m k  will not change normally. 
The modified the " i m t i o n  problem is 
Minimize 
4291 
Proceedings of the Third International Conference on Machine Learning and Cybernetics, Shanghai, 26-29 August 2004 
y , - ( w . x , ) - b < & , + t , *  
(w .x i )+b-y i  <&,+ti 
Subject to f o r l s i i l  
p: 2 0 
5, <I3 
(3') 
Respectively, maximum problem of equation (5) will 
become 
W(a, a')  = - - .E (a: - ai)(a; - a j  ) ~ ( x ~ ,  x j )  
-E ci (a,* +ai) + C (a: -cz,)yi 
Maxirrdze 
1 '  
2 i,,=l 
I i 
i= 1 i=l 
z ( a :  - a,) = 0 
i=l 
Subjectto Osaf <C, i=l,-,I 
Oia, sC, i=l,.-,l (57 
The value of E can be determined as: First by using 
equation (5 )  with a big E (denoted as ~ l )  to obtain a 
system model. Then change the E value of these related to 
the support vectors produced in the former step to a mall 
one,(denotedassZ).Nowthe E valuesare 
(14) 
~l for non - support vectors 
E2 for suppcot vectors 
To do the SVMR by this new E, a system model with both 
small nmber of support vectors and small empirical risks 
can be obtained. 
As an example, Fig.3 shows the result.for the above used 
tratning set with single noise e(1) = 0.2. First, &1=0.5 
(C=l) is used and this will p r o h e  a system model with 3 
support vectors & shown in the figure. Then &2 = 0.1 is 
used to refine the system model. The new model have three 
suppart vectors but a much small empirical risk which can 
be seen easily fiom the figure. Beside ~ 2 = 0 . 1 ,  
c2 = 0.01 is hied also to see the effect of the idea. It can be 
seen that when &Z=O.OI  is used, training set and the 
system model are nearly the same except in the staring 
p&od around x=-4.8 where the noise e(l) is available. 
Actually, The model when EZ = 0.01 is 
& = (  
f ( x ; P ) =  0 . 4 9 7 5 ~ 0 ~ - 0 . 2 4 8 7 ( ~ 0 ~ ( ~ + 3 ) + ~ 0 S ( ~ - 3 ) )  
(15) 
= 0 . 9 9 4 2 ~ ~  
And the model when 62 = 0.1 is 
f ( x , 8 )  = 0.4520cosx-0.2260(cos(x+3)+ccs(x-3)) 
= 0.9034cosx 
(16) 
f ( x ; P )  = 0.250 cosx - 0.1 250(cos(x + 3) + cos(x- 3)) 
(17) 
The one when both s l =  &2 = 0.5is 
= 0.scosx 
The efficient ofthe algorithmis obvious. 
Input 
Fig.3 Mcdehgredtswi th  C=l ,&I=0.5,  
&2 = 0.5,O.landO.Ol respectively. 
Ne* A more complicated 2-D sinc function added by 
white zero mean uniform noise is used to show the 
advantage of the proposed a l g o r i h .  The tr;umng set is 
generated by 
5 sin i ( x  -9.99)2 + (y - 9.99)2 
f ( x ,  Y)' . + e  (18) . . .._ . .  
\l(x-9.99)2 +(y-9.99)2 
where e has uniform (not normal) distribution in the 
interval [-025,0.25].(Here the Gaussian noise is not used). 
The input of the training set is given by 
{x , ) : !~  x {y ,  = {2i -  IO)^!^ x (pi - 10):~ ; the test set is 
given by ( X , ) ~ ~ ~ X ( ~ ~ ) ~ ~ ~  = (0 .4 i -10) ~1x(0 .4 i -10)~~1 
training set is smaller compared to the test set). 
Gaussian kernel with the following format is used 
Whenu2=2,C=5,  & = 2  i s u s e d w i t h n d S V M R , a  
system model with 9 support vectors are established shown 
in Fig.4.a. The used 9 support vectors and the related 
weight values are listed in Table. 1. The pictures are shown 
in Fig.4. 
4292 
Proceedings of the Third International Conference on Machine Learning and Cybernetics, Shanghai, 26-29 August 2004 
In Fig.4.a the upper three pictures are training set, test set 
with uniform noise and the pure test set without noise. The 
middle three are the system model produced hy the SVMR 
(The middle and the right one are the same). The Lower 
three pictures are the related errors. From the figure we can 
see due to the big E ,  the errors are big. 
sm M i - 2 .  S"l.".."m=9 
Fig 4.a. The experiment result given hy general SVR 
with C=5, E = 2 .  The upper three pictures are the training 
set, test set with noise and pure set without noise. The 
middle three are the system model produced hy the SVR 
(The middle and the right one are the same). The Lower 
three pictures are the related errors 
Sm s s  - 0  I ,SUPPO. n f l o n -  75 
Fig 4.b. The experiment result given hy general SVR with 
C=5, E = 0.1 . The upper three pictures are the training set, 
test set with noise and pure set without noise. The middle 
three are the system model produced by the SVR (The 
middle and the right one are the same). The Lower three 
pictures are the related errors 
When U' = 2 ,C=5, E = 0.1 is used, a system model with 
75 support vectors are established shown in Fig.4.h. It is 
ohviously that this model has much small empirical risk 
than the former one but with much more support vectors 
When try the 2-steps method proposed in this paper and use 
U* = 2 ,C=5, &I = 2 and &I = 0.1. a system model with 9 
(75-9). 
support vectors are established shown in Fig. 4.5 With the 
same support vectors as in Fig4.a, the weight of each 
support vectors has been changed and result in a system 
model with much small empirical risk than in Fig.4a. 
SA M 1 1  i 2 . m  I ,  SOI" I"c la  i o  
Fig 4.c. The experiment result given hy general SVR 
with C=5, &I = 2,&2 = 0.1 . The upper three pictures are the 
training set, test set with noise and pure set without noise. 
The middle three are the system model produced by the 
SVR (The middle and the right one are the same). The 
Lower three pictures are the related errors 
S",. S". 
Fig 4.d. The experiment result given hy general SVR 
with C=5, &1= 0.5,&2 = 0.02. The upper three pictures are 
the training set, test 'set with noise and pure set without 
noise. The middle three are the system model produced by 
the SVR (The middle and the right one are the same). 
The Lower three pictures are the related emors. A 
more result is given to show that when using 
&I = 0.5, &2 = 0.02 ; a system model with 42 support vectors 
can he produced which has similar empirical risk with the 
model in Fig4.h but the SVs number is much smaller (75- 
42). 
5. Conclusion 
The propose method can improve the accuracy of the SVR 
dramatically as shown in the paper. When same empirical 
4293 
Proceedings of the Third International Conference on Machine Learning and Cybernetics, Shanghai, 26-29 August 2004 
risk needs to be meet, this method can give a system 
model with less support vectors and this will result a 
system model with less regressors. 
6. References 
V.N.Vapnik. The nature of statistical learning 
theory; Springer. 2000 
B. Scholkopf and A. J. Smola. Lea- with 
Kemels. MIT Press 2002. 
C. J. C. .Burges. A tutorial on support vector 
machines for pattem recognition. Data Mini 
and Knowledge Discovery, 2(2):121-167,1998 
B.Boser, I. Guyon, and V.N. Vapnik “A 
training algorithm for optimal margin 
classifiers”, Fifth Annual Workshop on 
Computational learning Theoq, Piitsburgh 
A U  pp 144.152 
V. N.Vaonik S. Golowich and A Smola. 
- 
~ ~~~ ~ ~ 
Support vector’ method for function 
approximation, regression estimation and signal 
processing. In M C. Mom, M I. Jordan, and 
T. Petsche, editors, Advances in Neural 
Information Processing Systems 9, pages 281 - 
287, Cambridge, MA. 1597. MIT Press 
Smola and B. Scholkopf. A tutorial on support 
vector regression. Technical Report NC2-TR- 
1998-030, NeuroCOLT2, 1998. 
N. Aronszajn. Theory of reproducing kernels. 
Transactions of the American Mathematical 
Society, 68:337-404, 1950. 
0. Chapelle, V. Vapnik, 0. Bousquet, and S. 
Mukhejee. Choosing multiple parameters for 
support vector machines. Machine Learning, 
4q1): 13 1-1 59,2002. 
K. Duan, S.S. Keerth~, and A.N. Poo. 
Evaluation of simple performance measures for 
tuning svm hyperparameters. Neurocomputing, 
51:41-59.2003. 
C. S. &, A. J. Smola, and R C. Williamson. 
Hyperkemels. In Neural Information Processing 
Systems, volume 15. MIT Press, 2002. 
B. Scholkopf, A. Smola, R C..Williamson, and 
P. L. Bartlett. New support vector algorithms. 
Neural Computation, 12:1207-1245,2000, 
M.S.Baazraa, H.D.Sherali C.M.Shetty, 
“Nonlinear programming: theory and 
algorithms” second edition, John Wiley &Sons, 
Inc., 1993 
Table.2 The support vectors used in the SVR when 0’ = 2 .C=5, ~ l =  0.5, €2 = 0.1 
I SVI I s v 2  I s v 3  I s v 4  I s v 5  I SV6 I s v 7  I SV8 I s v 9  
& I  -4 I -4 I -2 I -2 ’ I 0 1  2 . I  ? I  4 1  4 
‘ X I  I -2 I 2 1  4 1 4 1  0 1  :4 I * - 4  I -2 I 2 
p I -0.5199 I -0.3802 I -0.4977 I -0.7413 I 5 ‘ 1  -0.7657 I -0.6944 I -0.6777 I -0.7231 
4294 

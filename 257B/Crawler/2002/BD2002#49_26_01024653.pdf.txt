63 
Complexity Metric of Data Enquiry Functions for Public Registers and 
Electronic Commerce 
Doc.dr.sc. Vjeran Strahonja 
University of Zagreb, Faculty of Organisation and Infomatics Varaidin, 
Pavlinska 2, 42000 Varaidin, Croatia 
vstrahonwoi. hr 
Abstract. A complex and extensible metric for 
data enquiry functions is needed to predict a 
system performance and to create an effective 
cost management policy for public registers and 
electronic commerce environments. This may be 
a significant undertaking. 
This article proposes an advanced complexity 
metric of data enquiry function. The advantage 
of the proposed method over standard methods 
used in pricing, such as counts of typical outputs, 
is that it can be applied on complex queries 
resulting in big data sets. The aim of the 
proposed method is also to calculate the efort 
required to create and disseminate required data 
output in complex and distributed systems. 
Keywords. complexity metric, cost 
management, operation complexity, technical 
compexity 
1. Introduction 
Communications and information tech- 
nologies are dominant technologies of the 
emerging “new economy” and e-society. Their 
enabling capabilities have added new 
opportunities in every facet of our society. 
An efficient transactional system is not 
enough to meet needs of successful companies as 
well as modem e-government concepts. Many 
observers have recognized the nature of data as 
an economic good, but also one of the 
bottlenecks in the case of “closed” systems and 
limited access to data. The ability to process a 
great number of data queries and aggregate data 
into meaningful reports is a conditio sine qua 
non. 
Traditional query and reporting tools are 
hiding the complexity of query’s underlying SQL 
statements and enabling a wider audience to 
access data and develop ad-hoc reports. 
Furthermore, OLAP (on-line analytical 
processing) technologies are providing far 
greater capabilities and provide the user a rich set 
of flexible queries. However, complex queries 
result in increased response times and risk 
degrading the performance of transaction 
processing and mission-critical operations. Ad- 
hoc queries, enabled by open system 
architectures and the Internet revolution, have 
added to the magnitude of data already generated 
by business transaction processing. A new 
buzzword “click stream” refers to these streams 
produced by a single click initiating complex 
queries. While IT people at business are 
concerning about decrease of system 
performance and data security caused by 
increasing requests for information, IT people at 
governments are conceming also about a data 
pricing policy and pricing methods. Both are 
conceming about problems of cost management 
and optimization for clients that access public 
registers and other intellectual property, or use 
some services. Clients will pay for accessing 
data, and optimizing costs will be as important as 
optimizing performance in traditional databases 
VI. 
The desirable pricing method is one where 
metric of data enquiry functions and the cost 
estimation methods are: 
- 
- Automated 
- Accurate and 
- 
Efficient and simple as much as possible 
From users point of view transparent. 
Standard value-based pricing methods rely 
- Counts of typical outputs and price 
associated to every output 
- Volume of the output data screen 
produced by data enquiry function and price 
associated to this volume of data, or on 
- Combination of these methods. 
on: 
24th Int. Conf. Information Technology Interfaces IT1 2002, June 24-27, 2002, Cavtat, Croatia 
Advanced cost-based pricing methods try also 
to take into account CPU usage costs, U 0  costs 
and communication costs, especially in the case 
of remote, parallel and distributed systems. Cost- 
based pricing methods have the added advantage 
of being based on the processing complexity 
metric of data enquiry functions and associated 
resources needed for their execution. These 
methods are well suited for measuring and 
pricing of data requests that result in complex 
processing and large data sets. 
2. Data enquiry functions and logical 
operations 
User’s request for information may be met by 
some automated data enquiry function. Data 
enquiry function is vied as a set of data 
processing operations resulting in a screen 
display, or a report, or a data set in digital form, 
proceeded by some input of parameters and data 
accessing. 
Over time, SQL has emerged as the standard 
declarative language for expressing data enquiry 
functions related to access and maintenance of 
data stored in relational databases. Stated thus, 
logical operations and data enquiry functions 
may be expressed by SQL statements. SELECT- 
FROM-WHERE is the basic logical operation 
that queries one or more tables or views, joins 
any number of tables, enables ordering (sorting) 
the result table and returns rows and columns of 
data. One SELECT statement may be used as a 
sub query in another statement. This feature 
results in a tree-structured query with defined 
parent-child relationships between nodes. 
Specification of a data enquiry function by 
using SQL implies implementation because some 
execution engine can execute a piece of SQL 
code. 
Therefore, the simplest way to think about 
data enquiry functions on a logical level is to 
express them by introducing logical operations. 
On a logical level, a data enquiry function is vied 
as a set of logical operations. Logical operations 
are low-level functional components used as 
building blocks for data enquiry functions. Every 
logical operation takes an input data stream and 
produces an output data stream. The input data 
stream comprises a set of input data to be 
transformed by this operation and a set of input 
parameters that control this operation. The input 
data can appear on an input screen, in an input 
table or file, or in an interface memory space. 
The output data stream comprises a set of output 
data to be displayed, or to be stored in an output 
table, file, or interface memory space. Logical 
operations may be for example: index scan, table 
scan, sort, join, loop join etc. 
An abstract representation of a data enquiry 
function by using logical operations is a logical 
operation tree. Formally, a logical operation tree 
can be represented as a directed graph G(F,O,E), 
where F is a data enquiry function, 0={ 01,  02, ...
0,} is a set of nodes that corresponds to the set 
of logical operations that can be executed, and 
E={ Ell, El*, ... E,} is a set of edges with EG 
indicating the data flow from operation Oi to 
operation 0,. Operations at the bottom of the 
graph are roots, and a data enquiry function at 
the top of the graph is vertex. The graph is 
traversed along the branches from the roots to the 
vertex visiting each node only once, e.g. the 
operations are executed along the branches in 
sequential order. Execution of parallel branches 
is concurrent. Operation trees may be linear or 
bushy. Because of commutative and associative 
properties of some operations (for example 
joins), some bushy trees can be transformed to 
logically equivalent linear trees. 
Different algebraic representations of the 
given data enquiry function may result in widely 
different usage of resources, throughput and 
response times. 
Abstract representations of a data enquiry 
function are used in query evaluation and 
optimization methods ([3] and [4]). This is also 
the underlying theory in the metric of data 
enquiry functions and cost estimation methods 
further discussed in this article. 
3. Traditional processing complexity 
metric of data enquiry functions 
Implementation of e-commerce and e- 
government concepts requires fair data pricing 
based on some processing metric of data enquiry 
functions and cost estimation methods ([2] to 
[71). 
Typical processing complexity metric rely on: 
- A set of statistic data maintained on data 
tables and indexes by the system (number of 
rows, number of columns, number of distinct 
values in a column) 
- Formulas to calculate the size (volume) of 
the output data stream for every data enquiry 
function 
- Formulas to calculate the CPU and YO 
costs of query execution for every data 
enquiry function. 
Simple processing complexity value may be 
calculated as follows: 
n 
PC = W I  * NI + WE * C (Ai * Rri) + A0 * Ro (1) 
i=l 
where: 
W ,  WE, WO are calibration weights 
NI is number of input parameters (attributes) 
i is a number of input entities (tables) 
referenced 
AIj is a number of attributes (columns) of the i 
input table or array used in the operation 
RIj is a number of rows of the i input table or 
array used in the operation 
A.  is a number of attributes (columns) of the 
output table or array resulting from the operation 
Ro is a number of rows of the output table or 
array resulting from the operation. 
Different sets of weights may be needed for 
different types of data enquiry functions. To 
achieve long term stability of weights over the 
wide range of purposes, weight sets must be 
calibrated. 
However, calculating complexity is much 
more than counting inputs, outputs and 
operations. Therefore, processing complexity 
metrics are based on statistical properties of 
input data streams, processing cost calculations, 
and output data streams volume. 
4. Advanced complexity metric of data 
enquiry function 
This article introduces and proposes an 
advanced complexity metric of data enquiry 
function, which can be used in data pricing 
methods. Some ideas for this metric are taken 
over from the functional point analysis methods 
(U1). 
Proposed complexity metric depends on two 
- Processing complexity (PC) - that is the 
amount and comdexitv of information 
components: 
65 
processing to be performed by a system, e.g. a 
measure concerned with the system inputs, 
processing and outputs 
- Technical complexity (TC) - that is a 
measure concerned with the technical 
requirements of the data enquiry function, 
such as data communications, distribution of 
data etc. 
PC score gives an unadjusted processing 
complexity measure of the data enquiry function. 
To calculate the Complexity of data enquiry 
function (FC), the Processing based complexity 
(PC) must be adjusted by the Technical 
complexity of the data enquiry function (TC). 
4.1. Operation complexity (OC) and 
processing complexity (PC) 
Operation complexity (OC) of every 
operation in an operation tree is calculated by 
summing its input, processing, and output 
complexity, e.g.: 
OC = f (Input, Processing, Output) (2) 
Input = f (A~i,Rri, A12,Rr2 ,....., AIn,Rrn) (3) 
Output = f ( AOI, Rol, A02, Ro~,..   ., Aonr, Ron,) (4)  
Processing = f (WP, NP,) ( 5 )  
where: 
Wp is a calibration weight of the operation 
Np is a number of parameters used in the 
operation 
AIi is a number of attributes (columns) of the i 
input table or array used in the operation 
RIj is a number of rows of the i input table or 
array used in the operation 
A@ is a number of attributes (columns) of the 
j output table or array resulting from the 
operation 
Roj is a number of rows of t h e j  output table 
or array resulting from the operation. 
Finally, processing based complexity (PC) of 
some data enquiry function is calculated by 
summing operation complexities of all 
comprised operations: 
n 
P C = C O C  
i=l 
66 
where: 
i is the number of logical operations in the 
data enquiry function. 
Counting a number of columns and rows of 
tables involved in some operation is a complex 
undertaking. This approach is similar to the well- 
known query optimizers in relational systems. 
Therefore, the proposed method introduces one 
simplification. It takes into account statistical 
values and relies on: 
- A count of set of statistic maintained 
number of rows and number of columns 
in data tables (for example SQL statement 
ANALYZE validates the structure of an 
index, table or cluster and collects 
performance statistics) 
- Formulas to calculate the size of the 
output data stream for every operation 
node, based on number of parameters 
used in the operation and on selectivity of 
predicates used in every operation node 
Formulas to calculate the CPU and YO 
costs of query execution for every 
operation, based on these statistical 
properties and specific calibration weight 
of operation. 
- 
4.2. Technical complexity (TC) 
In the proposed method, the technical 
complexity of the data enquiry function (TC) is 
calculated by estimating and scoring technical 
characteristics of the data enquiry function which 
represent the influence of the following TC 
factors: 
- Type of processing 
- Distribution of processing 
- Performance 
- Hardware configuration 
- Reusability 
- Operational ease 
- Data distribution 
- Audit, control and security. 
For some specific system architecture, every 
TC factor i may contribute to the overall 
technical complexity of the- data enquiry 
functions with the maximum theoretical weight 
WTcTj. These maximum theoretical weights must 
be estimated and calibrated first. This task is a 
part of the calibration of metric. 
Calibration of maximum theoretical weights 
means scoring eight general TC factors 
according to their relative influence represented 
by TC features, as shown in the following table: 
TC factor 
1. Type of 
processing 
[ W T C l )  
2. Distribution 
of processing 
( w T C 2 )  
3. Performan- 
ce ( w T C 3 )  
4. Hardware 
configuration 
(Wra)  
5. Reusability 
( w T C 5 )  
TC feature 
Screen display, report, or a 
data set in digital form 
preceded by interactive input 
of parameters and pure batch 
processing 
Screen display, report, or a 
data set in digital form 
preceded by interactive input 
of parameters and interactive 
processing 
Dynamically interactive input 
of parameters, processing 
and output 
Data result set is processed 
and stored or displayed on a 
single component of the 
system 
Data result set is prepared 
for processing on another 
component of the system 
and is transferred to another 
component of the system 
As above with feedback to 
the initiatina svstem 
Response time or throughput 
are standard 
Special response time or 
throughput are stated 
Standard production 
hardware configuration is 
used 
Data enquiry function 
requires additional devices 
(back-up device, plotter etc.) 
to the standard production 
hardware configuration 
Data enquiry function 
requires reconfiguration of 
the standard production 
hardware configuration 
Data enquiry function uses 
standard modules installed 
on the standard production 
hardware configuration 
Data enquiry function 
requires archived modules 
which must be installed on 
the standard production 
hardware configuration 
67 
security 
:wTC8) 
TC factor TC feature 
requires modules which 
must be installed from 
another hardware 
control processing 
Data enquiry function 
configuration 
Data enquiry function must 
Max. 
Min. 
-Ave. 
be prepared by using of 
standard query tool (SQL, 
QBE\ 
12,O 13,O 14,O 7,OO 8,OO 12.0 5,OO 8,OO 
l 0 ,O  11.0 9,oo 6,OO 6,OO 10,o 4,OO 7,OO 
11,3 11,6 11,3 6,33 6,67 11,O 4,33 7,33 
Data enquiry function must 
be prepared by using of 
standard reusable 
components 
Data enquiry function must 
be designed, developed, 
tested and documented 
No special operational 
considerations are stated by 
the user 
Manual operator’s activities 
I such as tape mounts, paper 
handling are required’ . 
Specific backup and data 
recovery processes are 
required 
Specific data preparation and 
data engineering processes 
1 are required 
I Data enquiry function uses 7. Data 
jistribution 
: WTC7) 
3. Audit, 
:ontrol and 
one database instance 
Data enquiry function uses 
more database instance at a 
single location 
Data enquiry function uses 
distributed databases 
Data enquiry function 
rewires standard audit and 
The above table of TC factors and features 
may be extended but only appropriate 
characteristics should be added which arise form 
specific requirements. 
The actual TC of the data enquiry function is 
calculated by first estimating and scoring actual 
weights of TC factors W T C ~  and then by applying 
the following formula: 
8 c WTCAi 
(7 )  
Fig. 1.  shows experimental results of one 
calibration of maximum theoretical weights for 
the specified case. This calibration has been 
performed by five programmers. The figure 
shows significant dissipation of WTcT values. 
This dissipation can be reduced by discarding 
extreme minimum and maximum values of each 
TC factor, as shown on Fig. 2. 
25,OO 
20,oo 
15,OO 
10,oo 
5,OO 
0 00 
Figure 1. Calibration of maximum theoretical 
weights WTCT 
15,OO 
10,oo 
500 
0,oo 
Figure 2. Results of calibration - extreme 
values are discarded 
4.3. Complexity of data enquiry function 
(FC) 
PC score gives an unadjusted processing 
complexity measure of the data enquiry function. 
To calculate the Complexity of data enquiry 
jiinction (FMC), the Processing complexity (PC) 
must be adjusted by the Technical complexity of 
the data enquiry function (TCP). Generally, 
complexity of data enquiry function is function of 
processing complexity, and technical complexity: 
Depending on purpose .of metric, different 
functional dependencies may be applied. At the 
current stage of the metric development project 
and based on the functional point analysis 
methods works ([ l]), the linear functional 
dependency is used: 
FC = PC * (1 + k * TC) (9) 
Depending on purpose of metric, different k 
values may be used. As a part of a metric 
development process, an Excel-based application 
has been developed. The application sets out 
templates 'in such a way that an average, 
spreadsheet literate person may follow the logic. 
This application has first been used for 
calibration of metric. 
The calibration entails determining all 
weights and k value for some specific problems. 
Current experimental results show that proposed 
metric and application developed upon may 
improve metric processes of data enquiry 
functions in production systems. The further 
operational usage of this application requires 
interface to the production system, i.e. to process 
and resource monitoring. The neatest way to 
interface is with an ODBC driver (Open Data 
Base Connectivity) or via an ASCII file. A 
possible enhancement of this application will be 
an automated collection of statistic data and 
creating the database log file by using DBMS 
features for auditing of specific SQL statements 
and types of objects. 
5. Conclusion 
Calculating complexity is much more than 
counting inputs, outputs and operations. This 
article has proposed an advanced complexity 
metric of data enquiry function that can be used 
68 
in prediction of system performance and in 
pricing methods. Proposed complexity metric 
depends on two components: processing 
complexity (PC) and technical complexity (TC). 
The processing complexity is the amount and 
complexity of information processing to be 
performed by a system, e.g. a measure concerned 
with the system inputs, processing and outputs. 
The technical complexity is a measure concerned 
with the technical requirements of the data 
enquiry function, such as data communications, 
distribution of data etc. 
The advantage of proposed method over 
standard methods used in pricing, such as counts 
of typical outputs, is that it can be applied on 
complex queries resulting in big data sets. The 
aim of the proposed method is also to calculate 
the effort required to create and disseminate 
required data output in complex and distributed 
systems. 
As a part of a metric development process, an 
Excel-based application has been developed. 
This application has first been used for 
calibration of metric. 
6. References 
[ 11 Drummond I. Estimating with M k  11 Function 
Point Analysis. CCTA, Crown; 1992, ISBN 
0113305788 
21 Franklin MJ, Jonsson BT, Kossmann D. 
Performance tradeoffs for client-server query 
processing. In Proc. of the ACM SIGMOD 
Conf. on Management of Data; pages 149- 
160; Montreal: Canada; June 1996. 
31 Graefe G. Query evaluation Techniques for 
Large databases. IACM Computing surveys; 
Vol. 25, No. 2, 1993. 
[4] Ioannidis Y. Query Optimization. ACM 
Computing Surveys; 28(1), 121-123, 1996. 
[5] Sistla P, Wolfson 0, Yesha J, Sloan R. 
Towards a Theory of Cost Management for 
Digital Libraries. ACM Transactions on 
Database Systems (TODS); Vol. 23/3, 1998. 
[6] Stillger M, Obermaier JK, Freytag JC. 
AQuES An Agent-based Query Evaluation 
System. Cooperative Information Systems, 
(CoopIS'97); Kiawah Island, South Carolina, 
USA; June 24-27,1997. 
[7] Yesha J, Adam N. Electronic commerce: An 
overview, In Nabil Adam and Yelena Yesha, 
editors, Electronic Commerce. Lecture Notes 
in Computer Science, Springer-Verlag, 1996. 

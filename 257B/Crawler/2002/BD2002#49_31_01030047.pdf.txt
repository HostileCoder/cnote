FEATURE SELECTION FOR OFF-LINE 
RECOGNITION OF DIFFERENT SIZE 
SIGNATURES 
George D. da C. Cavalcanti, Rodrigo C. D6ria and Edson C. de B. C. Filho 
Universidade Federal de Pernambuco ~ Centro de Informatica 
AV. Professor Luis Freire sin, Cidade Universitiria, 50740-540 
Recife-PE, Brasil 
Phone: +55 81 3271 8430 
Fax: 1 5 5  81 3271 8438 
Email: {gdcc, rcd, ecdbcf}@cin.ufpe.br 
Web: www.cin.ufpe.br 
Abstract .  The aim of this work is to select a se t  of features, which 
have good performance to solve the problem of signature recogni- 
tion of different sizes. T h e  signature database was formed for th ree  
sizes of signatures per user, small, median and big. This  s tudy  
used s t ructural  features, pseudo-dynamic features and Eve moment  
groups. T h e  feature selection method chosen was the one that se- 
lect the best individual features based on classifiers like bayes and 
k-NN. 
I N T R O D U C T I O N  
The handwriting recognition is a complex problem. Signatures form a spe- 
cial class of handwriting in which legible letters or words may not be exhib- 
ited [U].  So many efforts have been made to develop computational systems 
that are able to recognize and to verify signatures [Z, 7, 31. 
In signature verification, the system aims to authenticate a user signature, 
that is, to determine if a user is who it claims to be. It is a simpler problem 
than recognize signatures, in which the system analyses the entire database 
searching users who have similar signature to the one given as input, and 
offers a list of likely users in descending order of similarity. 
Signature classification systems can be split in two groups: on-line and off- 
line. In the first one, signatures are obtained though an electronic device in 
which the user signs. Under these conditions, dynamic information like speed 
of writing and pressure can be extracted. In off-line systems, the signatures 
are captured from documents using a scanner or a video camera. In these 
cases dynamic information w not available. Decision in off-line system is more 
difficult because the amount of information is smaller than in on-line system. 
The aim of this work is to select a set of features, which have good perfor- 
mance to solve the problem of signature recognition of different sizes. There- 
fore the signature database is formed for three sizes of signatures per user, 
small, median and big, like show in Figure 1. Structural features, pseudo- 
dynamic features and five moment groups were used. The feature selection 
method chosen was the one that select the best individual features based on 
classifiers like hayes and k-NN. 
This paper is divided into the following sections. In Section 2, signature 
features extraction is discussed. The experiments are presented in Section 4. 
Finally, conclusions of the study are noted in Section 5. 
Figure 1: Examples of small, median and big signatures, 
FEATURE EXTRACTION 
For each signature was extracted a set of 37 features that belongs to three 
distinct techniques: structural, invariant and pseudo-dynamic. 
Four structural features were used ratio, Positive Inclination (Inclination 
P),  Negative Inclination (Inclination N) and Vertical Inclination (Inclination 
V), that is, without inclination. Signature inclination can be calculated by 
the indination of each point individually. 
Moment sets were used. Hu [9] devived seven moment invariants that 
are invariant under translation, rotation and scaling of the object. Six affine 
moments (Flusser) [5] that are invariant under general &ne transformations. 
Five Maitra moments [ll], that extend Hu moments to  be invariant under 
image contrast. Three Simon moments [lo] and seven central moments. 
The pseudo-dynamic features were used by Ammar [l] to  detect skilled 
forgeries. Using the Ammar features, it  is possible to recover pressure in- 
356 
Figure 2: Threshold of High Pressure 
formation from the image of the signature. This technique is based on High 
Pressure Regions (HPR) that is given by Equation 1. In this equation, I is 
the original image, I h p  is the HPR image and THP is the Threshold of High 
Pressure. The THP value is defined using the image histogram, like show 
in Figure 2. The THP will be the point 0,7 in the normalized accumulated 
histogram of the image. 
255, if I ( i j )  2 THP 
IhP = [ 0, otherwise 
The features are: Threshold of High Pressure (THP), maximum pressure, 
min-max, high-pressure area and pressure factor. 
Maximum pressure corresponds to the smallest gray level found. The min- 
max feature is obtained subtracting the smallest gray level from the biggest 
one. High-pressure area is the number of black pixels in HPR image. Finally, 
the pressure factor corresponds to the ratio between Ihp and I .  
FEATURE SELECTION 
The aim of the feature selection is to find a subset of a set of feature, 
which presents better performance than the whole set, based on some clas- 
sifier. In [8], Jain presents some techniques to perform feature selection, 
besides showing some obstacles to execute feature selection on a small set of 
data (common factin signature classification problem). The ideal technique 
showed, it is the one which tests all the subsets, but this method will create 
a huge number of combinations increasing the computational time. 
A simple technique to perform feature selection is to join the best indi- 
vidual features based on some classifier. This method does not guarantee the 
best set of feature, but i t  can be a good way to select some initial features. 
In this paper two classifiers for that objective are used: hayes and k-NN. 
357 
Bayes classifier 
The Equation 2 represents the Bayes theorem. Through this equation it is 
possible to estimate the posteriori probability using the priori probability and 
the class-conditional probability. 
The Bay- decision rule can be simplified by the Maximum a Posteriori 
Rule (MAP), which puts the input pattern z to the dass C; if 
P(C,IX) > P(CjjX) for all a # j (3) 
It is common to use the classification rule based on the Normal distribu- 
tion or Gaussian, because it needs only two parameters: mean and covariance 
matrices, like showed in Equation 4. 
(4) 
1 1 
2 
v k ( Z )  = --(Z - /Ak)TC,1(2 - /Lk)  - 2lnlCkl + lnP(Ck) 
The classification rule expressed by Equation 4 represents a quadratic 
discriminant, and i t  assigns a input vector z for the class C, if y;(z) > yj(z)  
for all i # j .  
An important case occurs when the normal distributions have the same 
covariance matrix, the Bayes discrimination function is linear, S, = S. The 
linear discriminant is show in Equation 5.  
V k ( Z )  = w:z+'JkO 
where 
w: = & - I  
who = --I/L:C-l/Ak 2 + hP(Ck)  ( 5 )  
To suppose that the covariance matrices are identical, when a small train- 
ing set is used, can improve the results [SI. 
k-Nearest  Neighbor 
When a k-NN classifier is used, i t  is not needed to calculate the probability 
distribution of the classes. For that reason, it doesn't have the training phase. 
The k-Nearest Neighbor classification rule is as follow: given the training set 
from n classes, it assigns an unclassified pattern z to the class that is most 
represented among its k nearest neighbors based on a similarity measure. 
Normally, two distances are used: euclidian and Mahalanobis. Like show 
in Equations 6 and 7, respectively. 
(6 )  2 d: = (%I - U d 2  + (22 - Y2)2 + ' ' . + (2" - Y,.) 
358 
d& = (z - pL.)'C-'(x - pz) (7) 
x = [XI, 2 2 , .  . . , z,] and y = [y, , y2, . . . , yn]. p represents the average and 
C the covariance matrix. Through the covariance matrix, the Mahalanohis 
distance extracts class dispersion information. But, the Euclidian distance is 
computationally cheaper. 
EXPERIMENTS 
The experiments aim to select a set of features based on the individual clas- 
sification power of each feature. Two classifiers were used: bayes and k-NN. 
Signature Database 
To capture the signatures, a form holding twenty was used. These spaces are 
not of the same size, they had been divided in the following way: four small 
spaces, twelve median spaces and four big spaces. The areas of each space 
are, respectively, 7,Ocm x l,Ocm, 8,Ocm x 2,Ocm and 9,5cm x 4,Ocm. Each 
one of the twenty volunteers signed in the 20 spaces of the form. Figure 1 
shows examples of signatures small (S), median (M) and big (B). 
The data set was divided in the following way: eight median signatures 
for class were used for training, the remainder, four median, four small e four 
big, were used for test, A problem inherent to signature recognition is a small 
amount of pattern to train and to test, because it is difficult in commercial 
applications to request for a user to  sign many times. 
Feature Selection wi th  k-NN 
The first experiment used the 1-NN classifier with the Mahalanobis distance 
to evaluate the power of each one of the features. It is possible to  observe 
in Figure 3 the features that obtained the best results, they are: Inclination 
N, Inclination P, Simon 2, Central 3, Hu 1, Ratio, Central 2, Maitra-l?and 
Inclination V. While the features Max Pressure, Hu 5 and Flusser 4 obtained 
the worse results. The accuracy rate showed in Figure 3 corresponds to the 
arithmetic average of small, median and big signatures accuracy rate. 
Through the analysis of the results of this experiment it w a  possible to  
notice that some features obtained good results when applied to a specific 
class'. These results are congregated in Table 1. Also, in this table, it is 
possible to see that some features didn't discriminate well any class, they 
are: 2, 3, 13, 17, 20, 21, 23, 24, 25, 26, 27, 30, 34 and 3@. Moreover, the 
accuracy rate of the median size signatures was superior in 60% of the cases, 
compared to the accuracy rate of the small and big signatures. This confirms 
'For good result understands that the feature classify correctly, at least, 60% of the 
C l a s  samples. 
*In Figura 3 the features are indexed. 
359 
Figure 3: Feature evaluation using 1-hW 
the results found in [4], which express that when an user signs in spaces of 
different sizes, he modifies the scaling of his signature in a nonlinear way. - - 
Class I Feature 
1 6, 8, 19, 35, 37 
8 I 6 
Q 4 ' 4 " i  
9, 10, 11, 12, 14, 15, 28, 29, 32, 37 
TABLE 1: FEATURES THAT HAVE GOOD RESULTS FOR ISOLATED CLASSES USING 
1." CLASSIFIER. 
Considering that the signatures of each class are distributed in three sizes: 
small, median and big; a normalization procedure was thought. It consists 
of the following steps (Figure 4): 
1. Calculate the average size of all the median she signatures; 
2. Normalize all signatures, S, M and B, of all classes for this size 
After this normalization, all the 36 features3 were extracted for each one 
of the signatures. These features were used for another experiment. The 
results obtained with 1-NN classifier, using these normalized features; were 
inferior to 5% of correct classification per feature. For this reason the results 
of this experiment were omitted. However its important to point out that, 
'In view of that all the BignatitUreB were normalized for a specific dimension the ratio 
feature w.s removed. 
3-54 
using the normalization technique, all the features discriminated well some 
class. Moreover, only three classes didn't obtain satisfactory results with any 
feature, they are: 6, 11 and 15. 
L+,J 
Figure 4: Normalization processes. 
Different values for k, such as 3, 5 and 7, were tested. But the results 
weren't good. Sometimes, when increasing k, a little gain was observed, but 
it didn't justify the performance loss. For this reason, only the results with 
the 1-NN were presented. 
Feature Selection wi th  bayes classifier 
Using the bayes classifier three experiments were carried out. Motivated by 
its analytical tracyability, all the experiments used a normal distribution. In 
particular, it was assumed that the covariance matrices for all of the classes 
are identical. Hence the bayes discriminant become8 linear. The first experi- 
ment used the bayes classifier without normalization. The second experiment 
carried the image signature normalization as showed in Section 4.2. The last 
experiment is based on the following argument: the moment feature values 
represent numbers very big or very small, thus it was decided to make a 
normalization of the features based on its standard deviation. 
The performance evaluation results of the features using a bayes classifier 
without normalization can he observed in Figure 5 .  These results show that 
some features revealed interesting results when used separately to solve the 
problem of signature recognition. Among them its possible to emphasize 
the following ones: Inclination P, Inclination N, Maitra 1, Central 7, Hu 
2, Central 3, Hu 1 and Simon 2. The features High Pressure Threshold, 
Maximum Pressure, Min-Max and Pressure Factor obtained the worse results. 
Table 2 shows the features that were better adjusted to some particular 
class. Moreover, it is also possible to observe that some feature didn't dis- 
criminate well any class, they are: 2, 3, 14, 18, 20, 21, 24, 26, 29, 30, 34 and 
35. 
The bayes classifier results for normalized images by size and for normal- 
ized features by its standard deviation didn't present satisfactory results. For 
this reason, they weren't showed here. The results, using normalization, pre- 
sented equal problems to the same experiment done using 1-NN classifier and 
361 
Fa5u" 
Figure 5: Feature evaluation using bayesian decision rule. 
plus ones: the number of classes uncovered was bigger and the accuracy rate 
of each feature individually was very low. However, after the normalization, 
it was possible to  observe the same process occurred with the k-NN classifier, 
the correct classification rate of big and small signatures approximates the 
correct classification rate of the median size signatures. This demonstrates 
that the normalization would be becoming signatures invariants on nonlinear 
scsling. 
It was also tested the quadratic discriminant4. But, as related in [E ] ,  for 
a $mall set of sample better results are found when it assumes that all the 
classes have a common covariance matrix. 
The intersection of the hest features showed in Figure 3 with the best 
ones in Figure S'makes a set of features that may be interesting to solve 
the problem of signature recognition, they are: Inclination N, Indination P, 
Central 3, Hu 1, Maitra 1 and Simon 2. 
It is well done that the best individual feature set may not be the best set 
of feature. But based on the features performance, the next step is to join 
them and test if they are good to  solve the problem of signature recognition. 
CONCLUSIONS 
The bayes classifier presented better individual results then k-NN, that is, 
in the average, its results were better. Its important to  point out that in- 
dependently of the classifier a set of features obtained prominence, they are: 
Inclination N, Indination P, Central 3, Hu 1, Maitra 1 and Simon 2. 
The correct classification rate showed in Figures 3 and 5 was calculated 
by the accuracy rate average of signatures S, M and B. The signatures that 
had the biggest accurate rate were the median sue, around 60%5. This 
report that when a person is signing in spaces of different sizes, he varies his 
4When each rl- po88e89 its pmper ewariance matrix. 
5Sire used for training. 
362 
Class I Feature I 
TABLE 2: FEATURES THAT HAVE GOOD RESULTS FOR ISOLATED CLASSES USING 
COMMON COVARIANCE MATRlZ WITH BAYES CLASSIFIER. 
signature in a nonlinear form [4]. 
The problem of the biggest accuracy rate for signatures of median size 
was minimized using the signature image normalization technique, described 
in this paper. Without normalization few class were well discriminated by 
many features. After normalization the power of the features was distributed, 
that is, more classes were well discriminated by few features. It suggests a 
classitier that uses sets of independent features for class. 
Some features didn’t reach relevant results in any test, they are: Maxi- 
mum pressure, Min-Max, Maitra 5 ,  Flusser 1, Flusser 4, Flusser 6, Central 1 
and Central 5. These feature are the ones which have the smallest variance. 
Although, these features had presented problems when treated separately, it  
is possible that in a set of features, they may supply some lack of this set. 
Test other techniques of feature selection, investigate the power of other 
moment groups, such as: Zernick, Fourier-Merlin, Taubin and Legrand, and 
prove the effectiveness of the selected set of features to recognize signatures 
are still being development. 
REFERENCES 
[I] M. Ammar, Y. Yashida and T. Fukumura, “A New Effective Approach for 
Off-line Verification of Signatures by Using Pressure Features,” 8th ICPR, 
pp. 566-569, 1986. 
(21 R. Bajaj and S. Chaudhury, “Signature Verification Using Multiple Neural 
Classifiers,” Pattern Recognition, vol. 30, no. 1, pp. 1-7, 1997. 
(31 H. Baltzakis and N. Papamarkos, “A new signature verification technique 
based on a +*stage neural network classifier,” Engineering Applications 
of Artiflcial Intelligence, vol. 14, pp. 95-103, 2001. 
141 R. C. Doria, E. C. E. C. Filho and E. F. A. Silva, “How distortions in different 
size signatures inthence moment based techniques,” Conference of PRIP, 
vol. 1, pp. 219-226, 2001. 
363 
[5] J. Flusser and T. Suk, "Pattern recognition by affine moment invariants," 
Pattern Recognition, vol. 26, pp. 167-174, 1993. 
[6] J. P. Hoffbeck and D. A. Landgrebe, "Covariance Matrix Estimation and 
Classification With Limited Training Data," IEEE Transaction on Pattern 
Analysis and  Machine Intelligence, vol. 18, no. 7, 1996. 
[7] K. Huang and H. Yan, "Off-line signature verification based on geometric 
feature extraction and neural network classification," Pattern Recognition, 
vol. 30, no. 1, pp. 9-17, 1997. 
[SI A. Jain and D. Zangker, "Feature Selection: Evaluation, Application and 
Small Sample Performance," IEEE Transaction on Pattern Analysis and 
Machine Intelligence, vol. 19, no. 2, pp. 153-158, 1997. 
[9] Y. Li, "Reforming the Theory of Invariant Moments for Pattern Recognition," 
Pattern Recognition, vol. 25, no. 7, pp. 723-730, 1992. 
[lO].X. S. Liao and Q. Lu, "A Study of Moment Functions and Its Use in Chinese 
Character Recognition," Fourth ICDAR, pp. 572-575, 1997. 
[ll] S. Maitra, "Moment Invariants," Proceedings of the IEEE, vol. 67, no. 4, 
pp. 697499, 1979. 
1121 V. E. Ramesh and M. N. Murty, "Off-line signnature verification using genet- . .  .. 
i d l y  optimized weighted feat&," Pattern Recognition, vol. 32, no. 2, 
pp. 217-233, 1999. 
364 

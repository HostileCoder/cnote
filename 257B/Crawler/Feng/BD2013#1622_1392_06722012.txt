The Incremental Risk Functional:
 Basics of a Novel Incremental Learning Approach
 Andreas Buschermo¬hle, Jan Schoenke, Nils Rosemann, Werner Brockmann
 Smart Embedded Systems Group
 University of Osnabru¬ck
 Osnabru¬ck, Germany
 Andreas.Buschermoehle@uos.de, jschoenk@uos.de, Nils.Rosemann@uos.de, Werner.Brockmann@uos.de
 AbstractÑIncremental learning gets increasing attention in
 research and practice as it has the advantages of continuous
 adaptation and handling big data with a low computation and
 memory demand at the same time. Several approaches have
 been proposed recently for online learning, but only few work
 has been done to regard the influence of the approximation
 structure. Hence, we introduce the incremental risk functional
 which directly incorporates knowledge about the approximation
 structure into its parameter update. Exemplary, we apply this
 approach to regression estimation through linear-in-parameter
 approximators. We show that the resulting learning algorithm
 converges and changes the global functional behavior only as
 little as necessary with every learning step, thus resulting in a
 stable incremental learning approach.
 I. INTRODUCTION
 Machine learning is used in many complex tasks to auto-
 matically acquire details about a system of interest based on
 data from that system. A special case is the online learning
 task, which means that learning stimuli, i.e., the individual data
 points, are obtained sequentially and the performance of the
 function approximator is evaluated after each learning stimu-
 lus. Hence, classical batch learning is not applicable. Online
 learning is, on the one hand, important for handling big data
 which cannot be processed at once. On the other hand, it can be
 used for online generation of a system or environmental model
 as well as for online time series prediction or adaptive control.
 In these cases, a big or continuously growing batch of data
 is available. But often timeliness and computational costs are
 crucial demands. Hence it is not possible to build a hypothesis
 from all acquired data. Instead, it becomes necessary to revise
 the current hypothesis incrementally depending only on the
 most recent learning stimuli or in the extreme case a single
 stimulus.
 Besides the lower computing demand, online learning
 allows for fast and continuous adaptation to nonstationary
 systems. Consequently, an online learning algorithm is more
 widely applicable than batch learning, as it serves as well for
 an online scenario as for offline learning of data sets which
 are too large for batch learning.
 Formally, the online learning task is characterized by learn-
 ing on a sequence of data which can be described in rounds.
 In round t the learning algorithm is presented an instance
 xt ? Rd which is transferred from input space to parameter
 space by the approximation structure through a vector of basis
 functions ?(xt) ? Rn. This input is then used to predict its
 label yöt ? R through an approximator yöt = f(?(xt), ?t) with
 the parameter vector ?t, which is the hypothesis maintained
 by the learning algorithm. Afterwards, the correct label yt
 is given and the learning algorithm suffers an instantaneous
 loss L(yöt, yt) ³ 0 reflecting how wrong the prediction was.
 With the new pair of an instance and its corresponding label,
 henceforth called an example (xt, yt), the learning algorithm
 updates its hypothesis to ?t+1 with the aim to minimize the
 cumulative loss
 Lc =
 ND·
 t=0
 L(yöt, yt) (1)
 where ND is the length of the data sequence provided.
 But usually, for each learning step many of such revisions
 with zero error on the new example are possible. Hence, the
 problem of finding a hypothesis that accounts for the new
 information is potentially under-determined. So other con-
 straints are needed to define a unique solution. Furthermore,
 the resulting hypothesis of online learning depends not only on
 the data but also on their sequence of presentation, in contrast
 to batch learning. Here the dilemma arises to integrate a single
 example such that prior learned knowledge is only affected
 where appropriate.
 II. RELATED WORK
 In the batch-case, the goal is a minimal risk of loss, which
 means to find the hypothesis that minimizes the risk functional
 [11]
 R(?) =
 º
 L(y, f(?(x), ?)) dF (x, y) . (2)
 It measures the risk of any chosen hypothesis ? to incur
 some loss, given that the input and respective output values
 are distributed according to the joined probability distribu-
 tion F (x, y). But this distribution is usually unknown for a
 learning problem and hence only some value-pairs of (xi, yi)
 from training data are available. According to [11], the risk
 functional can be approximated by
 Rb(?) = 1t
 t·
 i=1
 L(yi, f(?(xi), ?)) , (3)
 for batch learning with a sufficiently large set, i.e., t ?° in
 the ideal case, of identically, independently drawn data. This
 method can be applied to an online scenario with an ever
 2013 IEEE International Conference on Systems, Man, and Cybernetics
 978-1-4799-0652-9/13 $31.00 © 2013 IEEE
 DOI 
1500
 DOI 10.1109/SMC.2013.259
increasing data set but the computational demand increases
 also with every new example.
 For an online setup the only available information is the
 most recent example (xt, yt) at a time step t, which fixes
 the memory and computation demand at design time to the
 minimum. As the minimization of the resulting loss generally
 is under-determined, a unique solution can only be found if
 a regularization-term is added. A widely applied technique
 is some form of Tikhonov regularization which incorporates
 the norm ???, e.g., the Euclidean norm, of the parameter
 vector or a norm ?f(?(x), ?)? of the function to be minimized
 as well (see e.g., [4], [8]). The impact of regularization is
 parametrized by a weighting factor. This way the resulting
 functional behavior can be shown to be as flat as possible, thus
 preventing over-fitting [9]. But as this regularization does not
 include any knowledge about prior learning data, it is usually
 only applicable for learning with a set of training data and not
 for online learning.
 Based on the classical perceptron algorithm [10], a unique
 solution for a parameter update is to minimize the loss along
 its gradient through a gradient descent (GD) on an error
 potential that is given by the example [13], also known as
 Passive-Agressive [1]. This is analogous to a regularization
 term penalizing the amount of change ?Æ?? of the parameter
 vector and is parameterizable as well through the step size
 of the GD, also referred to as the learning rate. The learning
 rate can be chosen in every learning step so as to directly
 minimize the loss, which is called normalized gradient descent.
 This learning algorithm allows for a continuous and flexible
 adaptation but is prone to noise.
 Besides these first order algorithms, in the approach of
 second order learning the way an example is used to update
 the parameter vector is changed through previous data as well.
 This way a higher robustness to noise is achieved at the cost
 of less flexibility to adapt to changes in the data. In the idea
 of the Relaxed Online Maximum Margin Algorithm [7] and
 the Ellipsoid Method [12] a set of possible parameter vectors
 is kept that is consistent with prior examples as well as the
 current example. This idea is further extended to a Gaussian
 distribution of parameter vectors in the work of Confidence
 Weighted Learning [5], Adaptive Regularization Of Weights
 [2] and Gaussian Herding [3]. These approaches are similar
 to the well known recursive least squares (RLS) [6]. Here, an
 estimation of the covariance matrix W of the parameter vector
 ? is incrementally updated together with a forgetting factor and
 used to increase or decrease the adaptation of single parameters
 in a learning step. The covariance matrix and the parameter
 vector together define a Gaussian distribution of parameter
 vectors. If the covariance matrix is the identity W = I , RLS
 and GD yield the same result for learning on one example.
 Hence, GD as a flexible first order and RLS as a noise-
 robust second order algorithm represent the state of the art
 of online learning algorithms. Yet, interpreting equation (3) as
 the sum over all examples in a sequence up to a time step t,
 online learning directly regards the last element of the sum.
 The remaining N ? 1 examples are only indirectly available
 through the current parameter vector ?t. All mentioned meth-
 ods incorporate this parameter vector into the optimization
 task, but none of them with respect to the actual functional
 behavior of the approximator f(?(x), ?t), i.e. the influence of
 the approximation structure ?(x). Thus the presented learning
 approaches do not minimize the risk in every learning step
 as the amount of influence of a parameter is not regarded in
 the additional constraints. The aim of this paper is hence to
 define a learning paradigm based on the optimization goal in
 an online setup with respect to the functional behavior, hence
 the incremental risk functional.
 III. APPROACH TO ONLINE LEARNING
 A. Incremental Risk Functional
 In online learning, the knowledge of prior examples
 {1, . . . , t ? 1} is condensed into the functional behavior
 f(?(x), ?t) of the current hypothesis. Accordingly, the global
 functional behavior should change as little as possible in order
 to not destroy prior information. At the same time, the example
 should be incorporated as good as possible which might be a
 conflicting demand.
 Following this idea, we propose an incremental formulation
 of the risk functional containing the most recent example and,
 similar to a regularization, the overall change of the output,
 with a factor ?t > 0
 Rinc(?) = ?t2 á
 º
 ?
 L(f(?(x), ?t), f(?(x), ?))dx
 +
 1
 2
 L(yt, f(?(xt), ?)) . (4)
 A new parameter vector is chosen according to the mini-
 mization of Rinc(?). As the probability density F (x, y) is
 unknown, the change of the functional behavior is accounted
 for by an equally weighted integral for the input x over the
 considered bounded input space ?.
 The factor ?t can be seen as to steer the stiffness of the
 approximator. The bigger its value is, the more a change of
 the approximator is punished. If a lot of data is condensed in
 the learning process and hence the contribution of the current
 example is comparably small, a big value of ?t accounts for
 this evidence. In contrast, in the initial learning phase with no
 or only low data background, this weighing factor should be
 low, thus putting more weight on the present example. Con-
 sequently it should be chosen as a monotonically increasing
 value (?t ³ ?t?1) as the learning process progresses.
 B. Application to LIP Approximators
 To investigate the properties of the incremental risk func-
 tional, we consider the case of regression estimation for Linear-
 In-Parameter (LIP) approximators. The problem of regression
 estimation can be expressed by minimizing the risk functional
 with squared loss L(a, b) = (a? b)2 [11]. A general N-node
 LIP-approximator is given by
 f(?(x), ?) =
 N·
 i=1
 ?i?i(x) = ?T ?(x) (5)
 defining a linear subspace of functions from Rd to R1 with
 the basis {?i(x)}ni=1 [6]. In this case, the parameter vector ?t
 is updated incrementally with respect to the minimization of
 Rinc(?) = ?t2 á
 º
 ?
 ((?t ? ?)T ?(x))2dx
 +
 1
 2
 (yt ? ?T ?(xt))2 . (6)
 1501
As Rinc(?) ³ 0 ? ?t > 0, if a unique solution of a zero
 partial derivative exists, it is the minimum of the incremental
 risk functional. For LIP-approximators the resulting equation
 ¶Rinc
 ¶?i
 = 0 ? i ? [1;N ] (7)
 has the form of a linear system of equations which can be
 written as
 (A+ 1
 ?t
 B(xt))?t+1 = A?t + 1
 ?t
 ?(xt)yt (8)
 with symmetric matrices
 (A)i,j =
 º
 ?
 ?i(x)?j(x)dx and (B(xt))i,j = ?i(xt)?j(xt) .
 With the basis {?i(x)}ni=1 in the function space L2(?), A is
 the Gramian matrix given by the standard inner product on
 functions. Hence the matrix A is positive definite for linearly
 independent ?i(x) and has an inverse A?1. Choosing the
 substitution u = v = 1Ã
 ?t
 ?(xt), the second part of (8) can be
 expressed as 1
 ?t
 B(xt) = uvT and thus the ShermanÐMorrison
 formula yields the entire inverse
 (A+ uvT )?1 = A?1 ? A
 ?1B(xt)A?1
 ?t + ?(xt)T A?1?(xt) . (9)
 So the minimization has a unique solution and as the matrix A
 is constant for any given approximation structure and can be
 inverted offline, equation (9) poses a numerically cheap way
 to compute the inverse for a new example [9].
 C. Local Convergence
 If the parameters ?t are not changed, the incremental risk
 functional has the value
 Rinc(?t+1 = ?t) = (yt ? ?Tt ?(xt))2 , (10)
 which is equivalent to the local error of the approximator. The
 partial derivative for this case
 ¶Rinc(?t+1 = ?t)
 ¶?t+1,i
 = 2(yt ? ?Tt ?(xt))?i(xt) (11)
 shows that the gradient is only zero, if the target value yt is
 already met by the approximator or the influence of all ?i(xt)
 vanishes. Otherwise, a change of parameters Æ = ?t+1??t =
 0 minimizes the risk functional and we can conclude
 0 < ?t á
 º
 ?
 (ÆT ?(x))2dx
 ?
 ((?t +Æ)T ?(xt)? yt)2 < ?t á
 º
 ?
 (ÆT ?(x))2dx
 +((?t +Æ)T ?(xt)? yt)2
 < (?Tt ?(xt)? yt)2 . (12)
 Consequently, the local error of the new parameter vector
 after learning is less than before. Thus learning converges
 locally. The global convergence properties are investigated by
 an example in the next chapter, as it cannot be expected that
 the global error is monotonically decreasing because of the
 insufficient information of a single example.
 0 50 100 150
 10?2
 100
 102
 104
 Number of Learning Stimuli (NL)
 Cumulative Loss (
 L C
 )
  
 
GD
 RLS
 IRMA
 Fig. 1. Cumulative loss of online learning algorithms.
 Looking at the influence of the stiffness ?t on equation (8)
 in one incremental learning step, obviously for ?t ? ° the
 old parameter vector ?t is kept, i.e., ?t+1 = ?t. On the other
 hand, for ?t ? 0 equation (8) takes the form
 B(xt)?t+1 = ?(xt)yt (13)
 ?
 N·
 i=1
 ?j(xt)?i(xt)?t+1,i = ?j(xt)yt ? j ? [1;N ] .
 ?
 N·
 i=1
 ?i(xt)?t+1,i = yt .
 So, in one extreme case, the result is a parameter vector ?t+1
 that does not change the approximation at all. In the other
 extreme case, it is forced to reproduce the example exactly.
 The magnitude of change in parameters Æ increases for a
 decreasing ?t and the adjustment of ?t hence allows to choose
 how much the functional behavior might change.
 IV. EXEMPLARY APPLICATION
 The investigation scenario is chosen to satisfy two require-
 ments. On the one hand the setup is as simple as possible, as
 it is restricted to a one-dimensional function, to allow a focus
 on the learning algorithm and its methodological behavior. On
 the other hand the used approximator is chosen to constitute
 nonlocal basis functions {?(x)}i with each having a different
 amount of influence on the functional behavior. Thus the
 demand on the stability of the learning algorithm is strong. The
 approach is compared to the two basic state-of-the-art methods
 of parameter adaptation, gradient descent and recursive least
 squares. So to demonstrate the resulting learning algorithm
 IRMA (Incremental Risk Minimization Algorithm) for a LIP
 approximator, a polynomial fp : R? R of order N
 fp(x, ?) =
 N·
 i=0
 ?ix
 i (14)
 is used, starting with the initial parameter vector ?0 = 0.
 The stiffness is initialized to a small value ?0 = 0.1 to
 1502
0 1 2 3
 0
 0.2
 0.4
 Input: x
 Output: 
y
 0 1 2 3
 0
 0.2
 0.4
 Input: x
 Output: 
y
 0 1 2 3
 0
 0.2
 0.4
 Input: x
 Output: 
y
 0 1 2 3
 0
 0.2
 0.4
 Input: x
 Output: 
y
 0 1 2 3
 0
 0.2
 0.4
 Input: x
 Output: 
y
 0 1 2 3
 0
 0.2
 0.4
 Input: x
 Output: 
y
 0 1 2 3
 0
 0.2
 0.4
 Input: x
 Output: 
y
 0 1 2 3
 0
 0.2
 0.4
 Input: x
 Output: 
y
 0 1 2 3
 0
 0.2
 0.4
 Input: x
 Output: 
y
 Polynomial Order: 4 Polynomial Order: 6 Polynomial Order: 10
 Incremental Learning
 No. Learning
  
Stimuli: 1
 0
 No. Learning
  
Stimuli: 8
 0
 No. Learning
  
Stimuli: 15
 0
 Reference: Complete Batch
 No. Learning
  
Stimuli: 15
 0
  
 
Target Function Learning Stimuli Approximation
 0 1 2 3
 0
 0.2
 0.4
 Input: x
 Output: 
y
 0 1 2 3
 0
 0.2
 0.4
 Input: x
 Output: 
y
 0 1 2 3
 0
 0.2
 0.4
 Input: x
 Output: 
y
 Fig. 2. Results of online learning by minimizing the incremental risk functional in comparison to standard batch regression.
 represent that the starting point supplies only low information.
 As the information increases with every example, the stiffness
 increases as well stepwise by ?t = ?t?1 á 1.05. As a target
 function ft we use the nonlinear nonmonotonic function
 ft(x) = x á e?x2 , x ? [0, 3] (15)
 with output values in the interval [0, 0.43]. The online learning
 algorithm is provided with random examples whose target
 values are disturbed by equally distributed noise in the range
 ±0.05, i.e. about ±10% of the codomain range. In a first
 scenario we compare IRMA to GD and RLS with respect to
 the cumulative loss (1) after NL examples (xi, yi), i.e., the
 prediction error along the online learning. 500 trial runs with
 a sequence of 150 examples each are randomly generated and
 used to train a polynomial of 6th order incrementally. GD
 is set up with a normalized learning rate. RLS is initialized
 1503
with the identity as a covariance matrix and a forgetting factor
 of 0.9. Both are combined with a Tikhonov regularization
 of ? á ?T ? with ? = 0.05. These parameters are optimized
 by systematically scanning the range of possible values. The
 mean cumulative loss as well as its upper and lower standard
 deviation are shown in Figure 1 on a logarithmic scale. The
 results show that IRMA leads to the lowest cumulative loss
 and shows nearly no variance of the results for different
 learning sequences. Especially GD is an unsuitable method
 for online learning of polynomials as the different amounts of
 influence of the parameters and their global effect destabilize
 the gradient learning. Contrariwise RLS and IRMA are stable
 and grow sub-linearly, hence guaranteeing convergence. Yet,
 the cumulative loss of IRMA is lower, as it minimizes the
 worst case in every step.
 Two additional aspects of this scenario are investigated to
 show details of IRMA. On the one hand, we take polynomials
 of 4th, 6th and 10th order for the approximation. The expres-
 siveness with 4th and 6th order is appropriate for the target
 function whereas the 10th order tends to over-fitting because
 it is too expressive. On the other hand, we take snapshots of
 the resulting functional behavior after 10, 80 and 150 examples
 have been presented in order to demonstrate the convergence
 properties. This way we see how the approximation changes
 with increasing information and expressiveness. For compari-
 son, additionally a batch regression is made as a benchmark
 with standard polynomial fitting of Matlab (through the Van-
 dermonde matrix) using the total amount of 150 examples
 altogether. Thus, we can compare the result to that of an
 algorithm with complete knowledge of all data which would
 be unfeasible for big sets of data.
 The results of these investigations are shown qualitatively
 in Figure 2. At first with only 10 examples, the approximation
 is still bad and especially the 10th order polynomial is not fully
 set up. The provided information with these sparse samples
 is too low to represent the target function appropriately. But
 yet at an input of about x = 1.5, every learned output nearly
 meets the target function as here some data cumulate. The 10th
 order polynomial shows that in regions of low data density the
 initial value of zero is preferred. Hence, a localized impact of
 examples is achieved even for an approximator with globally
 active basis-functions.
 As expected, with more examples, the approximation gets
 better. With 80 presented examples, the regression has con-
 verged fairly well for any order of the polynomial as the total
 target function is covered. Even though the target values are
 subject to noise, presenting 70 more examples does not affect
 the approximation much. A notable feature is that even the
 polynomial of higher order is stable in the online scenario
 with sparse and noisy data. And the more data are presented,
 the more similar get the results for different polynomial orders.
 In comparison to the benchmark of batch regression over
 the total data set, nearly no difference can be seen. This is
 supported by a numerical investigation. To account for the
 influence of the randomly chosen learning sequence, here
 the above investigation is repeated 1000 times and for every
 sequence the mean squared error (MSE) of the resulting
 approximation with respect to the ground truth is calculated
 on 1000 test points. The average and the standard deviation
 of the MSE are shown in Table I. The results show that
 the MSE as well as the standard deviation decreases with
 an increasing amount of data. As expected, with a low data
 density, the simple batch learning tends to over-fitting which
 results in a big difference to the ground truth, whereas IRMA
 without any additional regularization is stable. With more data,
 batch learning achieves nearly no error and the comparison
 shows a similar error for IRMA even though it has much
 less information available at a time. Thus IRMA is more
 stable than its batch-counterpart and it provides a reasonable
 method to inhibit over-fitting. Accordingly, online learning is
 no drawback in the quality of the result, but provides the
 possibility to incorporate new knowledge into the approximator
 at every time step and still has a low demand on computation
 and memory, even for big data sets.
 V. DISCUSSION
 In a nutshell, along with the minimal local error every
 approach optimizes different optimization criteria. While GD
 learning minimizes the change of the parameter vector, RLS
 learning minimizes the parameters variance. The newly intro-
 duced IRMA minimizes the change of the global functional
 behavior. Consequently, the advantage of the incremental risk
 functional is that the resulting learning algorithm thus enforces
 a localized learning which does not depend on whether the
 basis {?(x)}i consists of localized or global functions. This
 behavior is also reflected in the exemplary investigation as
 the stability of the resulting learning algorithm can be seen
 even for overly expressive and nonlocal approximators and a
 stable low cumulative loss is achieved contrary to other online
 methods. If the basis {?(x)}i consists of functions with the
 same amount of influence on the global functional behavior,
 the result of IRMA is similar to that of GD. In comparison
 to the batch algorithm a good result is achieved that is just as
 stable even though the data are only available one at a time.
 This way, a low computational demand is achieved even for
 big data.
 In this work we focused on regression estimation and LIP
 approximators as an exemplary online learning scenario. Both
 restrictions can be relaxed, keeping the fundamental principles.
 In an online setup, one example should have only localized
 effects as its expressiveness is limited. Hence the idea of
 minimal change of the global functional behavior and maximal
 local incorporation of an example at the same time is the same
 for other tasks and for approximators with nonlinear parameter
 influence.
 With the nonlinearity of parameter influence, the mini-
 mization of the incremental risk functional gets more com-
 plex. Especially local minima can exist for both parts of
 the minimization (the functional behavior and the local er-
 ror) and accordingly an analytical minimization will be hard
 to get and an approximate solution is confronted with the
 well known problems of nonlinear minimization. Furthermore,
 pattern recognition or density estimation as well as other
 learning tasks can be expressed within the incremental risk
 functional through the choice of another loss function. Its basic
 idea is hence not unique to regression estimation so that the
 resulting behavior of a learning algorithm for other tasks can
 be expected to be comparable.
 With the approach of the incremental risk functional it
 is possible to get more stable online learning algorithms for
 1504
TABLE I. THE TABLE SHOWS THE AVERAGE MSE TO THE GROUND TRUTH AND ITS STANDARD DEVIATION OVER 1000 RANDOMLY DRAWN LEARNING
 SEQUENCES. THE MSE IS COMPARED FOR ONLINE AND BATCH LEARNING WITH DIFFERENT AMOUNTS OF EXAMPLES.
 No. of examples Polynomial Order: 4 Polynomial Order: 6 Polynomial Order: 10
 10 (incr.) 3.1 á 10?3 ± 2.9 á 10?5 5.5 á 10?3 ± 6.0 á 10?5 1.2 á 10?2 ± 1.2 á 10?4
 10 (batch) 4.4 á 10+2 ± 1.9 á 10+8 7.2 á 10+7 ± 5.1 á 10+18 1.8 á 10+14 ± 1.3 á 10+31
 80 (incr.) 3.0 á 10?4 ± 1.2 á 10?8 2.2 á 10?4 ± 1.5 á 10?8 3.1 á 10?4 ± 2.3 á 10?8
 80 (batch) 2.0 á 10?4 ± 5.3 á 10?9 1.0 á 10?4 ± 1.2 á 10?8 4.0 á 10?4 ± 5.3 á 10?6
 150 (incr.) 2.0 á 10?4 ± 2.1 á 10?9 9.4 á 10?5 ± 3.0 á 10?9 1.2 á 10?4 ± 3.1 á 10?9
 150 (batch) 1.6 á 10?4 ± 6.4 á 10?10 5.0 á 10?5 ± 6.7 á 10?10 8.4 á 10?5 ± 8.7 á 10?9
 different tasks and approximators than previous adaptation
 methods provided. But still this is just a starting point for a
 new learning paradigm. More theoretical investigations of our
 proposed algorithm need to be done to ensure not only local
 but global convergence on the long run and to further clarify
 the advantages over GD and RLS. Additionally, approximators
 with nonlinear influence of parameters are of interest for
 further investigations as well as different online learning tasks
 like pattern recognition.
 VI. CONCLUSION
 In this work we introduced the incremental risk functional
 as a novel approach for online learning, i.e. on big data
 or continuously at runtime. We showed that for the case of
 regression estimation by approximators with linear parameter
 influence, the minimization can be solved analytically and
 the resulting learning algorithm is locally contracting while
 maintaining a minimal change of the global functional be-
 havior in every online learning step. Thus, the worst case
 of an increasing total loss is minimized. As an example, the
 resulting online learning algorithm was applied to a polynomial
 approximator and demonstrated that even with sparse data the
 approximation is stable and with dense data it achieves similar
 results as a regression over the complete batch of data, but
 with much lower computational demands. At the same time
 localized learning is achieved with incrementally presented
 data although a basis of nonlocalized functions is used. And
 the approach of online learning allows to flexibly adapt to
 changes in the underlying learning data.
 Concerning the different optimization criteria of online
 learning approaches, it can be seen that the risk minimization
 learning leads to a similar behavior as gradient descent learning
 if the amount of influence of all parameters is the same. But it
 is a more general approach as it incorporates knowledge about
 the approximation structure and the effects of its parameters
 on the output into the parameter update. The incremental risk
 functional is thus very reasonable for online learning problems
 and for dealing with big data.
 Regardless of the task and approximator it provides a
 method to derive a stable online learning algorithm as it allows
 to incorporate new examples incrementally only locally and
 thus ensures that prior learned knowledge is kept as far as
 possible. Hence the worst case risk is minimal in every single
 step.
 REFERENCES
 [1] Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and
 Yoram Singer. Online passive-aggressive algorithms. The Journal of
 Machine Learning Research, 7:551Ð585, 2006.
 [2] Koby Crammer, Alex Kulesza, Mark Dredze, et al. Adaptive regular-
 ization of weight vectors. Advances in Neural Information Processing
 Systems, 22:414Ð422, 2009.
 [3] Koby Crammer and Daniel D Lee. Learning via gaussian herding.
 Advances in Neural Information Processing Systems, 23:451Ð459, 2010.
 [4] E. De Vito, L. Rosasco, A. Caponnetto, U. De Giovannini, and
 F. Odone. Learning from examples as an inverse problem. Journal
 of Machine Learning Research, 6(1):883Ð904, 2006.
 [5] Mark Dredze, Koby Crammer, and Fernando Pereira. Confidence-
 weighted linear classification. In Proceedings of the 25th International
 Conference on Machine Learning, pages 264Ð271. ACM, 2008.
 [6] J. Farrell and M. Polycarpou. Adaptive Approximation Based Control.
 Wiley-Blackwell, Hoboken, 2006.
 [7] Yi Li and Philip M Long. The relaxed online maximum margin
 algorithm. Machine Learning, 46(1):361Ð387, 2002.
 [8] S. Mukherjee, R. Rifkin, and T. Poggio. Regression and classification
 with regularization. Lectures Notes in Statistics: Nonlinear Estimation
 and Classification, 171:107Ð124, 2002.
 [9] W.H. Press, Flannery B.P., S.A. Teukolsky, and W.T. Vetterling. Numer-
 ical Recipes: The Art of Scientific Computing. Cambridge University
 Press, Cambridge, 2007.
 [10] Frank Rosenblatt. The perceptron: A probabilistic model for information
 storage and organization in the brain. Psychological Review, 65(6):386Ð
 408, 1958.
 [11] V.N. Vapnik. The Nature of Statistical Learning Theory. Springer,
 Berlin, 2000.
 [12] Liu Yang, Rong Jin, and Jieping Ye. Online learning by ellipsoid
 method. In Proceedings of the 26th Annual International Conference
 on Machine Learning, pages 1153Ð1160. ACM, 2009.
 [13] Y. Ying and M. Pontil. Online gradient descent learning algorithms.
 Foundations of Computational Mathematics, 8(5):561Ð596, 2008.
 1505

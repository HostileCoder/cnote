 
 
Survey of Research on Big Data Storage 
 
 
Xiaoxue Zhang 
College of Computer and Information 
Hohai University 
Nanjing, China 
zhang_zxx@163.com 
Feng Xu 
College of Computer and Information 
Hohai University 
Nanjing, China 
njxufeng@163.com
  
 
Abstract—With the development of cloud computing and 
mobile Internet, the issues related to big data have been 
concerned by both academe and industry. Based on the 
analysis of the existed work, the research progress of how to 
use distributed file system to meet the challenge in storing big 
data is expatiated, including four key techniques: storage of 
small files, load balancing, copy consistency, and de-
 duplication. We also indicate some key issues need to concern 
in the future work. 
Keywords-big data; storage;distributed file system 
I.  INTRODUCTION 
According to IDC (Internet Data Center), the global big 
data will increase by 50 times in next decade. How to store 
these fast-growing, vast amounts of data? How to analysis 
and process these big data? A series of related problems 
become a common challenge faced by all enterprises, also 
become today's research focus. The reason why big data 
takes so much attention cab be summarized as follows: the 
network terminal changed from a single desktop to multi-
 terminal such as desktop, tablet PCs, book-reader PCs, 
mobile phones, television and so on, which greatly expands 
the content and scope of the network services, and improves 
the user's reliance on the Internet; the rapid growth of 
network users and the average time a user spends on the 
network, which makes a significant increase in the user 
network behavior data; the network services have changed 
from a single form of words to the multimedia form such as 
pictures, voice, video, leading to significant increase in the 
amount of data. All above lead to the increasing amount of 
data. Not just the Internet, the phenomenon of large data 
already exists in the fields of physics, biology, environmental 
ecology, automatic control and other scientific area. What’s 
more, big data also exist in military, communications, 
finance and other industries for some time. It is not a new 
problem, it is widespread. There is no in-depth study on it 
because of restricted by condition in the past. But now, with 
the hardware costs come down and more and more develop 
in science and technology, people can focus into the big data 
which implied the great value of. 
II. THE CHARACTERISTICS OF BIG DATA  
The main sources of big data are data information within 
the organization, sensory information in the Internet of 
things and interactive information in the Internet world. It is 
difficult to form a unified concept of big data. IDC defines it 
this way: Big data technologies describe a new generation of 
technologies and architectures, designed to economically 
extract value from very large volumes of a wide variety of 
data, by enabling high-velocity capture, discovery, and 
analysis [1]. This definition includes three main 
characteristics of big data .Two other characteristics seem 
relevant: value and complexity. Summarize as “4V+C”. 
• Volume: The data volume is huge, especially the 
huge amounts of data generated by a variety of 
devices, the size of the data is very large, much 
larger than the flow of information on the Internet, 
PB level will be the norm. This implies the need for 
large storage space and computing power. 
• Velocity: The data related to perception, 
transmission, decision making, control open loop 
have very high requirements on real-time data 
processing. The late result is of little value. It is 
essentially different with traditional data processing. 
• Variety: Big data may be blogs, videos, pictures, 
location information, etc. Even with the same kind 
of data, encoding, data format or other aspects may 
be different. 
• Value: Big Data having a high commercial value. It 
is the reason why more and more people are 
concerned about the big data. But the density of the 
value is low. Take the video for example, during 
uninterrupted monitoring process, useful data may 
only few seconds? 
• Complex: Big Data is not only complex and diverse 
in data types and representation. There is often a 
complex dynamic relationship between each data. 
The change of one data may have an impact to a lot 
of data. 
III. BIG DATA STORAGE CHALLENGES 
With the expansion of the application scale, the amount 
of data will show the trends of explosive growth. The 
conventional data storage system reaches a bottleneck, 
2013 12th International Symposium on Distributed Computing and Applications to Business, Engineering & Science
 978-0-7695-5060-2/13 $26.00 © 2013 IEEE
 DOI 10.1109/DCABES.2013.21
 76
 
 
TABLE I.  NEW CHALLENGES ON STORAGE 
Characteristics Social networks Challenges 
Large amount of data Large amount of data 
and growing rapidly Real-time 
Intelligence 
Security 
Reliability 
Integrity 
Low consumption 
High concurrency 
High efficiency 
Many kinds of data 
Unstructured 
Semi-structured 
Structured 
High potential value 
Social relationship 
mining 
Personalized 
recommendation 
Large fluctuations 7-8 times 
unable to complete the operational task timely. Storage 
systems face the challenge of complex big data applications. 
Take social networks for example, as shown in the TABLE I. 
• Because the fluctuation of load is high and uncertain, 
the storage system for big data needs to dynamically 
match the load characteristics. 
• Because of the need of high real-time in big data 
applications, the delay and the length of IO path in 
data processing should be reduced. 
• High accumulation of the amount of data, high 
concurrent user access and high data growth require 
the storage system large capacity, high aggregate 
bandwidth, high IOPS. 
Currently, there are many storage technologies to deal 
with big data, such as distributed file system, new type of 
database based on MapReduce and so on. On one hand, the 
great value in big data promotes development on these 
technologies. On the other hand, these technologies provide 
technical support for big data storage and make big data 
becoming a research hotspot in recent years. Existing 
technology just can meet the demand of big data storage 
barely. So many improvements are needed and have high 
research value. 
IV. DISTRIBUTED FILE SYSTEM 
File system is the foundation of upper layer application 
[2]. With the continuous development of internet 
applications, data is growing rapidly. So the large-scale data 
storage became the arduous task of the companies and 
research focus. Because of the limit on the expansion of 
storage capacity, traditional storage system is difficult to 
meet the big data storage. 
So have to use the distributed file system to transfer 
system load to multiple nodes. By grouping hard disks on 
multiple nodes into a global storage system, distributed file 
system provides polymeric storage capacity and I/O 
bandwidth, and easy to extension according to the system 
scale. Generally speaking, the mainstream distributed file 
systems, such as Lustre, GFS, HDFS, separately store 
metadata and application data because of the different 
characteristics of store and access. Divide and rule can 
improve the performance of the entire file system 
significantly. Despite these advantages, distributed file 
system still has many shortcomings when it faces explosive 
growth of data, complex variety of storage needs. Then these 
shortcomings are paid more and more attention, and become 
the research focus today. 
A. Small Files Problem 
The distributed file systems, such as GFS and HDFS, are 
designed for larger files. But most of the data on the Internet 
represent by the high frequency of small files, and more 
storage access are for small files in the application. 
TABLE II shows the problems of traditional distributed 
file system in the small file management aspect. 
• Small file access frequency is higher, need to access 
the disk for many times, so the performance of the 
I/O is low; 
• Small files will form a large amount of metadata, 
which can affect the management and retrieval 
performance of metadata server, and cause overall 
performance degradation. 
• Because the file is relatively small, easy to form file 
fragmentation resulting in a waste of disk space; 
• To create a link for each file can easily lead to 
network delays. 
There are some common optimization methods [3-10] as 
the TABLE II shown. Of course, this is not comprehensive. 
In [11], the files belong to same directory will written into 
the same block as much as possible. This will help increase 
the task speed distributed by MapReduce in the future. 
B. Load Balancing 
In order to make the multiple nodes can be a very good to 
complete the task together , a variety of load balancing 
algorithm is proposed to eliminate or avoid unbalanced load, 
data traffic congestion and long reaction time. 
The load balancing [10-12] can be done by two ways. 
One is to prevent. One is to prevent. The I/O request is 
equally distributed on each storage node by the right I/O 
scheduling policy, so each node can be a very good job, and 
the situation that a part of the nodes are overloaded, but 
another part of the nodes are light load will not be happened. 
Another way is to adjust after happened. When load 
imbalance phenomenon has emerged, it can be eliminated 
effectively by migration or copy. In general, load balancing 
algorithm through many years research has been relatively 
mature, usually divided into two types: static load balancing 
algorithm and dynamic load balancing algorithm.  
Static load balancing algorithm has nothing to do with 
the current state of the system. It assigns the task by 
experience or pre-established strategy. This algorithm is 
easy, and spends little. But it does not consider the dynamic 
changes of the system status information, so it has blindness. 
It is mainly suitable for smaller and homogeneous service 
systems. Classical static load balancing algorithm includes 
polling algorithm, ratio algorithm, priority algorithm. 
Dynamic load balancing algorithm assigns the I/O task or 
adjust the load between nodes according to the current load 
of the system. Classical load balancing algorithm includes 
minimum connection algorithm, weighted least connection 
algorithm, destination address hash algorithm, source 
address hash algorithm. 
 
77
 
 
TABLE II.  CLASSIFICATION AND ANALYSIS OF ALGORITHMS 
method illustration advantage disadvantage 
Metadata 
Management 
Metadata compression 
reduces metadata size Improve space utilization 
the metadata read 
performance was hurt due to 
extra steps of lookup file . 
Performance 
Optimization 
Utilizing prefetching and caching technologies to 
improve access efficiency 
e.g. Hot Files Caching, Metadata Caching 
The improvement of the Cache hit 
ratio Just for particular application 
Small file merging 
A set of correlated files is combined into a single large 
file to reduce the file count.  
An indexing mechanism has been built to access the 
individual files from the corresponding combined file. 
Reduce the metadata Two indexes, affect the speed 
sequence files Form by a series of binary, where key is the name of the file, the value of the file content. 
Free access for small files, nor 
restrict how much users and files 
Platform dependent 
Way to store Small files stored separately in separate areas Reduce disk fragmentation Complexity of the movement 
Dynamic load balancing algorithm can be divided into 
centralized and distributed strategy. TABLE III shows the 
advantages and disadvantages of them. 
Load balancing algorithm in the distributed file system 
can also be divided into sender starting method and recipient 
starting method. Sender starting method starts load 
distribution activities by overload point, by which part of the 
load of the overloaded nodes is sent to light load nodes, is 
suitable for the system as a whole in light load condition, 
because of more light load nodes in system, light load node 
is easy to found, so frequent migration won’t happen. 
Recipient starting method starts load distribution activities 
by light load nodes which apply for part of the load of the 
overloaded nodes, is relatively effective when the system as 
a whole is in a state of overload, the reasons are similar to 
the above. 
TABLE III.  COMPARISON BETWEEN CENTRALIZED AND DISTRIBUTED 
STRATEGY 
Principle 
Centralized strategy  Distributed strategy 
The function of the 
dynamic load balancing 
is concentrated in a 
special load management 
server. The server is 
responsible for the load 
information collection 
and maintenance of the 
whole system. 
There is no specific load 
management server, so 
each node need to collect, 
record and manage the 
load information of the 
surrounding nodes. 
Advantages 
Simple and less 
communication cost; The 
best node for  migration 
or copy can be found; 
Suitable for large system 
model 
High reliability, easy to 
extend; There is no single 
point failure, paralysis of 
any host will not affect the 
normal work of the whole 
system. 
Disadvantages 
Low reliability, hard to 
extend;  
Once the load 
management server fails, 
the whole load balancing 
system will be paralyzed; 
The larger the size of the 
system, the more 
complex the 
management. 
When the system is very 
large, the communication 
cost of load information 
collection will 
geometrically growth; 
Because each node only 
master a small amount of 
load information, and the 
load regulation is local, the 
result may be not 
satisfactory, and even have 
a chance to cause the load 
to move back. 
Each type of load balancing strategy above has 
advantages and disadvantage, so some mix strategy emerged. 
In [12], keep three queues (light load, optimal load, overload) 
in master server, use priority algorithm among these queues, 
and weighting polling algorithm in the queues. 
Because the centralized load balancing has the 
advantages of less communication overhead that is 
particularly important for the large data storage, so a lot of 
research enhances its reliability by changing the system 
structure. A more common way is to decompose complex 
load balancing management tasks by layering or grading and 
accomplished by multiple servers. In [13], a hierarchical 
dynamic load balancing strategy was proposed. An 
intermediate layer was added between the metadata server 
and the client, which was designed to collect load status 
information especially to reduce the load information 
acquisition cost, and by adding a backup server of load 
management to solve the problem of low reliability. In [14], 
a load balancing strategies of two-stage meta server cluster 
file system was proposed, using the idea that the high level 
manage the low level, the task allocation and task processing 
functions of the meta server were decentralized, which 
improves the parallel processing ability and extensibility of 
the system, and put forward a model of the heat reflecting 
accessing frequency of the storage node to determine the 
ideal location of the file to be saved. 
The process of load balancing scheduling is complex, 
especially in dynamic equilibrium. In addition to the 
scheduling policy, there are many other factors, such as how 
to collect information, what information should be collected, 
when should migrate task, where the task should be migrated. 
More attention is paid to the related research of load 
evaluation index which is used to judge when to migrate. 
HDFS’s equilibrium strategies use the single evaluation 
index of disk usage, which has considerable limitations. In 
[15], the evaluation function is determined by multiple 
attributes and the server's load condition is determined by 
double threshold value.  
However, the method above still can't solve the 
limitations of threshold. Usually thresholds are calculated 
and set in advance, if the thresholds are set too high, when 
the average system load is lighter, the phenomenon that part 
of the nodes are idle while the other part are busy may 
appear; If the threshold is set too low, when the average 
78
 
 
system load is heavier, not only the phenomenon of frequent 
migration will appear, but also that a migration triggers a 
new migration will happen. That will cause the system 
instability. Dynamic threshold, which determines the 
threshold by the current system load situation, has the 
potential to become one of the ways to solve this problem 
and has very high research value. 
C. Replica Consistency 
In distributed file system, replication technology is 
widely used to improve the reliability and performance of 
system. Ensure the safety and reliability of the data and fault 
tolerance of the system by storing multiple copies, with that 
just using another copy can normally access the data when a 
copy is destroyed. In addition, when the system is larger, the 
use of replication technology on different servers to store a 
copy of the same file helps to achieve the load balancing of 
the system, thereby improving overall performance. 
Nowadays the master - slave replica model is adopted by 
the most study of the Replica consistency, and it is the most 
original way for the salve replica to receive the update 
passively from the master replica. In [16] two consistency 
protocols based the tree are proposed: aggressive copy 
coherence and lazy copy coherence. With the root node 
regarded as the master copy, in the aggressive copy 
coherence, once the data of the root node update, the update 
will broadcast to each copy nodes, and the copy nodes will 
be updated; in the lazy copy coherence, the copy nodes are 
accessed by comparing timestamp to check their own data 
whether the latest version or not, if not, they will update. 
Therefore, though the aggressive copy coherence can 
guarantee update from the copy in a timely manner, in the 
big data environment, the system will be given a great deal 
of load pressure; while though the lazy copy coherence can 
reduce the network load, but will have access latency, and 
when the master copy is damaged it may not be restored 
from a copy of which has not been updated. 
However, in the master - slave replica model, as the 
master copy is the only update source, the efficiency will by 
reduced and it will be the bottleneck of the system 
performance improvement. In [17] a non-centralized copy 
update consistency model based on the replica identification 
of timestamp was proposed, which effectively solves the 
bottleneck problem of the original model by substituting the 
master copy nodes to the slave copy nodes to trigger the 
update process. In addition, it also referred to a strategy of 
creating copy dynamically when a request exceeds the 
threshold, but no extra copies of deletion policy was referred, 
which makes a number of file copy used to be hot in the 
system resulting in a large number of redundant. In [18] the 
dynamic replication strategy was referred, dynamically 
changing the number of copies, including the creation and 
deletion. 
In the actual application, load fluctuation of distributed 
file system of is very big, so if consistency detection can be 
done when the load is light, not only the data will be more 
reliable, but also the system resources will be fully used. 
D. De-duplication 
Extensive redundancy exists in the distributed file 
system, such as multi-user storing the same file, different 
versions of the unified file, similar file header of the same 
type file and so on. De-duplication is a very popular storage 
technology, effectively optimizing storage capacity. It 
deletes duplicate data in data set, leaving only one, thus 
eliminating redundant data. 
The mainstream method to determine whether the data 
block is repeated is by the way of computing their hash value. 
If the hash value of the data block matches with a value from 
the hash index table, it indicates that data block is repeated, 
and substitute it with a pointer to the data storage. With the 
increase of the index, the memory performance will rapidly 
decline. So some researches focus on the approach by which 
more redundant data can be reduced, and some investigate 
how to do data de-duplication at high speed.  
Another key technology is the division of the data. In file 
units to detect the speed will be very fast, but even if many 
of the same data exist in many different files, the duplicate 
data in the file can’t be deleted, so the file are needed to be 
divided, and are tested in block units. In [19], five 
representative chunking algorithms of data de-duplication are 
introduced and their performance on real data set is 
compared. 
By de-duplication the rapid growth of data is effectively 
controlled, effective storage space is increased, storage 
efficiency is improved, and so on. However the reliability of 
data will be affected, and multiple files rely on the same data 
block, that is to say, multiple files are damaged, if the data 
block is damaged. Therefore, we can consider conducting the 
de-duplication firstly, and then backup appropriately for the 
data set with low access rate, by which storage space is 
saved, and the reliability of the data is ensured. 
V. CONCLUSION 
Despite the big data is popular recently, the study of it is 
not just started. There have been many countermeasures and 
related technologies to meet the challenge of big data. This 
paper summarizes and analyzes four technologies of 
distributed file system on big data storage, and gives some 
outlook for further study.  
ACKNOWLEDGMENT 
This paper is supported by National Natural Science 
Foundation of China: “Research on Trusted Technologies for 
The Terminals in The Distributed Network Environment” 
(Grant No. 60903018) and “Research on the Security 
Technologies for Cloud Computing Platform” (Grant No. 
61272543). 
 
REFERENCES 
[1] J. A. E. R. Gantz, "Extracting Value from Chaos," IDC’s Digital 
Universe Study 2011. 
[2]  X. Ci and X. Meng, "Big Data Management:Concepts,Techniques 
and Challenges," journal of Computer Research and Development, 
2013. 
79
 
 
[3] X. Li, B. Dong, L. Xiao, L. Ruan, and Y. Ding, "Small files problem 
in parallel file system," in 2011 International Conference on Network 
Computing and Information Security, NCIS 2011, May 14, 2011 - 
May 15, 2011, Guilin, Guangxi, China, 2011, pp. 227-232. 
[4] B. Dong, Q. Zheng, F. Tian, K. Chao, R. Ma, and R. Anane, "An 
optimized approach for storing and accessing small files on cloud 
storage," Journal of Network and Computer Applications, vol. 35, pp. 
1847-1862, 2012. 
[5]  B. Dong, J. Qiu, Q. Zheng, X. Zhong, J. Li, and Y. Li, "A novel 
approach to improving the efficiency of storing and accessing small 
files on hadoop: A case study by PowerPoint files," in 2010 IEEE 7th 
International Conference on Services Computing, SCC 2010, July 5, 
2010 - July 10, 2010, Miami, FL, United states, 2010, pp. 65-72. 
[6]  G. MacKey, S. Sehrish and J. Wang, "Improving metadata 
management for small files in HDFS," in 2009 IEEE International 
Conference on Cluster Computing and Workshops, CLUSTER '09, 
August 31, 2009 - September 4, 2009, New Orleans, LA, United 
states, 2009. 
[7]  S. Chandrasekar, R. Dakshinamurthy, P. G. Seshakumar, B. 
Prabavathy, and C. Babu, "A novel indexing scheme for efficient 
handling of small files in Hadoop Distributed File System," in 2013 
3rd International Conference on Computer Communication and 
Informatics, ICCCI 2013, January 4, 2013 - January 6, 2013, 
Coimbatore, India, 2013, p. Gov. India, Dep. Sci. Technol., Minist. 
Sci. Technol.; Council for Scientific and Industrial Research (CSIR). 
[8] Y. Zhang and D. Liu, "Improving the efficiency of storing for small 
files in hdfs," in 2012 International Conference on Computer Science 
and Service System, CSSS 2012, August 11, 2012 - August 13, 2012, 
Nanjing, China, 2012, pp. 2239-2242. 
[9]  [9] X. Li, B. Dong, L. Xiao, and L. Ruan, "Performance 
optimization of small file I/O with adaptive migration strategy in 
cluster file system," in 2nd International Conference on High-
 Performance Computing and Applications, HPCA 2009, August 10, 
2009 - August 12, 2009, Shanghai, China, 2010, pp. 242-249. 
[10] [10] N. Mohandas and S. M. Thampi, "Improving hadoop 
performance in handling small files," in 1st International Conference 
on Advances in Computing and Communications, ACC 2011, July 
22, 2011 - July 24, 2011, Kochi, India, 2011, pp. 187-194. 
[11] J. Liu, L. Bing and S. Meina, "The optimization of HDFS based on 
small files," in 2010 3rd IEEE International Conference on 
Broadband Network and Multimedia Technology, IC-BNMT2010, 
October 26, 2010 - October 28, 2010, Beijing, China, 2010, pp. 912-
 915. 
[12] C. Zhang and J. Yin, "Dynamic Load Balancing Algorithm of 
Distributed File System," Journal of Chinese Computer Systems, vol. 
32, pp. 1424-1426, 2011. 
[13] W. Wu, "Research on mass storage metadata management,". vol. D: 
Huazhong University of Science and  Technology, 2010. 
[14] J. Tian, W. Song and H. Yu, "Load-Balance Policy in Two Level-
 cluster File System," Computer Engineering, vol. 33, pp. 77-79,82, 
2007. 
[15] F. Gu, "Research on Distributed File System Load Balancing in 
Cloud Environment,". vol. D: Beijing Jiaotong University, 2011. 
[16] Y. Sun and Z. Xu, "Grid replication coherence protocol," in 
Proceedings - 18th International Parallel and Distributed Processing 
Symposium, IPDPS 2004 (Abstracts and CD-ROM), April 26, 2004 - 
April 30, 2004, Santa Fe, NM, United states, 2004, pp. 3197-3204. 
[17] S. Zheng, M. Li and W. Sun, "DRCSM:a Novel Decentralized 
Replica Consistency Service Model," Journal of Chinese Computer 
Systems, vol. 32, pp. 1622-1627, 2011. 
[18] S. Zheng, M. Li and W. Sun, "DRCSM:a Novel Decentralized 
Replica Consistency Service Model," Journal of Chinese Computer 
Systems, vol. 32, pp. 1622-1627, 2011. 
[19] B. Cai, F. L. Zhang and C. Wang, "Research on chunking algorithms 
of data de-duplication," in International Conference on 
Communication, Electronics, and Automation Engineering, 2012, 
Xi'an, China, 2013, pp. 1019-1025. 
 
80

Locally Smoothed Modified Quadratic Discriminant Function
 Xu-Yao Zhang Cheng-Lin Liu
 National Laboratory of Pattern Recognition (NLPR)
 Institute of Automation, Chinese Academy of Sciences, Beijing, P.R. China
 xyz@nlpr.ia.ac.cn liucl@nlpr.ia.ac.cn
 AbstractÑModified quadratic discriminant function (MQD-
 F) is a state-of-the-art classifier for handwriting recognition.
 However, the big gap between accuracies on training and
 testing sets indicates that MQDF has a good capability to
 fit training data but the generalization performance is not
 promising. To solve this problem, we propose a new mod-
 el called locally smoothed modified quadratic discriminant
 function (LSMQDF) by smoothing the covariance matrix of
 each class with its nearest neighbor classes. LSMQDF can be
 viewed as a regularization to avoid over-fitting. The covariance
 matrix estimated by local smoothing is more accurate and
 robust. LSMQDF can be also viewed as an extension of the
 global smoothing method, namely regularized discriminant
 analysis (RDA). Experiments on both offline and online Chinese
 handwriting databases demonstrate that: with local smoothing,
 the accuracy on training set is decreased (over-fitting avoided),
 and the accuracy on testing set is improved significantly and
 consistently (generalization improved).
 I. INTRODUCTION
 The modified quadratic discriminant function (MQDF) [1]
 has been applied prevalently and successfully in handwritten
 Chinese character recognition [2] [3] over the past 25 years.
 However, the large number of free parameters of MQDF
 usually lead to over-fitting of the training data. To solve this
 problem, this paper proposes a local smoothing technique
 as a regularization to obtain a more robust and accurate
 estimation of the covariance matrix. The local smoothing
 can alleviate the over-fitting problem and improve the gen-
 eralization performance of MQDF.
 A. Quadratic Discriminant Function (QDF)
 In Bayes decision theory, the quadratic discriminant
 function (QDF) is derived from the assumption of class-
 conditional Gaussian distribution
 p(x|i) = exp
 [
 ?
 1
 2 (x? µi)??1i (x? µi)
 ]
 (2pi) d2 |?i| 12
 , (1)
 where µi ? Rd and ?i ? Rd?d are the mean vector
 and covariance matrix of class i respectively. Considering
 a M-class classification problem, a pattern x is classified
 into the class of maximum a posterior (MAP) probability
 x ? argmaxMi=1 p(i|x) = p(i)p(x|i)p(x) , where p(i) is the
 prior probability and p(x) is the mixture density function.
 When assuming equal prior probabilities, the MAP decision
 rule becomes x ? argmax p(x|i) which is equivalent to
 x ? argmin? log p(x|i). Therefore QDF is defined as:
 fQDF(x, i) = (x? µi)??1i (x? µi) + log |?i| , (2)
 which is actually a distance metric between x and class i:
 x ? class arg
 M
 min
 i=1
 fQDF(x, i) . (3)
 The performance of QDF is highly dependent on the
 computation of the inverse matrix ??1i . The covariance
 matrix ?i is usually singular and under-estimated due to
 the estimation errors and the sensitivity of the estimation
 to small sample size. Therefore QDF can not achieve high
 accuracy in real applications. To achieve an accurate and
 robust estimation of ??1i , many improvements have been
 proposed, among which the modified quadratic discriminant
 function (MQDF) [1] is the most effective one for large
 category Chinese handwriting recognition.
 B. Modified Quadratic Discriminant Function (MQDF)
 By utilizing eigen-decomposition algorithms, the covari-
 ance matrix can be diagonalized as ?i = ?i?i?i , where
 ?i = diag[?i1, á á á , ?id] with ?ij ? R+, j = 1, á á á , d
 being the eigenvalues (ordered in decreasing order) of
 ?i, and ?i = [?i1, á á á , ?id] with ?ij ? Rd, j =
 1, á á á , d being the ordered eigenvectors. Inserting the
 eigen-decomposed covariance matrix into QDF, we get
 fQDF(x, i) =·dj=1 1?ij [?ij(x? µi)]2 +·dj=1 log ?ij . The
 minor eigenvalues are usually prone to be underestimated
 due to small sample size, therefore MQDF [1] replaces the
 minor eigenvalues (?ij , j > k) with a constant ?i to stabilize
 the generalization performance
 fMQDF(x, i) =
 k·
 j=1
 (
 1
 ?ij
 ?
 1
 ?i
 )[?ij(x? µi)]2
 +
 1
 ?i
 ||x? µi||2 +
 k·
 j=1
 log ?ij + (d? k) log ?i,
 (4)
 where k denotes the number of principal axes. The above
 derivations utilize the properties of ?i ?i = I and ||x ?
 µi||2 = ·dj=1[?ij(x ? µi)]2. Compared with QDF, MQDF
 involves only the principal eigenvectors and eigenvalues,
 2013 12th International Conference on Document Analysis and Recognition
 1520-5363/13 $26.00 © 2013 IEEE
 DOI 10.1109/ICDAR.2013.11
 8
therefore, both the storage of parameters and the compu-
 tational complexity are reduced. More importantly, high-
 er classification accuracy can be obtained by setting the
 minor eigenvalues to be a class-independent constant as
 ?1 = ?2 = á á á = ?M = ? 1Md
 ·M
 i=1
 ·d
 j=1 ?ij and using
 cross-validation to select ? from [0, 1] on the training data.
 Although MQDF is a state-of-the-art classifier for Chinese
 handwriting recognition, many improvements have been
 proposed in the past 25 years. We first give a summariza-
 tion of the improvements on MQDF in Section II. After
 that, the motivation and definition of the newly proposed
 locally smoothed MQDF are described in Section III. The
 experimental results on both offline and online Chinese
 handwriting datasets are presented in Section IV, and the
 concluding remarks are drawn in Section V.
 II. RELATED WORKS
 Many improvements have been proposed for MQDF from
 different aspects, such as discriminative training, memory
 reduction, instance selection and generation, ensemble learn-
 ing, and discriminative feature extraction.
 Because MQDF is a generative model, Liu et al. [2]
 proposed to use the minimum classification error (MCE) [4]
 criterion for discriminative training of MQDF. After that,
 the sample separation margin criterion [5] and perceptron
 criterion [6] were also adopted for this purpose. The re-
 training of MQDF based on instance importance weight-
 ing [7], instance selection [8] and virtual instance generation
 (mirror image) [9] can be also viewed as some kinds of
 discriminative training. Discriminative training of MQDF
 can directly optimize the classification boundaries, therefore,
 high classification accuracy can be achieved especially when
 the size of training data is large.
 The memory requirement of MQDF is usually very large
 (e.g. 120MB for a 3,755-class and 160-dimension problem).
 To reduce the memory requirement of MQDF, Long and
 Jin [10] proposed to use the vector quantization and split
 quantization techniques to compress the parameters (mean-
 s, eigenvectors and eigenvalues) of MQDF. Modeling the
 inverse covariance matrices by the expansion of some tied
 basis matrices was proposed by Wang and Huo [11]. The
 precision constrained Gaussian model was combined with
 the minimum classification error (MCE) training criterion
 to simultaneously compress the parameters and improve
 the accuracy [12]. With parameter compression, the high
 accuracy MQDF classifier can be embedded into some hand-
 held devices such as mobile phones and tablet computers.
 There are also many other improvements of MQD-
 F derived from different viewpoints. The kernel MQDF
 was proposed by Yang and Jin [13] to extend MQDF
 from original feature space to kernel space (implicit high-
 dimensional space). The ensemble learning methods such as
 cascade classifier training [14], boosting [15] and pairwise
 discrimination [16] were adopted to improve the accuracy
 of MQDF. The graphical lasso method was used to estimate
 a sparse inverse covariance matrix ??1i for QDF [17]. The
 normalization of the determinant of the covariance matrix
 was shown to achieve better classification accuracy [18]. The
 asymmetric Mahalanobis distance [19] can be viewed as an
 extension of the symmetric Gaussian distribution. Combin-
 ing MQDF with the discriminative feature extraction [20]
 can further improve the performance.
 III. LOCALLY SMOOTHED MQDF
 Although MQDF is a generative model, it has a very high
 accuracy on training set (over 99%). However, the accuracy
 on testing set is much lower (around 89% and 93% for
 offline and online Chinese handwriting recognition). This
 indicates that MQDF has a good capability to fit training
 data but the generalization performance is not promising.1
 The covariance matrix estimated by maximum likelihood
 (ML) is usually under-estimated due to small sample size,
 and the large number of free parameters gives MQDF a
 strong memory of training samples (over-fitting). To improve
 the generalization performance of MQDF, we propose a new
 model of locally smoothed modified quadratic discriminant
 function (LSMQDF) by smoothing the covariance matrix of
 each class with its nearest neighbor classes. In the following
 sections, we first give a description of the ML estimation,
 and then present the details of LSMQDF. After that, we
 compare LSMQDF with a global smoothing method.
 A. Class-wise ML Estimation
 Given a training dataset {xij ? Rd} (i = 1, á á á , M and
 j = 1, á á á , ni), where ni is the number of training samples
 in class i, and M is the number of classes. Let xij denotes
 the j-th training sample from class i. The function of the
 negative log-likelihood of the data from class i w.r.t. the
 mean µ and the covariance matrix ? can be defined as:
 NLL(µ,?, i) = ?
 ni·
 j=1
 log p(xij |i)
 ?
 ni·
 j=1
 (xij ? µ)??1(xij ? µ) + ni log |?| .
 (5)
 The maximum likelihood (ML) estimation of µi and ?i
 is conducted class-by-class (i = 1, á á á , M ) as:
 {µi,?i} = argmin
 µ,?
 NLL(µ,?, i) . (6)
 By solving this convex optimization problem, the ML esti-
 mation is:
 µi =
 1
 ni
 ni·
 j=1
 xij , (7)
 ?i =
 1
 ni
 ni·
 j=1
 (xij ? µi)(xij ? µi) . (8)
 1This explains why discriminative training of MQDF can only improve
 the testing accuracy slightly.
 9
B. Locally Smoothed MQDF
 The class-wise ML estimation is proven to be sensitive
 to small sample size (small ni), and the estimation error in
 covariance matrix (8) is more serious than the estimation
 error in mean vector (7). To achieve a robust estimation, we
 propose a local smoothing method:
 ÷?i = argmin
 ?
 (1? ?) NLL(µi,?, i) +
 ? 1
 K
 ·
 j?KNN(i)
 NLL(µj ,?, j) , (9)
 where KNN(i) denotes the K nearest neighbors of class
 i from the remaining classes (defined by the Euclidean
 distance between the class means). The first term of (9) is the
 ML estimation and the second term is a local smoothing of
 the covariance matrix with the nearest neighbor classes. The
 ? ? [0, 1] is a tradeoff parameter. Because the estimation of
 the class-mean is accurate enough, µi is fixed as the ML
 estimation (7). The new covariance matrix ÷?i is estimated
 to maximize the likelihood of not only class i but also the
 neighboring classes. Therefore, both the samples from class
 i and the neighboring classes are used simultaneously to
 achieve a robust estimation. This is based on the assumption
 that the covariance matrices of neighboring classes are close
 to each other, which is satisfied in handwriting recognition,
 because similar characters usually have the same types of
 distortion (variation).
 The optimization problem of (9) is also a convex problem
 which has a closed-form solution:
 ÷?i =
 (1? ?)ni?i + ? 1K
 ·
 j?KNN(i)
 nj?j
 (1? ?)ni + ? 1K
 ·
 j?KNN(i)
 nj
 , (10)
 where ?i is the ML estimation (8). From this definition, we
 can find that ÷?i is a smoothing of ?i with the neighboring
 classes ?j : j ? KNN(i). Therefore, ÷?i should be more
 accurate than ?i especially when ni is small. By using
 µi (7) and÷?i (10) to train the MQDF classifier (Section I-B),
 much better generalization performance can be achieved, and
 we denote this method as LSMQDF. With local smoothing,
 some discriminative information of the training data is lost,
 but the new covariance matrix ÷?i will be more robust.
 Therefore, although the accuracy on training set is decreased
 (over-fitting avoided), the accuracy on testing set will be
 improved (generalization improved).
 C. Local Smoothing V.S. Global Smoothing
 LSMQDF can be viewed as an extension of the global
 smoothing method, namely regularized discriminant analy-
 sis (RDA) [21], which constrains the range of parameter
 values by interpolating the class covariance matrix with the
 common covariance matrix ?0 and the identity matrix I:
 ??i = (1? ?)[(1? ?)?i + ??0] + ??2i I, (11)
 where ?0 = (·Mi=1 ni?i)/(·Mi=1 ni), ?2i = 1d tr(?i), and
 0 ² ?, ? ² 1. With appropriate values of ? and ?
 empirically selected, RDA can improve the generalization
 performance, and some special cases are included: (i) o-
 riginal QDF: ? = ? = 0; (ii) linear discriminant function
 (LDF): ? = 0, ? = 1; and (iii) Euclidean distance: ? = 1.
 Improvements can be achieved when using µi (7) and
 ??i (11) to train the MQDF classifier (Section I-B), and we
 still denote this method as RDA for simplification.
 RDA is a global smoothing method, while LSMQDF is
 a local smoothing method. When the number of classes
 is large, global smoothing can not achieve any improve-
 ment, while local smoothing is still effective to improve
 the accuracy. This is because we can not assume all the
 covariance matrices to be close to each other, and contrarily,
 the neighboring classes are more likely to have the same
 covariance matrices. The superior performance of LSMQDF
 is also verified by experiments in the following sections.
 IV. EXPERIMENTS
 We evaluated the performance of LSMQDF on two 3,755-
 class Chinese handwriting databases [22]: the offline hand-
 writing database CASIA-HWDB1.1 and the online hand-
 writing database CASIA-OLHWDB1.1. Both of them con-
 tain handwritten Chinese characters from 300 writers (240
 for training and 60 for testing). Each writer has about 3,755
 characters (one for each class). The extracted feature data
 can be downloaded from our website.2
 A. LSMQDF for Offline Handwriting Recognition
 For representing an offline character sample, we extract
 features from gray-scale character images (back-ground e-
 liminated) using the normalization-cooperated gradient fea-
 ture (NCGF) method [23]. The original feature dimension-
 ality is 512 which is further reduced to 160 by Fisher linear
 discriminant analysis (FDA).
 For the LSMQDF method, we set K = 10 and ? = 0.5
 for (10). Sensitive analysis of these parameters will be shown
 in the following sections. For the RDA method (11), we have
 ?0 = I (the common covariance matrix is I in the FDA
 transformed subspace), and now the RDA can be simplified
 as??i = (1??)?i+??2i I and we report the best performance
 of RDA on testing set by searching ? from 0 to 1.
 The accuracies of MQDF, RDA and LSMQDF on both
 training set and testing set with various numbers of principal
 eigenvectors are shown in Figure 1(a). We can find that:
 (i) with either global smoothing (RDA) or local smoothing
 (LSMQDF), the training accuracy is decreased (over-fitting
 avoided), and the testing accuracy is improved (generaliza-
 tion improved); (ii) LSMQDF outperforms RDA consistently
 and significantly on the testing set; and (iii) compared with
 the baseline MQDF, LSMQDF is effective to improve the
 2http://www.nlpr.ia.ac.cn/databases/handwriting/Download.html
 10
97.5
 98.5
 99.5
 93.5
 94.5
 95.5
 96.5
 u
 ra
 cy
 (%
 )
 MQDFTraining MQDFTesting
 RDATraining RDATesting
 88.5
 89.5
 90.5
 91.5
 92.5
 Ac
 c
 LSMQDFTraining LSMQDFTesting
 87.5
 5 10 15 20 25 30 35 40 45 50 55
 Numberofprincipaleigenvectors
 (a)
 89 5
 90
 90.5
 88
 88.5
 89
 .
 a
 cc
 u
 ra
 cy
 (%
 )
 86.5
 87
 87.5
 Te
 st
 in
 g MQDF
 LSMQDF
 86
 50 60 70 80 90 100 110 120 130 140 150 160 170
 Dimensionality
 (b)
 89.5
 90.5
 87.5
 88.5
 g
 a
 cc
 u
 ra
 cy
 (%
 )
 85.5
 86.5
 Te
 st
 in
 g MQDF
 LSMQDF
 84.5
 40 60 80 100 120 140 160 180 200 220 240
 Numberoftrainingsamplesperclass
 (c)
 90.2
 90.4
 89 6
 89.8
 90
 g
 a
 cc
 u
 ra
 cy
 (%
 )
 MQDF
 LSMQDF
 89.2
 89.4
 .
 Te
 st
 in
 g
 89
 3 5 7 9 11 13 15 17 19 21 23 25 27 29
 NumberofnearestneighborsinLSMQDF
 (d)
 90.3
 90.5
 89.7
 89.9
 90.1
 g
 a
 cc
 u
 ra
 cy
 (%
 )
 MQDF
 LSMQDF
 89.3
 89.5
 Te
 st
 in
 g
 89.1
 0.1 0.2 0.3 0.4 0.5 0.6 0.7
 Valueof inLSMQDF
 (e)
 99
 100
 96
 97
 98
 cu
 ra
 cy
 (%
 )
 MQDFTraining MQDFTesting
 RDA Training RDA Testing
 93
 94
 95Ac
 c  
 LSMQDFTraining LSMQDFTesting
 92
 5 10 15 20 25 30 35 40 45 50 55
 Numberofprincipaleigenvectors
 (f)
 Figure 1. (a) Different models for offline handwriting recognition; (b) MQDF v.s. LSMQDF on different dimensions; (c) MQDF v.s. LSMQDF with
 different number of training samples; (d) LSMQDF with different numbers of K in (10); (e) LSMQDF with different values of ? in (10); (f) Different
 models for online handwriting recognition.
 testing accuracy, e.g., from 89.53% to 90.27% when the
 number of principal eigenvectors is 50.
 B. Varying Dimensionality
 By using FDA to reduce the dimensionality from 512 to
 different subspaces, we compare the performance of MQDF
 and LSMQDF (K = 10 and ? = 0.5 for (10) and the number
 of principal eigenvectors is 50). The results are shown in
 Figure 1(b). When the dimensionality is small, the difference
 between MQDF and LSMQDF is not significant, but with
 the increasing of dimensionality, the difference becomes
 larger and larger. This is because the curse of dimensionality,
 i.e., in the high-dimensional space, the training data are not
 sufficient to get an accurate parameter estimation, and in
 this case, smoothing is an important tool for improving the
 generalization performance.
 C. Varying the Size of Training Data
 We also compare the performance of MQDF and
 LSMQDF with different number of training samples. In this
 case, we fix the dimensionality as 160, use K = 10, ? = 0.5
 for (10), and set the number of principal eigenvectors as
 50. The results are shown in Figure 1(c). We can find that:
 LSMQDF can improve the testing accuracy consistently and
 significantly, especially when the number of training samples
 is small (e.g. the accuracy is improved from 84.79% to
 86.80% when there are only 60 training samples per class).
 D. On Selection of K
 In this section, we evaluate the performance of LSMQDF
 by varying the number of nearest neighbors K in (10).
 Other parameters are fixed as the same of Section IV-C.
 The results are shown in Figure 1(d). When K = 3, the
 testing accuracy of LSMQDF is already higher than MQDF,
 and with the increasing of K, the performance is further
 improved. When 10 ² K ² 30, the accuracy is nearly
 not changed. Therefore, the performance of LSMQDF is not
 sensitive to the selection of K.
 E. On Selection of ?
 We also evaluate the performance of LSMQDF by varying
 the values of the tradeoff parameter ? in (10). Other param-
 eters are fixed as the same of Section IV-C. The results
 are shown in Figure 1(e). When ? = 0.1, the accuracy of
 LSMQDF is higher than MQDF, because the information of
 neighboring classes is incorporated into the estimation of the
 covariance matrix. With the increasing of ?, the performance
 is further improved. However, when ? > 0.5, the accuracy is
 decreased due to the over-smoothing. Therefore, in practice,
 we should pay attention to the value of ?, and ? = 0.5 is
 shown to be a good choice in our experiments.
 F. LSMQDF for Online Handwriting Recognition
 In this section, we use LSMQDF for online handwriting
 recognition. For representing an online character sample,
 11
we use a benchmark feature extraction method [24]: 8-
 direction histogram feature extraction combined with pseudo
 2D bi-moment normalization (P2DBMN). We also add the
 direction values of off-strokes (pen lifts) to real strokes with
 a weight of 0.5 [25]. The settings of different parameters
 are the same as Section IV-A.
 The results are shown in Figure 1(f). Although MQDF
 can achieve much higher testing accuracy for online data
 (compared with offline data), LSMQDF is still effective
 in improving the generalization performance, e.g., from
 93.22% to 93.70% when the number of principal eigen-
 vectors is 50. The local smoothing of LSMQDF again
 outperforms the global smoothing method of RDA.
 V. CONCLUSION
 To improve the generalization performance of MQDF,
 we proposed a locally smoothed modified quadratic dis-
 criminant function (LSMQDF) by smoothing the covariance
 matrix of each class with its neighboring classes. The idea of
 local smoothing is simple and effective. In the future, we will
 explore the local smoothing from the theoretical viewpoint
 such as Bayesian learning [26]. Using the parameters of
 LSMQDF as initialization for discriminative training [2],
 and combining LSMQDF with improved dimensionality
 reduction methods [27] can further improve the accuracy.
 ACKNOWLEDGEMENTS
 This work was supported in part by the National Natural
 Science Foundation of China (NSFC) Grant 60933010 and
 National Basic Research Program of China (973 Program)
 Grant 2012CB316302.
 REFERENCES
 [1] F. Kimura, K. Takashina, S. Tsuruoka, and Y. Miyake, ÒMod-
 ified quadratic discriminant functions and the application to
 Chinese character recognition,Ó IEEE Trans. Pattern Analysis
 and Machine Intelligence, 1987.
 [2] C.-L. Liu, H. Sako, and H. Fujisawa, ÒDiscriminative learning
 quadratic discriminant function for handwriting recognition,Ó
 IEEE Trans. Neural Networks, 2004.
 [3] X.-Y. Zhang and C.-L. Liu, ÒWriter adaptation with style
 transfer mapping,Ó IEEE Trans. Pattern Analysis and Machine
 Intelligence, 2013.
 [4] B.-H. Juang and S. Katagiri, ÒDiscriminative learning for min-
 imum error classification,Ó IEEE Trans. Signal Processing,
 1992.
 [5] Y. Wang and Q. Huo, ÒSample separation margin based min-
 imum classification error training of pattern classifiers with
 quadratic discriminant functions,Ó IEEE IntÕl Conf. Acoustics
 Speech and Signal Processing, 2010.
 [6] T.-H. Su, C.-L. Liu, and X.-Y. Zhang, ÒPerceptron learning of
 modified quadratic discriminant function,Ó IntÕl Conf. Docu-
 ment Analysis and Recognition, 2011.
 [7] Y. Wang, X. Ding, and C. Liu, ÒMQDF discriminative learn-
 ing based offline handwritten Chinese character recognition,Ó
 IntÕl Conf. Document Analysis and Recognition, 2011.
 [8] Y. Wang, X. Ding, and C. Liu, ÒMQDF retrained on selected
 sample set,Ó IEICE Trans. Information and Systems, 2011.
 [9] H. Liu and X. Ding, ÒHandwritten Chinese character recog-
 nition based on mirror image learning and the compound
 Mahalanobis function,Ó J. Tsinghua Univ. (Sci. & Tech.) (in
 Chinese), 2006.
 [10] T. Long and L. Jin, ÒBuilding compact MQDF classifier
 for large character set recognition by subspace distribution
 sharing,Ó Pattern Recognition, 2008.
 [11] Y. Wang and Q. Huo, ÒModeling inverse covariance matrices
 by expansion of tied basis matrices for online handwritten
 Chinese character recognition,Ó Pattern Recognition, 2009.
 [12] Y. Wang and Q. Huo, ÒBuilding compact recognizers of
 handwritten Chinese characters using precision constrained
 Gaussian model, minimum classification error training and
 parameter compression,Ó IntÕl J. Document Analysis and
 Recognition, 2011.
 [13] D. Yang and L. Jin, ÒKernel modified quadratic discriminant
 function for online handwritten Chinese characters recogni-
 tion,Ó IntÕl Conf. Document Analysis and Recognition, 2007.
 [14] Q. Fu, X. Ding, and C. Liu, ÒCascade MQDF classifier for
 handwritten character recognition,Ó J. Tsinghua Univ. (Sci. &
 Tech.) (in Chinese), 2008.
 [15] Q. Fu, X. Ding, and C. Liu, ÒA new adaboost algorithm
 for large scale classification and its application to Chinese
 handwritten character recognition,Ó IntÕl Conf. Frontiers in
 Handwriting Recognition, 2008.
 [16] T.-F. Gao and C.-L. Liu, ÒHigh accuracy handwritten Chinese
 character recognition using LDA-based compound distances,Ó
 Pattern Recognition, 2008.
 [17] B. Xu, K. Huang, I. King, C.-L. Liu, J. Sun, and N. Satoshi,
 ÒGraphical lasso quadratic discriminant function and its ap-
 plication to character recognition,Ó Neurocomputing, 2013.
 [18] T. Kawatani, ÒHandwritten Kanji recognition with determi-
 nant normalized quadratic discriminant function,Ó IntÕl Conf.
 Pattern Recognition, 2000.
 [19] N. Kato, M. Suzuki, S. Omachi, H. Aso, and Y. Nemoto,
 ÒA handwritten character recognition system using directional
 element feature and asymmetric Mahalanobis distance,Ó IEEE
 Trans. Pattern Analysis and Machine Intelligence, 1999.
 [20] C.-L. Liu, ÒHigh accuracy handwritten Chinese character
 recognition using quadratic classifiers with discriminative
 feature extraction,Ó IntÕl Conf. Pattern Recognition, 2006.
 [21] J. Friedman, ÒRegularized discriminant analysis,Ó J. American
 Statistical Association, 1989.
 [22] C.-L. Liu, F. Yin, D.-H. Wang, and Q.-F. Wang, ÒCASIA
 online and offline Chinese handwriting databases,Ó IntÕl Conf.
 Document Analysis and Recognition, 2011.
 [23] C.-L. Liu, ÒNormalization-cooperated gradient feature ex-
 traction for handwritten character recognition,Ó IEEE Trans.
 Pattern Analysis and Machine Intelligence, 2007.
 [24] C.-L. Liu and X.-D. Zhou, ÒOnline Japanese character recog-
 nition using trajectory-based normalization and direction fea-
 ture extraction,Ó IntÕl Workshop Frontiers in Handwriting
 Recognition, 2006.
 [25] K. Ding, G. Deng, and L. Jin, ÒAn investigation of imaginary
 stroke techinique for cursive online handwriting Chinese
 character recognition,Ó IntÕl Conf. Document Analysis and
 Recognition, 2009.
 [26] S. Srivastava, M. Gupta, and B. Frigyik, ÒBayesian quadratic
 discriminant analysis,Ó J. Machine Learning Research, 2007.
 [27] X.-Y. Zhang and C.-L. Liu, ÒEvaluation of weighted Fisher
 criteria for large category dimensionality reduction in appli-
 cation to Chinese handwriting recognition,Ó Pattern Recogni-
 tion, 2013.
 12

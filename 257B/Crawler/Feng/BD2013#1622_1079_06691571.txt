GPU Accelerated Item-Based Collaborative Filtering for Big-Data Applications
 Chandima Hewa Nadungodage, 
Yuni Xia 
Department of Computer & 
Information Science 
Purdue School of Science 
IUPUI 
Indianapolis, USA 
{chewanad, yuxia}@iupui.edu 
 
John Jaehwan Lee 
Department of Electrical & 
Computer Engineering 
Purdue School of Engineering & 
Technology 
IUPUI 
Indianapolis, USA 
johnlee@iupui.edu 
Myungcheol Lee,  
Choon Seo Park 
Big-Data Software Platform 
Research Department 
Software Research Laboratory 
Electronics & Telecommunications 
Research Institute, Korea 
{mclee, parkcs}@etri.re.kr
   
Abstract—Recommendation systems are a popular marketing 
strategy for online service providers. These systems predict a 
customer’s future preferences from the past behaviors of that 
customer and the other customers.  Most of the popular online 
stores process millions of transactions per day; therefore, 
providing quick and quality recommendations using the large 
amount of data collected from past transactions can be 
challenging. Parallel processing power of GPUs can be used to 
accelerate the recommendation process. However, the amount 
of memory available on a GPU card is limited; thus, a number 
of passes may be required to completely process a large-scale 
dataset. This paper proposes two parallel, item-based 
recommendation algorithms implemented using the CUDA 
platform. Considering the high sparsity of the user-item data, 
we utilize two compression techniques to reduce the required 
number of passes and increase the speedup. The experimental 
results on synthetic and real-world datasets show that our 
algorithms outperform the respective CPU implementations 
and also the naïve GPU implementation which does not use 
compression.  
Keywords - recommendation systems; GPU; CUDA; 
collaborative filtering; big-data 
I. INTRODUCTION 
Internet-based service providers such as Amazon and 
Netflix widely use Collaborative Filtering (CF) to provide 
personalized recommendations based on past behaviors of the 
users. CF-based recommender systems can be divided into 
two categories; the user-based CF and the item-based CF [1], 
[2]. Due to the large volume of transactions generated by the 
online stores, providing quick and quality recommendations 
is a challenging task. 
Graphics Processing Units (GPUs) are designed to handle 
highly parallel workloads and can execute thousands of 
concurrent threads. With the introduction of CUDA (Compute 
Unified Device Architecture)1  platform, a general purpose 
parallel computing architecture, GPU computing has become 
more and more popular in general-purpose, large-scale, data 
mining applications [3-5]. Parallel processing power of GPUs 
can be used to speed up the CF process [6-8]. However, the 
amount of memory available on a GPU card is limited; 
                                                          
1 http://www.nvidia.com/object/cuda_home_new.html 
therefore, a number of runs may be required to completely 
process a large-scale dataset. This limits the performance gain 
that can be achieved by using GPUs. In most scenarios, the 
user-item data is very sparse because there are large numbers 
of users and large variety of items but each customer only 
purchases a relatively small number of items. Thus, 
representing the user-item data in a compressed format can 
significantly reduce the memory requirements and the 
processing time. The existing CUDA implementations [6-8] 
of user-based CF systems do not consider the sparsity of the 
user-item data.  
Various studies have shown that the item-based CF 
methods outperform the user-based CF methods in many 
scenarios [1], [2], [9], [10]. Due to its ability to scale to large 
numbers of users and its predictive accuracy, item-based CF 
is used by many large-scale online service providers such as 
Amazon, Google, YouTube, and Netflix. Some 
recommendation algorithms compute recommendations 
based on the user ratings of the items. However, most of the 
times, user ratings are not available but only the purchase 
information. In such situations, Deshpande and Karypis [1] 
have shown that the item-based CF approach combined with 
the conditional probability-based similarity measure produces 
higher-quality recommendations than the user-based CF 
approach.  
The conditional probability-based similarity of two items 
depends on the co-occurrence of those two items in the 
transaction history. Thus, it is only required to know whether 
the user has purchased an item or not (i.e. the user rating of 
the item is not necessary). This property of conditional 
probability-based similarity measure enables us to compress 
the user-item data and significantly reduces the memory 
requirements, thereby making it a good candidate for GPU 
implementation. In this paper we propose two GPU 
accelerated item-based CF algorithms, using conditional 
probability as the similarity measure. Considering the high 
sparsity of the user-item data, we utilize compression 
techniques to reduce the required number of passes to 
completely process large-scale datasets. Data compression 
also allows us to reduce the number of computations required 
in populating the item-item similarity matrix. The experiments 
2013 IEEE International Conference on Big Data
 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 175
show that the proposed GPU implementations are very 
efficient in processing large-scale datasets in a timely manner 
and achieved up to 30X and 125X speedups compared to the 
respective CPU implementations and up to 5X and 18X 
speedups compared to the naïve GPU implementation which 
does not use data compression. 
II. RELATED WORK 
There is very limited work on using GPUs to accelerate 
recommendation system applications. Kato and Hosino [6] 
proposed a CUDA implementation of user-user k-nearest 
neighbor search for user-based CF. Li et al. [7] proposed a 
social network-aware top-N recommender system using 
GPUs. This approach also uses user-based CF and requires 
social network information to produce recommendations. 
Both above work do not consider the sparsity of the user-item 
matrix, and thus, require a number of passes to process large-
 scale datasets using GPUs. Zhanchun and Yuying [8] also 
proposed a GPU accelerated user-user similarity calculation 
method, emphasizing on addressing the accuracy limitation of 
user-based CF techniques by using implied similarity and 
default values to fill the missing values. This might increase 
the accuracy of the recommendation process, but still require 
a number of passes to completely process large-scale datasets 
as the amount of memory available in GPU is limited.  
Our proposed methods consider the sparsity of user-item 
data and use compression techniques to reduce the number of 
runs required to process large-scale datasets in GPU, thereby 
increasing the possible speedup. Furthermore, our methods 
are based on item-item CF which is proven to be more accurate 
and scalable than user-user CF methods [1], [2], [10]. 
III. PRELIMINARIES 
A. Item-based Collaborative Filtering 
Let n denote the number of distinct users and m denote the 
number of distinct items in the dataset. Then, this dataset can 
be represented by an n?m binary matrix R that will be referred 
to as the user-item matrix, such that Ri,j is one if the ith user has 
purchased the jth item, and zero otherwise. Let N denote the 
number of recommendations that need to be computed for a 
particular user. The top-N recommendation problem is 
formally defined as follows: 
Given the user-item matrix R and the set of items Xui 
purchased by a given user ui (i = 1 to n), identify a set of items 
Xri (Xui ??Xri = {} and |Xri | ? N) which is most likely to be 
purchased by the user ui. 
Item-based CF process consists of two stages. The first 
stage is to compute the similarities between the different 
items; this produces an m?m similarity matrix S. Each column 
j in S is then sorted to get the top-k similar items for a 
particular item j. The second stage is to derive the top-N 
recommendations for a particular user ui, using the sorted 
similarity matrix S and the items already purchased by the user 
Xui. The focus of this paper is on accelerating the first stage 
(i.e. item-item similarity calculation) using GPUs. Since the 
similarity of each pair of items <Ii, Ij> can be individually 
calculated without depending on the other items, this stage can 
be highly parallelized using GPUs.  
B. Conditional Probability-Based Similarity Calculation 
The similarity between a pair of items i and j can be 
calculated based on the conditional probability of purchasing 
one of the items given that the other has already been 
purchased. The conditional probability of purchasing item j 
given that item i has already been purchased [P(j|i)] is defined 
as the number of users that purchase both items i and j divided 
by the total number of users that purchased i. If P(j|i) is 
directly used as the similarity measure, each item i will tend 
to have high similarities with the frequently purchased items 
(such as milk or bread), thus those items will appear on the top 
of the recommendation list. According to Kitts et al. [10], this 
problem can be corrected by penalizing the frequently 
purchased items by dividing P(j|i) with a quantity that depends 
on the frequency of item j, thus avoiding them dominating the 
recommendations. 
In the original form, the rows of the user-item matrix R 
correspond to the binary purchase information, in which case, 
the similarity calculation gives equal weight to all users. It is 
shown in [1] that the co-purchasing information derived from 
the users that have bought fewer items is a more reliable 
indicator for the similarity of two co-purchased items than the 
information derived from the users that tend to buy a large 
number of items. By scaling each row of R to be of unit length, 
we can make users that have purchased fewer items contribute 
higher weight to the similarity measure than the users that 
have purchased more items. Based on these facts, Deshpande 
and Karypis [1] proposed the following formula to compute 
the similarity between two items using the normalized user-
 item matrix R [1], 
 ???(?? , ??)  =  
? ??,???:{??,?  ?? && ??,???} 
[????(??) ? (????( ?? ))?]
  (1) 
Here, ?????????????????????????????????????????????????????
 and 1. The numerator in (1) is the sum of the corresponding 
entries of the jth column in the matrix R, for the set of the users 
who have purchased both items i and j. In the remainder of the 
paper we refer this as “weighted co-occurrence frequency”. 
IV. PROPOSED APPROACH 
A. CUDA Implementation 
From a programmer’s point of view, the CUDA 
programming model is a collection of threads running in 
parallel. A CUDA program consists of a host program running 
on the host CPU, and one or more parallel kernel functions 
executed on a GPU. Each kernel is executed by multiple 
blocks, and each block contains multiple threads. We split the 
similarity matrix computation into three steps and use a 
separate GPU kernel function for each step. First, Kernel 1 
computes the weighted item-item co-occurrence frequencies 
(numerator in (1)) for possible pairs of items. Then, using that 
information Kernel 2 computes the item-item similarity 
according to (1). Finally, Kernel 3 sorts each column of the 
similarity matrix to find top-k similar items for each item. 
Before starting the GPU computation, input data (user-
 item matrix) needs to be transferred from the host memory to 
the GPU’s memory which is called the global memory. If the 
176
GPU does not have enough global memory space to hold the 
entire input dataset, multiple iterations are required to 
complete processing the entire dataset. When the user-item 
matrix is very large, this will have an effect on the runtime of 
the algorithm and reduce the performance gain. To address 
this issue, we propose two memory efficient implementations 
for step 1 (weighted co-occurrence frequency calculation).  
The first method, GPU_BP_CF (GPU accelerated Bit-
 Packed Collaborative Filtering), uses bit packing to compress 
the data to reduce the memory requirement and increase the 
speedup. The second method, GPU_CMP_CF (GPU 
accelerated Compact Collaborative Filtering), uses a compact 
format of the user-item matrix which only stores non-zero 
elements. When user-item matrix is sparse, this method also 
reduces the memory requirement and gives significant 
speedup. 
B. GPU_BP_CF Algorithm 
In this implementation we do not normalize the rows of R 
beforehand. Each column of the user-item matrix is packed 
into n/32 (n is the number of users)2 32-bit integers; such bit-
 packing reduces the size of input data by 32 times, making it 
possible to store more transactions in the GPU memory at a 
time; thus reducing the number of iterations require to process 
the entire dataset.  We also compute the weight of each user 
wu (which is the reciprocal of the number of items purchased 
by the user) and transfer this information separately to the 
GPU to be used for row normalization. 
To facilitate coalesced memory access, the user-item 
matrix is stored in the transposed (item-user) format in GPU 
memory. Each column of the user-item matrix is packed into 
32-bit integers and then stored as a row in the item-user 
matrix. Fig. 1 depicts the bit packing process and computing 
the co-occurrence frequency of the pair of items Ii and Ij. 
When the data sparsity is high bit packing allows to efficiently 
rule-out the users who have not purchased the interested pair 
of items without checking the individual bits. This is done 
using bitwise AND operation and significantly improves the 
processing time. After performing the bitwise AND operation, 
we only need to unpack the entries with non-zero values.  
Then we multiply the non-zero bits with weight of the 
corresponding user (wu) and take the summation to get the 
weighted co-occurrence frequency. 
 
Figure 1.  GPU_BP_CF algorithm - compressing the user-item matrix 
using bit-packing. 
                                                          
2 If n is not a multiple of 32, add padding bits of zeros to the 
end. 
1) Kernel 1 – Weighted co-occurrence frequency 
calculation 
As opposed to the serial CPU implementation, in GPU we 
can compute the co-occurrence frequency of multiple pairs of 
items in parallel. We divide the item-item similarity matrix 
into m x By two dimensional blocks, and each block is divided 
into Tx x Ty threads. The value of Tx needs to be a multiple of 
32 to utilize full power of coalesced memory access. Then 
value of By is given by m/Ty. Each row in each thread block 
computes the co-occurrence frequency of the pair of items <Ix, 
Iy> where Ix = bx and Iy = by*Ty + ty; here (bx, by) is the block 
index and ty is the row index within the block. Each row of 
threads reads the corresponding Tx elements (or columns) 
from the bit-packed item-user matrix and computes the 
weighted sum in parallel.  This is repeated (n/32)/Tx times 
until all the elements of the corresponding row are processed. 
Fig. 2 illustrates the block and thread layout for Kernel 1 
implementation of GPU_BP_CF algorithm. 
 
Figure 2.  GPU_BP_CF Algorithm - thread layout for Kernel1 - 
computing weighted co-occurrence frequency of items in parallel. 
2) Kernel 2 – Compute the similarity values  
Once Kernel 1 is executed for all grids, we have the 
weighted co-occurrence frequencies for all pairs of items. 
Using this information Kernel 2 computes the similarity 
values for each pair of items in parallel. The similarity matrix 
is divided into BxB blocks of TxT threads. The value of T 
needs to be a multiple of 32 to achieve coalesced global 
memory access. Then value of B is given by m/T. Each thread 
computes the similarity of the pair of items <Ix, Iy> according 
to (3). Item indices are given by Ix = bx*T + tx and Iy = by * T 
+ ty, where (bx, by) is the block index and (tx, ty) is the thread 
index within the respective block. 
3) Kernel 3 – Sort the similarity matrix 
Once the item-item similarities are computed, we need to 
find the top-k similar items for each item. Kernel 2 sorts 
columns of the similarity matrix in parallel to get top-k similar 
items for each item. Each column is only required to be sorted 
partially until we get the top-k items. The value of k is 
relatively small compared to the number of items. Therefore, 
in Kernel 3 we use the partial insertion sort algorithm 
implemented by Garcia et al. [4]. 
177
C. GPU_CMP_CF Algorithm
 In this algorithm we compress the user-item matrix by 
storing only the non-zero entries. As shown in Fig. 3 we use 
two arrays to store the compressed data. In the data array, for 
each user, we store the corresponding item numbers of the 
items purchased by that user. Row boundary for each user is 
maintained in a separate indices array of size (n+1) where n 
is the number of users. For each user we record the 
corresponding transaction starting index of the data array. For 
the last user we need to store the transaction ending index as 
well. Using this compact format significantly reduces the data 
volume; thus, reducing the number of iterations required to 
process the entire dataset. Moreover, if the data is 
uncompressed, it is required to compute the co-occurrence 
frequency of all possible pairs of items. When we use the 
compact format, it is only required to count the item pairs that 
actually occurred in the transactions. Here also we do not 
normalize the rows of R beforehand. The number of items 
purchased by a particular user can be found using the indices 
array, and will be used for normalization at the time of 
weighted co-occurrence frequency calculation. 
 
Figure 3.  Compressing the user-item matrix into compact data and indices 
arrays. 
Kernel 1 – Weighted co-occurrence frequency calculation 
Fig. 4 depicts the block and thread layout for Kernel 1 in 
GPU_CMP_CF algorithm. The data array is divided into G 
grids. The number of grids depends on the available GPU 
memory. The GPU loads and processes one grid at a time. 
Using the compact format reduces the number of iterations 
required. Each grid is divided into 1 x B one dimensional 
blocks which work in parallel. Each block is divided into Tx x 
Ty threads. The value of Tx needs to be a multiple of 32 to 
utilize the full power of coalesced memory access. Then the 
value of B is given by n/Ty. Each row of threads in each thread 
block processes a corresponding user of the data array given 
by the index u = by * Ty + ty, where by is the block index and ty
 is the thread index within the respective block. For a particular 
user index u, row length (i.e. the number of items purchased 
by the user (Cu)) is found using the indices array. Each thread 
tx finds the co-occurring items Iy for the item Ix from the set of 
items purchased by the user u and atomically increases the 
weighted co-occurrence frequency count of the respective 
index <Ix, Iy> in the item-item similarity matrix by 1/Cu. 
Kernel 1 is executed on all G grids, one grid at a time, 
before proceeding to the other kernels. Implementations of 
                                                          
3 http://www.flixster.com/ 
Kernel 2 and Kernel 3 are similar to what is described in the 
previous section. 
 
Figure 4.   GPU_CMP_CF Algorithm - thread layout for Kernel1 - 
computing weighted co-occurrence frequency of items in parallel. 
V. RESULTS AND OBSERVATIONS 
We compared the performance of our parallel algorithms 
with the respective serial implementations on the CPU and 
with a naïve GPU implementation which does not use data 
compression. The CPU version of the bit-packed algorithm is 
named as CPU_BP_CF and the CPU version of the compact 
matrix algorithm is named as CPU_CMP_CF. The naïve GPU 
implementation is named as GPU_UNCMP_CF. 
The GPU versions of the algorithms are implemented 
using C++ and CUDA, and CPU versions are implemented in 
C++. All the experiments were carried out on a server running 
64bit Fedora 17 with an Intel Xeon 3.3GHz CPU and 64GB 
memory, and an NVIDIA GeForce GTX680 GPU card with 
2GB memory, CUDA runtime version 5.0, and compute 
capability 3.0. We evaluated the performance and scalability 
of the proposed algorithms using synthetic datasets of 
different volumes and sparsity. We also tested our algorithms 
on real-world datasets derived from the movie rental website 
Flixster3. The runtimes reported here are the total runtimes for 
the respective versions, including the time to read input data 
from the database and the data transfer time between the CPU 
and the GPU. 
A. Runtime comparison with data volume 
To test the runtime variation of the algorithms according 
to the increasing number of items, we fixed the number of 
users to 100K and increased the number of items from 5000 
to 10000. The sparsity of the datasets is set to 90%, k is set to 
???????????????????????? First we compared the runtime of the 
proposed GPU implementations with the respective serial 
CPU implementations. Fig. 5 presents the runtime comparison 
and Fig. 6 presents the respective CPU/GPU speedups. It is 
visible that the runtime of the CPU versions increases rapidly 
with the increasing number of items, whereas the runtime of 
the GPU versions scales well with the increasing number of 
items. The GPU_CMP_CF algorithm has above 20X speedup 
compared to its CPU counterpart, and the speedup slightly 
increases with the increasing number of items. The 
GPU_BP_CF algorithm has above 60X speedup compared to 
178
its CPU counterpart, and there is more noticeable increase in 
the speedup (up to 80X) with the increasing number of items. 
 
Figure 5.  Runtime vs number of items [sparsity 90%, #users 100K]. 
Figure 6.  CPU/GPU speedup vs number of items [sparsity 90%, #users 
100K]. 
Next we compared the runtime of the proposed GPU 
implementations with the runtime of the GPU_UNCMP_CF 
implementation which does not use any data compression. 
According to Fig. 7, it is visible that the runtimes of the 
proposed GPU implementations scale well with the number of 
items in the dataset and outperform the GPU_UNCMP_CF 
implementation which does not use any data compression.  
 
Figure 7. Comparison of GPU implementations - runtime vs number of 
items [sparsity 90%, #users 100K]. 
We also compared the runtime variation of the algorithms 
according to the increasing number of users by fixing the 
number of items to 10K and increasing the number of users 
from 50K to 100K. The sparsity of the datasets is set to 90%, 
k ??? ???? ??? ???? ???? ?? ??? ???? ??? ???? In these tests also, the 
GPU_CMP_CF algorithm achieved up to 20X speedup 
compared to its CPU counterpart, and the GPU_BP_CF 
algorithm achieved up to 80X speedup compared to its CPU 
counterpart. We have omitted the graphs due to the space 
limitations. 
B. Runtime comparison with data sparsity 
This section presents the runtime variation of the 
algorithms according to the sparsity of the user-item matrix. 
The number of users in the dataset is set to 100K and the 
number of items is set to 10K. The sparsity of the dataset is 
varied from 97% to 80%, k is set to 50, ?????????????????????Fig. 
8 presents the runtime comparison of the proposed GPU 
implementations with the respective serial CPU 
implementations and Fig. 9 presents the respective CPU/GPU 
speedups. It is visible that runtime of the CPU versions 
increases rapidly with the increasing density of the dataset, 
whereas runtime of the proposed GPU versions scales well 
with the increasing density of the dataset. The speedup 
achieved by the GPU_CMP_CF algorithm increased from 
20X to 30X whereas speedup of the GPU_BP_CF algorithm 
increased from 40X-125X. 
 
Figure 8.  Runtime vs sparsity of the dataset [#items 10K, #users 100K]. 
 
Figure 9.  CPU/GPU speedup vs sparsity of the dataset [#items 10K, 
#users 100K]. 
We also compared the runtime of the proposed GPU 
implementations with the runtime of the GPU_UNCMP_CF 
implementation which does not use data compression, at 
different sparsity levels of the dataset. Fig. 10 depicts the 
runtimes of the three GPU implementations against the 
sparsity of the dataset. When the sparsity of the dataset is over 
80%, it is visible that the proposed GPU implementations 
outperformed the GPU_UNCMP_CF implementation and 
achieved 2X-18X speed increase.  
0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
 5000 6000 7000 8000 9000 10000
 Ru
 nt
 im
 e (
 s)
 Number of itemsGPU_CMP_CF GPU_BP_CF
 CPU_CMP_CF CPU_BP_CF
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 5000 6000 7000 8000 9000 10000
 Sp
 ee
 du
 p
 Number of itemsCPU_CMP_CF/GPU_CMP_CF CPU_BP_CF/GPU_BP_CF
 0
 50
 100
 150
 200
 250
 300
 5000 6000 7000 8000 9000 10000
 Ru
 nt
 im
 e (
 s)
 Number of itemsGPU_CMP_CF GPU_BP_CF GPU_UNCMP_CF
 0
 2000
 4000
 6000
 8000
 10000
 12000
 14000
 16000
 18000
 97% 95% 93% 90% 87% 85% 83% 80%
 Ru
 nt
 im
 e (
 s)
 Sparsity of the datasetGPU_CMP_CF GPU_BP_CF
 CPU_CMP_CF CPU_BP_CF
 0
 20
 40
 60
 80
 100
 120
 140
 97% 95% 93% 90% 87% 85% 83% 80%
 Sp
 ee
 du
 p
 Sparsity of the datasetCPU_CMP_CF/GPU_CMP_CF CPU_BP_CF/GPU_BP_CF
 179
It is also noticeable that the GPU_CMP_CF algorithm is 
faster than the GPU_BP_CF algorithm when data sparsity is 
above 90% and vice versa when data sparsity below 90%. The 
reason behind this is, in the compact algorithm we only 
compute the similarity for the item pairs that actually co-occur 
in the transactions.  When the data is very sparse, the average 
number of items purchased by a user is low; thus, the required 
number of co-occurrence frequency computations is low. In 
the bit-packed implementation, although we rule out most of 
the irrelevant users using bitwise AND operations, it is still 
required to consider all possible pairs of items. When the data 
get denser, the number of co-occurring pairs of items 
increases; thus, the performance gain realized by the bit-pack 
implementation increases compared to the performance gain 
realized by the compact implementation.  
 
Figure 10.  Comparison of GPU implementations - runtime vs data sparsity 
[#items 10K, #users 100K] 
To evaluate the performance of the proposed algorithms 
on real-world data, we used the version of the Flixster dataset4  
prepared by Mohsen Jamali [11]. We only considered the 
users that have watched at least two movies. The original 
dataset consists of around 109K such users and for these users 
we extracted five different datasets with the number of items 
varying from 10K to 20K. The sparsity of these datasets is 
around 99%. The results obtained on Flixter datasets 
confirmed our observations on the synthetic datasets. 
Displaying a similar behavior to the experiments on the 
synthetic datasets, here also the speedup achieved by the 
proposed GPU implementations increased with the number of 
items in the dataset.  The GPU_CMP_CF algorithm achieved 
up to 28X speedup and the GPU_BP_CF algorithm achieved 
up to 36X speedup compared to their respective CPU 
counterparts. Both algorithms also outperformed the 
GPU_UNCMP_CF algorithm which does not use data 
compression. We have omitted the graphs due to the space 
limitations. 
VI. CONCLUSION 
In this paper we proposed two GPU accelerated similarity 
calculation algorithms for item-based collaborative filtering 
systems. Considering the sparsity of large-scale user-item 
data, and limited memory available on GPUs, we use 
compression techniques to reduce the data volume. By doing 
                                                          
4 http://www.cs.ubc.ca/~jamalim/datasets/flixster.zip 
so, we reduce the number of iterations required to completely 
process large-scale datasets and also reduce the number of 
computations required for item-item similarity calculation; 
thereby increasing the speedups achieve by the GPU 
implementations. The experimental results show that our 
proposed parallel GPU implementations outperform not only 
the respective serial CPU implementations but also the naïve 
GPU implementation which does not use data compression. 
The proposed algorithms scale well with the increasing 
volume of the user-item data and produce results in a timely 
manner, making them applicable for large-scale 
recommendation systems. 
ACKNOWLEDGMENT 
This work was supported by the IT R&D program of 
MSIP/KEIT, Korea. [10041709, Development of Key 
Technologies for Big Data Analysis and Management based 
on Next Generation Memory]. 
REFERENCES 
[1] M. Deshpande and G. Karypis, “Item based top-n 
recommendation algorithms,” ACM Transactions on 
Information Systems, 22:143–177, 2004. 
[2] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl, “Item-based 
collaborative filtering recommendation algorithms,” ACM 
International conference on World Wide Web (WWW), 285-
 295, 2001 
[3] D. Steinkraus, I. Buck, and P.Y. Simard, “Using GPUs for 
machine learning algorithms,” IEEE International Conference 
on Document Analysis and Recognition, 1115–1120, 2005. 
[4] V. Garcia, E. Debreuve, and M. Barlaud, “Fast k nearest 
neighbor search using GPU,” IEEE Computer Vision and 
Pattern Recognition Workshops (CVPRW), 1–6, 2008. 
[5] R. Farivar, D. Rebolledo, E. Chan, and R. Campbell, “A 
parallel implementation of k-means clustering on GPUs,” 
International Conference on Parallel and Distributed 
Processing Techniques and Applications (PDPTA), 340–345, 
2008. 
[6] K. Kato and T. Hosino, “Solving k-Nearest Neighbor Problem 
on Multiple Graphics Processors,” IEEE/ACM International 
Conference on Cluster, Cloud and Grid Computing, 769-773, 
2010. 
[7] R. Li, Y. Zhang, H. Yu, X. Wang, J. Wu, and B. Wei, “A social 
network-aware top-N recommender system using GPU,” 
ACM/IEEE joint conference on Digital libraries (JCDL), 287-
 296, 2011. 
[8] Z. Gao, and L. Yuying, "Improving the Collaborative Filtering 
Recommender System by Using GPU," IEEE International 
Conference on Cyber-Enabled Distributed Computing and 
Knowledge Discovery (CyberC), 2012. 
[9] G. Linden, B. Smith, and J. York, "Amazon.com 
Recommendations: Item-to-Item Collaborative 
Filtering," IEEE Internet Computing, vol. 7, no. 1, 76-80, 2003 
[10] B. Kitts, D. Freed, and M. Vrieze, “Cross-sell: A fast 
promotion-tunable customer–item recommendation method 
based on conditional independent probabilities,” ACM 
SIGKDD International Conference, 437–446, 2000. 
[11] M. Jamali and M. Ester, "A Matrix Factorization Technique 
with Trust Propagation for Recommendation in Social 
Networks,” ACM Conference on Recommender Systems 
(RecSys), 135-142, 2010.
 0
 50
 100
 150
 200
 250
 300
 97% 95% 93% 90% 87% 85% 83% 80%
 Ru
 nt
 im
 e (
 s)
 Sparsity of the dataset
 GPU_CMP_CF GPU_BP_CF GPU_UNCMP_CF
 180

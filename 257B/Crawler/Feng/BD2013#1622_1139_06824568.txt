Study of a Virtual Machine Migration Method 
Dancheng Li, Wei Wang, Quanzuo Li, DongLin Ma 
Software College 
Northeastern University, NEU 
Shenyang, 110819, China 
ldc@mail.neu.edu.cn 
 
 
Abstract—With the rapid development of cloud computing, 
physical resources of the cloud data centres have been 
increasing and how to improve the utilization of server 
resources is an important research direction. The virtual 
machine migration method proposed in this paper achieves the 
reallocation of server hardware resources by migrating the 
virtual machines between servers in order to realize the 
resource scheduling and improve the resource utilization. The 
proposed method solves the problems that how to determine 
the trigger condition of virtual machine migration, select the 
target virtual machine and choose the destination node. The 
experimental results show that the proposed method can 
improve the utilization of server resources under the cloud 
computing environment effectively. 
Cloud computing; resource scheduling; virtual machine 
migration; resource utilization  
I.  INTRODUCTION 
As the cloud computing develops rapidly, a majority of 
service providers focus on the construction of data centres, as 
a result of which the scale of data centres has been increasing 
and the energy consumption has been also constantly rising. 
Despite numerous techniques present many solutions to 
lower the energy consumption of the data centre, but the 
consumption is still growing. In order to utilize the hardware 
resources of data centre more efficiently, many scholars 
study the resource scheduling issues of data centre deeply to 
lower the energy consumption of server clusters and then 
improve the energy efficiency of server clusters. 
There are two types of resource scheduling algorithms 
which regard lowering the energy consumption of data 
centre as goal. The first type achieves the purpose of energy 
conservation by adjusting the CPU frequency dynamically 
[1]. Reference [2] presented a scheduling mechanism that 
lowers the processor speed to reduce energy consumption, 
proposed the corresponding formula of energy consumption 
and then drew the conclusion that lowering processor speed 
can lower energy consumption effectively. And the second 
type achieves the energy saving by shutting down idle server 
resources. Reference [3] presented a green cloud computing 
architecture, an energy-aware virtual machine optimization 
method and a selection algorithm for the study of energy 
consumption issue in data centre. 
In order to improve the resource utilization, current 
scheduling algorithms use the strategy that allocates and 
optimizes physical resources dynamically for virtual 
resources [4]. French Jean Marc Menaud and Hien Nguyen 
Van presented a virtual resource dynamic scheduling 
algorithm for cloud computing [5], and the algorithm gave a 
solution for choosing the virtual machine and determining 
the destination server [6]. The resource scheduling method 
proposed in this thesis achieves the reallocation of server 
hardware resources by adopting the migration of virtual 
machines between servers, and then achieves the purpose of 
resource scheduling. It determines the trigger condition of 
virtual machine migration in the process of resource 
scheduling and selects the targets involved in the process of 
virtual machine migration. The proposed method improves 
the utilization of running servers combined with the 
shutdown policy for idle servers, reduces the number of 
running servers to lower energy consumption and improve 
the energy efficiency of server clusters. 
II. DEFINITIONS AND PRELIMINARIES 
Reference [7] presented SLA (Service-Level Agreement) 
which is a contract between network service providers and 
customers to evaluate the quality of cloud services. In this 
paper, two SLA fulfilment level strategies based on 
sequential SLA strategy are defined in order to meet 
different demand, which are general strategy and strict 
strategy respectively. 
Define 2.1 SLA fulfilment level of general strategy lg  is 
defined as following 
( )
 ( )
 1 exp
 lg
 exp1 m ax 0,1 exp
 exp
         
 <
 =
 ?
 ? ? ≥
 T T
 real
 T T
 real T T
 realT
       (1) 
Define 2.2 SLA fulfilment level of strict strategy ls  is 
defined as following 
( )
 ( )
 1 T Texprealls
 0 T Texpreal
 
 <
 =
 ≥
                   (2) 
Remark 2.1 The value of lg  and ls  is between 0 and 1, 
and the higher the value is, the higher the capacity that a 
server complete tasks is. realT  is the real time length that a 
server spend in executing a specific task, and expT  is the 
expected time length to execute the same task. 
Define 2.3 given a server r and SLA levels of a task set 
( )1 2 3 nL l , l , l ,..., l= , SLA overall level rL  is defined as 
following 
2013 International Conference on Advanced Cloud and Big Data
 978-1-4799-3261-0/14 $31.00 © 2014 IEEE
 DOI 10.1109/CBD.2013.33
 27
( )ir
 n li 1L n 1
 n
 	
 =
 = ≥                         (3) 
Remark 2.2 The value of rL  is between 0 and 1, and the 
higher the value of rL  is, the higher the current computing 
capacity of server r is.  
III.
  
VIRTUAL MACHINE MIGRATION METHOD
  
The virtual machine migration method includes the 
trigger policy for virtual machine migration, the selection 
method for target virtual machine and the selection method 
for destination node [8]. The trigger policy for virtual 
machine migration points out the type of the migration tasks, 
which include upper migration task and lower migration task. 
The selection method for target virtual machine is used to 
determine the virtual machines which need to be migrated. 
The selection method for destination node is utilized to 
determine the destination servers which the target virtual 
machines are migrated to. 
A. Trigger Policy for Virtual Machine Migration 
The quality requirements of cloud platform users for 
service are relatively high, so the service quality is the 
principal consideration of dynamic scheduling algorithms. In 
order to minimize the overall impact on clusters, the switch 
timing of different servers needs to be considered. According 
to the analysis of migration steps, principal scheduling steps 
can be divided into two types. The first type is the 
determination of migration timing, and the second type is the 
selection of migrated objects, which involves the selection of 
migrated virtual machines and destination physical server. 
A factor that directly determines the migration timing is 
the setting of migration condition, which checks each test 
item of each running node including CPU utilization and 
memory utilization after the status information collection of 
all running nodes in resource pool is completed. The 
proposed trigger policy checks each item whether the 
migration conditions are satisfied, and the migration 
conditions involve two restrictions which are upper trigger 
condition and lower trigger condition respectively. Before 
migration tasks are triggered, CPU utilization’s SLA overall 
level upper and lower trigger threshold and memory’s upper 
trigger threshold need be determined. If the CPU utilization 
is higher than the upper trigger threshold, upper migration 
tasks are triggered, and if the CPU utilization is lower than 
the lower trigger threshold, lower migration tasks are 
triggered. The principal role of upper trigger condition is to 
reduce the load of high-load running nodes and ensure that 
the computing capacity of each virtual machine in these 
nodes can meet users’ needs. Then policy selects appropriate 
virtual machines from running nodes meeting upper trigger 
condition and migrates them to other running nodes to ensure 
the computing capacity of the remaining virtual machines. 
With regard to energy conservation, lower trigger 
condition is used to migrate all virtual machines of low-load 
running nodes to other running nodes and transforms these 
running nodes into idle nodes. Policy shuts idle nodes down 
and marks them as candidate nodes to reduce the number of 
running nodes, which directly lowers overall energy 
consumption of data centre and improves resource utilization 
of the remaining running nodes. 
The sensitivity of upper trigger condition is different 
from that of lower trigger condition. In terms of upper 
migration tasks, if physical resources cannot meet the needs 
of all the virtual machine instances, the performance of the 
virtual machines will be seriously affected and user 
experience will be drastically reduced. In terms of lower 
trigger condition, if the resource pool is not processed 
timely, merely some resources will be idle and energy will 
be wasted to a certain extent. So the upper trigger condition 
has priority to the lower trigger condition in the 
determination of trigger condition and the scheduling 
process. 
Policy traverses each running node, performs scheduled 
testing tasks and collects CPU utilization, memory utilization 
simultaneously. And then upper trigger and lower trigger 
condition are judged successively. Upper migration tasks 
directly affect the performance of the running nodes, while 
lower migration tasks have no effect on the performance, 
which have a lower priority compared with upper trigger 
tasks. Having completed migration tasks, policy clears the 
counters of corresponding running nodes in order to avoid 
re-triggering migration. 
Since resource utilization of cloud platforms is variable, 
time series and the relationship between target trends and 
time can be used effectively to forecast the time points when 
physical servers meet the upper and lower trigger conditions, 
and then virtual machine migration tasks can be triggered. 
B. Selection Method for Target Virtual Machine  
The selection of target virtual machine focuses on the 
migration tasks triggered by upper trigger condition. In the 
migration tasks triggered by lower trigger condition, all the 
virtual machine instances in running nodes need to be 
migrated, so we don’t need to select target virtual machine. 
In determining the target virtual machine, the principal 
issue to be considered is the time cost required for migrating 
virtual machines. In the online migration process of virtual 
machine, what needs to be done includes the copy, 
migration, recovery and run of virtual machines. Therefore 
it’s quite obvious that migration costs mainly concentrates in 
the size of the CPU and memory utilized by virtual machine 
instances. 
Since it’s impossible to evaluate how much computing 
capacity each virtual machine instance occupies actually, it’s 
difficult to define precisely how many virtual machine 
instances need to be migrated to make the SLA overall level 
of CPU satisfy the highest requirement of the remaining 
virtual machines. The flowchart of target virtual machine 
determination is shown in Figure 1. 
In the target virtual machine set’s determination of upper 
migration tasks, CPU utilization and memory utilization 
need to be taken into account. Having completed the 
migration, if the SLA overall level of CPU is still inadequate, 
the next migration task will be triggered. 
28
 
Figure 1.  Flowchart of Target Virtual Machine Determination. 
C. Selection Method for Destination Node  
In the selection process of destination node, the principal 
issue to be considered includes the selection of destination 
node set, the avoidance of cluster effect, and the setting of 
switch threshold. 
The selection method for destination node mainly solves 
the problem that how to determine the destination set D in 
the running node set, of which each node is able to 
accommodate the target virtual machine. The remaining 
CPU and memory utilization of the running nodes that can 
accommodate the target virtual machine is higher than the 
needs of target virtual machine, and the utilization of the 
existing virtual machine is lower than the threshold of 
migration trigger. In order to solve the problem of chain 
migration, the running nodes with the higher unoccupied 
CPU utilization will be selected from destination set D as the 
destination node preferentially. 
However, if the running node with the maximum 
unoccupied CPU utilization is selected, clustering effect may 
be caused. In order to avoid clustering effect, this method 
considers the specific remaining resources of each running 
node and adopts a random approach to determine destination 
node. Initially test tasks are performed on each running node 
to gain the maximum SLA overall level l max . We suppose 
that the set of the destination nodes which satisfy the 
condition is 1 2 3 nD (d ,d ,d ,...,d )=  and the current SLA 
overall level and the maximum SLA overall level of running 
node id  is il  and il max  respectively, the idle coefficient of 
running node id  is defined as following 
( )
 i i
 i n
 j jj 1
 lmax l
 I
 l max l
 =
 ?
 =
 ?	
                                (4) 
 
Figure 2.  Schematic diagram of the destination node determination. 
It’s apparent that the more the remaining resource of a 
running node is, the higher the idle coefficient is. According 
to the proportion that the idle coefficient of a running node in 
the destination set occupies, a section between 0 and 1 which 
is proportional to the proportion is determined, and the 
sections of all the destination nodes are consecutive and fill 
the interval between 0 and 1. The schematic diagram of the 
destination node determination is shown in Figure 2. 
Each section representing a destination node distributes 
between 0 and 1, and the length determines the probability 
that the section is selected. The length of a specific section is 
calculated as following 
( ) ii n
 jj 1
 Ilen d
 I
 =
 = 	
                               (5) 
Having determined the consecutive section distribution, a 
random tag between 0 and 1 is used to generate a random 
number between 0 and 1, and then the destination node is 
determined according to the section that this random number 
belongs to. The proposed method introduces random factors 
to the control of virtual machine distribution in the 
destination node determination. These random factors 
consider the idle status of specific resources sufficiently, and 
take into account the avoidance of the clustering effect and 
the balance of the overall load distribution. 
Eventually the setting of switch threshold needs to be 
taken into account. The server’s switch timing depends on 
the proportion that high-load servers occupy in all running 
nodes. The high-load servers are the running nodes whose 
monitor factor’s value is higher than the threshold. When the 
proportion of the high-load nodes exceeds 
max? , new 
running nodes need to be turned on as supplementary 
resources. When the proportion of the high-load nodes is 
lower than min? , idle nodes need to be shut down and 
marked as candidate nodes to save energy. The setting of 
these two thresholds directly affects the overall performance 
of the cluster and the average SLA overall level is also 
related to max?  and min? . The influence of these two 
thresholds on SLA is shown in Figure 3. 
 
Figure 3.  Diagram of Two Threshold Values’ Influence on SLA. 
29
 
Figure 4.  Diagram of Two Threshold Values’ Influence on Energy 
Consumption. 
In addition, the average SLA overall levels of all running 
nodes increase along with the decline of max?  and min? . The 
influence of these two thresholds on Energy Consumption is 
shown in Figure 4. 
The lower the value of max?  and min?  is, the lower the 
overall energy consumption is, which is opposite to the 
influence on SLA. As can be seen from the above discussion, 
excessive threshold setting can reduce the overall energy 
consumption of data centre, but the computing capacity may 
also be lowered, the specific needs of the virtual machine 
may be dissatisfied and the ability to deal with sudden load 
increase may be lowered. On the contrary, low threshold 
setting can ensure a sufficient number of running nodes and 
adequate overall computing capacity, but the situation of idle 
resources may be quite serious and the overall energy 
consumption may be quite high. 
IV. EXPERIMENTAL RESULTS AND ANALYSIS 
The scheduling for CPU utilization is the core issues of 
the proposed resource scheduling method, and the issues to 
be considered include the distribution of all loads and the 
resource utilization of all running nodes in the data centre. In 
order to verify these issues, relevant indicators of the 
proposed resource scheduling method will be compared with 
those of non-dynamic scheduling algorithms. 
Prior to running the proposed method, some unknown 
parameters concerning CPU work quality need to be set. In 
the experiment, the SLA overall level threshold of CPU is 
=0.97, the interval of prediction and data collection is 50 
second, and the lower migration trigger thresholds of CPU 
utilization and memory utilization are both 20%, and the 
upper migration trigger thresholds of memory utilization is 
80%. The CPU utilization of dynamic scheduling method is 
shown in Figure 5. 
 
 
Figure 5.  CPU Utilization Graph of Dynamic Scheduling Method. 
 
 
Figure 6.  CPU Utilization Graph of Non-dynamic Scheduling Method. 
As can be seen from the graph, the server’s SLA overall 
level is almost opposite to its CPU utilization, which means 
that the SLA overall level will be low when the utilization of 
CPU is quite high. In the experimental preliminary phase, the 
CPU utilization of some servers such as server 1 and server 5 
is very high, and then the trigger of upper migration tasks 
makes the CPU utilization of other servers such as server 2 
and server 4 rise. In the middle and late phase, the load of 
each server continues to decrease and the servers which 
satisfy the lower migration condition trigger lower migration 
tasks. Most of virtual machines are migrated to server 2 and 
server 3 and the consumption of the idle servers declines to 0, 
which means that the idle servers are shut down. 
In the next stage, the same load is run on the data centre 
using non-dynamic scheduling method, and the CPU 
utilization is shown in Figure 6. 
Apparent from the Figure 6, if the data centre adopts non-
 dynamic scheduling method, load aggregation exists in the 
experimental preliminary phase. The load of some servers is 
quite high while that of others hovers at 60%, and some of 
the low-load servers complete their tasks and are shut down. 
In the late phase, most of the remaining servers are in a low-
 load state. 
V. SUMMARY 
In this paper, the proposed virtual machine scheduling 
method can realize the dynamic scheduling of virtual 
machine resources by judging the migration trigger 
conditions of virtual machines, selecting target virtual 
machines and choosing destination nodes. The experimental 
results show that the proposed dynamic scheduling method 
can improve the resource utilization effectively compared 
with the non-dynamic scheduling method. However, how to 
set rational trigger thresholds and how the other secondary 
factors except CPU utilization impact servers’ energy 
consumption deserve further research. 
REFERENCES 
[1] G. von Laszewski et al., “Power Aware Scheduling of Virtual 
Machines in DVFS enabled Clusters,” Prod of IEEE International 
Conference on Cluster Computing 2009, Aug. 2009, pp. 1-10, 
doi:10.1109/CLUSTR.2009.5289182. 
[2] R. Ge, X. Feng, and K. W. Cameron, “Performance constrained 
distributed dvs scheduling for scientific applications on power aware 
clusters,” Proceedings of 2005 ACM/IEEE Conference on 
30
Supercomputing IEEE Computer Society, IEEE Computer Society, 
Nov. 2005, p. 34, doi:10.1109/SC.2005.57. 
[3] A. Beloglazov, J. Abawajy, and R. Buyya, “Energy Aware Resource 
Allocation Heuristics for Efficient Management of Data Centers for 
Cloud Computing,” Future Generation Computer Systems, vol. 28, 
May 2012, pp.755-768, doi:10.1016/j.future.2011.04.017. 
[4] D. Ergu et al., “The analytic hierarchy process: task scheduling and 
resource allocation in cloud computing environment,” The Journal of 
Supercomputing, vol. 64, June 2013, pp. 835-848, 
doi:10.1007/s11227-011-0625-1. 
[5] H. N. Van, F. D. Tran, and J. M. Menaud, “SLA Aware Virtual 
Resource Management for Cloud Infrastructures,” Ninth IEEE 
International Conference on Computer and Information Technology, 
vol. 1, Oct. 2009, pp. 357-362, doi:10.1109/CIT.2009.109. 
[6] H0 N. Van, F. D. Tran, and J. M. Menaud,  “Autonomic virtual 
resource management for service hosting platforms,” ICSE Workshop 
on Software Engineering Challenges of Cloud, IEEE Computer 
Society, May 2009, pp. 1-8, doi:10.1109/CLOUD.2009.5071526. 
[7] K. T. Kearney, F. Torelli, and C. Kotsokalis,  “SLA: An Abstract 
Syntax for Service Level Agreements,” In Proceeding of 3rd Service 
Level Agreement in Grids Workshop, Oct. 2010, pp. 217-224, 
doi:10.1109/GRID.2010.5697973. 
[8] K. R.  Keahey,  Nimbus: Cloud Computing for Science. Chicago, IL. 
 
31

Approximate triangle counting algorithms on Multi-cores
 Mahmudur Rahman and Mohammad Al Hasan
 Dept. of Computer and Information Science Indiana University—Purdue University, Indianapolis
 {mmrahman, alhasan}@cs.iupui.edu
 Abstract—Counting triangles in a large network is an
 important research task because of its usages in analyzing
 large networks. However, this task becomes expensive when
 runs on large networks with millions of nodes and millions
 of edges. For efficient triangle counting on such networks,
 researchers in recent years have adopted approximate counting
 or have proposed parallel or distributed solutions. In this
 work, we propose an approximate triangle counting algorithm,
 that runs on multi-core computers through a multi-threaded
 implementation. We show that for a given speedup factor,
 our method has a better approximation accuracy; further,
 the multi-threaded implementation that we propose is much
 superior to the Hadoop based distributed methods that earlier
 algorithms propose.
 I. INTRODUCTION
 A key task in network analysis is to count the frequency
 of various small subgraphs to discover network motifs—
 subgraphs that are significantly more frequent in a network
 relative to their occurrence in a randomized network of
 identical degree distribution [1]. Researchers have shown
 that network motifs are basic building blocks of different
 networks, including social networks, molecular interaction
 networks, and transportation networks [2], [3], [1]. To obtain
 effective algorithms for finding such motifs, researchers have
 developed a number of methods for counting the frequency
 of small subgraphs in a large networks [4], [5], [6]. Besides
 exact methods, approximate counting algorithms are also
 considered [7], [8], [9].
 Triangle, a complete graph of size three is an impor-
 tant network motif in various networks, including social
 networks, information networks, and molecular interaction
 networks. The reason for triangle being a motif is that, in
 these networks interactions among entities exhibit transitiv-
 ity property. For example, in social networks, people who
 have common friends tend to be friend themselves [10],
 which justifies the unusual high count for triangles in a
 social network compared to the same count in a randomized
 networks of similar scale. This phenomenon also prompted
 social science researchers to define two important metrics
 for graph analysis: clustering coefficient and transitivity ratio
 [11], [12]. Counting triangles is a sub-routine to obtain the
 values of these metrics. Given the importance of triangle
 count in a network, in recent years, researchers in the data
 mining community have shown an overwhelming interest in
 the problem of triangle counting [13], [14], [15], [16], [17],
 [18], [19]. Although this is an old problem in graph theory,
 the renewed interest in this problem in mainly due to the fact
 that gigantic networks with millions of nodes and billions
 of edges are being available in recent years, on which the
 existing triangle counting methods perform poorly.
 In the existing literatures, the best practical algorithm for
 counting triangles exactly has a cost of O(m3/2), where m
 is the number of edges in the graph [20]. The algorithm
 iterates over the edges of the graph, and counts the number
 of triangles in which each of the edges contributes. Such an
 algorithm is known as EDGEITERATOR method. NODEIT-
 ERATOR, a method which is dual to the EDGEITERATOR,
 iterates over the nodes and counts triangle in O(d2max · n)
 time, where n is the number of nodes, and dmax is the
 maximum degree of a node in the graph. Most of the existing
 methods belong to the EDGEITERATOR category. However,
 in this work we show that for counting triangles in most
 of the real-life graphs, the EDGEITERATOR shows a better
 performance than the NODEITERATOR.
 In terms of time complexity, the fastest triangle counting
 methods are based on fast matrix multiplication. Alon et
 al. [21] gave such an algorithm that has a time complexity
 of O(m 2??+1 ) where ? is the matrix multiplication cost; so
 the cost of this algorithm becomes O(m1.41), using the
 best known ? value which is 2.37 at present. However,
 the methods that are based on matrix multiplication require
 large amount of memory, and hence they are not suitable for
 counting triangles in very large graphs.
 For today’s large network with millions of vertices and
 edges, all the exact methods for triangle counting can be
 deemed as expensive; so, the majority of the recent efforts
 of triangle counting either adopt a method for approximate
 counting, or design a parallel or distributed framework for
 solving the counting task effectively. For approximate count-
 ing, [13] proposes DOULION , which uses a probabilistic
 sparsification technique to obtain a sparser graph; then, it
 computes the exact triangle count on the sparse graph, from
 which it extrapolates an approximate triangle count of the
 original graph. Authors of this work also offer a Hadoop
 (An implementation of MapReduce[22]) based solution for
 this work. Hadoop is also used in an exact triangle counting
 which is recently proposed by Suri and Vassilvitskii [23].
 A linear algebraic method is also proposed for approximate
 triangle counting [14],
 In this paper, we propose a variant of EDGEITERATOR
 method for approximate triangle counting. Our method has
 127
 2013 IEEE International Conference on Big Data
 978-1-4799-1293-3/13/$31.00 ©2013  IEEE 
a surprisingly high accuracy, with a generous speedup. On
 large real-life graphs with millions of nodes and edges,
 the single processor version of our algorithm consistently
 achieve a 30-fold speedup (compared to the best exact
 method) with an accuracy that is around 99%. The most
 attractive feature of our method is that, both the speedup,
 and the accuracy of our method improve as the input graph
 becomes larger, so it is particularly suitable for very large
 graphs.
 The simplicity of our algorithm also allows a simple
 multi-threaded implementation of our method for execut-
 ing on today’s multi-core architecture—this improves the
 speedup even further without harming the counting accuracy.
 We indeed found that for all the real-life graphs that we
 encountered, the multi-core version that we propose is a
 better choice by a wide margin than all the Hadoop based
 solutions. For a specific example, on a Wikipedia graph with
 1.63 millions vertices, and 18.5 millions of edges; using 32-
 threads out method obtains a whopping 837-fold speedup
 with an accuracy of 98.2%. None of the Hadoop based
 solution reports a speedup that is as high as this work.
 Below we list the contribution of this work.
 • We propose a simple, yet powerful, method for ap-
 proximate triangle counting. It has a surprisingly high
 accuracy and a high speedup factor; both the metrics
 observably improve as the graph grows larger.
 • We develop two variants of thread-based multiprogram-
 ming solutions of our approximate triangle counting
 algorithm. The parallel implementation of these algo-
 rithms are simple, and effective.
 • We compare the performance of our methods with those
 of the state-of-the-art approximate triangle counting
 methods that are available at present, on a collection
 of large real-life networks to validate the superiority of
 our methods.
 II. METHOD
 Assume, G(V, E) is a graph, where V is the set of of
 vertices and E is the set of edges. Each edge e ? E is
 represented by a pair of vertices (vi, vj) where, vi, vj ? V .
 A graph is called simple, if it does not contain a self loop,
 and at most one edge exists between two of its vertices. In
 this work, we consider simple, connected, and undirected
 graphs. We denote the adjacency list of a vertex v by
 adj(v), which contains all the vertices that are adjacent to
 v. In our implementation, all the adjacent lists are sorted
 in the ascending order of the vertex-id. Since the graph is
 undirected, for an edge (u, v) u appears in v’s adjacency
 list and vice-versa. A triangle is represented by a triple
 of (u, v, w), where u, v, w ? V and there exists an edge
 between every pair of vertices in the triple.
 The approximate triangle counting method that we pro-
 pose in this work is based on the EDGEITERATOR algorithm
 that we discuss below.
 Algorithm 1 EXACTEI
 Require: Large network G(V, E)
 1: count = 0
 2: for each edge (vi, vj) ? E do
 3: adj1 = {x|x ? adj(vi), x > max(vi, vj)}
 4: adj2 = {x|x ? adj(vj), x > max(vi, vj)}
 5: counte = |intersection(adj1, adj2)|
 6: count+ = counte
 7: end for
 8: return count
 Algorithm 2 APPROXEI
 Require: Large network G(V, E),
 :Sample factor p
 1: count = 0
 2: Ep = sample p ? |E| edges from E uniformly
 3: for each edge (vi, vj) ? Ep do
 4: adj1 = {x|x ? adj(vi), x > max(vi, vj)}
 5: adj2 = {x|x ? adj(vj), x > max(vi, vj)}
 6: counte = |intersection(adj1, adj2)|
 7: count+ = counte
 8: end for
 9: count = count/p
 10: return count
 A. Exact triangle count by EXACTEI
 An edge iterator algorithm iterates over each edge
 e(vi, vj) ? E for counting the total number of triangles
 for which e is a participating edge. Let’s call the number
 of triangles incident to the edge e, the partial triangle
 count with respect to e, and represent it by counte. Since
 a triangle is composed of 3 edges, a triangle will appear in
 exactly 3 of these partial counts. EXACTEI can obtain the
 total count of triangles in G by simply adding the partial
 counts of all the edges followed by a division by 3. The
 triple counting of a triangle in the above method can be
 avoided by imposing a restriction that the third vertex of
 the triangle (say, vk) has an id which is larger than the id’s
 of both the vertices vi and vj of the edge e; this yields
 a more efficient version of EXACTEI; a pseudo-code for
 which is shown in Algorithm 1. It computes the counte for
 an edge e = (vi, vj) as follows: it takes the adjacency lists
 of the contributing vertices vi (adj(vi)) and vj (adj(vj)).
 Then, it finds the subsets adj1 and adj2 to ensure that the
 id of the possible third vertex (vk) is strictly higher than
 the ids of both vi and vj , and then it finds the number
 of vertices that are common in both adj1 and adj2 i.e.,
 counte = |intersection(adj1, adj2)|. More than 50% of
 execution time can be saved using this method.
 B. Approximate triangle count by APPROXEI
 We design an approximate triangle counting algorithm,
 called APPROXEI, by computing the partial count, counte,
 only for a fraction of edges. For this, APPROXEI takes
 128
Graph EXACTEI APPROXEI APPROXEI2
 time Accuracy speedup Accuracy speedup
 (sec) (%) (%)
 Wiki-1 124 98.5 5.81 99.62 3.83
 Wiki-2 273 99.57 5.12 99.86 3.33
 Wiki-3 299 99.67 5.18 99.75 3.24
 Wiki-4 361 99.75 4.66 99.93 3.12
 Zewail 0.043 95.64 6.02 97.04 5.39
 Flikr 38.92 99.57 9.54 99.76 6.02
 EN 0.72 98.51 7.62 99.52 6.01
 EAT RS 0.37 97.56 7.22 99.45 5.3
 Table I
 EXECUTION TIME FOR EXACTEI. AVERAGE ACCURACY AND SPEEDUP
 OF APPROXEI AND APPROXEI2. HERE, p = 0.1. SPEEDUP IS WITH
 RESPECT TO EXACTEI. STATISTICS OF THE GRAPHS ARE IN TABLE II.
 an additional parameter, a sample factor p ? [0, 1], which
 defines the fraction of edges in E for which we compute
 the partial count. A pseudo-code is shown in Algorithm 2.
 APPROXEI first chooses (Line 2) a set of p fraction of edges
 from the set E uniformly. Then, it computes the partial count
 of each of the chosen edges (Line 4 ? 6). Finally, the sum
 of the partial count is divided by p to obtain an approximate
 count of triangles in the graph G (Line 9).
 A second version of approximate counting algorithm can
 be obtained by lifting the constraint (imposed by Line 4?5)
 that the id of the third vertex (vk) of a triangle is larger than
 the ids of both vi and vj . As a consequence, we may end
 up counting each triangle at most thrice. So, the final count
 has to be normalized by 3?p (at Line 9 of Algorithm 2). We
 will call this version, APPROXEI2. As expected, this version
 is slower than the earlier version of approximate counting
 algorithm as it performs more works when computing the
 partial counts by intersecting a pair of unfiltered adjacency
 lists. However, this version has better sampling performance,
 as the sampling space of this version is less restricted. So,
 APPROXEI2 typically achieves a better counting accuracy
 than APPROXEI. For a comparison, see Table I.
 C. Parallel algorithm, PARAPPROXEI
 In this section, we discuss the parallel version of the
 APPROXEI, which we call PARAPPROXEI; the idea for this
 is quite simple. Since, the APPROXEI algorithm only per-
 forms read operations on the graph data structures, multiple
 threads can access these data structures without requiring
 any exclusive access. So, PARAPPROXEI splits the p ? |E|
 edges, and assign each part to several threads so that each
 thread can compute the partial count of the edges in its part
 independently. Each thread th maintains it’s own counter
 (say countth) which contains the sum of all the counte
 processed by it. After every thread has done its share of com-
 putation, PARAPPROXEI sums the partial triangle counts
 from each thread (countth) to get the countpartial. Then, it
 divides the countpartial by appropriate normalization factor
 (p for APPROXEI, and 3 ? p for APPROXEI2) to get the
 approximate triangle count. The process is illustrated in
 Figure 1.
 1) Optimization of PARAPPROXEI: As shown in Figure
 1, PARAPPROXEI provides each of the threads approxi-
 mately |E|?p/tc edges from the set Ep, with the expectation
 that all the threads are assigned the same amount of work for
 parallel processing. But, most often it is not the case. Even
 though all the threads have to process the same number of
 edges, the computation associated with the edges can be
 significantly different based on the size of the adjacency
 lists of the vertices incident to an edge. As a result, when
 we execute PARAPPROXEI (as shown in Figure 1), most
 of the threads finish their computation within a short span
 of time; however, there exist some of the threads that take
 significantly longer time to finish their share of computation;
 thus, resulting a longer overall execution time (see, Figure
 2). This problem is also discussed in earlier works with the
 phrase “the curse of last reducer” in the context of Hadoop
 based parallelization [23].
 Ep
 Ep1 Ep2 Eptc
 T1 T2 Ttc
 Figure 1. Illustration of parallel workload distribution among tc number of
 threads. Here, Ti indicates thread i. Ep is the set of edges to be processed.
 Epi ? Ep is a disjoint set of edges assigned to the thread i.
 0 5 10 15 20 25 30 350
 0.4
 0.8
 1.2
 1.6
 2
 Threads
 Waiting time (sec
 )
 Figure 2. Thread waiting time over 32 threads of PARAPPROXEI.
 Execution with AtomicW orkLoad = (|Ep|/32) for graph “Wiki-4”
 To work around the above limitation, we propose a
 different variant of the algorithm PARAPPROXEI. In this
 129
Ep
 Epi Epi+1 Epi+tc?1
 T1 T2 Ttc
 Figure 3. Illustration of parallel workload distribution among tc number
 of threads using queue. Here, Ti indicates thread i. Ep is the set of edges
 to be processed, each small box in Ep is a packet of edges that is assigned
 to a thread at a given iteration. Dark packets of Ep are the edges that have
 already been processed by some thread. Gray packers are being processed,
 and finally the white packets will be assigned to the next available thread.
 variant, each thread takes a small fraction of the total job
 at the beginning of the execution and upon finishing the
 execution, it takes additional fractions of job in subsequent
 iterations, until the entire job is finished. To ensure that
 different threads work on different fractions of the total job,
 a queue is used for storing the list of edges that are yet to be
 processed; this queue mediates the job allocation to different
 threads. At the beginning of every iteration of job allocation,
 each thread acquires an exclusive access of this queue to
 request new job. The process is explained in Figure 3.
 Though the above variant of PARAPPROXEI distributes
 the total work among different threads more evenly, it also
 suffers from a bottleneck caused due to the exclusive access
 to the job queue. So, there is a trade-off that is based
 on the size of job (edges) assigned in response to a job
 request. A large job assignment (the highest possible is,
 AtomicW orkLoad = (|Ep|/tc) edges assigned to each
 thread) ensures that no time is spent for enforcing mutual
 exclusion but it yields larger waiting time for finishing the
 last thread. On the other hand, a smaller job assignment
 results smaller waiting times for finishing the last thread,
 but it suffers from a high waiting time in the semaphore
 queue due to the large number of exclusive accesses to the
 job allocation queue. In our experiments, we find that the
 differences in speed-up factors vary within a range of 5%
 to 10% based on the choice of packet size.
 III. EXPERIMENTS
 We perform several experiments to observe the perfor-
 mance of approximate triangle counting. For this we use
 a collection of real-life graphs available from http://www.
 cise.ufl.edu/research/sparse/matrices/. We choose the largest
 graphs used in [13]. The statistics of the graphs are shown
 Graph Vertices Edges
 Wikipedia 2005-11-05 (Wiki-1) 1, 634k 18, 540k
 Wikipedia 2006-09-25 (Wiki-2) 2, 983k 35, 048k
 Wikipedia 2006-11-04 (Wiki-3) 3, 148k 37, 043k
 Wikipedia 2007-02-06 (Wiki-4) 3, 566k 42, 375k
 Zewail 6k 54k
 Flikr 820k 6, 625k
 Epinions network (EN) 75k 405k
 Edinburgh Associative Thesaurus (EAT RS) 23k 305k
 Table II
 GRAPH USED IN EXPERIMENTS (SOURCE:
 HTTP://WWW.CISE.UFL.EDU/RESEARCH/SPARSE/MATRICES/).
 in Table II. To reflect the performance of an approximate
 counting algorithm, we use two metrics: speedup and accu-
 racy (%). The speedup of a method M defines the ratio of
 the execution time between EXACTEI and the corresponding
 algorithm, M and accuracy defines the counting accuracy
 of the algorithm M , in percentage. All experiments are
 executed on a 64 core 2.3GHz AMD machine.
 A. EDGEITERATOR vs NODEITERATOR
 In this experiment we compare the performance of
 PARAPPROXEI with that of PARAPPROXNI. PARAP-
 PROXNI algorithm is a NODEITERATOR algorithm to ap-
 proximate triangle count of a network using multiple threads.
 Both the PARAPPROXEI and PARAPPROXNI algorithms
 are given same set of parameters (a network G(V, E),
 sampling factor p, thread count tc). A NODEITERATOR
 algorithm works by iterating over the nodes in V . For, each
 node v ? V the PARAPPROXNI computes partial count of
 triangles incident on node v (countv). Finally, the sum of
 all the partial counts gives the total triangle count count (
 count =
 ∑
 v?V
 countv ). For approximate triangle count with
 sampling factor p, we sample a set Vp, where |Vp| = p? |V |
 and each node has equal probability to be selected. Then,
 the approximate triangle count using PARAPPROXNI will
 be, count =
 ∑
 v?Vp
 countv
 p . The method is very similar with
 that of PARAPPROXEI as explained in Section II-C. The
 most significant difference here is the process of computing
 partial triangle count countv. To compute countv of node
 v we need to go over all the possible pairs of nodes from
 adj(v) and check if the pair represents an edge (countv =
 |{(x, y) : x = y and x, y ? adj(v) and (x, y) ? E}|).
 For this experiment we consider tc = 16 and p = 0.1
 for both the algorithms. Each approximation is executed
 10 times. The average and variance of execution time
 and accuracy is reported in table IV. From Table IV, we
 can see that, EDGEITERATOR algorithm is almost always
 better than NODEITERATOR in execution time and accuracy.
 NODEITERATOR algorithm is also shows higher variance
 on execution time and accuracy. The reason behind it is
 that, in many graphs node degree is exponentially distributed
 130
Variance APPROXEI PARAPPROXEI Speedups
 Graph Sample accuracy of speedup threads
 Factor p (%) accuracy 4 8 16 32
 Wiki-1 0.1 99.21 0.40 4.49 24.23 42.84 68.57 91.68
 0.01 98.2 2.36 33.54 239.94 418.75 664.37 837.74
 Wiki-2 0.1 99.56 0.10 4.14 20.1 35.6 55.43 63.42
 0.01 98.95 0.49 32.42 199.56 351.02 548.01 614.27
 Wiki-3 0.1 99.61 0.12 4.21 19.87 35.54 54.22 60.96
 0.01 98.72 1.93 32.85 198.44 341.15 515.91 592.26
 Wiki-4 0.1 99.60 0.09 4.33 19.54 34.95 52.55 56.84
 0.01 98.28 1.61 33.71 197.13 346.92 504.8 547.29
 Zewail 0.1 98.45 0.08 4.29 11.35 12.46 10.0 6.19
 0.01 92.26 12.28 9.92 40.14 30.01 15.14 10.03
 Flikr 0.1 99.74 0.07 5.18 28.92 51.46 77.67 96.67
 0.01 99.43 0.19 33.26 277.44 501.83 730.21 796.85
 EN 0.1 99.03 0.62 4.93 22.69 33.1 40.6 38.71
 0.01 97.03 5.82 18.83 147.4 164.77 143.46 97.96
 EAT RS 0.1 98.21 1.66 4.15 17.74 24.79 27.92 23.62
 0.01 96.64 3.88 13.62 101.7 111.67 86.82 56.3
 Table III
 AVERAGE ACCURACY WITH RESPECT TO SAMPLE FACTORS AND SPEEDUPS WITH RESPECT TO SAMPLE FACTORS AND TOTAL THREADS USED.
 Graph Execution Time Accuracy
 Average (s) Variance Average (%) Variance
 EI NI EI NI EI NI EI NI
 Wiki-1 3.13 11.99 0.01 79.78 96.17 96.39 0.19 8.87
 Wiki-2 7.89 23.00 0.06 132.43 98.83 97.03 0.03 2.75
 Wiki-3 8.60 45.90 0.03 2824.02 98.80 96.36 0.06 9.69
 Wiki-4 10.68 47.98 0.08 3056.87 99.07 96.02 0.05 26.47
 Zewail 0.00 0.00 0.00 0.00 99.65 95.47 0.02 12.56
 Flikr 0.61 1.24 0.00 0.03 99.86 93.40 0.01 9.68
 EN 0.02 0.03 0.00 0.00 99.35 92.97 0.28 60.87
 EAT RS 0.01 0.01 0.00 0.00 99.39 93.52 0.54 13.28
 Table IV
 AVERAGE AND VARIANCE OF EXECUTION TIME AND ACCURACY OF PARAPPROXEI AND PARAPPROXNI. (tc = 16,p = 0.1)
 (Power Law model [24]). That is, number of nodes with low
 degree is very high compared to the number of nodes with
 high degree. In general, high degree nodes contribute higher
 in triangle count. Consequently, when we are performing
 uniform sampling from nodes in V , we are sampling from a
 very skewed distribution of partial-count countv; as opposed
 to EDGEITERATOR which samples from a less skewed
 distribution of partial-count counte. As can be observed
 from the Table IV, execution time of NODEITERATOR is
 also higher than that of EDGEITERATOR.
 B. Performance of APPROXEI and PARAPPROXEI
 We show the performance of APPROXEI and PARAP-
 PROXEI in Table III. For both the methods we show results
 for both p equal to 0.1 and 0.01; for these two cases, the
 sampler samples 10% and 1% of edges of the original graph.
 For PARAPPROXEI we set AtomicW orkLoad = 50edges,
 and thread-count tc = 4, 8, 16 and 32. For every parameter
 setting we execute the algorithm for 5 times and show the
 average of the counting accuracies and the speedups. As
 we can see, the speedup factor increases as we increase
 the number of threads for PARAPPROXEI . However, the
 speedup does not increase linearly (see Figure 4) with the
 number of threads, due to the bottleneck, as explained in
 Section II-C1. In ideal case, where there is no bottleneck
 for gaining mutual exclusion the speedup factor should be
 increased linearly with tc. For example, in case of “Wiki-
 1” graph and p = 0.1, speedup with tc = 4 is 24.23.
 So, for tc = 8 ideally the speedup should be close to
 48.46. But, it is 42.84 for our experiment. Similarly, for
 tc = 16 speedup is 68.57 instead of 85.68. The more number
 of threads competing for mutual exclusion the higher the
 waiting time would be. As a result, increasing the thread
 count indefinitely does not ensures steady speedup. For
 example, the speedup of PARAPPROXEI decreases when
 we increase total threads from 16 to 32 for graph “EAT
 RS” for p = 0.1 (see Table III). Figure 4 shows this
 relation graphically. The speedup increases with thread count
 up to certain point, but eventually increased number of
 thread damages the performance. Also, important to note that
 our approximate algorithm has shown better approximation
 accuracy for larger graphs.
 131
0 5 10 15 20 25 30 35
 0
 100
 200
 300
 400
 500
 600
 700
 800
 Numbers of threads
 Speedu
 p
  
 
Wiki?2005?11?05
 Wiki?2006?09?25
 Wiki?2006?11?04
 Wiki?2007?02?06
 Zewail
 Flikr
 Epinion
 EAT
 Figure 4. Speedup Vs Thread count tc for PARAPPROXEI
 Mutex Speedup
 Access
 Count
 1 32.5
 2 31.71
 4 32.4
 8 33.14
 16 33.53
 32 33.96
 64 33.99
 128 34.34
 256 34.2
 512 34.64
 1024 34.16
 2048 34.2
 Table V
 SPEEDUP WITH RESPECT TO MutexAccessCount FOR GRAPH
 “WIKI-4” USING 16 THREADS
 C. Comparing the performance of PARAPPROXEI with dif-
 ferent MutexAccessCount
 For this experiment, we define an additional parameter
 MutexAccessCount. MutexAccessCount = 1 identifies
 that, every thread will try to obtain mutual exclusion (job
 assignment) only once. Where MutexAccessCount = 10
 indicates that on average every thread will try to ob-
 tain new job assignment 10 times. Increased value of
 MutexAccessCount indicates smaller portion of job as-
 signed to a thread at single access to mutually exclusive
 portion of program.
 AtomicW orkLoad = |Ep|MutexAccessCount?tc
 In this experiment, we execute EXACTEI with differ-
 ent MutexAccessCount for graph “Wiki-4”. For a con-
 stant number of threads tc, as we increase the value of
 MutexAccessCount, the waiting time of all other threads
 for last thread to finish decrease but the waiting time to
 get exclusive access to job queue Ep increases. The result
 is presented in Table V. As we can see, for a good span
 of possible value of MutexAccessCount the speedup is
 Graph DOULION APPROXEI
 Accuracy(%) Speedup Accuracy(%) Speedup
 Wiki-1 55.2 19.61 98.2 33.54
 Wiki-2 88.52 20.54 98.95 32.42
 Wiki-3 90.13 20.72 98.72 32.85
 Wiki-4 94.31 21.64 98.28 33.71
 Zewail 92.41 3.61 92.26 9.92
 Flikr 99.04 17.69 99.43 33.26
 EN 97.4 7.16 97.03 18.83
 EAT RS 53.5 4.94 96.64 13.62
 Table VI
 AVERAGE ACCURACY AND SPEEDUP OF OUR IMPLEMENTATION OF
 DOULION (p = 0.1) (NOT PARALLEL) AND APPROXEI. p = 0.01
 Graph time(sec)
 GP Parallel
 Exact
 web-Berk-Stan 102 12.04
 as-Skitter 124.8 9.75
 LiveJournal 654 26.46
 Table VII
 AVERAGE EXECUTION TIME OF GRAPHPARTITION AS STATED BY [23]
 AND PARAPPROXEI WITH p = 1 AND tc = 32
 approximately same.
 D. APPROXEI vs DOULION
 In this experiment we compare the performance of AP-
 PROXEI with that of DOULION [13]. For that, we repeat
 the APPROXEI with p = 0.01 and DOULION with
 p = 0.1 for all the graphs from Table III. The speed up is
 with respect to EXACTEI. The implementation delivered by
 the authors of [13] performs worse than (shows less speed
 up) our implementation of DOULION . So in this result
 we report the speed up and accuracy of our implementation
 of DOULION . The result is shown in Table VI. Clearly,
 APPROXEI (single-threaded) is better than DOULION both
 in terms of speedup and accuracy for most of the graphs.
 E. PARAPPROXEI vs GraphP artition
 In this experiment we compare the performance of
 PARAPPROXEI with that of GraphP artition(GP ) [23].
 Since, GP is an exact counting method, We conduct the
 exact triangle counting using PARAPPROXEI with p = 1
 (100% sampling), and tc = 32, and compare the execution
 time. The results are shown in Table VII. In this table, times
 under GP column are taken from corresponding paper [23],
 which is the time of running GP on a 1636-node Hadoop
 cluster. In all three cases for which we were able to collect
 graph from SNAP 1, our method significantly wins over GP
 using only 32 threads!
 IV. CONCLUSIONS
 In this paper we present an approximate triangle counting
 algorithm which is built on an edge iterator algorithm. Our
 1http://snap.stanford.edu/
 132
method is simple, yet it achieves a surprisingly high accuracy
 and speedup. We also present a multi-threaded version of
 our algorithm which is suitable for multi-core machines.
 We show experimental results that validate that our approx-
 imate counting method is better compared to the state-of-
 the-art approximate triangle counting algorithm. Also, for
 exact triangle counting, our thread based implementation is
 significantly faster than an exact triangle counting method
 built on Hadoop cluster. Distributed computing paradigm is
 probably a better choice for graphs that are too large to fit in
 the main memory. However, we found that real-life graphs
 with as many as 6 millions nodes and 20 millions of edges
 easily fits in the memory of a typical desktop PC with 4GB
 of RAM, and for such graphs, our method is obviously a
 better alternative.
 REFERENCES
 [1] R. Milo, S. Shen-Orr, S. Itzkovitz, N. Kashtan, D. Chklovskii,
 and U. Alon, “Network motifs: Simple building blocks of
 complex networks,” Science, vol. 298, pp. 824–827, 2002.
 [2] A. Arenas, A. Fernandez, S. Fortunato, and S. Gómez,
 “Motif-based communities in complex networks,” Journal of
 Physics A: Mathematical and Theoretical, vol. 41, no. 22, p.
 224001, 2008.
 [3] S. Itzkovitz and U. Alon, “Subgraphs and network motifs in
 geometric networks.” Phys Rev E Stat Nonlin Soft Matter
 Phys, vol. 71, no. 2 Pt 2, 2005.
 [4] P. Ribeiro and F. Silva, “Querying subgraph sets with g-
 tries,” in Proceedings of the 2nd ACM SIGMOD Workshop on
 Databases and Social Networks, ser. DBSocial ’12. ACM,
 2012, pp. 25–30.
 [5] O. Kuchaiev, A. Stevanovic´, W. Hayes, and N. Pr?ulj,
 “Graphcrunch 2: Software tool for network modeling, align-
 ment and clustering,” BMC Bioinformatics, vol. 12:24, 2011.
 [6] S. Wernicke, “Efficient detection of network motifs,”
 EEE/ACM Transactions on Computational Biology and Bioin-
 formatics Proceedings, vol. 3(4), pp. 347–359, 2006.
 [7] M. Rahman, M. Bhuiyan, and M. A. Hasan, “GRAFT: an
 approximate graphlet counting algorithm for large graph
 analysis,” in Proceedings of the 21st ACM international con-
 ference on Information and knowledge management (CIKM).
 ACM, 2012, pp. 1467–1471.
 [8] N. Kashtan, S. Itzkovitz, R. Milo, and U. Alon, “Efficient
 sampling algorithm for estimating subgraph concentrations
 and detecting network motifs,” Bioinformatics, vol. 20(11),
 pp. 1746–1758, 2004.
 [9] M. Bhuiyan, M. Rahman, M. Rahman, and M. Al Hasan,
 “GUISE: Uniform sampling of graphlets for large graph
 analysis,” in IEEE 12th International Conference on Data
 Mining (ICDM), 2012, pp. 91–100.
 [10] S. Wasserman, Social network analysis: Methods and appli-
 cations. Cambridge university press, 1994, vol. 8.
 [11] T. Schank and D. Wagner, Approximating clustering-
 coefficient and transitivity. Universität Karlsruhe, Fakultät
 für Informatik, 2004.
 [12] M. E. Newman, “The structure and function of complex
 networks,” SIAM review, vol. 45(2), pp. 167–256, 2003.
 [13] C. E. Tsourakakis, U. Kang, G. L. Miller, and C. Faloutsos,
 “Doulion: Counting triangles in massive graphs with a coin,”
 in Proc. of KDD, 2009.
 [14] E. C. E. Tsourakakis, “Fast counting of triangles in large real
 networks without counting: Algorithms and laws,” in Proc.
 of IEEE ICDM, 2008, pp. 608–617.
 [15] L. S. Buriol, G. Frahling, S. Leonardi, A. Marchetti-
 Spaccamela, and C. Sohler, “Counting triangles in data
 streams,” in Proceedings of the twenty-fifth ACM SIGMOD-
 SIGACT-SIGART symposium on Principles of database sys-
 tems, 2006, pp. 253–262.
 [16] H. Jowhari and M. Ghodsi, “New streaming algorithms for
 counting triangles in graphs,” in Computing and Combina-
 torics, ser. Lecture Notes in Computer Science, 2005, vol.
 3595, pp. 710–716.
 [17] E. C. E. Tsourakakis, “Counting triangles in real-world net-
 works using projections,” Knowledge and Information Sys-
 tems, vol. 26(3), pp. 501–520, 2011.
 [18] H. Avron, “Counting triangles in large graphs using random-
 ized matrix trace estimation,” in KDD-LDMTA’10 Proceed-
 ings, 2010.
 [19] R. Pagh and C. E. Tsourakakis, “Colorful triangle counting
 and a mapreduce implementation,” Information Processing
 Letters, vol. 112(7), pp. 277–281, 2012.
 [20] T. Schank, “Algorithmic aspects of triangle-based network
 analysis,” Ph.D. dissertation, Dept. of Computer Science,
 University of Karlsruhe, 2007.
 [21] N. Alon, R. Yuster, and U. Zwick, “Finding and counting
 given length cycles,” Algorithmica, vol. 17, pp. 354–364,
 1997.
 [22] J. Dean and S. Ghemawat, “Mapreduce: simplified data
 processing on large clusters,” Communications of the ACM,
 vol. 51(1), pp. 107–113, January 2008.
 [23] S. Suri and S. Vassilvitskii, “Counting triangles and the curse
 of the last reducer,” in Proceedings of the 20th international
 conference on World Wide Web, 2011, pp. 607–614.
 [24] A. L. Barabàsi and R. Albert, “Emergence of Scaling In
 Random Networks,” Science, vol. 286, pp. 509–512, October
 1999.
 133

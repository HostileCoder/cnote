"
 An IO optimized Data Access Method in Distributed KEY-VALUE 
Storage System 
Li Chao
 Institute of Computing Technology Chinese 
Academy of Sciences
 Beijing,China
 lc_lichao@126.com
 Wu Guangjun
 Institute of Information Engineering Chinese 
Academy of Sciences
 Beijing,China
 wuguangjun@iie.ac.cn
 Wang Shupeng
 Institute of Information Engineering Chinese 
Academy of Sciences
 Beijing,China
 wangshupeng@iie.ac.cn
 Li Yixi
 National Computer network Emergency Response 
technical Team/Coordination Center of China
 Beijing,China
 15901118657@126.com
 Abstract—Distributed KEY-VALUE storage system
 is a new storage framework for cloud computing. It
 can enable an application to dynamically adapt to 
growing workloads by increasing the number of 
servers. However, current distributed KEY-VALUE 
storage systems are still inefficient on range query 
for larger result set. When the result set become large, 
the file layout, cache hit rate are both key points for 
IO efficiency. In this paper, we will introduce our 
experience under the development of China Mobile 
Big Cloud KEY-VLAUE DB (BC-kvDB). We will 
discuss how we increase IO efficiency in BC-kvDB.
 BC-kvDB is based on single-table space data model 
and provides SQL-LIKE DDL and DML language. 
BC-kvDB’s high throughput is benefit from data 
locality storage, column-storage structure and multi-
 layer caches.  Data can be accessed in local cache or 
local blocks through block index. Experimental 
results show that the random writing performance of 
BC-kvDB is 2.5 times better than HBase and the
 random reading performance is 1.8-2 times than
 HBase.
 Key words: KEY-VALUE DB; Distributed Storage; 
Cloud Storage; Column Store
 B. Introduction
 With the popularity of e-commerce, mobile 
internet, wireless sensor networks applications, the 
amount of data is increasing rapidly. Big data has 
4V characteristics (volume, velocity, variety, 
value). In order to explore the value of big data, we 
have to deal with the large-scale (volume) and
 rapid-growth (velocity) challenges for big data. 
Distributed Key-Value DB is an alternative storage 
system for massive structured big data storage [1].
 Distributed KEY-VALUE data storage systems 
can achieve high throughput and scalability even in 
PB-scale storage. Open source softwares include 
MongoDB[2], Kyoto Cabinet, Redis, Cassandra, 
HBase[3] etc.. KEY-VALUE DB now has also 
been widely used in commercial system, such as
 PNUTS in Yahoo![4], Dynamo in Amazon [5], and 
Big Table in Google [6]. Based on the schema, the 
current key-value DBs can be divided into two 
types, point query system and range query system.
 The point query system puts or gets item is 
distributed based hash function. The range query 
system uses column-family schema to organize 
data attributes. Point query system supports fast to 
fetch a single item. For example DynamoDB can 
read an item in 50ms. But the point query can only 
support PUT or GET operations on the given key. 
Data mining is a kind of large-scale data 
computing and the range query systems are needed. 
The range query systems are more powerful than 
point query systems. They are cable of query large 
data sets. It can provide prefix query, range query 
or even wildcard query. The range query systems
 are efficient for batch data processing. However, 
when the result data set become large, the query 
efficiency decreases dramatically. HBase, 
hypertable are typical range query KEY-VALUE
 2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber,
 Physical and Social Computing
 978-0-7695-5046-6/13 $26.00 © 2013 IEEE
 DOI 10.1109/GreenCom-iThings-CPSCom.2013.222
 1276
DB. They face with the problem of storage locality
 in distributed file system. For example, HBase 
stores data file into HDFS. When the query starts, 
Hbase region server first get data file from HDFS,
 do the query computing locally and then return the 
results to client. Usually, the region server is not 
the server which storage data file. The network 
traffic cost will become significant when the return 
result date set get larger.
 At present, there are two methods to improve
 large-scale query efficiency. The first method is to 
optimize Map Reduce process. For example, 
impala introduces pipeline format between Map 
job and Reduce job. Alough it can improve query 
efficiency, when the date set is large enough for a 
single job, the IO efficiency is still very low.
 PowerDrill [9] proposed distributed columnar 
storage structure to improve efficiency for OLAP
 query. RCfile introduces row and column mixed 
storage structure, and it can improve the efficiency 
for statistical analysis[10]. The data file layout is 
the key points for block scan, but until now, there 
is no study on multi-layered cache for KYE-
 VALUE DB, it will also greatly improve the query 
efficiency.
 In this paper, we will discuss some key methods 
on improving the range-query efficiency in BC-
 kvDB. We will introduce the framework and data 
model in BC-kvDB in section 2. The data 
localization storage structure will be introduced in 
section 3, and multi-layered cache for point and 
range query in section 4. In section 5, we give 
concrete experiments, and the brief conclusion 
described in section 6.
 C. Framework and Data Model
 A. The BC-kvDB framework
 BC-kvDB is a kind of range query distributed 
KEY-VALUE DB. The entire system includes four 
subsystems. As showed in figure 1, there are client 
subsystem, metadata subsystem, region server 
subsystem and network communication subsystem.
 Figure 1. The BC-kvDB components
 Client subsystem provides C/C++ API. It also 
provide multi-language programming interface 
through thrift framework. Some useful tools, such 
as command shell, web interface, unit test are also 
provided at client subsystem. 
Metadata subsystem is composed of Zookeeper 
cluster and multi-master cluster. The function of 
BC-kvDB master is to support the basic operations 
within the cluster scope, such as create table, drop 
table, add a new server or load balance etc.. 
Multi-master clusters include two roles. One is 
primary master and the others are slave masters. 
Primary master support the basic table space 
operations, and the slave masters are watching the 
primary master working state. If the primary 
master is down, one of the slave masters is elected 
as the primary master. 
The zookeeper cluster stores the metalog for the 
whole cluster reliably. The metalog is used to 
recover the master state when the slave masters
 startup. The metalog includes the current primary 
master address, table schema, operation logs and 
the ROOT region location in the cluster. 
Region server subsystem consists of 
independent PC servers. These PC servers connect 
each other through high-speed Ethernet. Region 
server(RS) provides a key-value data read and 
write service. BC-kvDB uses local data storage 
engine. For each client write operation, it will be 
first write into log, then to cache. Cache refreshes 
at periodical time to disk and produces data files 
locally. Because of the read and write is 
implemented on RS directly and can achieve better 
performance. 
Network communication subsystem can provide 
high-speed data communication and message 
service. In BC-kvDB the client and RS send and 
receive data block directly. But when the client and 
RS communicate with master, they use the RPC
 message service based on the network 
communication subsystem.
 B. The BC-kvDB data model
 The data model of BC-kvDB is single table 
space. And the table space is further partitioned by 
row-key. Each range of row-key are called region, 
and stored in a region server. Each region includes 
many CS files. Region is the basic unit of 
distribution and its location information are 
recorded in metatable.
 1277
OgvcTcpig
 Entry point RootRange
 …
 Ogvcvcdng
 RangeServer:IP
 [1]
 ObRootMeta2 1024X1023
 [2]
 server_info
 [3]
 server_info
 [1]
 server_info
 [2]
 ObRootMeta2
 [3]
 ObRootMeta2
 [4]
 ObRootMeta2
 tabletinfo
 replica[2]
 vcdngvaxgtukqpa
 replica[3]
 vcdngvaxgtukqpa
 replica[1]
 vcdngvaxgtukqpa
 Rqtv
 Others?????
 TqqvTcpig
 [1]
 MetaRange1
 [2]
 MetaRange2
 OgvcTcpig
 Vcdngv
 Wugtvcdng
 Q dT qqvO gvc4"
 Qvjgtu
 Tqqvvcdng
 1024X1023
 Figure 2.The metatable structure for BC-kvDB
 Metadata table is a kind of central indexing for 
region distribution. The metatable is distributed as 
the normal user table. When a table is created, a 
XML schema is also created and stored into 
zookeeper cluster. The schema has replica policy, 
database name, column-family name, and other 
basic table space information. 
When a client writes data into a table space, it 
can directly sent data to the dedicated RS. The RS 
first accepts the updated data and writes into local 
log, caching the data in local memory and then 
responds the client. The RS will flush the write 
cache into local file periodically. When a region 
size is beyond some threshold, the region is split 
into two new regions. The newly produced region 
will move to other RS in balance period.
 D. Data File Structure for BC-kvDB
 When the write cache is large enough, RS will 
create a new write cache for the updating data and 
the old write cache will flush into a local data file. 
The data file in BC-kvDB is also called cell store 
as hypertable. The data file in composed of data 
blocks, bloom filter, block index and file trailer. 
Each part is listed in the following figure.
 Figure 3.Data file format in BC-kvDB
 The trailer is the data file metadata. Its size is 
1024 bytes. It includes the total records number, 
the total file length and the magic number. The 
block index is the index for all blocks in the file.
 Each item in the block index has the offset and the 
end row-key for each data block. 
Bloom filter is used to predict whether a given 
key in the cs file. It uses the bit array succinctly 
represents a set, and be able to predict if an 
element belongs to the set in O(1) times. Although 
bloom Filter has some false positive disadvantages, 
the correct query process will be done in the data 
block finally, and the correct item will be fetched. 
The data block store the compressed key/value 
pairs blocks and the concrete key structure lists as 
figure 4.
 Figure 4. KEY structure  
Row key is a user defined unique key, with '\0' 
for the end identifier. Column family are the 
attributes cluster define in the table schema. 
Column qualifier is a column name under a
 column family. Flag item indicates that if the item 
is newly created or has been deleted. At query time, 
the deleted item will not be fetched. Timestamp is 
the reverse order of timestamp string, and the 
newly created item will be fetched firstly.
 The new data file is created at the compact or 
split stage. In KEY-VALUE DB the compaction 
1278
and splitting are common methods to produced a
 new data files. In BC-kvDB the new data file are 
all stored in local file. For detailed compaction and 
splitting discussion, reader can refer to Hbase for 
further knowledge. 
E. Multi-level cache structure
 In BC-kvDB, we use multi-layered cache 
structure to increase the efficiency of reading and 
writing operations.
 A. Client Cache
 Client cache module is mainly used to cache the 
RS location information and the table schema 
information which the user retrieved. Client cache 
module store two types of data: table cache and 
location cache.
 Table cache stores the retrieval information of
 table schema. Location cache stores region 
information, such as start row, end row and the 
location information of corresponding region
 server. It used the hash map structure. When a user 
makes retrieval, it will find the information from 
local Cache at first time. If there is, then get it 
directly from the cache. It will speed up the query 
process.
 If the table schema changes, Master will 
modifies the corresponding table structure. Master 
also informs the main RS stores of the table, the 
main RS will update the table structure information
 on locally stored region. At the same time, 
according to the local cache backup plan, put a
 schema update request to the backup node, the
 backup nodes at the same time update the schema 
cache. When RS receives a client to retrieval, The 
Region management module on RS would 
compare table structure version number. If it older 
than local cache table structure version number, 
then update the table structure and return to the 
client. That makes it possible to update all of the 
client's local Schema cache.
 Because of the Metadata table changes, it will 
lead to failure in the location cache of client cache, 
which led to the client failed to retrieve the 
corresponding region. The clients read the 
metadata table again, and update the location cache.
 B. Storage server cache
 Storage server cache is divided into three 
modules: Cell Cache, Query Cache and Block
 Cache.
 Figure 5.storage server cache mechanism structure diagram
 Cell Cache module is used to store the write data 
cache, in order to speed up the data write operation. 
The interior of the module uses the RB-Tree to 
save the user input data. And also it can support 
the sequential search and random search. In order 
to ensure the data into the cell cache is safe, the 
user writes the key-value data to the commit-log at 
first, and then written to the cell cache. If the 
amount of data in the cell cache reached a certain 
threshold, then the data in cell cache by cell store 
module is written to the file. Finally, returned a
 successful response to the user.
 Data written process: :
 (1) The user writes the key-value data to the 
commit-log at first?Then the system
 according to the replica strategy synchronous 
writes the information to Log.
 (2) Key-Value data will be written to the cell
 cache.
 (3) It determines whether cell cache reaches the 
set threshold. If it not reached, then return and 
according to the need to accept user requests 
to write data.
 (4) We should create a new cell cache and accept 
the write data. 
(5) The full cell cache assigned to immutable cell 
cache, and then refreshes data to the cell store
 by immutable cell cache. 
(6) Call the CellStore module to write data, 
1279
generate related block index, and written to 
disk file.
 Query Cache module is to accelerate the read 
operation. Query cache saved the user retrieve 
result data. Query cache uses the LRU elimination 
strategy.
 When new data is written to the cell cache, if it
 has the same data in query cache, and then deletes
 the same one from query cache.
 The internal of query cache used of the B-Tree 
structure, each data in the B-Tree format like this: 
<md5(Tablename, ScanSpec), QueryResult(key-
 value)>? Table name means that the user to 
retrieve the name of the table; ScanSpec means 
that the last retrieval provides the retrieval 
conditions. Query cache uses the B-Tree structure 
to support fast random search and interval search.
 When the user needs to query the specified data,
 the system will determine existence of the same 
operation in the query cache at first. If query cache 
have the same data?then read the required data 
directly from the query cache. If not find, then
 insert the query results into query cache.
 When the data update operation is executed, first 
of all, according to the table name and scan spec to 
determine whether it exists the update data in the 
query cache, if it has the same data in query cache, 
then delete it from query cache. When the query
 cache reached the set threshold, system will use 
LRU algorithm to delete the cache data.
 Data reading process:
 (1) Lock operating on QueryCache.
 (2) When it has been 1000 times query operation
 since the last query, output the log information
 and show that the hit rate of QueryCache.
 (3) Search the corresponding key form query
 cache. If the key does not exist, it will return
 False. If query cache has the key, it will be 
stored in result and return true.
 Block Cache is a data file for the entire storage 
server. When a piece of data files accessed by the 
user, then the data block is added to the block
 cache. When users query the data in the block
 cache, you can return immediately to the specified 
data block. When it is not in the block cache, adopt 
the method of querying the data file. At the same 
time, the corresponding data block in the file is
 added to the block cache.
 F. Experiments
 The purpose of the experiment is to measure the 
read and write efficiency with a certain amount of 
data in BC-kvDB, at the same time gives a 
comparison test in the same environment with 
HBase .Through the contrast test shows that the 
superiority of the system in the performance . 
The experimental used 15 servers for the 
performance test. During the test, we mainly grasp 
the trend of read and write performance. There 
were 13 nodes as a cluster of the BC-kvDB service, 
and 2 nodes as a client. Then the following is the 
system information: OS was centos5.5.CPU was 2-
 way and 4-core cups, it had Gigabit Ethernet, 21tb
 hard disk and 16GB memory.
 The Experiment used an AG and a column
 family as the test data pattern. The key of length is 
set to 12 KB, the value of length is set to 1 KB, the
 value of random and sequential read and write are 
the same construction method.
 The experimental method is shown as follows: 
(1) In the cluster it already had a certain amount of 
data (2TB). (2) In the client we started the random 
writing process, and inserted data in BC-kvDB 
cluster. Statistical the data insertion efficiency and 
then record the random write efficiency. (3)After 
perform the random written, we had start the 
random reading process. Read data in the BC-
 kvDB cluster, Statistical data read efficiency and 
then record random read efficiency. (4) We should 
record test results in the form. 
Experimental data and test results are showed in 
the following table. 
Figure 6. The throughput of the cluster
 In this figure, the abscissa indicates the scale of 
the current cluster data (GB); the vertical axis 
represents the throughput of the entire cluster 
(KB/s). This experiment mean that when the data 
is increasing, the throughput of the entire cluster 
still maintain a good state.
 Figure 7. Process efficiency of reading and writing
 In this figure, the abscissa indicates the process 
of writing / reading (MB) data, the vertical axis 
represents that each client's random read-write 
0
 10
 20
 30
 40
 50
 60
 Random write
 Random read
 MB/s
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
 4.5
 Random 
write 
2?100GB
 Random 
read 
2?1GB
 Random 
read 
2?10GB
 Random 
read 
2?20GB
 current data total 
(200GB)
 current data total 
(800GB)
 ms
 1280
efficiency. The experiment data shows that the 
BC-kvDB storage system can able to work 
efficiently in big data environment. The efficiency 
of read and write operations is still remained high 
efficiency with the amount of data increases and 
the transformation of the data environment.
 And then in the same environment, experiments 
were performed in HBase, Start single client and 
pre split table into 100, at the same time random 
write 100 records with ten threads.
 Figure 8. BC-kvDB and HBase experimental results
   
Experimental results show that the random 
writing performance of BC-kvDB is 2.5 times of 
HBase and its random reading performance is 1.8-
 2 times for HBase. 
G.Conclusions
 This article describes a high-performance 
distributed KEY-VALUE storage system (BC-
 kvDB) and the data access method. In BC-kvDB 
the key-value pairs are compressed into blocks and 
indexed in a local data file. The region server can 
retrieve local data file directly for the range query. 
This can boost the query process for larger data set 
greatly. Besides, the cell cache, block cache and 
the shadow cache make the results are mostly hit in 
the local cache. The multi-layered cache can 
further increase the query efficiency. As space is 
limited, the detailed data processing mechanism 
for BC-kvDB are not introduced in this paper; we 
will discuss them in other papers.
 References
 [1] SQL Databases Don’t Scale [EB/OL] . [2011-03-09]
 [2] F. Chen,Z. K. Yong,W. X. Zhi. Index for Cassandra:a
 novel schema for multi-dimensional range queries in 
Cassandra[C]. Semantics Konwledge and Grid (SKG),
 2011 IEEE International Conference on :130-136.
 [3] HBase .http://hadoop.apache.org/hbase/.
 [4] B. F. Cooper, R. Ramakrishnan, U. Srivastava. PNUTS:
 Yahoo!'s hosted data serving platform. In Proceedings of 
the VLDB Endowment, 2008, 1(2):1277-1288
 [5] G. DeCandia, D. Hastorun, M. Jampani. Dynamo: 
amazon’s highly available key-value store. In 
Proceedings of twenty-first ACM SIGOPS symposium 
on Operating systems principles (SOSP’07), 2007:205-
 220
 [6] F. Chang, J. Dean, Ghemawat S. Bigtable: A distributed 
storage system for structured data. Usenix Association 
7th Usenix Symposium on Operating Systems Design 
and Implementation, 2006: 205-218
 [7] D. Giuseppe, H. Deniz, J. Madan, K. Gunavardhan, L. 
Avinash,Dynamo: Amazon’s highly available key-value 
store[J]. ACM, 2007:205-220.
 [8] Hadoop .http://hadoop.apache.org/.
 [9] A. Hall,O. Bachmann, R. Bussow, Processing a Trillion 
Cells per Mouse Click [j]. PVLDB,2012.5(11):1436-
 1446
 [10] Y. Q. He, R. Lcc, Y. Huai. RCFile: A Fast and Space-
 efficient Data Placement Structure in MapReduce-based 
Warehouse Systems.IEEE 27th International 
Conference,2011:1199-1208
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
 Random write 
2*100GB
 Random read 
2*1GB
 Random read 
2*10GB
 Random read 
2*20GB
 HBase
 BC-kvDB
 ms
 1281

Mining Repetitive Sequences Using A Big Data 
Ecosystem 
 
Michael Phinney1, Hongfei Cao1, Andi Dhroso1,2, Chi-Ren Shyu1,2 
1Department of Computer Science, 2Informatics Institute, University of Missouri, Columbia, MO
   
Abstract— Identifying repetitive gene sequences occurring 
within DNA sequences that span a collection of species is a 
challenge that is conceptually simple yet computationally 
challenging. Biological research suggests that certain regions 
within genomic sequences may be unchanged for hundreds of 
millions of years; understanding and identifying these highly 
preserved regions is a major challenge faced by 
bioinformaticians. Taking an evolutionary perspective on 
DNA, pinpointing these repetitive sequences is the first step to 
understanding functional similarities and diversities. The 
difficulty of this problem arises from the volume of the data 
required for analysis; it grows with every genome that is 
sequenced. Traditional approaches used to identify repetitive 
sequences often require the pair-wise comparison of 
chromosomes, which takes a significant amount of time to 
gather results. When comparing n chromosomes, n(n-1) 
individual comparisons must be made. To avoid exhaustive 
pair-wise comparisons, we designed an algorithm that 
partitions genomic sequences into search key values 
representing potential repetitive sequences, which are hashed 
into bins. With the introduction of new genomes, we only 
process the new sequences and aggregate new results with 
those that were previously processed. 
Keywords— repetitive sequences, Big Data, sequence matching, 
MapReduce, Hadoop. 
I. INTRODUCTION 
Research on animal and plant genomes suggests that 
specific regions within the genomes are highly preserved, 
remaining unchanged for nearly 300 million years [1]. There 
is great mystery associated with these seemingly static 
sequences; as an evolutionary mechanism, the exact role 
they play is uncertain[2,6]. However, identifying their 
existence is the first step to gaining insight. One of the most 
common approaches to identifying ultraconcerved elements 
(UCEs) is to perform a pair-wise sequence alignment [2]. 
This method is known to be computationally expensive. 
Modern approaches utilize information retrieval based 
techniques to achieve similar results and have shown to be 
successful [1]. We suggest an alternative approach that 
utilizes the Hadoop MapReduce framework to address this 
issue.  
This problem could be considered as a topic of Big Data 
because the data is not clean, there is uncertainty in the data, 
the amount of intermediate results generated by pair-wise 
comparison approaches is difficult to manage, and the 
availability of genomic data is increasing rapidly. Utilizing 
common pair-wise comparison techniques to compare new 
genomes with existing genomic datasets requires an 
enormous amount of redundant computation. In addition, 
alignment algorithms may not provide optimal results; they 
may not identify all matching sequences. 
II. SYSTEM ARCHETECTURE 
A. Framework 
By utilizing the Hadoop [3] framework, we are able to 
parallelize the processing across multiple machines and 
multiple threads. Our repetitive sequence algorithm utilizes 
a parallel, distributed programming model developed for use 
in a cluster environment, MapReduce [4]. The algorithm can 
be thought of as a data flow consisting of two phases: Map 
and Reduce conceptually, shown in Fig. 1. The map phase is 
responsible for sorting and filtering data. First, the large-
 scale genomic input data is divided amongst a collection of 
mappers (the number of mapper is defined based on the size 
of our input data) and is then manipulated and translated 
into a set of key-value pairs; in our case, keys are 
subsequences and values are their respective chromosome 
identifiers and starting positions in the input sequence. 
These key-value pairs are passed through a hashing function 
that will deterministically map a key to one of potentially 
many reducers; this ensures pairs sharing a common key 
will be sent the same reducer. In the reduce phase, reducers 
aggregate the output subsequences received from mappers. 
The MapReduce programming model is appropriate for 
sequence matching applications. That is, by assigning a 
sequence to be the key, the hashing behavior of the 
MapReduce model may be utilized to locate iidentical 
sequences; exact matching sequences will be hashed to the 
same reduce task. 
B. Cluster Setup 
For the purpose of this study, we simulate a cluster 
environment and run Hadoop [3] in pseudo-distributed 
mode. We constructed a virtual cluster by partitioning a 
single server’s resources and allocating them to a collection 
of virtual machines. In total, our server has 128GB of RAM, 
24 2.0GHz Intel CPUs, and 6TB of disk space. For our 
experiments, we have configured our cluster to have seven 
data nodes and one master node. Each data node virtual 
machine is running CentOS Linux, has approximately 6GB 
of RAM, and a single CPU. The master node has 8GB of 
RAM and 4 CPUs. Hadoop is running in pseudo distributed 
mode as opposed to fully distributed mode, since the cluster 
is housed on one physical machine. The advantage to using 
this sort of environment, we have complete control over 
4235"KGGG"Kpvgtpcvkqpcn"Eqphgtgpeg"qp"Dkqkphqtocvkeu"cpf"Dkqogfkekpg
 ;9:/3/69;;/3532/91351&53022"Æ4235"KGGG 82
available resources and are able evaluate different cluster 
configuration settings such as the number of nodes and the 
number of CPUs and RAM to be allocated to a node.  
III. METHOD 
Common methods for identifying UCEs across a set of 
chromosomes utilize an exhaustive pair-wise comparative 
approach; given n chromosomes, identifying inter- and 
intra-chromosomal UCEs requires n2 comparison. We 
would prefer a technique that requires n operations, 
analyzing each chromosome once. Our approach, shown in 
Algorithm 1, utilizes the Hadoop MapReduce model to 
perform a single global aggregation to identify identical 
sequences across chromosomes simultaneously. 
Algorithm1 : Repetitive Sequence in MapReduce  
// Map Function: input <k,v> k is offset for current file 
block (in bytes); v is a sequence in chromosome C 
1: v = P(v)       // remove invalid characters 
2: for i = 0 to m-n do 
2:       TSI = code (v[i to i+n])   
3:       start_pos = i + k 
4:       return (TSI, (start_pos, C)) 
  
// Reduce Function: input <k,v> k is a subsequence (TSI); v 
is the starting position of the subsequence w.r.t the 
chromosome sequence 
1: if(count(v) >= 2) 
2:       uce = decode(k) 
3:       pos = merge(v) 
4:       return (uce, pos) 
  
As previously mentioned, the role of our Map function is to 
construct a set of key-value pairs; all subsequences of length 
n (100 base pairs in our implementation) found in a 
chromosome form keys and their respective starting 
positions along with the current chromosome identifier form 
the values. To avoid potential capacity issues provoked by 
trying to feed in data files that are too large for a single 
mapper, the system first splits the file into smaller chunks. 
Hence, in the map layer, our input data will be a partition of 
one of the original files. In the Map function, the value in 
each input key-value pair is a segment of the raw 
chromosome sequence containing values A, C, G, T and a 
few others such as N, M, R, etc. A few preprocessing steps 
are performed before generating any key-value pairs. First, 
only valid characters A, C, G and T are considered. That is, 
all subsequences that contain an N, M, R, etc., are neglected 
because this represents some level of uncertainty within the 
sequence. Second, we convert each subsequence from a 
string to binary representation using the code function; we 
refer to this conversion as a Translated Sequence Identifier 
(TSI). Since each position in our sequence can be occupied 
by one of only four possible values, using the base-2 
numeric system led to a significant reduction in data size. 
The character data type in Java requires two bytes of storage 
space; we reduce this to two bits. This will significantly 
reduce the size of intermediate results generated by Mapper. 
In our experience using subsequences of length 100 as our 
key, we were able to halve the size of our intermediate 
results by using TSIs. The next step in the mapper is to 
calculate the starting position of a subsequence with respect 
to the raw chromosome by adding the block offset, k, to the 
current line offset, i. To illustrate the concept, considering a 
subsequence, TSI*, starting at the 50th position in the 
original file containing chromosome A; it would take the 
following key-value pair representation: <TSI*, (50,A)>. 
After constructing key-value pairs in the map tasks, all like 
keys will be mapped to the same reducer. In the Reduce 
function, the input key is a TSI (potential UCE) and values 
are lists of starting positions paired with a chromosome id. 
The main job of a reducer is to count the number of 
positions present in the value; if there are more than two, we 
have found a UCE. Following our simple example above, 
all key-value pairs with a key of TSI* will arrive at the same 
reduce task. Upon arrival to a reducer, the positions are 
Figure 1. System Architecture for MapReduce setting for identifying repetitive sequences 
83
aggregated together. This set of positions represents the 
location of UCEs. A sample output from a reducer may look 
as follows: <TSI*,((50,A);(2300,B))>. Using this 
representation, it becomes easy to see that a sequence of 
length 100 occurs in both chromosome A and B starting at 
the 50th and 2300th positions respectively. The final job for 
the reducer is to decode the TSI, translating it back to the 
original character representation, and storing the results. 
IV. RESULT 
Our MapReduce implementation is written in Java and 
utilizes the Hadoop MapReduce framework. We consider 
the task of identifying UCEs across six chromosomes, three 
human and three from rat, in an incremental fashion as to 
demonstrate the behavior of our run-time, intermediate data 
size, and final result data size as the number of base pairs 
contained in the input sequences is increased. We compare 
our results with that of an existing method described in [1] 
in terms of accuracy. We do not consider a direct 
comparison of the results in terms of runtime because both 
approaches require vastly different computing 
environments. However, we report the overall runtime for 
our method. 
The first case, processing one chromosome consisting of 
250 million base pairs for intra-chromosomal UCEs, 
resulted in a runtime of approximately 25 minutes, 68 GB of 
intermediate data, and 512 MB of final data containing 
discovered UCEs. The subsequent cases iteratively 
introduced new chromosomes, increasing the number of 
base pairs in a roughly linear manner. Our method was able 
to identify all known UCEs across the six chromosomes 
having an average length of 250,000,000 base pairs within 
4.5 hours, approximately 5.6 minutes per one million base 
pairs. The method scaled linearly as the number of base 
pairs increased (see Fig. 2). Our results were validated 
against results achieved from [1]. To be more precise, every 
UCE identified by the prior method was also identified by 
our proposed method. It is important to note that storage 
space is the greatest limiting factor for our method; we 
generate roughly 1 GB of intermediate results per every 1 
million base pairs. A fully distributed cluster containing 
more nodes would certainly have improved performance. 
V. CONCLUSIONS 
This ongoing study demonstrates the potential for 
conducting sequence matching with the Hadoop 
MapReduce framework. Approaches defined in previous 
studies perform pair-wise comparisons, whereas our 
approach is comprised of a single global aggregation step to 
identify matches across all sequences simultaneously. The 
simplicity of our method in combination with the scalability 
of Hadoop is what makes our approach novel. Furthermore, 
introducing more nodes into the computational cluster will 
improve our performance until the total number of map and 
reduce tasks is exceeded by the number of nodes in our 
cluster. With the advent of online pay-per-use cluster 
services, individuals could quickly spin up a powerful 
environment composed of an arbitrary number of nodes to 
complete this task quickly. Additional improvements are 
being made on our method by employing HBase to 
efficiently manage memory and storage utilization; we can 
minimize disk I/O by accumulating results in memory and 
writing them to disk in one flush. 
ACKNOWLEDGMENTS 
This material is based upon work supported by the IBM 
Students for a Smarter Planet Project Award. MP has been 
supported by the U.S. Department of Education GAANN 
grant no. P200A100053. 
REFERENCES 
[1] J. Reneker, E. Lyons, G. C. Conant, J. C. Pires, M. Freeling, 
C. R. Shyu, D. Korkin, Long identical multispecies elements 
in plant and animal genomes Proc Natl Acad Sci U S A. 2012 
May 8;109(19):E1183-91. doi: 10.1073/pnas.1121356109. 
Epub 2012 Apr 10. 
[2] G. Bejerano, M. Pheasant, I. Makunin, S. Stephen, W. J. 
Kent, J. S. Mattick, et al., "Ultraconserved elements in the 
human genome," Science, vol. 304, pp. 1321-1325, 2004. 
[3] T. White, Hadoop: the definitive guide: O'Reilly, 2012. 
[4] J. Dean and S. Ghemawat, "MapReduce: simplified data 
processing on large clusters," Communications of the ACM, 
vol. 51, pp. 107-113, 2008. 
[5] S. Ghemawat, H. Gobioff, and S.-T. Leung, "The Google file 
system," in ACM SIGOPS Operating Systems Review, 2003, 
pp. 29-43.  
[6] L. F. Lareau, M. Inada, R. E. Green, J. C. Wengrod, and S. E. 
Brenner, "Unproductive splicing of SR genes associated with 
highly conserved and ultraconserved DNA elements," 
Nature, vol. 446, pp. 926-929, 2007. 
Figure 2. The charts display the relationship between the input size (# of base pairs) and the following three metrics: 
run-time, size of intermediate data generated, and size of final results generated by our method. 
84

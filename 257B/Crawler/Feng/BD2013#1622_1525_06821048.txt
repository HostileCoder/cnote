 
 
Suffix Tree Clustering with Named Entity 
Recognition 
 
Jiwei Zhang 
School of Information and 
Communication 
Engineering 
Beijing University of Posts 
and Telecommunications 
Beijing, China 
p40.fate@gmail.com 
Qiuyue Dang 
School of Information and 
Communication 
Engineering 
Beijing University of Posts 
and Telecommunications 
Beijing, China 
qiuyue_dang@163.com 
Yueming Lu 
School of Information and 
Communication 
Engineering 
Beijing University of Posts 
and Telecommunications 
Beijing, China 
ymlu@bupt.edu.cn 
Songlin Sun 
School of Information and 
Communication 
Engineering 
Beijing University of Posts 
and Telecommunications 
Beijing, China 
cn.songlin.sun@ieee.org 
 
 
Abstract—The news searching is challengeable in providing 
web users with clear and readable lists of news reports. This 
paper proposes the Suffix Tree Clustering with Named Entity 
Recognition (STC-NER). STC-NER is supposed to cluster news 
searching results returned by the search engine. STC-NER uses 
the snippets returned from the searching results and then derives 
patterned information by means of named entity recognition. 
STC-NER makes a great contribute to the reduction of storage 
as well as the time complexity. Experiments show that STC-NER 
has a better performance in precision and efficiency than the 
traditional Suffix Tree Clustering (STC). 
KeyWords—suffix Tree; clustering; news reports; named entity 
recognition 
I. INTRODUCTION 
As is known to all that news reporting has become the 
main competition between web portals. Nowadays, the search 
engines also take part and break out a fierce competition. A 
survey [1] made by Nielsen/ NetRatings shows that news 
browsing and searching has become one of the important 
online activities. There are many commercial News search 
engines. Google News retrieves news information by more 
than 4,000 sources, organizes it in categories and automatically 
builds a Web page with the most important news for each 
category. Yahoo News runs an analogous service on more than 
5,000 sources [2]. The advantages of online news searching 
are timeliness and richness. The disadvantage is that the search 
engine goes through all the web pages and returns 
reduplicative news reports, which in turn increases the burden 
on users. 
News searching results need to be clustered before 
presented to the user. Clustering news reports searching results 
facilitates users browsing through search results more quickly 
and provides a better access to the information users are 
looking for. 
Document clustering is a special data clustering technique 
that clusters a document collection into meaningful 
sub-collections. The documents in each sub-collection are 
highly related (in some sense) to each other and vastly 
different (in the same sense) to the documents placed in other 
sub-collections [ 3 ]. Document clustering has long been 
investigated as a post-retrieval document visualization 
technique [4]. Among them K-means [5] clustering algorithm 
based on Vector Space Model (VSM) [6] is a classic algorithm 
and it has a wide range of application in scientific fields, for 
example, digital image processing, data mining, medical 
researches. However, the K-means clustering algorithm has its 
limits in document clustering, because it treats documents as 
bags of words, ignoring word sequences in the documents, on 
which the meaning of natural languages strongly depends [7]. 
The biggest shortcoming of K-means clustering algorithm is 
that it needs a stop-condition, specifically, the value of K. 
What’s more, K-means clustering algorithm has a restriction 
that each text be included in just one cluster, regardless of the 
possibility that one text may contain multiple subjects. 
The Suffix Tree Clustering (STC) [8] algorithm proposed 
by Zamir and Etzioni gave a good solution on these issues. 
There are two reasons why we study suffix tree model and 
STC. Firstly, this algorithm identifies sets of documents that 
share common phrases extracted from the snippets and treats a 
document as a set of words and is appropriate to Chinese 
document clustering. Secondly, one or several phrases are 
naturally selected to generate a topic summary to label the 
corresponding cluster during building the clusters [9]. 
STC is an incremental clustering algorithm and its time 
complexity is linear with regard to the document size [10]. A 
conventional STC algorithm construct a suffix tree model with 
documents and then clusters those documents by analyzing 
nodes in the model. We made some improvements to STC to 
2013 International Conference on Cloud Computing and Big Data
 978-1-4799-2829-3/13 $26.00 © 2013 IEEE
 DOI 10.1109/CLOUDCOM-ASIA.2013.102
 549978-1-4799-2830-9/14 $31.00 © 2014 IEE
 
 
make it more suitable in clustering Chinese news searching 
results. 
The unique feature of a news report is that it consists of 
certain elements, i.e. time, location, character and event. A 
method called Named Entity Recognition [11] can extract 
these elements in news reports. Named entities are phrases that 
contain the names of persons, organizations, locations, times 
and quantities [ 12 ]. The main task of Named Entity 
Recognition is to identify and classify Proper Nouns as well as 
meaningful phrases. In this paper we concentrate on four types 
of named entities: names of organizations, names of 
individuals, names of places, time expressions. By identifying 
these elements in each news report, we extract patterned 
information. These derived information then assemble a new 
document to replace the original one. This process is achieved 
by using a lexical tool for Chinese language named Natural 
Language Processing & Information Retrieval (NLPIR), which 
is also called Institute of Computing Technology, Chinese 
Lexical Analysis System 2013 (ICTCLAS2013)[13]. NLPIR is 
also an appropriate and necessary tool to implement word 
segmentation and POS tagging (part-of-speech tagging) [14]. 
The precision of word segmentation reaches as high as 
97.58%; the recall rate of unknown words recognition exceeds 
90% as well. NLPIR integrates word segmentation, POS 
tagging, which makes it a superior in terms of practicalness 
and precision. 
II. SUFFIX TREE MODEL AND STC ALGORITHM 
Zamir and Etzioni gave a good analysis on document 
clustering with STC algorithm. For example, the algorithm 
should take the document snippets instead of the whole 
documents as input, since the downloading of original 
documents is time-consuming; the clustering algorithm should 
be fast enough for online calculation; and the generated 
clusters should have readable descriptions for quick browsing 
by users, etc.[15] The conventional STC algorithm has five 
steps as following: 
A. Preprocessing of documents 
The document snippets are retrieved from search engine 
and make up the textual sources. Sentences are then identified 
and split into strings of words. During this process, stop words 
and symbols are removed.  
B. Building a Suffix Tree Model 
Build a Suffix Tree Mode by inserting the strings 
associated with each document into it. A suffix tree is a data 
structure that presents the suffixes of a given string in a way 
that allows for a particularly fast implementation of many 
important string operations. The suffix tree for a string S is a 
tree whose edges are labeled with strings, and such that each 
suffix of S corresponds to exactly one path from the tree's root 
to a leaf. It is thus a radix tree (more specifically, a Patriciatrie) 
for the suffixes of S [16]. 
Here is an example of construction of Suffix Tree Model 
with a set of documents: “cat ate cheese”, “mouse ate cheese 
too” and “cat ate mouse too”. 
Fig. 1 illustrates a Suffix Tree, which is constructed with 
these 3 documents.  
 
Fig. 1. Suffix Tree of three documents 
The label below a leaf node shows where the substring 
comes from. The first number in the squared label is the index 
of documents. The second number in the label is the start point 
of the substring in its original string. 
C. Merging base clusters 
Once the suffix tree has been built, each node on the suffix 
tree that contains multiple documents can be considered a base 
cluster. To reduce the number of clusters, base clusters are 
merged according to the following steps. 
Firstly, calculate the similarity value between two  
base clusters according to a certain algorithm. Base clusters 
are merged if the similarity value is 1. Given two base clusters 
Bm and Bn, the number of documents each base cluster has are 
|Bm| and |Bn|, |Bm Bn| is the overlap of their documents set.  
If:     	
  and     	
  
Then: set the similarity value of these two base 
clustersand to 1; 
Else: set the similarity value to 0; 
Where  is the threshold, which value is settled between 0 
and 1. The conventional algorithm is: while two base clusters 
are merged, these two base clusters are connected logically. 
These connected base clusters then form a base cluster graph, 
from which the final clustering result is derived. Fig. 2 
illustrates a base cluster graph of these 3 documents, each node 
represent a base cluster in the suffix tree. 
550
 
 
 
Fig. 2. A base clusters graph 
D. Acquiring clusters 
A cluster is defined as being a connected component in the 
base cluster graph. Each cluster contains the all the documents 
of all its base clusters. It can be implied that a document may 
emerge in different clusters simultaneously, which is an 
instance of soft-clustering. 
It is obviously that STC prefers to cluster text documents, 
other than numeric data. As the time complexity of this 
algorithm is linear with regard to the document size, the 
original document should be refined to reduce the complexity 
and time consummation of the clustering processing. 
III. STC-NER 
The distinctive methodology of the Suffix Tree Clustering 
with Named Entity Recognition (STC-NER) algorithm is that 
the input data of the algorithm is documents of name entities 
extracted from snippets of news searching results. This helps 
to simplify the documents. As the conventional method to 
construct a suffix tree with a document of m words takes  
 time, it is obviously that the size of document can 
affect the efficiency of STC algorithm on a large scale. 
In this paper, we make some improvements on the 
procedures of preprocessing and merging base clusters. And 
due to the special feature of news reports, we add a procedure 
called named entities extraction after the word segmentation to 
refine the documents before clustering.  
Here we also improve the suffix tree model to make it 
easier to build and operate during the clustering algorithm.  
Fig. 3 shows the framework of STC-NER. 
 
 
Fig. 3. Framework of STC-NER 
A. Preprocessing with Chinese documents 
The STC is proposed based on English web page. But the 
STC-NER in this paper aims at clustering news reports written 
in Chinese. The English written language is composed of 
words between which there are nature segmentations such as 
space character. Thus it is not difficult for the computer to 
analyze, understand and manipulate English written language. 
The Chinese written language, to the contrary, has no 
segmentation of any kind between words, which makes it hard 
for the computer recognition processing [17]. To the computer, 
Chinese written language is a string made up of Chinese 
characters and punctuations. Characters make up words, words 
make up phrases, phrases make up sentence, and then other 
language structures like paragraphs, chapters and sections. But 
not all the words are meaningful; these indistinctive words are 
called stop words, which are frequently occurring, insignificant 
words that appear in the documents. They are useless to index 
or use in search engines or other search indexes. Stopwords 
lists and stemming algorithms are two commonly used 
information retrieval techniques for preparing text document 
[9]. We hold a list of 842 stop words to filter out these 
meaningless words. 
Expressing a document in a simple and accurate way is the 
purpose of preprocessing. Preprocessing includes automatic 
word segmentation, removing punctuations and stop words, 
POS tagging and key words extraction [18]. Due to the special 
characteristic of news reports, we deploy named entity 
extraction within the process of preprocessing, to reduce the 
size of documents. 
1) Word segmentation 
Word segmentation is the first priority in preprocessing; it 
splits a sentence written in Chinese into words or phrases by 
their meanings. After the word segmentation, stop words and 
symbols are removed and the left words are regarded as 
meaningful ones. In order to put these words into different 
categories (or Name Entities), POS tagging (part-of-speech 
tagging) [14] is also a necessity. We use the open source 
lexical tool NLPIR (or ICTCLAS2013) [1313] to implement 
word segmentation and POS tagging. The tags that we concern 
are: “/t” for time, “/ns” for China, “/nsf” for foreign countries, 
“/nr” for names of people, “/nt” for names of organizations, 
“/n” for nouns and “/v” for verbs. Among them, words with 
tags of “/t”, “/ns”, “/nsf”, “/nr”, “/nt” are extracted as named 
551
 
 
entities, in other words, basic elements in news reports. Other 
words with tags of “/n” and “/v” are also extracted in 
sequences. 
2) Named entities extraction 
In the field of news reporting, the basic elements are: time, 
location, character and event, so it is very convenient for us to 
apply Name Entity Recognition to news reports. After the 
preprocessing tags words or phrases with their POS 
information, the Named Entity Recognition extracts elements 
according to the POS information.  
Here we extract six kinds of information and put them into 
a structure: 
struct NameEntity{ 
vector<WORD_CN> Time;// “/t” tag for time 
vector<WORD_CN> Location;//“/ns” tag for China, “/nsf” 
for foreign countries 
vector<WORD_CN> Name;//“/nr” tag for names of 
people,  
vector<WORD_CN> Organ;//“/nt” tag for names of 
organizations 
vector<WORD_CN> Nouns;//“/n” tag for nouns 
vector<WORD_CN> Verbs;// “/v” tag for verbs. 
}; 
Where “WORD_CN” is a customized structure that stands 
for a Chinese word.  
The pseudo-code for the implementation of the named 
entities extraction is shown in Algorithm 1. 
Algorithm 1: Extract Entities 
1: input <-- set of Chinese words with POS 
tags 
2: output <-- set of named entities 
3: for each entity (time, location, name, 
organization, nouns, verb){ 
4:   for each word (text){ 
5:     if word is an instance of entities  
6:     then{ 
7:       if this word is not involved 
8:       then add this word to the set of 
named entities 
9:       else discard this word 
10:     } 
11:   } 
12: } 
All these extracted words or phrases are then transformed 
in sequence of time, location, name of people, name of 
organization and event, where the event is a string of nouns 
and verbs with a same word sequence as in the original 
document. These documents are the final input of SCT-NER 
algorithm in this paper.  
Here is an example of how a snippet of news report is 
processed. The snippet is: “???????????? 5?
 53 ???.?u??????x 5???? J9P; ???n
 ?.?????????” After the word segmentation, the 
sentence is cut into words and labeled with POS tag:” ???
 ?????????/nt, 5 ?/t, 53 ?/t, ??/v, ?u?/ns, 
???/ns, ?x/v   3/m, ?/q, ?/n, ??/v, H7N9/n, ?
 ??/n, n?/n, ?/m, ?/n, ??/vi”, note that stop words 
and punctuations are already removed. Then the named entities 
are extracted and transformed into a new sentence in sequence 
of time, location, name of people, name of organization and 
event. The result is: “time{5?"53?}; location{?u?"??
 ?}; name of people{ }; name of organization{??????
 ??????}; event{??"?x"?"??"J9P;"???"
 n?"??}”. 
B. Improvements of Suffix Tree Model 
We can see from the Figure 1 that the suffix tree model 
uses a path definition while constructing the tree. The contents 
of a substring are labeled on the path from root to the leaf, in 
other words, edges between nodes in the suffix tree model. 
Each suffix of a string S corresponds to exactly on path from 
the tree’s root to a leaf. This may look good but it is hard to 
manipulate. In this paper, we put all these labels into the 
corresponding node. Each node is seen as a storage unit, 
together with all the necessary information like document 
index number, start point of substring, child node pointer and 
so on, which makes it easier to construct the tree model, 
traverse the tree and operate the node. 
The structure of each node contains four main elements 
(see Fig. 4): 
 
Fig. 4. Node structure of suffix tree 
 Node label: it stores the content of a substring, in 
other words, the corresponding content of the string 
S. 
 Document index label: this is an array of variable 
length. Each element in this array is the label of a 
document which contains substring of this node. 
 Start point of substring: it is used to mark the start 
point of the substring in the string S. The substring 
is stored in the node label mentioned above. In 
order to find a substring more quickly, the start 
point of a substring is the start position in the 
original string before preprocessing. 
 Child node pointer: this is an array of variable 
length. Each element in this array is a pointer that 
point to a node in the next tier. Note that each node 
itself is a pointer too.  
Fig. 5 shows the improved suffix tree model with 
document “ABDCABCD” based on the new node structure. 
552
 
 
This model has the advantage over the conventional suffix tree 
clustering algorithm as following:  
 
Fig. 5. The structure of improved suffix tree model 
 Change node string from edge label to node storage. 
We use node definition instead of path definition 
for flexibility in clustering operations. Besides, 
Each node contain all the information needed in the 
process of clustering, which facilitated for users to 
traverse the suffix tree. 
 We transform the data structure of the suffix tree 
model into linked nodes. To traverse the whole 
suffix tree, we start from the root node, and look 
into every node that the child node pointer point to. 
When build the suffix tree, we need not a fully 
depth traverse. If the node label in a node does not 
match the substring from the beginning, then all its 
sub nodes will not match too. This can save a lot of 
time and operation for traversing. 
C. Merging base clusters 
Each node of the suffix tree represents a group of 
documents and a phrase that is common to all of these 
documents. Therefore each node is a base cluster. Before 
merging two base clusters, we use the same method as defined 
in the Zamir and Etzioni paper and we found the threshold  
 = 0.8 is appropriate to conduct the document clustering. 
IV. EXPERIMENT EVALUATION 
We implemented STC-NER algorithm in C++ on a 
Windows PC with a Core2 Duo 3.0GHz processor and 2GB 
memory. 
A. Data sets 
Two groups of data sets are used to conduct the 
experiments. One group is news reports with similar themes 
and the other is news reports within same time period. All 
these documents are snippets of news reports from news 
search engine like Baidu News and Google News and are 
written in Chinese. We also merged the duplicated items. They 
are different in terms of documents sizes, cluster sizes, number 
of classes. The detail information of these textual resources is 
lists in the tables below: 
TABLE I. DATA SETS OF NEWS REPORTS IN CHINESE WITH 
SIMILAR THEMES 
Data 
Set 
Theme 
(Query) 
Number 
of 
Classes 
Number of 
Documents Details of classes 
A1 
???
 ? 
3 28 
???????15????
 ?????8???????
 ??5? 
A2 ??? 2 25 
?u????? 3??16??
 C?????? 4??9? 
A3 
?? 
0?0?0?0?? 
2 18 
‚???0?0?0?0???11????
 ??0?0?0?0???7? 
A4 
?Â?
 ? 
2 18 
?Â??? \ \?9???Â?
 ????9? 
TABLE II. DATA SETS OF NEWS REPORTS IN CHINESE WITHIN SAME TIME 
PERIOD 
Data 
Set 
Time 
Period 
Number 
of 
Classes 
Number of 
Documents Details of classes 
B1 3.29~3.31 2 31 
???????15???
 u????? 3??16? 
B2 3.29~3.31 3 39 
??S????15???
 ??????15??‚?
 ?????9? 
B3 3.29~3.31 4 55 
??S????15???
 ??????15??‚?
 ?????9???u??
 ??? 3??16? 
B. Evaluation of preprocessing 
 We introduced the procedure of entities extraction by 
means of named entity recognition. The whole preprocessing 
steps including: word segmentation, stop words and symbols 
removal, entities extraction. We monitored the number of 
words as the input of constructing the suffix tree model. Fig. 6 
shows a remarkable decline in the number of words in the 
Suffix Tree Model. 
553
 
 
0.00
 500.00
 1000.00
 1500.00
 2000.00
 2500.00
 A1 A2 A3 A4 B1 B2 B3
 N
 um
 be
 r 
o
 f 
w
 o
 rd
 s
 Documents Sets
 STC-NER
 STC
  
Fig. 6. The number of key words in the model of the STC-NER vs. the STC
 The less the words there are in the suffix tree model, the 
faster the algorithm performs. We can see from the figure that 
the STC-NER reduces about 40% of total number of words in 
the suffix tree model.  
C. Evaluation method of clustering algorithm 
To evaluate the effectiveness of the STC-NER algorithm, 
we use F-measure value to the quality of the clustering result. 
The F-measure is a harmonic combination of the precision and 
recall values used in information retrieval [19].It is commonly 
used in evaluating the effectiveness of clustering and 
classification algorithms. 
F-measure evaluates the result of clustering with the idea 
of considering both the precision P(i , j) and recall R(i , j) of 
each cluster j for class i. The definition of precision and recall 
are: 
P(i , j) = Nij / Ni 
R(i , j) = Nij / Nj 
where Nij is the number of documents of class i that are 
included in cluster j; Nj is the number of documents in cluster 
j; Ni is the number of documents in class i. 
The definition of corresponding F-measure F(i , j) is: 
                 
Then, the F-measure for the whole clustering result is 
defined as: 
   
  !" #$%
 $
  
where n is the total number of documents in data set. In 
general, the larger the F-measure is, the better the clustering 
result is [20]. Fig. 7 shows the improvements in F-measure. 
554
 
 
 
Fig. 7. F-measures of STC-NER vs. STC 
V. CONCLUSION 
In this paper we propose a new text document clustering 
algorithm named STC-NER, which stands for Suffix Tree 
Clustering with Named Entity Recognition. This algorithm is 
supposed to cluster news reports written in Chinese returned 
from news search engine. Due to the characteristics of news 
reports, we import the method of entities extraction by means 
of named entity recognition into the document preprocessing. 
This helps to reduce the high dimension of the documents by 
reducing the number of key words. After the preprocessing, 
number of nodes in the suffix tree model decreases, which 
makes the algorithm more effective. 
As the STC algorithm was proposed for clustering English 
documents, we adapt Chinese words segmentation to make the 
algorithm suitable for Chinese documents as well. The Chinese 
words segmentation is accomplished with the open source tool 
NLPIR. In this way all documents written in Chinese are also 
treated as strings and they are appropriate for the STC 
algorithm. 
We also proposed a new structure for the suffix tree model 
by moving labels of the string from edges to the nodes. Each 
node is a storage unit, together with all the necessary 
information like document index numbers, start point of 
substrings, child node pointer and so on. This facilitates the 
traversing and operation of the algorithm. 
During the experiments, we found that the threshold 
8.0=?
  is appropriate to conduct the document 
clustering. We used two groups of data sets to conduct the 
experiments. We can see from the results that our STC-NER 
algorithm gets better performance than conventional STC 
algorithm in both precision and efficiency when clustering 
news searching results returned by the search engines. 
ACKNOWLEDGEMENT 
This research was supported by National 863 Program 
(No. 2011AA01A204), P. R. China. 
REFERENCES 
 
[1]  http://www.nielsen-netrations.com/ 
[2]  A. Gulli, The Anatomy of a News Search Engine, Dipartimento di 
Informatica, University of Pisa 
[3]  Muhammad Rafi, Mehdi Maujood, Murtaza Munawar Fazal, Syed 
Muhammad Ali. “A comparison of two suffix tree-based document 
clustering algorithms”, Information and Emerging Technologies 
(ICIET), National University of Computer & Emerging Sciences, 2010. 
[4]  M. A. Hearst and J. O. Pedersen, “Reexamining the cluster hypothesis: 
Scatter/Gather on retrieval results”, Proceedings of the 19th International 
ACM SIGIR Conference on Research and Development in Information 
Retrieval (SIGIR’96), 1996, pp.76-84. 
[5]  MacQueen J B. “Some Methods for Classification and Analysis of 
Multivariate Observations”. Proceedings of the Barkley Symposium on 
Mathematical Statistics and Probability, Berkeley: Berkeley University 
of California Press, 1967. 
[6]  Salton G; Yang C S. “On the specification of term values in automatic 
indexing”, 1973 (04) 
[7]  Yanjun Li, Soon M. Chung, John D. Holt. “Text document clustering 
based on frequent word meaning sequences”, Data & knowledge 
Engineering, 64, 2008, 381-404. 
[ 8 ]  Zamir O, Etzioni O. “Web Document Clustering: a Feasibility 
Demonstration”. Proceedings of the 19th International ACM SIGIR 
Conference on Research and Development in Information Retrieval 
(SIGIR’ 98), Melboume, 1998. 
[9]  Hung Chim, Xiaotie Deng. “A new suffix tree similarity measure for 
document clustering”, International World Wide Web Conference 
Committee (IW3C2), Canada, 2007. 
[10] Yang Jian-wu. “A Chinese Web Page Clustering Algorithm Based on the 
Suffix Tree”, Wuhan University Journal of Natural Sciences, 2004, 9(5) 
[ 11 ] Pawlak Z. Rough sets. International Journal of Computer and 
 
0.00%
 10.00%
 20.00%
 30.00%
 40.00%
 50.00%
 60.00%
 70.00%
 80.00%
 90.00%
 100.00%
 A1 A2 A3 A4 B1 B2 B3
 F-
 M
 e
 a
 su
 re
 Document Sets
 STC-NER
 STC
 555
 
 
 
Information Science. 1982, 11(5): 341~356 
[ 12 ] Erik F. Tjong Kim Sang, Fien De Meulder. Introduction to the 
CoNLL-2003 shared task: language-independent named entity 
recognition, CONLL ’03 Proceedings of the seventh conference on 
Natural language learning at HLT-NAACL, 2003, Volume 4. 
[13] Zhou Li-na, Zhang Dong-song. “NLPIR: a theoretical framework for 
applying natural language processing to information retrieval”, Journal 
of the American Society for Information Science and Technology, 2003 
[14] TURISH B. “Part-of-speech tagging with finite-state morphology[C]”// 
Proceedings of the International conference on Collocations and Idioms: 
Linguistic, Computational, an Psycholinguistic perspective. Berlin 
German, 2003: 18-20 
[15] Hua-Jun Zeng, Qi-Cai He, Zheng Chen, Wei-Ying Ma, Jinwen Ma. 
“Learning to cluster web search results”, Proceedings of the 27th annual 
International ACM SIGIR Conference on Research and Development in 
Information Retrieval (SIGIR’ 04), New York, USA, 2004. 
[16] Zamir O., Etzioni O., Grouper, “A Dynamic Clustering Interface to Web 
Search Results”, Proceedings of the English International World Wide 
Web Conference (WWW), Toronto, Canada, May 1999. 
[ 17 ] Liu Ting. “Study and Application of Chinese Automatic Word 
Segmentation in Full Text Retrieval”, Nanjing University Of 
Aeronautics And Astronautics, 2007,01 
[18] Zhang Chun-xia, Hao Tian-yong. “The state of the art and difficult in 
automatic Chinese word segmentation”, Xitong Fangzhen Xuebao / 
Journal of System Simulation, 2005, v17: p138-143,147 
[19] C. J. van Rijsbergen. Information Retrieval. Second Edition. Butterworts, 
London, 1979. 
[20] M. Steinbach, G. Karypis, V. Kumar, “A comparison of document 
clustering techniques”, KDD-2000 Workshop on Text Mining, 2000. 
556

Polarity Identification of Sentiment Words based on
 Emoticons
 Shuigui Huang?, Wenwen Han?, Xirong Que? and Wendong Wang?
 ? State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications,
 hsguidemail@gmail.com, hww860810@gmail.com, rongqx@bupt.edu.cn, wdwang@bupt.edu.cn
 Abstract—The orientation of sentiment words plays an impor-
 tant role in the sentiment analysis, but existing methods have
 difficulty in classifying the orientation of Chinese words, espe-
 cially for the newly emerged words in Internet. Most approaches
 are mining the association between sentiment words and seed
 words using the big corpora and manually labeled seed words
 with definite orientation. But less work has ever focused on the
 efficient seed words selection. As we observed, emoticons, which
 are widely used on social network because of the simplicity
 and visualization, are good indicators for sentiment orientation.
 Thus this paper proposes the sentiment word model based on
 emoticons, which built orientation model of sentiment words with
 the orientation of emoticons, and train the model with the SVM
 classifier. Meanwhile, this work proposes a high efficient way to
 automatically classify the orientation of emoticons. Experiments
 show the precision rate of emoticon classification could reach
 93.6%, and that of sentiment words classification could be 81.5%.
 Keywords-sentiment analysis; emoticon; sentiment words; e-
 moticon based model; similarity; SVM
 I. INTRODUCTION
 Sentiment analysis[1] is used to analysis the users’ opinions
 expressed in positive or negative comments. It has become
 one of the major research topics in the subjective informa-
 tion processing. Using sentiment analysis, users’ opinions
 could be classified as having positive orientation or negative
 orientation[2], [3], which benefits to companies and govern-
 ment to know people’s emotion.
 The fact that sentiment words are strong predictors of
 subjectivity[4] makes them be the important role to sentiment
 analysis. Sentiment words are words that convey positive
 or negative sentiment orientation, which could be adjective,
 adverb and idiom etc. The work[2], [3] category sentiment
 words as having positive orientation or negative orientation.
 There are two main methods to classify the orientation of
 sentiment words: one is based on corpora[5], [6], [7] and the
 other is based on dictionary[2], [8], [9]. The one based on
 corpora is to compare the association of sentiment words with
 positive seed words and negative seed words, concretely the
 sentiment words are positive if the sentiment words are more
 associated with positive seed words, otherwise negative. But
 this method has a problem: it didn’t tell us the selection of seed
 words. Whereas, seed words selection is not only inevitable,
 but also a difficult task in sentiment classification, because
 different seed words usually yield results of diverse quality.
 The other based on dictionary is to use the synonymous and
 antonyms of seed sentiment words with definite orientation to
 predict the polarity of sentiment words. Particularly, the syn-
 onymous of positive seed words and the antonyms of negative
 seed words have positive orientation, while the synonymous of
 negative seed words and the antonyms of positive seed words
 have negative orientation. However this method could not be
 used to classify the newly emerged words which didn’t exist
 in the dictionary. The drawbacks encourage us to seek polarity
 assignment method which could deal with both the seed words
 selection problem and new merged words problem.
 Emoticons are widely used in the social network, Weibo for
 instance, to express users’ personal emotion, which are a set
 of dynamic images and are portrayed like people’s expression.
 For example: “Suzhou has an explosion! ”
 Obviously, emoticons can convey users’ positive and neg-
 ative sentiment clearly and visually, and the orientation is
 usually definite, which make them suitable to serve as an-
 notated seed opinion words. The work[10] also points out that
 emoticons can convey strong sentiment. They help the users
 to express their mood when post statuses. The work[11] uses
 the orientation of emoticons to represent the orientation of
 statuses. For example, “Life is wonderful ” is considered as
 a positive status because “ ” is a positive emoticon while
 “Fatso is terrible ” is considered as a negative status because
 “ ” is a negative emoticon.
 As we observed, sentiment words appearing with emoticons
 have nearly the same orientation of emoticons. For example
 above, “Wonderful” and “ ” are both positive, and “terrible”
 and “ ” are both negative. The findings inspire us to use
 emoticons in guiding the recognition of polarities.
 Our paper is based on the intuition that the orientation of
 sentiment words share the same orientation of emoticons
 when they co-occur. The differences between our work and
 the previous work are: 1. use the emoticons to build the model
 for sentiment words. 2. find an effective method to compute
 the orientation of emoticons. As indicated above, our approach
 is performed in three steps:
 1) Classify the orientation of emotions into positive and
 negative classes.
 2) Construct a dimension-reduced vector space model for
 emoticon-based word sentiment classification, and prin-
 ciple component extraction is used to reduce dimensions.
 3) Train the classifier with SVM algorithm and optimized
 the parameters via 10-fold cross validation.
 2013 Ninth International Conference on Computational Intelligence and Security
 978-1-4799-2548-3/13 $31.00 © 2013 IEEE
 DOI 10.1109/CIS.2013.35
 134
II. RELATED WORK
 Our work is related to orientation identification for senti-
 ment words. The existing work for orientation classification
 of sentiment words could be categorized into two approaches:
 corpora-based approaches and dictionary-based approaches.
 Our work falls into the category of corpora-based approaches.
 The work[6] predicts orientation of sentiment words by
 detecting pairs of such words conjoined by conjunctions such
 as “and”, “or”, “but”, in a large document set. The underlying
 intuition is that the orientation of conjoined sentiment words
 are subject to some linguistic constraints. Two sentiment words
 are usually the same orientation when they are conjoined
 by “and” while the two sentiment words are the opposite
 orientation when they are conjoined by “or”. The weakness of
 this method is that it relies on the conjunction relations and
 needs a large amount of manually tagged training data.
 In the work[5], the orientation of a sentiment word is
 calculated as the point mutual information between the given
 word and the set of manually labelled positive words, minus
 the point mutual information between the given word and
 the set of manually labelled negative words. The mutual
 information is estimated by issuing queries to a search engine
 and noting the number of hits. If the result is a positive
 value, then the sentiment word has the positive orientation.
 If the result is a negative value, then the sentiment word has
 the negative orientation. The underlying intuition is that the
 positive word is more likely to associate with positive words,
 otherwise, the negative word is more likely to associate with
 negative words. The weakness of this method is that it is
 hard to select the proper positive and negative seed words
 in Chinese. And also their work requires additional access to
 the Web Search Engine, which is time consuming.
 The work[2], [9] takes advantage of WordNet to predict
 the orientations of sentiment words. The underlying intuition
 is that in WordNet, the synonymous words have the same
 orientation, while the antonymous words have the opposite
 orientations. In the work[8], a words’ network is constructed
 by connecting pairs of synonymous words in WordNet. The
 orientation of a word is decided by its shortest path to the
 positive sentiment seed words and the negative sentiment seed
 words. The work[12] determines the orientation of words
 based on glosses in an online dictionary. The classifier is
 trained on glosses of an unknown word to categorize the
 sentiment words as positive or negative. The weakness of this
 method is that for dictionary-based methods, they are unable
 to classify the orientation of sentiment words that are not in
 the dictionary, for instance, the newly cyberwords.
 III. EMOTICON-BASED SENTIMENT WORD MODEL
 Fig. 1 gives the framework to classify the orientation of
 sentiment words. Recall that the task of classifying senti-
 ment words’ orientation is telling whether the orientation of
 sentiment words is positive or negative, this paper performs
 this task in three main steps: 1. classifying the orientation
 of emoticons; 2. constructing the vector space model for
 sentiment words; 3. training the model with the SVM.
 Fig. 1. Proposed Words’ Classification Approach by Using Emoticons
 A. Classifying the Orientation of Emoticons
 To use emoticons in guiding the word sentiment classifica-
 tion, above all we need to obtain a set of annotated emoticons
 with known semantic orientation. In this section, this prepro-
 cess work is done with small proportion of manually labeling
 and large part of automatic annotation. Before handling the
 classification of emoticons, we define some symbols first:
 Definition 3.1: EOC represents the set of emoticons con-
 tained in the corpora, and ne is the size of the set EOC.Then
 EOC = {e1, e2, e3, · · · , ene}, where e1, e2, e3, · · · , ene rep-
 resents the emoticon.
 Definition 3.2: n(ei) represents the count of the emoticon
 ei occurring in the corpora, i = 1, 2, · · · , ne.
 Definition 3.3: n(ei, ej) represents the count of co-
 occurrence between emoticon ei and ej in the corpora, i, j =
 1, 2, · · · , ne. Especially, n(ei, ei) = n(ei)Definition 3.4: Ve represents the co-occurrence vector of
 the emoticon e with each emoticon in EOC, then Ve is:
 Ve = [n(e, e1), n(e, e2), · · · , n(e, ene)] (1)
 where e1, e2, · · · , ene ? EOC
 Definition 3.5: ind-aff(ei) represents the orientation of
 emoticon ei. That is:
 ind-aff(ei) =
 {
 1 ei is positive
 ?1 ei is negative
 , ei ? EOC (2)
 This paper uses a method of both manually tagging and au-
 tomatically annotation to classify the orientation of emoticons:
 1. Manually tagging: The emoticons whose orientation are
 obvious are easy to label, so we can label them manually.
 For the emoticons whose orientation are not that obvious, the
 automatically annotation is used. We tag positive emoticons
 with 1 and negative ones with -1, resulting in 116 emoticons.
 2. Automatically annotation: For the emoticons whose ori-
 entation are not obvious to label, we use an unsupervised
 learning method to label them. We automatically annotated
 524 emoticons in this phase.
 135
The intuition of automatically annotation of the emoticon
 orientation is: if different emoticons appear in the same status,
 it is the usual case that the emoticons express the same
 orientation, because the attitude of writers within a short
 message is usually stable. Suppose the set of emoticons that
 manually tagged as positive is P E, and the set of negative
 emoticons is NE. The orientation estimation measure of an
 unlabelled emoticon could be formulated as (3):
 ind-aff(e) =
 ∑
 pe?PE
 S(e, pe)? ind-aff(pe)+
 ∑
 ne?NE
 S(e, ne)? ind-aff(ne)
 (3)
 ind-aff(e) calculates the orientation difference between e
 and seed emoticons. S(e1, e2) means the similarity of two
 emoticons e1 and e2, as formulated in (4), which measures the
 weight of each seed emoticons in predicting the orientation of
 a new emoticon. Here, Ve follows the definition in (1).
 S(e1, e2) = cos(Ve1 ,Ve2)
 =
 Ve1 • Ve2
 ||Ve1 || ? ||Ve2 ||
 (4)
 Experiments in Section IV show that this method can clas-
 sify the orientation of emoticons with high accuracy. With this
 method, we can classify the set EOC into positive emoticons
 EOCp and negative emoticons EOCn, which will be used to
 build sentiment word model.
 B. Construct the Model for Sentiment Words
 After classifying the orientation of emoticons, we describe
 the task of constructing the model for sentiment words. We
 propose an emoticon-based sentiment word model ECWM. In
 this model, the orientation of sentiment word is represented
 by the co-occurrence information of sentiment words with
 emoticons and the orientation of emoticons. We describe some
 definitions first:
 Definition 3.6: n(sword, ej) means the sentiment word
 sword and the emoticon ej are co-occurred in n(sword, ej)
 statuses, j = 1, 2, · · · , ne.
 Definition 3.7: Vsword represents the feature vector of the
 sentiment word sword. The jth vector component of Vsword
 is the association between the sentiment word and the emoti-
 con ej , whose value is f(sword, ej), j = 1, 2, · · · , ne, then:
 Vsword = [f(sword, e1), · · · , f(sword, ene)] (5)
 And f(sword, ej) is computed as (6)
 f(sword, ej) = n(sword, ej)? ind aff(ej) (6)
 In this model, two problems are solved: selecting the
 features and extracting Vsword’s principal components.
 1) Selecting the Features: We use emoticons as features,
 because their clear and definite orientation make them more
 appropriate to serve as features than ordinary words. Moreover,
 its broad usage on the Internet could contribute largely to the
 coverage of messages. Hence, emoticons are used to construct
 VSM model of sentiment words.
 2) Extracting the Principal Components: There are hun-
 dreds of thousands of emoticons in a large corpora, which
 makes the vector Vsword a high dimensional vector. Hence,
 there is a need to extract the principal components. In fact,
 some emoticons appear rarely while some are redundant,
 which increases the computation workload and decrease the
 accuracy of the model.
 Singular value decomposition(SVD) is used to extract the
 principal components of the model[13]. The method can be
 depicted as the following four steps.
 Firstly, suppose the set of sentiment words is SW , the size
 of SW is n(SW ), that is SW = {sword1, · · · , swordnw},
 and sword1, · · · , swordnw represents the sentiment word.
 The feature matrix is constructed for the set SW , which we
 called A:
 A =
 ?? Vsword1.
 .
 .
 Vswordnw
 ?? (7)
 Secondly, SVD is applied to A, to decompose A into a
 product of three matrices as follows:
 A = U ??? V T (8)
 where U is nw?nw column orthogonal form, ? is a nw?ne
 rectangular diagonal matrix with nonnegative real numbers on
 the diagonal, and V is ne?ne column orthogonal form. The
 diagonal entry ?i,i of ? is the singular value of matrix A.
 Thirdly, let ?k be the diagonal matrix formed from the top
 k singular values, where k < ne, and the top k singular values
 are ?1,1, ?2,2, · · · , ?k,k. Then we define r to be:
 r =
 ∑k
 i=1?i,i∑ne
 j=1?j,j
 (9)
 What r means is the rate of information that the top k
 singular values contain. We make r equal to 99% to let the
 top k singular values depict 99% information of the original
 matrix.
 Finally, when selecting the top k singular values to form
 the diagonal matrix ?k, and let Uk and Vk be the matrices
 produced by selecting the corresponding columns from U and
 V , then extracting the k principal components from original
 matrix could be done by following:
 Bk = A? Vk (10)
 Bk is a nw ? k matrix, and the information rate it contains
 is r. Remember, the feature num is k, which is much smaller
 than that of original feature num ne.
 C. Train the Vector Space Model of Sentiment Words
 The last step is to train the model using the machine learning
 method. In this paper, SVM is chosen, which is widely used in
 machine learning and demonstrated effective. We also chose
 the radial basis function(RBF) kernel as the kernel function.
 For training the model, LibSVM[14] is used. LibSVM is an
 efficient SVM implements package for Java language. There
 are two parameters we should set manually, and 10-fold cross
 validation is used to find the best parameters.
 136
IV. EXPERIMENTS AND DISCUSSION
 A. Build the Corpora
 The Weibo statuses were crawled by Xiaoxin crawler. There
 are totally 12,543,164 statuses from date time August the 18th,
 2009 to December the 27th, 2011. To make sure the quality of
 statuses, improper statuses are filtered out, including emoticon-
 free statuses and too short statues(less than 4 words), resulting
 in 3,921,794 statuses and 640 different kinds of emoticons. In
 addition, we indexed all filtered statuses to improve efficiency
 of the approach using Lucene 4.0(http://lucene.apache.org/)
 B. The Accuracy of Orientation Classification of Emoticons
 To validate accuracy of orientation classification of emoti-
 cons, the manually labeled emoticons set [11] is used, which
 contains 121 emoticons. In the set, emoticons are classified
 into four different sentiment categories, including “angry”,
 “disgusting”, “joyful” and “sadness”. The sentiment categories
 in our paper are positive and negative. So the mapping is built:
 “angry”, “disgusting”, “sadness” are classified as negative
 while “joyful” is classified as positive. After reclassifying, pos-
 itive emoticons set ET p contains 58 emoticons and negative
 emoticons set ET n contains 63 emoticons.
 The method of validating for (3) is following. The accuracy
 is used to measure the performance of the algorithm, which
 is defined as (11)
 accuracy =
 #(num of emoticons classified correctly)
 #(num of total tagged emoticons) (11)
 To avoid the uneven distribution of emoticons in the corpora,
 we use different training emoticons and testing emoticons to
 make multiple trials, and synthesis the average accuracy rate
 of these trials as the final accuracy rate. We randomly select k
 positive emoticons ET pk and k negative emoticons ET nk as the
 training set, with remaining emoticons ETrest as the testing
 set. Here, we run p = 100 times for each separate k. Defining
 the accuracy rate of each trial for each value k as fki just as
 (11) says, and we use the average accuracy rate to measure
 the p trials, which is calculated as (12):
 avgk =
 ∑p
 i=1 fki
 p
 (12)
 Fig.2 shows the average accuracy of the algorithm. To
 measure the average accuracy, we use different number of seed
 emoticons. Here, k is in the range [5, 55] with step 5.
 From Fig.2, we can see that the average accuracy is high.
 When we use 5 seed emoticons, the average accuracy rate is
 86.2%, and the average accuracy rate can reach to 90.9% when
 we use 10 seed emoticons. Moreover, when we use more seed
 emoticons, the average accuracy is enlarged consequently.
 C. Experiment of Classifying Sentiment Words’ Orientation
 To validate the accuracy of sentiment words’ orientation
 classification, the labeled sentiment words’ ontology[15] from
 the Dalian University of Technology Information Retrieval
 laboratory are used(http://ir.dlut.edu.cn/). There are 27,466
 sentiment words in this ontology. These sentiment words are
 Fig. 2. The Average Accuracy with the Number of Seed Emoticons
 tagged with three features: the sentiment strength, orientation
 and part-of-speech.
 But not all the words are proper for the experiments. We
 filter the ontology to get 1500 words. The filter rules are:
 1. Filter the noun and verb. Intuitively, most sentiment
 words are adjective, adverb and prepositional phrase. While
 lots of the noun and verb sentiment words in the ontology are
 not exactly sentiment words. So we filter them out.
 2. Filter the words with weak sentiment strength and am-
 biguous meanings. Sentiment words with definitely orientation
 are considered to be more proper than those weak sentiment
 ones. So we just keep the sentiment words with sentiment
 strength value equal to or larger than 5.
 3. Filter the words that are not in the corpora. Most words
 in the ontology are too official to use in the Internet language.
 So we only keep the words that are in our crawled corpora.
 Suppose Ttest is the set of 1500 sentiment words. All the
 sentiment words in Ttest are labeled as either positive or
 negative. Suppose the correctly classified words set is Tcorrect,
 then the accuracy of this approach fw is Tcorrect/Ttest.
 To validate the effectiveness of the algorithm, two baselines
 are used:
 1) PMI+W[5]: PMI+W is short for PMI with word seeds.
 This approach is an effective approach to classify the orienta-
 tion of sentiment words, which uses the point mutual informa-
 tion to compute the association of two sentiment words. It first
 tags two seed sets: one set is P w which contains the positive
 seed sentiment words, and the other set is Nw which contains
 the negative seed sentiment words. The orientation of a given
 word is calculated from the strength of its association with the
 positive seed words, minus the strength of its association with
 the negative seed words, just as (13) shows:
 P MI(sword) =
 ∑
 p?Pw
 A(sword, p)?
 ∑
 n?Nw
 A(sword, n) (13)
 A(word1, word2) is the measure of association between
 word1 and word2, which is the point mutual information of
 two words. In [5], 7 positive words and 7 negative words are
 selected as the seed words. But the author didn’t tell how to
 select the 14 seed words. The rules we selected are: 1. seed
 words should appear frequently in the corpora. 2. seed words
 should have definitely orientation. The selected seed words are
 listing as Table I.
 137
TABLE I
 SEED SENTIMENT WORDS USED IN PMI+W
 Positive seeds Negative seeds
 Word n(Word) Word n(word)
 (joyful) 113,991 (poor) 82,401
 (happy) 83,250 (wronged) 36,677
 (happiness) 54,095 (contempt) 27,627
 (wonderful) 27,894 (despair) 25,027
 (beautiful) 23,362 (sad) 14,089
 (handsome) 20,450 (disgusting) 9,629
 (fine) 18,128 (catty) 9,400
 TABLE II
 SEED EMOTICONS USED IN PMI+E
 Positive seeds Negative seeds
 Emoticon n(Emoticon) Emoticon n(Emoticon)
 515,559 272,282
 271,247 126,016
 262,008 92,197
 234,699 85,064
 162,469 82,401
 151,573 72,998
 145,506 69,151
 TABLE III
 THE ACCURACY OF DIFFERENT ALGORITHMS
 Approaches Accuracy Learning type Feature number
 PMI+W 68.5% unsupervised 14 words
 PMI+E 78.7% unsupervised 14 emoticons
 ECWM 81.5% supervised 80
 2) PMI+E: PMI+E is short for PMI with emoticon seeds.
 It’s similar with PMI+W. But what we select emoticons as
 the seeds. That means, the orientation of a given word is
 calculated from the strength of its association with a set of
 positive emoticons, minus the strength of its association with
 a set of negative emoticons. The rules that we select the seed
 emoticons are the same as the approach of the PMI+W. The
 seed emoticons are listing as Table II.
 The overall result is showed as Table III. For the result of
 ECWM, the feature number of original feature vector Vsword
 is 640. After the reduction of the features, the feature number
 of principal components of Vsword is 80, which is much less
 than the original features. What means that the original vector
 does have lots of redundant features.
 Table III shows that emoticons are better features. The
 accuracy of PMI+W is just 68.5%, compared to that of
 PMI+E, which is 10.2% higher, and ECWM is 2.8% higher
 than that of PMI+E. The seeds play the key role. In fact, the
 selection of seeds is the process of human intervention, and
 sentiment words are more widely used in terms of usage. We
 have a large range to select the seed sentiment words, but the
 range of emoticons are much more limited. So the error that
 selection of emoticons brings is smaller.
 V. CONCLUSION
 This work aims to classify the polarity of sentiment words.
 The way we did it is using the emoticons. First we proposed an
 efficient approach to classify the polarity of emoticons. Based
 on the polarity of the emoticons, as well as the association
 between the sentiment word and emoticons, we build the
 sentiment word’s model through the emoticons, which is called
 ECWM. Experimental results demonstrate that ECWM was a
 good model to represent the orientation of sentiment words.
 For the future work, more data would be crawled to cap-
 ture the rich co-occurrence between emoticons and sentiment
 words. Moreover, the polarity of sentiment words would be
 used to classify the orientation of statuses in Weibo.
 ACKNOWLEDGMENT
 This work was partially supported by the Fundamental Re-
 search Funds for the Central Universities, the National Natural
 Science Foundation of China under Grant No. 61271041;
 National Basic Research Program of China (973 Program)
 under Grant No. 2009CB320504
 REFERENCES
 [1] B. Pang and L. Lee, “Opinion mining and sentiment analysis,” Found.
 Trends Inf. Retr., vol. 2, no. 1-2, pp. 1–135, Jan. 2008.
 [2] M. Hu and B. Liu, “Mining and summarizing customer reviews,” KDD,
 August 2004.
 [3] P. D. Turney, “Thumbs up or thumbs down?: semantic orientation
 applied to unsupervised classification of reviews,” in Proceedings of
 the 40th Annual Meeting on Association for Computational Linguistics,
 ser. ACL ’02. Stroudsburg, PA, USA: Association for Computational
 Linguistics, 2002, pp. 417–424.
 [4] V. Hatzivassiloglou and J. M. Wiebe, “Effects of adjective orientation
 and gradability on sentence subjectivity,” ser. COLING ’00, 2000.
 [5] P. D. Turney and M. L. Littman, “Measuring praise and criticism:
 Inference of semantic orientation from association,” ACM Transactions
 on Information Systems, vol. 21, pp. 315–346, October 2003.
 [6] Hatzivassilogou, Vasileios, and K. R. McKeown., “Predicting the seman-
 tic orientation of adjectives,” Proceedings of ACL, pp. 174–181, 1997.
 [7] B. Pang and L. Lee, “A sentimental education: Sentiment analysis using
 subjectivity summarization based on minimum cuts,” in Proceedings of
 the 42nd annual meeting on Association for Computational Linguistics.
 Association for Computational Linguistics, 2004, p. 271.
 [8] J. Kamps, M. Marx, R. J. Mokken, and M. De Rijke, “Using wordnet
 to measure semantic orientations of adjectives,” 2004.
 [9] S.-M. Kim and E. Hovy, “Determining the sentiment of opinions,”
 in Proceedings of the 20th international conference on Computational
 Linguistics, ser. COLING ’04, 2004.
 [10] S. AOKI and T. University, “A method for automatically generating the
 emotional vectors of emoticons using weblog articles,” ACACOS, 2011.
 [11] J. Zhao, L. Dong, J. Wu, and K. Xu, “Moodlens: An emoticon-based
 sentiment analysis system for chinese tweets,” KDD, August 2012.
 [12] A. Esuli and F. Sebastiani, “Determining the semantic orientation of
 terms through gloss classification,” in Proceedings of the 14th ACM
 international conference on Information and knowledge management,
 ser. CIKM ’05. New York, NY, USA: ACM, 2005, pp. 617–624.
 [13] I. Jolliffe, Principal component analysis. Wiley Online Library, 2005.
 [14] C.-C. Chang and C.-J. Lin, “LIBSVM: A library for support
 vector machines,” ACM Transactions on Intelligent Systems and
 Technology, vol. 2, pp. 27:1–27:27, 2011, software available at
 http://www.csie.ntu.edu.tw/ cjlin/libsvm.
 [15] L. Xu, H. Lin, and Y. Pan, “The construction of the emotional vocabulary
 ontology[j],” Journal of The China Society For Scientific and Technical
 Information, vol. 27(2), pp. 180–185, 2008.
 138

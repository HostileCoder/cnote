Adapting CakeDB To Integrate High-Pressure Big
 Data Streams With Low-Pressure Systems
 Peter Membrey and Keith C.C. Chan
 Department of Computing
 Hong Kong Polytechnic University
 Hong Kong SAR, China
 Email: {cspmembrey,cskcchan}@comp.polyu.edu.hk
 Yuri Demchenko
 System and Network Engineering Group
 University of Amsterdam
 Amsterdam, The Netherlands
 Email: y.demchenko@uva.nl
 Abstract—Big Data continues to be one of the hottest topics
 in the computer science field and itself takes many forms. One
 way Big Data manifests is in the form of streams. These streams
 can be generally defined by their update frequency and the
 bandwidth they consume. They can however be further defined
 by the characteristics of the data they carry. The producers of
 these streams are generally tuned to perform a given role (such
 as moving large quantities of data with low latency) which can
 often be at odds with the requirements of a given consumer. In
 many cases the logistics of consuming such a stream can make
 the task impractical.
 This paper discusses the concept of data streams as sequential
 data sets and having different pressures. The paper demonstrates
 through a use case of a financial trading company and a High
 Performance Compute Cluster how different applications require
 different pressures and why it is necessary to be able to scale
 down high pressure streams for low pressure applications without
 impacting the applications that require the full high pressure feed
 and the high pressure feed itself. A proposed system for classifying
 streams and related consumers is discussed as well as the concept
 of conflation as it applies to these data streams. Features in
 the prototype stream oriented database (CakeDB) that support
 adapting high-pressure streams to low-pressure applications are
 then discussed and further work is identified.
 I. INTRODUCTION
 Big Data continues to be one of the hottest topics in the
 computer science field today. Database research is increasingly
 looking to shared nothing solutions with the ability to scale
 out over many nodes. This approach effectively uses the
 power of cloud computing and the relatively low cost compute
 cycles to create fast databases that scale in a near linear
 fashion. However this approach to database design has a
 severe limitation, that of physical memory. As long as the
 value derived from the dataset exceeds the overhead of storing
 it, a distributed memory model can easily be appropriate.
 However with terabytes of low value data (data that does
 not bring immediate value for its storage), this approach
 becomes an unrealistic solution. With the cost of fast hard disk
 storage plummeting, it is worth investigating disk based storage
 solutions. Most computers have disk storage space many times
 that of the physical memory in the machine. By considering
 the value and type of data being stored, the need for a different
 type of database becomes more evident [1].
 This paper builds upon previous work describing different
 types of Big Data [1] and introduces the concept of different
 types of Data Streams (DS) that a Big Data system may
 include, these being Low Pressure, Medium Pressure and High
 Pressure feeds respectively.A use case describing a financial
 trading company highlights the sorts of issues that a modern
 distributed system can face and demonstrates how the situation
 is getting increasingly more complex and difficult to manage.
 The paper then touches on the design for a prototype stream
 oriented database (currently called CakeDB) with a focus on
 the components that allow for low latency and high throughput
 with respect to mediating between streams of differing pres-
 sures. The key features in CakeDB that allow for these are
 discussed followed by a discussion of future work.
 The remainder of this paper is as follows. First two use
 cases are discussed along with the requirements for a proposed
 system that would satisfy their needs. This is followed by the
 proposed definitions for Big Data Streams and the different
 categories of consumer. A solution is then proposed detailing
 the implemented features that would fulfil the requirements. A
 summary then follows along with a discussion on future work.
 II. USE CASES & SYSTEM REQUIREMENTS
 When considering Big Data Streams, it can be easy to
 overlook the requirements of the consuming applications. It
 is common for producers to publish data at a rate that suits
 the producer’s needs. This could quite conceivably be a case
 of ‘send as much data as quickly as possible’. However the
 downstream applications that actually consume this feed often
 have varied requirements and although some may be required
 to handle the full data stream, others are only interested in parts
 of it or perhaps simply wish to store the data stream with no
 need (or even capacity) to handle the load in real time.
 This section details two use cases, one of a financial trading
 company that has multiple high throughput data streams but
 a variety of applications that utilize that data stream in very
 different ways. This challenge is not limited to scenarios with
 diverse requirements, but can also be found in compute clusters
 where it is quite possible that a given data stream may exceed
 for a short (or potentially long) time, the ability of the cluster
 to process it.
 The ability then to adapt high throughput data streams
 so that they may be consumed appropriately by a given
 application is of considerable interest. First, it is necessary
 to categorize different types of data stream by the impact they
 have on applications and requirements rather than by using the
 generally accepted approach of using bandwidth. Second, it is
 2013 International Conference on Cloud Computing and Big Data
 978-1-4799-2829-3/13 $26.00 © 2013 IEEE
 DOI 10.1109/CLOUDCOM-ASIA.2013.33
 414978-1-4799-2830-9/14 $31.00 © 2014 IEE
necessary to categorize different types of consumer so that they
 can be matched up to the data streams that were previously
 defined. Together, these sets of definitions make describing a
 particular scenario or use case significantly easier.
 A. Use Case – Financial Trading Company
 Financial Exchanges are at the centre of world finance.
 They provide regulated and structured places for trading a wide
 range of instruments such as stocks, bonds, futures, options,
 warrants and commodities. In exchange for providing these
 regulated trading platforms, the Exchange charges commission
 to market participants, generally based on the amount of
 volume traded. To ensure they have an environment conducive
 to increasing trading volume, Exchanges are constantly im-
 proving their technology so that orders sent to the Exchange
 can be processed faster and market information (such as prices
 and trades) can be disseminated fairly and efficiently to all
 participants. Quiet markets can easily generate 10 million trade
 updates in a trading day, and a state of the art Exchange can
 handle 10 billion trades a day. That does not include the orders
 placed that were never filled and those orders could easily be
 an order of magnitude higher.
 Although each Exchange is slightly different, there is a
 trend to move towards order-based price feeds. Snapshot-based
 price feeds publish the best price (and available volume) for a
 given instrument and can include various levels of price depth
 below the best price. There is no calculation or processing
 done by the consumer and the price update can be consumed
 directly. However these feeds are relatively slow because the
 Exchange must calculate the current best price (and levels
 of price depth) from their internal order book. This puts
 considerable load on the Exchange and effectively limits how
 fast price updates can be published. Often instruments are
 published in a cycle that is measured in milliseconds. Order-
 based price feeds however are very different. Rather than the
 Exchange calculating the best price, it simply forwards any
 orders that it accepts to all the other market participants. It is
 then down to each participant to take the order update, apply
 it to the order book and from that, calculate the prices that
 were previously provided by the Exchange in the snapshot-
 based feed. This approach greatly increases the amount of
 data that consumers need to process but effectively turns the
 Exchange into a routing device. These feeds tend to publish
 data in real time with gaps between updates being measured
 in microseconds.
 The continued improvements in performance at the Ex-
 change have meant that the computer systems used by market
 participants have needed similar upgrades and enhancements.
 This includes infrastructure such as computer hardware (newer
 and faster machines), improved network infrastructure (low
 latency switches and fibre optic inter-connects) and software
 enhancements (ability to consume the newer and faster feeds
 in real time). Whichever participant can receive data from the
 exchange, make a trading decision and (if necessary) send
 an order to the Exchange in the least time, will have a very
 significant (and most likely very profitable) advantage over
 other participants. It is now considered standard in the industry
 to benchmark such systems in nanoseconds.
 Market data is only one of many potential data sources
 in such a trading company and most will have distributed
 trading platforms of some kind. The components of such a
 system have differing requirements (and capabilities) in terms
 of the data each can handle and the ability to adapt high speed
 data streams into a form that these disparate applications can
 consume remains a significant challenge. Figure 1 highlights
 how different systems may inter-operate in a given trading
 system.
 Fig. 1. Trading System Message Flow
 Financial companies are under increasing stress to manage
 and process more data from a myriad of sources. The ability
 to arbitrate between streams of different categories is a big
 concern as applications may not increase in performance, but
 the demands of a given stream will.
 B. Use Case – High Performance Computing
 High Performance Computing solutions that take advantage
 of commodity hardware have matured greatly in recent years.
 Systems such as the High Performance Computing Cluster
 (HPCC) [2] and similar systems such as Google’s MapReduce
 [3], [4], Hadoop [5], [6], SCOPE [7], Sector/Sphere [8],
 Datomic [9] all use commodity hardware in their respective
 solutions.
 However, regardless of which technology is chosen, one of
 the challenges such a system faces is being able to process
 High Pressure Data Streams (HPDS) where the data coming
 into the cluster can exceed the clusters ability to process it.
 Solutions such as Redis would not be appropriate in this case
 as if the cluster is unable to process the data in real time, the
 backlog will build up in Redis. If the stream is ‘always on’
 there would never be an opportunity for the cluster to catch up.
 Apache Kafka would be a superior choice for this challenge as
 it is designed specifically for handling such streams. However
 it does not provide database features, and so while it could
 buffer data, it would not be able to provide support for more
 complex queries such as requesting data over a time range.
 CakeDB however supports basic database queries that are
 time bound and also supports functionality to enable streaming
 data both conflated and non-conflated. Such a solution would
 not only allow data to be buffered into the HPCC cluster itself,
 but would also allow for CakeDB to store data for historical
 queries as well. This approach gives the best of both worlds
 where only simple database functionality is required, but the
 ability to handle diverse and high speed data streams (and
 make them available to a range of differing consumers) is a
 requirement.
 415
III. PROPOSED MECHANISMS & SOLUTIONS
 Data Streams (DS) are generally categorized by the amount
 of data that the stream generates every second. This figure
 (generally given in megabits per second) allows for effective
 resource allocation from a network level, but does not give a
 great deal of information about the DS itself. When consider-
 ing Big Data Streams (BDS) it is often beneficial to consider
 the type of data being sent and how exactly that data is being
 sent. For example, a multicast UDP stream will behave quite
 differently to a TCP stream.
 Streams are effectively serial in that one message or event
 arrives at a given time. Even streams with multiple sources
 ultimately arrive one behind the other in sequential order.
 Whether the stream is considered to contain messages (such
 as in a market price feed) or events (data capture points from
 remote sensors), these individual elements are the fundamental
 component of a stream. The general definition of a stream
 can be expanded to better describe a BDS. First, the size of
 a specific message in the stream determines how much data
 will be received in each message. The bigger the message,
 the greater the load on the infrastructure (although for various
 reasons - such as compression - smaller messages could be
 more CPU intensive). The second defining factor is the time
 between the messages. This is significant because a message
 that’s twice as large but takes twice as long to arrive is
 usually less intensive on the infrastructure than smaller updates
 arriving at much higher frequency.
 A. High Pressure Streams
 High Pressure Streams (HPS) are those that put significant
 stress on infrastructure and consuming devices and applica-
 tions. The individual message size can be large as data is
 packed to increase throughput (often to the size of the network
 MTU) and the time between each packet being sent can
 generally be measured in microseconds. These streams are
 currently found in many financial exchanges where in many
 cases a single computer is unable to process the entire feed
 and it must be broken down into segments called partitions.
 Individual streams that may not normally meet this category
 can be considered HPS if they combine data from other
 sources. For example, two Medium Pressure Streams being
 sent to one destination could when combined be enough to
 rate as an HPS.
 B. Medium Pressure Streams
 Medium Pressure Streams (MPS) also stress infrastructure
 but generally do so in only one of the two dimensions
 highlighted earlier. Either the packet size is large but the
 transmit frequency is low or the transmit frequency is high
 with much smaller packets. These streams can generally be
 consumed by most systems in its entirety although care needs
 to be taken to ensure that applications consuming this feed do
 not become overloaded. Although MPS are less intensive, it
 is still possible for applications to either build a backlog or to
 start failing under load.
 C. Low Pressure Streams
 Low Pressure Streams (LPS) are streams that place no real
 stress on the infrastructure and the vast majority of applications
 would have little difficulty consuming and processing this feed.
 This is the level that most general applications (particularly
 those with business logic) would operate at. These applications
 would be network bound rather than CPU or memory bound.
 D. Storing the data
 Regardless of the type of data stream, there is often the
 requirement to store it for future use. This can be especially
 challenging with current database technologies as those that
 can handle ‘black box’ data or highly structured data (such
 as JSON) tend to leverage physical memory in order to
 provide high performance thresholds. As physical memory is
 significantly less than available hard disk storage, the need to
 scale-out such systems (that is add additional nodes) greatly
 increases their cost and is therefore unable to fully utilize avail-
 able disk storage. This is especially important when dealing
 with High Volume Low Value (HVLV) data [1]. Therefore it is
 not only the handling of the streams that needs to be considered
 but also how such streams are ultimately going to be stored.
 E. Types of stream consumer
 Stream consumers can be broken down into four types
 regardless of the type of stream that is being consumed.
 F. Real time non-conflated
 Real time non-conflated (RTNC) consumers need to be
 able to process each message received in the stream within a
 specified time window. In addition each message in the stream
 must be processed. This is the most intensive form of consumer
 as not only must updates be handled as quickly as possible but
 it is a requirement for each and every update to be processed.
 For these consumers, either failing to meet the time window
 or dropping a message designates a failure of the system. Such
 systems would include ‘order based market price data’.
 G. Real time conflated
 Whilst Real time conflated (RTC) consumers have the same
 time constraints as RTNC consumers, there is no requirement
 for every message to be processed. Some systems only care
 about the most recent update. If a message is being processed
 and a new message then arrives for the same feed, such a
 consumer may cancel processing the current message and
 process the new message instead. These systems are common
 such as ‘snapshot based market price data’ where only the
 current price is of interest, as only the current price can be
 acted upon. Therefore in order to ensure that the most current
 value is available, such consumers should drop updates that
 are not relevant or can be skipped.
 H. Non-real time non-conflated
 Non-real time non-conflated (NRTNC) consumers have no
 implicit constraints on how fast each message must be handled
 but they are required to process each and every message.
 Such systems are usually ancillary in nature. For example, an
 application that stores the real-time feeds discussed previously
 would need to see each message, but as it is only storing them,
 there is no requirement for the process to be inherently fast.
 416
I. Non-real time conflated
 Non-real time conflated (NRTC) consumers only care about
 the latest values and have no constraints on the time required
 to process them. For example, a system that updates a price
 on a web page once every minute, neither needs to process
 the data within a limited time frame and nor does it need to
 process each message in the feed. It is only interested in the
 latest price.
 J. Conflation
 Conflation allows an application to effectively consume a
 data stream that would otherwise overwhelm it. As discussed in
 the previous section, when the full feed cannot be processed in
 real time, either more time is needed or the effective message
 rate must be decreased. By conflating a data stream, messages
 are effectively dropped but in a way that does not impact
 the usage of the stream. This is similar in concept to MP3
 compression where significant data is lost, but the overall
 impact on the use of the data stream is minimized.
 1) Passive Conflation: Passive Conflation is where a data
 stream is conflated by the client rather than by the sender.
 For example with a polling based mechanism, a client with a
 suitably equipped server would be able to request the latest
 message, process it and then again request the latest message.
 Any data received in between those two requests is effectively
 dropped from the clients point of view. This is often desirable
 because it means the client can effectively rate limit the data
 stream to meet its own capacity to process it. However it does
 add additional complexity in that the client must be able to
 handle certain situations, such as when the latest message has
 not updated since the last request.
 2) Active Conflation: Active Conflation is where a data
 stream is conflated by the sender. Conflation can be done in
 several ways, the simplest being that of a timed pulse. With
 this approach, the sender processes message as normal but does
 not actually send them. When the timer is triggered, the sender
 will then publish whatever the current value is. With a pulse
 cycle of 100ms, this would limit a given stream to 10 updates
 per second, regardless of how many messages were actually
 arriving at the sender for processing. Conflation could also
 be done on individual components inside the data stream. For
 example in a single stream there might be ten different message
 types. These could all be conflated separately. In addition,
 some message types may not be conflated at all.
 A more application specific form of conflation is where
 the sender applies logic to determine if a message should be
 sent or if it should be dropped and replaced with a newer
 message. For example, if message two arrives before message
 one is sent, message one is dropped and only message two
 will actually be published.
 K. Proposed implementation
 As discussed in a related paper CakeDB is a stream
 oriented database designed specifically to handle high-pressure
 low-latency data streams [1]. To enable the database to handle
 such workloads, a new database design was necessary to meet
 these specific requirements. Thus the following features were
 key to CakeDB’s original design:
 • Does not block the client
 • Has low impact on the client
 • Can accept data at high speed
 • Takes full advantage of available resources
 • Copes with burst traffic
 • Can operate on a single machine
 • Queries are executed in a reasonable time
 In addition to these features, CakeDB also uses a simple
 binary format (for both network communication and data
 storage) and stores data in Natural Order (NO) on disk. These
 features enabled CakeDB to outperform MongoDB on raw
 insert performance [1] and also make it possible for CakeDB to
 act as an intermediary for handling high-pressure data streams
 with low-pressure consumers.
 1) Optimized binary format: CakeDB was optimized for
 throughput and minimal overhead. Figure 2 shows the structure
 of a message and these are stored one after the other in natural
 order.
 Fig. 2. CakeDB Format - Data File
 Figure 3 shows a similar structure is used in the index files.
 This structure is very efficient for determining the closest byte
 offset to a given time stamp. This in turn allows data files of
 many hundreds of gigabytes to be accessed with a minimal
 performance penalty.
 Fig. 3. CakeDB Format - Index File
 2) Natural Order Storage: CakeDB implements a Natural
 Order Storage (NOS) system that stores data in the order
 it was received. This means that when the dataset is also
 naturally ordered ( such as prices from a stock market ) they
 are inherently stored in the correct order; that is events stored
 after a given event must also have occurred after that event.
 This approach makes streaming data to disk very efficient
 and allows for high throughput and minimum overhead. The
 database simply buffers data in FIFO order and streams to
 disk when either a given amount of time has passed or a given
 amount of data has built up in the buffer.
 Fig. 4. Natural Order Storage
 417
3) Adopting Erlang: Erlang was designed initially in 1986
 by a small team at Ericsson [10]. Lead by Joe Armstrong, the
 team developed the language (which was later open sourced
 in 1998) to address key issues that Ericsson were experiencing
 in developing software for their telecommunications hardware;
 that of stability and reliability in systems known to be suscep-
 tible to errors [11]. Armstrong’s approach takes the opposite
 approach of shared-everything designs (which use threads and
 shared memory) as he believed them to be too tightly inter-
 connected to allow faults to be properly isolated. Instead a
 process based message passing model (similar to the actor
 model [12]) was believed to be more than sufficient [13].
 This design coupled with a powerful scheduling model makes
 Erlang exceptionally good at making full use of multi-core
 systems as this development model prevents any shared state
 and therefore there is very little locking in Erlang [14].
 Although the message passing paradigm is not unique
 to Erlang, it is still a relatively rare approach to solving
 the concurrency problem. Scala, written for the JVM (Java
 Virtual Machine) [15] is one such implementation. However
 the challenges that are faced when trying to implement an
 actor based model on top of a thread based model can be
 significant and there are limits to what such languages as Scala
 can accomplish. Erlang has a custom VM that was written
 in C and was designed specifically from the ground up to
 support this paradigm [16]. This combined with libraries for
 implementing Erlang clusters in C and Java [17] make it very
 easy to integrate legacy applications into an Erlang cluster.
 These features made Erlang the perfect candidate for CakeDB.
 L. Commands
 CakeDB implements the following commands which pro-
 vide the basis for stream conflation. CakeDB acts as a pressure
 regulator between the incoming high-pressure feed and the
 various low-pressure consumers. Because CakeDB acts as an
 intermediary, it effectively detaches slower applications from
 the critical path which in turn protects upstream applications
 from back-pressure caused by slow consumers. Consumers that
 require only the latest update from a given stream can use the
 ‘Get Last Message’ command whereas those that require the
 full feed, just in smaller chunks can use the ‘Get All Messages
 Since’ command.
 1) Get Last Message: The ‘Get Last Message’ command
 returns the latest message on a given stream. This is very
 useful for applications where only the most recent value is of
 importance and where historical data (all data other than the
 most recent) is of no interest. This command is very efficient
 as it does not need to access the index or data file stored on
 disk. In CakeDB the last message on each stream is held in
 memory, and so it can reply immediately to this command.
 2) Get All Messages Since: The ‘Get All Messages Since’
 command takes a given timestamp (each message is guaranteed
 to have a monotonically increasing timestamp) and returns all
 messages since that time. It does not include the message that
 has that particular timestamp. This is useful for applications
 that need to process the whole feed but are unable to handle the
 full feed in real time. With this approach, CakeDB serves up
 data in chunks that the client can control. This query does use
 the index and data files, but as the most recent writes are still
 usually in the cache, data is still usually served from memory.
 IV. RELATED WORKS
 There are two key requirements for handling such data
 streams. There needs to be a way to store them and there
 needs to be a way to manage those streams so that they may be
 effectively consumed by the relevant applications. Combining
 these requirements shifts the focus away from a specific
 database technology. This makes designing, implementing and
 even understanding the issues surrounding Big Data usage a
 significant challenge [18].
 Databases are not generally designed for this sort of
 workload. State of the art databases such as those created
 by Michael Stonebraker and his fellow researchers (H-Store
 [19] and C-Store [20]) offer very impressive performance
 over distributed datasets. However they are not designed to
 stream data (i.e. push updates) to multiple clients and although
 something similar could be implemented, this would not yield
 optimum performance. In addition their database structures
 are relational in nature and as such do not support either
 ‘binary blob’ or highly structured ‘NoSQL’ datasets which is
 a useful feature when dealing with unstructured or proprietary
 data feeds. Datomic treats time as a first class citizen [9] and
 supports ‘NoSQL’ datasets, however its schemas still need
 to be determined in advance, and aren’t as flexible when
 structured data such as JSON must be handled.
 There are two main choices for when handling this style of
 data becomes necessary; MongoDB [21] and CouchDB [22].
 CouchDB stores data in native JSON format and takes a unique
 approach to indexing data. Where RDMS systems have static
 data and dynamic queries, CouchDB implements dynamic data
 and static queries [23] leading to fast reads only when indexes
 are available. CouchDB also has a REST (REpresentational
 State Transfer) over HTTP interface which makes querying less
 efficient. Insert performance, even with batch jobs is relatively
 low, at around 700 updates per second.
 In contrast, MongoDB is more like traditional SQL
 databases. It is possible to do dynamic queries and due to
 its BSON (Binary JSON) storage format offers JSON-like
 structures but with very fast search and parse times. However
 being memory mapped, physical memory is a highly limiting
 factor. Once data exceeds this limit, performance drops to the
 point of being unusable.
 Although these systems could potentially handle and store
 the required data (although initial research suggests that there
 are severe bottlenecks when trying to do so [1]), there is still
 no native way of streaming that data to clients. The same is
 true for key-value store systems such as memcached [24] and
 Riak. Whilst highly performant and distributed in nature they
 were not designed to handle this sort of workload.
 A potential alternative is Redis which is generally consid-
 ered an evolution of memcached [25]. It offers publish and
 subscribe support (PUB/SUB) which does give the ability to
 handle real-time streaming data. However, like MongoDB it is
 a memory-based solution (although it can backup this store
 to disk) and it attempts to deliver all queued messages to
 a given consumer. This causes problems where a consumer
 cannot keep up with the data stream and Redis is then required
 to start buffering undelivered messages in memory. In many
 cases a consumer is unable to catch up, and ultimately this
 will cause Redis to run out of memory and fail.
 418
Kafka is an Apache project that was designed to unify
 multiple messaging systems into a single system [26]. The
 design specifically placed throughput rather than features as
 the primary design constrain [27]. A key design difference
 between Kafka and other solutions such as Redis, was the
 design decision to leverage a disk based system for holding
 data after considering research that demonstrated disk based
 systems when implemented appropriately can offer faster ac-
 cess times than a memory based system [28]. By using disk
 space as the primary storage medium, Kafka removed physical
 memory as a restriction for the amount of data that could be
 stored. By simply tracking where a client was in relation to a
 particular feed, it was possible to allow even slow consumers
 to consume large data streams.
 However Kafka is not designed as a permanent storage
 system, it aggregates data streams rather than stores them. As
 such streams can often be very large themselves, even though
 Kafka might provide a solution for managing the streams in
 the initial case (where slower applications need to be able to
 manage fast data streams), it does not solve the issue of storing
 this data for historical purposes, archiving or a repository of
 data for future analysis.
 An ideal solution then would implement features found in
 existing database technology (such as the permanent storage
 of data and the ability to query it) along with features found
 in streaming solutions. CakeDB is a prototype database that
 unifies features from both fields to provide a simple database
 that supports the following use cases.
 V. SUMMARY & FUTURE DEVELOPMENTS
 Big Data Streams are generally geared towards suiting the
 requirements of the producer rather than that of the consumer.
 Indeed it is usually the producer that defines the format
 and requirements for a given data stream. However there
 are many cases where high-pressure data streams need to be
 consumed by applications that cannot fulfil the expectations of
 the producer (low-pressure consumers) and so an intermediary
 is required to regulate the pressure between the two. As data
 streams increase in throughput, complexity and number, a
 simple way of defining these streams and the related consumers
 becomes necessary.
 Support for active conflation is currently being added to
 CakeDB. This feature allows clients to specify the number of
 updates per second that they can support, and then streams the
 data to them in real time. Although CakeDB was originally
 conceived primarily as a system for storing large amounts of
 data, the need to be able to adapt that data in real time as
 well as for historical uses, is driving its continued research
 and development.
 REFERENCES
 [1] P. Membrey, K. C. Chan, and Y. Demchenko, “A disk based stream
 oriented approach for storing big data,” in Collaboration Technologies
 and Systems (CTS), 2013 International Conference on, 2013, pp. 56–64.
 [2] A. Middleton, “Hpcc systems: Introduction to hpcc (high-performance
 computing cluster),” White paper, LexisNexis Risk Solutions, 2011.
 [3] J. Dean and S. Ghemawat, “Mapreduce: a flexible data processing tool,”
 Communications of the ACM, vol. 53, no. 1, pp. 72–77, 2010.
 [4] ——, “Mapreduce: simplified data processing on large clusters,” Com-
 munications of the ACM, vol. 51, no. 1, pp. 107–113, 2008.
 [5] J. Venner, Pro Hadoop. Apress, 2009.
 [6] T. White, Hadoop: the definitive guide. O’Reilly, 2012.
 [7] R. Chaiken, B. Jenkins, P.-A˚. Larson, B. Ramsey, D. Shakib, S. Weaver,
 and J. Zhou, “Scope: easy and efficient parallel processing of massive
 data sets,” Proceedings of the VLDB Endowment, vol. 1, no. 2, pp.
 1265–1276, 2008.
 [8] R. Grossman and Y. Gu, “Data mining using high performance data
 clouds: experimental studies using sector and sphere,” in Proceedings
 of the 14th ACM SIGKDD international conference on Knowledge
 discovery and data mining. ACM, 2008, pp. 920–927.
 [9] A. Kiel, “Datomic-a functional database,” 2013.
 [10] F. Cesarini and S. Thompson, Erlang programming: A concurrent
 approach to software development. O’Reilly Media, Incorporated,
 2009.
 [11] J. Armstrong, “Making reliable distributed systems in the presence of
 software errors,” Ph.D. dissertation, KTH, 2003.
 [12] C. Hewitt, “Actor model for discretionary, adaptive concurrency,” CoRR,
 abs/1008.1459, 2010.
 [13] J. Armstrong, “How erlang views the world and what we have learned
 in the last 25 years of programming distributed systems.” EPTCS 58,
 2011.
 [14] ——, “erlang,” Communications of the ACM, vol. 53, no. 9, pp. 68–75,
 2010.
 [15] M. Odersky, P. Altherr, V. Cremet, B. Emir, S. Maneth, S. Micheloud,
 N. Mihaylov, M. Schinz, E. Stenman, and M. Zenger, “An overview of
 the scala programming language,” Technical Report IC/2004/64, EPFL
 Lausanne, Switzerland, Tech. Rep., 2004.
 [16] B. Hausman, “Turbo erlang: Approaching the speed of c,” Implemen-
 tations of logic programming systems, pp. 119–135, 1994.
 [17] J. Larson, “Erlang for concurrent programming,” Communications of
 the ACM, vol. 52, no. 3, pp. 48–56, 2009.
 [18] Y. Demchenko, P. Grosso, C. de Laat, and P. Membrey, “Addressing
 big data issues in scientific data infrastructure,” in Collaboration Tech-
 nologies and Systems (CTS), 2013 International Conference on, 2013,
 pp. 48–55.
 [19] R. Kallman, H. Kimura, J. Natkins, A. Pavlo, A. Rasin, S. Zdonik,
 E. Jones, S. Madden, M. Stonebraker, Y. Zhang et al., “H-store: a high-
 performance, distributed main memory transaction processing system,”
 Proceedings of the VLDB Endowment, vol. 1, no. 2, pp. 1496–1499,
 2008.
 [20] M. Stonebraker, D. Abadi, A. Batkin, X. Chen, M. Cherniack, M. Fer-
 reira, E. Lau, A. Lin, S. Madden, E. O’Neil et al., “C-store: a column-
 oriented dbms,” in Proceedings of the 31st international conference on
 Very large data bases. VLDB Endowment, 2005, pp. 553–564.
 [21] P. Membrey, E. Plugge, and T. Hawkins, Definitive Guide to MongoDB.
 Apress, 2010.
 [22] J. Lennon, Beginning CouchDB. Apress, 2009.
 [23] J. Anderson, J. Lehnardt, and N. Slater, CouchDB: The Definitive Guide:
 Time to Relax. O’Reilly Media, 2010.
 [24] J. Petrovic, “Using memcached for data distribution in industrial envi-
 ronment,” in Systems, 2008. ICONS 08. Third International Conference
 on. IEEE, 2008, pp. 368–372.
 [25] C. M. Hayden, E. K. Smith, M. Denchev, M. Hicks, and J. S. Foster,
 “Kitsune: Efficient, general-purpose dynamic software updating for c,”
 in Proceedings of the ACM international conference on Object oriented
 programming systems languages and applications. ACM, 2012, pp.
 249–264.
 [26] J. Kreps, N. Narkhede, and J. Rao, “Kafka: A distributed messaging
 system for log processing,” in Proceedings of the NetDB, 2011.
 [27] K. Goodhope, J. Koshy, J. Kreps, N. Narkhede, R. Park, J. Rao, and
 V. Y. Ye, “Building linkedin’s real-time activity data pipeline.” IEEE
 Data Eng. Bull., vol. 35, no. 2, pp. 33–45, 2012.
 [28] A. Jacobs, “The pathologies of big data,” Communications of the ACM,
 vol. 52, no. 8, pp. 36–44, 2009.
 419

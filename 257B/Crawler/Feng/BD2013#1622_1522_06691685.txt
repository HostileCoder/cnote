Advancing value creation and value capture in data-intensive contexts
 Roman Ferrando-Llopis 
Research Unit 
R-Knowing 
UK 
romanfllopis@rknowing.co.uk 
 
David Lopez-Berzosa 
Business School 
University of Exeter 
UK 
d.lopez-berzosa@exeter.ac.uk 
 
Catherine Mulligan 
Innovation and Entrepreneurship group 
Imperial College, London 
UK 
c.mulligan@imperial.ac.uk
  
Abstract—.Realizing the vast potential for value creation that 
Big Data has to offer to firms and public agencies requires a 
radical departure from the traditional data warehouse model 
currently in place in most organizations. Given the inability of 
current approaches to integrate the four dimensions of volume, 
variety velocity and veracity into a single and coherent 
framework, new business models around the Big Data 
paradigm will likely be developed in a collaborative regime in 
which technology firms, public entities and end customers will 
organize around ecosystems, strategic partnerships or private-
 collective modes of technology development and 
commercialization. 
 
Keywords-component; business models, Big Data, 4Vs, value 
creation. 
I. BIG DATA: OLD WINE IN A NEW BOTTLE ? 
Big data has been used to convey all sorts of concepts, 
including: enormous quantities of data, social media 
analytics, unstructured data analysis, next generation data 
management capabilities, real-time response, and online data 
management among others. McKinsey [1], for example, 
refers to Big Data as datasets whose size is beyond the ability 
of typical database software tools to capture, store, manage, and 
analyze.  
One of the most commonly recognized applications of 
big data is social media data analysis, probably due to the 
extensive press coverage about social network impact on 
customer behavior and experiences. It would be easy to 
conclude that big data means social media data, but this 
assumption would miss to capture both existing industrial 
applications as well as the potential of the Big Data 
paradigm. 
Recently, it has emerged the use of Big Data techniques 
by national intelligence offices worldwide [2,3]. In fact 
organizations have already been handling big data for years. 
A global telecommunications company for example, collects 
billions of detailed call records per day from 120 different 
systems and stores each for at least nine months [30].  
Big Data also refers to the huge potential to create value 
in unrelated contexts to social media such as in genomics [4], 
public service delivery [5, 6] or cities [7]. Smart meters and 
smart grid devices installed across the US electric grid will 
produce more than 400 terabytes of new data every day [29]. 
It is foreseen to have 50 billion of connected devices by 2025 
[8]. Some of that information will be part of mission critical 
systems for the society like electrical grid or 
telecommunications system.  
Big Data is not only about data storage, in addition to the 
storage challenge,companies have to deal with the online 
response time in a handful of situations (e.g. subscribers 
roaming away, mobile advertising, changing peaks in 
electricity demand, or influxes of new passengers on an 
underground station). In all these cases the system has to be 
aware and keep track of multiple variables. For many 
companies, therefore, the concept of big data is not new; 
what is the new the ability to perform large-scale analy,tics. 
In this sense the Big Data paradigm refers not to a new 
method but the expansion of an existing one.  
Big Data is often related to economic and social value 
creation. It remains unclear, however, the manner in which 
Big Data may create value for firms and public agencies. 
Further research is also required to understand how part of 
that value can be captured by citizens (oftentimes the ones 
providing the data) as well as by private firms. Both value 
creation and value capture mechanisms are key elements for 
sustainable business models [9] and markets for technology 
[10]. 
Notwithstanding some of the hype, it is commonly 
agreed that we are in the early stages of enterprise big data 
adoption. In this paper, we refer to big data adoption to 
represent the evolution of the data, sources, technologies and 
skills that are necessary to create a competitive advantage in 
the globally integrated marketplace. We foresee that 
companies and institutions able to incorporate the skills to 
integrate, correlate and analyze corporative and worldwide 
data will gain a competitive advantage in the big data race. 
By following this traditional approach firms are assuming 
that Big Data is just a bigger data warehouse where you 
already know what you are looking for and the answer is just 
“hidden” in a larger data set, these firms assume that it is like 
swimming in a bigger pool, but still business as usual [34].  
However, two important trends render invalid the 
business as usual approach to the Big Data paradigm: (1) 
The digitization of virtually “everything” now creates new 
types of “large” and real-time data across a broad range of 
industries. Much of this is non-standard data: for example, 
streaming, geospatial or sensor-generated data that does not 
fit neatly into traditional, structured, relational 
warehouses,(2) today’s advanced analytics and machine 
learning technologies enable organizations to extract 
valuable knowledge from data with previously unknown 
2013 IEEE International Conference on Big Data
 5978-1-4799-1293-3/13/$31.00 ©2013  IEEE 
levels of complexity, speed and accuracy. Technologies like 
Stream/online Analytics, embedded Analytics, automatic 
knowledge reinforcement, scalable frameworks among 
others, provide answers to new and exciting challenges. 
Moreover, and contrary to the traditional data Warehouse 
paradigm in which the data was created and maintained by 
single firms, in the case of Big Data several stakeholders are 
usually involved. For example, in so-called smart cities 
citizens and public agencies provide the data, which is in 
turn processed and delivered by third parties. As a result 
conventional approaches to create value (e.g. a multinational 
harvesting data from customer´s purchases) and capture 
value from it (e.g. the same multinational conducting 
customer profiling and segmentation) will not suffice in 
presence of heterogeneous and unrelated parties [31-33]. 
This would call for novel organizational arrangements either 
in the form of ecosystems [11-13], strategic partnerships 
[14,15] or private-collective modes of technology 
development and commercialization [16-18]. 
 
The purposes of this paper are therefore threefold, (1) 
situate the concept of business model in the context of the 
Big Data paradigm, (2) identify the lack of convergence of 
the three dimensions volume, variety and velocity as a main 
limitation of existing business models and (3) describe four 
technologies which are required to fully realize the potential 
of Big Data to create economic and social value.  
II. DEFINING THE BIG DATA PARADIGM 
Probably the origin of the confusion about big data 
comes from the definition itself. One valid definition for big 
data is described by “today’s extreme increase of available 
data, from new sources and formats. The necessity to 
combine them with existing data repositories in order to 
create complex knowledge sources and the advance of 
analysis techniques able to extract business insights from 
new data environment.” 
 
From a strategic and managerial point of view such 
definition needs further refinement. In this sense it is useful 
to characterize the Big Data paradigm according to the well-
 know dimensions of: Volume, Variety, Velocity and 
Veracity [19].  
Volume. Refers to the amount of data to be processed 
and analyzed. Perhaps the characteristic most associated with 
big data, volume denotes to the mass quantities of data that 
organizations are trying to harness to improve decision-
 making process across the organization.  
Variety. Different types of data and data sources. Variety 
is about managing the complexity of multiple data types, 
including structured, semi-structured and unstructured data. 
Organizations need to integrate and analyze data from a 
complex array of both traditional and non-traditional 
information sources, from within and outside the company. 
In a connected environment, the data correlation between 
inside and outside data is as important as internal analytics. 
With the explosion of sensors, smart devices and social 
collaboration technologies, data is being generated in 
countless forms, including: text, web data, tweets, sensor 
data, audio, video, click streams, log files and more. 
Velocity. Data in motion. The speed at which data is 
created, processed and analyzed continues to accelerate. 
Contributing to higher velocity is the real-time nature of data 
creation, as well as the need to incorporate streaming data 
into business processes and decision-making. Velocity 
impacts latency – the lag time between when data is created 
or captured, and when it is accessible. Today, data is 
continually being generated at a pace that is impossible for 
traditional data base systems to capture, store and analyze. 
For time-sensitive use cases where real-time is a key for the 
success (location aware applications, automatic grid 
accommodation, real-time fraud detection), or those which 
data generation is too high and not all data necessary 
represent useful information (Unattended intelligent 
monitoring systems, sensor monitoring) the online data 
management without full data storage involved technology is 
required. 
Veracity. Talking about veracity in big data is not just 
about technology and data correlation but it is more related 
to the human condition instead. Some data is inherently 
uncertain, sentiment in humans; weather conditions or 
economic factors. Dealing with these types of data, there is 
no data cleaning technique able to correct it, at least not 
today. However, despite uncertainty, the data still contains 
valuable information. Apparent uncertainty manifests itself 
in big data in some ways. It is in the uncertainty that 
surrounds data created in human environments like social 
networks; in the unknowingness of how the future will 
unfold and of how people, nature or unseen market forces 
will react to the variability of the world around them.  
In order to manage uncertainty, models around the data 
are needed. One way to achieve this is data aggregation, 
where combining multiple less reliable sources creates a 
more accurate and useful data point, such as social 
comments appended to geospatial location information. 
Another way to manage uncertainty is through advanced 
mathematics that embraces it, such as robust optimization 
techniques and fuzzy logic approaches 
 
While these dimensions cover the key attributes of big 
data itself, it is the convergence of these four dimensions into 
a single coherent entity what characterizes the potential of 
Big Data to open up new value creation avenues for firms 
and public agencies. 
 
This integration enables companies to transform the ways 
they interact with and serve their customers, and allows 
organizations – even entire industries – to transform 
themselves. The next section investigates how value is 
captured and created from the perspective of business 
models. 
 
 
 
6
III. BUSINESS MODELS IN BIG DATA CONTEXTS 
Formal definitions of a business model are still lacking 
[20]. According to Zott [21] a business model is a system of 
interdependent activities that transcends the focal firm and 
spans its boundaries. Gambardella [22] defines it as a 
mechanism for turning ideas into revenue at reasonable cost, 
whereas Casadesus [23] considers a business model as the 
logic of the firm, the way it operates and how it creates value 
for its stakeholder. 
According to Teece [9], a business model describes the 
design or architecture of the value creation, delivery and 
capture mechanisms employed. The essence of a business 
model is that it crystallizes customer needs and ability to 
pay, de?nes the manner by which the business enterprise 
responds to and delivers value to customers, entices 
customers to pay for value, and converts those payments to 
pro?t through the proper design and operation of the various 
elements of the value chain. 
An important characteristic of a business model is the 
interdependency of the focal firm with external resources and 
stakeholders [24], this is especially relevant in the case of 
Big Data as: (1) data is produced, owned and processed by 
different parties, (2) strong network effects are likely to 
emerge [25]. Therefore it is likely that business models 
around Big Data will result from the evolution of current 
platforms, in high-tech industries such as Internet services, 
mobile or cloud computing [11, 12, 26]. 
 
As much data you are able to capture, more qualified 
your decisions will be. Organizations engaged in big data 
require increasingly more advanced capabilities to find 
consumption or even movement patterns in order to target 
the advertising campaigns and increase the efficiency.  
 
Advertising companies used to be concerned about one 
question. What is the effect of my last campaign? Can I 
follow the impact in real-time? Answering that question in 
an accurate manner (i.e. veracity) is not a simple task. It 
requires combined structured and unstructured data analysis. 
It implies things like measuring how many people access to 
my website or how many people are talking about my 
campaign in the social networks. 
 
For many companies is not just about what but when. 
Fraud is a very real challenge for many companies in many 
industries around the world. If our credit card or our mobile 
phone gets stolen, we do not want our bank or telecom 
operator to do something about the day after. We want the 
companies to be able to detect that an abnormal pattern is 
happening and do something about that now. 
 
Firms wanting to develop business models around the Big 
Data paradigm can opt for two options, (1) replicate and 
further extend what is being done by leaders such as Google, 
Amazon, or (2) Develop new business models based on the 
integration of all (or part of) the 4Vs. 
In regards to this second strategic choice, the firm must 
be able to put in place technologies that supports the rapidly 
growing volume, variety and velocity of data created within 
the organization (e.g. customer purchases) as well as outside 
the organization (e.g. weather forecast, social media, medical 
data).  
In order to enact novel ways of value creation and value 
capture around the Big Data paradigm, we contend that four 
main information management components are required, (1) 
Stream processing, (2) Stream analytics, (3) Scalable storage 
processing and (4) Data Orchestration. 
A. Stream processing 
As previously defined in this document, big data comes 
from the necessity to offer a technical solution to the 
increasing rate from sensor applications, measurements in 
network monitoring and traffic management, log records, 
manufacturing processes, call detail records, email, blogging, 
twitter posts and others.  
All those applications share a commonality; all of them 
require streaming data management systems. In fact, all data 
generated can be considered as streaming data or as a 
snapshot of streaming data, since it is obtained from an 
interval of time. 
In time sensitive applications, the discussion goes around 
Complex Event Processing (CEP) and the benefits of its 
capabilities to process huge amount of flowing data avoiding 
to store everything. CEP is the analysis of event data in real-
 time to generate immediate insight and enable instant 
response to changing conditions. Likewise, a seemingly 
similar concept is that of Business Rules engine. These again 
work in enterprises and help in triggering actions on the basis 
of the set of rules defined.  
But the question emerges: how is CEP different from a 
Rules Engine (RE)? One main difference is that RE are 
stateless. An input payload is expected for a RE which is 
processed and output is produced. CEP engines are state-full 
and can be persistentin the case of failure. Another difference 
is that RE expects a single payload of data while as CEP can 
accept data and output data to multiple channels.  
CEP works on real-time event data which is coming into 
the system where as RE would working on stored data or 
payload data (now some RE’s support pseudo real-time 
event data too) Hence RE should be used where the situation 
does not need to be state-full and there is no need for either 
real-time events or time based event correlation. CEP and 
Business Rules can work together and complement each 
other. 
B. Stream Analytics 
Online learning algorithms are an important type of 
stream processing algorithms: In a repeated cycle, the 
learned model is constantly updated to reflect the incoming 
examples from the stream. They do so without exceeding 
their memory and time bounds. After processing an 
incoming example, the online learning algorithms are always 
able to output a model. Typical learning tasks in stream 
scenarios are classification, outlier analysis, and clustering.  
In data stream scenarios data arrives at high speed strictly 
constraining processing algorithms in space and time. To 
adhere to these constraints, specific requirements have to be 
7
fulfilled by the stream processing algorithms that are 
different from traditional batch processing settings. The most 
significant requirements are the following:  
 
• Requirement 1: Process an example at a time, and 
inspect it at most once 
• Requirement 2: Use a limited amount of memory 
• Requirement 3: Work in a limited amount of time  
• Requirement 4: Be ready to provide with a 
prediction model at any time 
 
In the context of data streams the learning algorithms 
need to be capable of handling very large (potentially 
infinite) streams of examples.  
 
C. Scalable storage and processing 
During the last 10 years researchers and organizations 
have attempted to tackle the big data problem from many 
different angles. The angle that is currently leading the pack 
in terms of popularity for massive data analysis is an open 
source project called Hadoop. 
Hadoop was inspired by Google’s work on its Google 
Distributed File System (GFS, the base for Big Table paper) 
and the MapReduce programming paradigm, in which work 
is broken down into mapper and reducer tasks to manipulate 
data that is stored across a cluster of servers for massive 
parallelism. Hadoop is generally seen as having two main 
parts:  
• A file system (the Hadoop Distributed File System)  
• A programming paradigm Map-Reduce which is 
considered the heart of Hadoop. 
The term MapReduce actually refers to two separate and 
distinct tasks that Hadoop programs perform. The first is the 
“Map” task, which takes a set of data and converts it into 
another set of data, where individual elements are broken 
down into tuples (key/value pairs). The “reduce” job takes 
the output from a map as input and combines those 
datatuples into a smaller set of tuples. As the sequence of the 
name Map-Reduce implies, the “reduce” job is always 
performed after the “Map” job. 
Unlike traditional transactional systems, Hadoop is 
designed to scan through large data sets to produce its results 
through a highly scalable, distributed batch processing 
system. Hadoop is not about immediate response times, in-
 memory real-time warehousing, online data management or 
blazing transactional speed; it is about discovery and making 
the once near-impossible possible from a scalability and 
analysis perspective. 
The Hadoop methodology is built around a function-to-
 data model as opposed to data-to-function; in this model, 
because there is so much data, the analysis programs are sent 
to the data. 
There are a number of Hadoop-related projects. Some of 
the more notable include: Apache Avro (for data 
serialization), Cassandra and HBase (databases), Chukwa (a 
monitoring system specifically designed with large 
distributed systems in mind), Hive (provides ad hoc SQL-
 like queries for data aggregation and summarization), 
Mahout (a machine learning library), Pig (a high-level 
Hadoop programming language that provides a data-flow 
language and execution framework for parallel computation), 
ZooKeeper (provides coordination services for distributed 
applications), among others. 
D. Data orchestration 
Integrated information is a core component of any 
analytics effort, and it is even more important with big data. 
An organization’s data has to be readily available and 
accessible to the people and systems that need it. 
Master data management and the integration of key data 
types –sensor, customer, product, vendor, employee and the 
like– require cross-enterprise data that is governed according 
to a single enterprise standard. The inability to connect data 
across organizational and department silos has been a 
business intelligence challenge for years. This integration is 
even more important, yet much more complex, with big data 
[34].  
IV. CONCLUSSIONS 
Baden-Fuller [20], suggests that business models have a 
multivalent character as models. They can be found as 
exemplar role models that might be copied, or presented as 
nutshell descriptions of a business organisation; also 
business model can represent relevant instances for a class of 
things. 
Given the state of development of the Big Data concept 
at present, there is not enough data from firms to characterize 
successful business models and extract best-in-kind patterns. 
This paper contends that disruptive business models will 
leverage from the ability to integrate the 4Vs (Volume, 
Variety, Velocity and Veracity) into a single, coherent, value 
proposition. Moreover given some similarities shared 
between Big Data and existing technological paradigms (e.g. 
mobile, cloud computing) we foresee an emergence of 
strategic federations of stakeholders [14,15] organized 
around technological platforms [12, 26-28]. 
 
ACKNOWLEDGMENTS 
The authors thank the support of RCUK through the 
sustainable society network (EP/K003593/1) and new 
economic models in the digital economy network 
(NEMODE, EP/K003542/1) research programs. 
 
REFERENCES 
 
[1] J. Manyika, et al. “Big data: The next frontier for innovation, 
competition, and productivity”, McKinsey 2011. 
[2] B. Walsh, “The NSA's Big Data Problem”. The times 2013. 
[3] The Economist, “Look who’s listening”. The Economist 2013. 
[4] D, Howe et al. "Big data: The future of biocuration." Nature 455.7209 
(2008): 47-50. 
[5] N. Shadbolt, et al., “Linked open government data: lessons from 
Data.gov.uk”. IEEE Intelligent Systems, 27, (3), Spring Issue, 16-24. 
(doi:10.1109/MIS.2012.23). 
8
[6] J. Hendler,et al. "US government linked open data: semantic. data. 
gov." IEEE Intelligent Systems 27.3 (2012): 0025-31. 
[7] J. Shapiro, "Smart cities: quality of life, productivity, and the growth 
effects of human capital." The review of economics and statistics 88.2 
(2006): 324-335. 
[8] EU, “M/490 EN Smart Grid Mandate”. European Union 2011. 
[9] D, Teece, "Business models, business strategy and innovation." Long 
range planning 43.2 (2010): 172-194. 
[10] A. Arora, A. Fosfuri, and A. Gambardella. “Markets for technology: 
The economics of innovation and corporate strategy”. The MIT Press, 
2004. 
[11] A. Gawer, M. Cusumano,"How companies become platform 
leaders." MIT/Sloan Management Review 49 (2012). 
[12] Gawer, Annabelle, and Rebecca Henderson. "Platform owner entry 
and innovation in complementary markets: Evidence from 
Intel." Journal of Economics & Management Strategy 16.1 (2007): 1-
 34. 
[13] A. Leiponen. "Competing through cooperation: The organization of 
standard setting in wireless telecommunications." Management 
Science 54.11 (2008): 1904-1919. 
[14] J. Hagedoorn, "Inter-firm R&D partnerships: an overview of major 
trends and patterns since 1960." Research policy 31.4 (2002): 477-
 492. 
[15] J. Hagedoorn, A. N. Link, and N. S. Vonortas. "Research 
partnerships." Research Policy 29.4 (2000): 567-586. 
[16] E. Von Hippel, G. Von Krogh. "Open source software and the 
“private-collective” innovation model: Issues for organization 
science."Organization science 14.2 (2003): 209-223. 
[17] E. Von Hippel and G. Von Krogh. "Free revealing and the private-
 collective model for innovation incentives." R&D Management 36.3 
(2006): 295-306. 
[18] A. Oliver, M. Reitzig. "Private–collective innovation, competition, 
and firms’ counterintuitive appropriation strategies." Research 
Policy (2013). 
[19] A. McAfee, E. Brynjolfsson. "Big data: the management revolution." 
Harvard business review (2012). 
[20] C. Baden-Fuller, M. S. Morgan. "Business models as models." Long 
Range Planning 43.2 (2010). 
[21] C. Zott,R. Amit. "Business model design: an activity system 
perspective." Long range planning 43.2 (2010). 
[22] A. Gambardella, A. M. McGahan. "Business-model innovation: 
General purpose technologies and their implications for industry 
structure." Long Range Planning 43.2 (2010). 
[23] R. Casadesus-Masanell, J.E. Ricart. "From strategy to business 
models and onto tactics." Long Range Planning 43.2 (2010). 
[24] D. J Teece., G. Pisano, A.Shuen. "Dynamic capabilities and strategic 
management." Strategic management journal 18.7 (1997). 
[25] M. Katz, C. Shapiro. "Systems competition and network effects." The 
Journal of Economic Perspectives 8.2 (1994). 
[26] T. Eisenmann, G. Parker, and M. W. Van Alstyne. "Strategies for 
two-sided markets." Harvard business review 84.10 (2006). 
[27] T. Eisenmann, G. Parker, M. Van Alstyne. "Platform envelopment." 
Strategic Management Journal 32.12 (2011). 
[28] T. Eisenmann, G. Parker, M. Van Alstyne. "6. Opening platforms: 
how, when and why?." Platforms, markets and innovation (2011). 
[29] IBM White Paper "Managing big data for smart grids and smart 
meters. Meet the challenge posed by the growing volume, velocity 
and variety of information in the energy industry " 
[30] Economist. “Data, data everywhere”. The Economist (2010). 
[31] Hagedoorn, John. "Inter-firm R&D partnerships: an overview of 
major trends and patterns since 1960." Research policy 31.4 (2002): 
477-492. 
[32] Hagedoorn, John, Albert N. Link, and Nicholas S. Vonortas. 
"Research partnerships." Research Policy 29.4 (2000): 567-586. 
[33] MaLunnan, Randi, and Sven A. Haugland. "Predicting and measuring 
alliance performance: a multidimensional analysis." Strategic 
Management Journal 29.5 (2008): 545-556. 
[34] Needham, Jeffrey. Disruptive Possibilities: How Big Data Changes 
Everything. O'Reilly Media, 2013. 
 
9

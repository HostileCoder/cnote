RANK MINIMIZATION FOR SUBSPACE TRACKING FROM INCOMPLETE DATA 
Morteza Mardani, Gonzalo Mateos, and Georgios B. Giannakis 
Dept. of ECE, University of Minnesota, Minneapolis, MN 
ABSTRACT 
Extracting latent low-dimensional structure from high-dimensional 
data is of paramount importance in timely inference tasks encoun?
 tered with 'Big Data' analytics. However, increasingly noisy, het?
 erogeneous, and incomplete datasets as well as the need for real-time 
processing pose major challenges towards achieving this goal. In this 
context, the fresh look advocated here permeates benefits from rank 
minimization to track low-dimensional subspaces from incomplete 
data. Leveraging the low-dimensionality of the subspace sought, 
a novel estimator is proposed based on an exponentially-weighted 
least-squares criterion regularized with the nuclear norm. After re?
 casting the non-separable nuclear norm into a form amenable to on?
 line optimization, a real-time algorithm is developed and its con?
 vergence established under simplifying technical assumptions. The 
novel subspace tracker can asymptotically offer the well-documented 
performance guarantees of the batch nuclear-norm regularized esti?
 mator. Simulated tests with real Internet data confirm the efficacy 
of the proposed algorithm in tracking the traffic subspace, and its 
superior performance relative to state-of-the-art alternatives. 
Index Terms- Low rank, online algorithm, matrix completion. 
1. INTRODUCTION 
Nowadays ubiquitous e-commerce sites, the Web, and Internet-friendly 
portable devices generate massive volumes of data. The undeniable 
consensus is that tremendous economic growth and improvement in 
quality of life can be effected by harnessing the potential benefits 
of analyzing this large volume of data. As a result, the problem 
of extracting the most informative, yet low-dimensional structure 
from high-dimensional datasets is of paramount importance [6]. The 
sheer volume of data and the fact that observations are acquired se?
 quentially in time, motivate updating previously obtained 'analytics' 
rather than re-computing new ones from scratch each time a new 
datum becomes available. In addition, large-scale datasets are often 
incomplete, and prone to corrupt measurements as well as communi?
 cation errors. In this context, consider the following streaming data 
model with incomplete observations at time t 
(1) 
where Xt E IRP is the signal of interest, and v, accounts for the 
noise. The set Wt C [1,2, ... , P] contains the indices of available 
observations, and the corresponding sampling operator Pw, ( -) sets 
the entries of its vector argument not in w, to zero, and keeps the 
rest unchanged. Suppose that the signal sequence {X'}?l lives in 
a low-dimensional (<< P) linear subspace Lt. Given the incom?
 plete observations {P WT (y T ), WT } ?=1' this paper deals with online 
(adaptive) estimation of L" and reconstruction of the signal Xt as a 
byproduct. 
Work was supported by an AFOSR MURI grant no. FA 9550-10-1-0567 
Relation to prior work. Subspace tracking has a long history in 
signal processing. An early noteworthy representative is the projec?
 tion approximation subspace tracking (PAST) algorithm [14]. Re?
 cently, an algorithm (termed GROUSE) for tracking subspaces from 
incomplete observations was put forth in [1], based on incremental 
gradient descent iterations on the Grassmannian manifold of sub?
 spaces. PETRELS is a second-order recursive least-squares (RLS)?
 type algorithm, that extends the seminal PAST iterations to handle 
missing data [4]. As noted in [5], the performance of GROUSE is 
limited by the existence of barriers in the search path on the Grass?
 manian, which may lead to GROUSE iterations being trapped at 10-
 cal minima; see also [5]. Lack of regularization in PETRELS can 
lead to unstable behaviors, especially when the amount of missing 
data is large. Relative to all aforementioned works, the algorithmic 
framework of this paper permeates benefits from rank minimization 
to low-dimensional subspace tracking (Section 3), and offers theo?
 retical performance guarantees (Section 4). 
Contributions. Leveraging the low dimensionality of the underly?
 ing subspace, a novel estimator is proposed based on an exponentially?
 weighted least-squares (LS) criterion regularized with the nuclear 
norm of the unknown signal of interest. Upon recasting the non?
 separable nuclear norm into a form amenable to online optimization, 
a real-time algorithm for susbspace tracking is developed and its con?
 vergence is established under simplifying technical assumptions. In?
 terestingly, under mild assumptions the proposed online algorithm 
attains the global optimum of the batch nuclear-norm regularized 
problem, whose quantifiable performance has well-appreciated mer?
 its [2, 3]. As a byproduct, the proposed online algorithm offers a 
viable approach to solving large-scale matrix completion problems. 
Simulated tests with Internet traffic data corroborate the effective?
 ness of the proposed algorithm for traffic estimation, and its superior 
performance relative to state-of-the-art alternatives [1,4]. 
Notation: Operators (.) ', (9, AminO, and amax(-) will denote trans?
 position, Kronecker product, minimum eigenvalue, and maximum 
singular value, respectively; I . I is the magnitude of a scalar and 
II 112 the Û2-norm of a vector. For matrix X, IIXIIF denotes the 
Frobenius norm, and X :::: 0 means that X is positive semidefinite. 
The n x n identity matrix will be represented by In, while On will 
stand for the n x 1 vector of all zeros, and Onxp := OnO?. 
2. NUCLEARáNORM REGULARIZATION 
Collect the indices of available observations up to time t in the set 
Ot := U?=lW', and the actual observations in the matrix Po, (Y,) := 
[PW1 (Yl), . . .  ,Pw, (y,)] E IRPxt. Likewise, introduce matrix X, 
containing the signal of interest. Since x, lies in a low-dimensional 
subspace, X, is a low-rank matrix. A natural estimator leverag?
 ing the low rank property of X, attempts to fit the incomplete data 
Po, (Yt) to X, in the LS sense, as well as minimize the rank of 
X,. Unfortunately, albeit natural the rank criterion is in general 
NP-hard to optimize [12]. Typically, the nuclear norm IIX, II. := 
978-1-4799-0356-6/13/$31.00 ©2013 IEEE 5681 ICASSP2013 
Lk ak (Xt) (ak is the k-th singular value) is adopted as a surrogate 
to rank(Xt). Accordingly, one solves [3] 
A {I 2 
} (Pl) Xt := argm?n '2 IIPrlt(Yt -X)IIF + AtIIXII. 
where At is a (possibly time-varying) rank-controlling parameter. 
Albeit convex, (Pl) is a non-smooth optimization problem (the nu?
 clear norm is not differentiable at the origin). In addition, scal?
 able algorithms to impute missing entries for streaming observations 
should effectively overcome the following challenges: (c I) the prob?
 lem size can easily become quite large, since the number of opti?
 mization variables is Pt; (c2) existing iterative solvers for (PI) typ?
 ically rely on costly SVD computations per iteration; see e.g., [2]; 
and (c3) different from the Frobenius-norm, (columnwise) nonsepa?
 rability of the nuclear-norm challenges online processing when new 
columns {PWt (Ytn arrive sequentially in time. In the following 
subsection, the 'Big Data' challenges (cl) and (c2) are dealt with to 
arrive at an efficient online algorithm in Section 3. 
2.1. A separable low-rank regularization 
To address (cl) and reduce the computational complexity and mem?
 ory storage requirements of the algorithm sought, it is henceforth 
assumed that the dimensionality of the underlying subspace Lt is 
bounded by a known quantity p. Accordingly, it is natural to require 
p :::: rank(Xt). As argued in Remark I, the smaller the value of p, 
the more efficient the algorithm becomes. Because rank(Xt) ::; p, 
(PI),s search space is effectively reduced and one can factorize the 
decision variable as X = LQ', where L and Q are P x p and 
t x p matrices, respectively. It is possible to interpret the columns of 
X (viewed as points in IRP) as belonging to the low-rank subspace, 
spanned by the columns of L. The rows of Q are thus the projections 
of the columns of X onto the subspace. Adopting this reparametriza?
 tion of X in (P 1) one arrives at a non convex problem where the num?
 ber of variables is reduced from Pt in (Pl), to p(P + t). The savings 
can be significant when p is small, and both P and t are large. 
To address (c2) [along with (c3) as it will become clear in Sec?
 tion 3], consider the following alternative characterization of the nu?
 clear norm [12] 
IIXII. := min -2
 1 
{IILII} + IIQII}}, s. t. X = LQ'. (2) 
{L.Q} 
The optimization (2) is over all possible bilinear factorizations of X, 
so that the number of columns p of L and Q is also a variable. Lever?
 aging (2), the following reformulation of (PI) provides an important 
first step towards obtaining an online algorithm: 
(P2) min -2
 1
 1IPrlt(Yt - LQ')II} + 
A
 2
 t 
{IILII} + IIQII}}á 
{L,Q} 
As asserted in [8, Lemma I], adopting the separable Frobenius-norm 
regularization in (P2) comes with no loss of optimality relative to 
(PI), provided p :::: rank(Xt). By finding the global minimum of 
(P2) [which could have considerably less variables than (Pl)], one 
can recover the optimal solution of (Pl). However, since (P2) is 
nonconvex, it may have stationary points which need not be glob?
 ally optimum. Interestingly, the next proposition shows that under 
relatively mild assumptions on rank(Xt) and the noise variance, sta?
 tionary points of (P2) qualify as global optimum solutions of (Pl). 
Proposition 1. f8} Let {Lt, Q'} be a stationary point of(P2J. If 
amax[Prlt (Yt - LtQ;)] ::; At, then Xt := LtQ; is the globally 
optimal solution of (P 1). 
3. ONLINE MATRIX IMPUTATION 
As stated in Section 1, the goal is to recursively estimate Xt at time 
t from historical observations {PWr (y r), Wr }?=1' naturally placing 
more importance on recent measurements. To this end, one possible 
adaptive counterpart to (P2) is the exponentially-weighted LS esti?
 mator found by minimizing the empirical cost (Q := [q1, ... , qt]) 
where.\t := At/ L!=l fJt-T, and 0 < fJ ::; 1 is the so-termed for?
 getting factor. When fJ < 1, data in the distant past are exponentially 
downweighted, which facilitates tracking in nonstationary environ?
 ments. In the case of infinite memory (fJ = 1), the formulation (3) 
coincides with the batch estimator (P2). This is the reason for the 
time-varying factor.\t weighting IlL II}á 
3.1. Subspace tracking from incomplete data 
Towards deriving a real-time, computationally efficient, and recur?
 sive solver of (3), an alternating-minimization (AM) method is adopted 
in which iterations coincide with the time-scale t of data acquisition. 
A justification in terms of minimizing a suitable approximate cost 
function is discussed in detail in Section 4.1. Per time instant t, a 
new datum {PWt (Yt), wt} is drawn and qt is estimated via 
q[t] = arg mm - IIPWt (Yt - L[t - l]q) 112 + -llql12 . . 
[ 1 2 At 2] 
q 2 2 
(4) 
Updating (4) entails an £2-norm regularized LS (ridge-regression) 
problem, that admits closed-form solution 
q[t] = (AtIp + L'[t - l]ntL[t - 1]) -1 L'[t - I]PWt (Yt) (5) 
where diagonal nt E IRPx P is such that [nt]p,p = 1 if p E Wt, and 
is zero elsewhere. In the second step of the AM scheme, the updated 
subspace matrix L[t] is obtained by minimizing (3) with respect to 
L, while the optimization variables {qT }?=1 are fixed and take the 
values {q[T]}?=l' namely 
L[t]=arg m?n [ ?
 t 
IILII}+ ??t-T ? IIPWT (YT-Lq[T]) II?] . (6) 
Notice that (6) decouples over the rows of L which are obtained in 
parallel via 
for p = 1, ... , P, where Wp,T denotes the p-th diagonal entry of 
nT¥ For fJ = 1 and fixed At = A, Vt, subproblems (7) can be effi?
 ciently solved using the RLS algorithm [13]. Upon defining sp[t] := 
L!=l fJt-T Wp,T(YP,T )q[T], Hp[t] := L?=l fJt-T Wp,Tq[T]q'[T] + 
AtIp, and Mp[t] := H;l[tJ, one simply updates 
sp[t] = sp[t - 1] + Wp,tYP,tq[t] 
M [t] = M [t - 1] _ W Mp[t - l]q[t]q' [t]Mp [t - 1] p P p,t 1 + q' [t]Mp [t - l]q[t] 
and forms Ip[t] = Mp [t]sp [tJ, for p = 1, ... , P. 
However, for 0 < fJ < 1 the regularization term (At/2 )IIIII? in 
(7) makes it impossible to express Hp [t] in terms of Hp [t - 1] plus a 
5682 
Algorithm 1 : Subspace tracking from incomplete observations 
input {PWT (YT)' WT };"'=1, {AT };"'=1, and (3. 
initialize Gp[O] = Opxp, sp[O] = Op, P = 1, ... , P, and L[O] at 
random. 
fort = 1, 2, ... do 
D[t] = (AtIp + L/[t - l]OtL[t - 1])-1 L/[t - 1]. 
q[t] = D[t]Pwt (Yt). 
Gp[t] = (3Gp[t - 1] + Wp,tq[t]q[tJ', p = 1, ... , P. 
sp[t] = (3sp[t - 1] + w1'.iYP,tq[t], p = 1, ... , P. lp[t] = (Gp[t] + AtIp) sp[t], p = 1, ... , P. 
return Xt := L[t]q[t]. 
end for 
rank-one correction. Hence, one cannot resort to the matrix inversion 
lemma and update Mp[t] with quadratic complexity only. Based 
on direct inversion of Hp [t], p = 1, ... , P, the overall recursive 
algorithm for subspace tracking from incomplete data is tabulated 
under Algorithm 1. 
Remark 1. [Computational cost] Careful inspection of Algorithm 1 
reveals that the main computational burden stems from p x p inver?
 sions to update the subspace matrix L[t]. The per iteration complex?
 ity for performing the inversions is O(pp3) (which could be further 
reduced if one leverages also the symmetry of Gp ltD, while the cost 
of multiplication as well as additions is O(pp2). The overall cost of 
the algorithm per iteration can be safely estimated as O(P p3), which 
is affordable since p is typically small (cf. the low rank assumption). 
Remark 2. [Tuning Ad In practice, to tune At one can resort to 
the heuristic rules proposed in [3], which build upon the following 
reasonable assumptions: i) Vp,t ? N(O, a2), ii) elements of Ot are 
independently sampled with probability Jr, and iii) P and t are large 
enough. Accordingly, one can pick At = (VP + 00) vna which 
naturally increases as time evolves, and where te := L?=l (3t-T is 
the effective time window. 
4. PERFORMANCE GUARANTEES 
This section studies the performance of Algorithm 1 for the infinite 
memory special case i.e., when (3 = 1. In the sequel, to make the 
analysis tractable the following assumptions are made: 
AI) {W,}?l and {PWt (Yt)}?l are independent and identically 
distributed (i.i.d.) random processes; 
A2) {PWt(Yt)}?l is uniformly bounded; and 
A3) Iterates {L[t]}?l are in a compact set. 
To clearly delineate the scope of the analysis, it is worth com?
 menting on the assumptions AI)-A3) and the factors that influence 
their satisfaction. Regarding AI), the acquired data is assumed sta?
 tistically independent across time as it is customary when studying 
the stability and performance of online (adaptive) algorithms [13]. 
While independence is required for tractability, AI) may be grossly 
violated because the observations {PWt (Yt)} are correlated across 
time (cf. the fact that {x,} lies in a low-dimensional subspace). 
Still, in accordance with the adaptive filtering folklore e.g., [13], as 
(3 -+ 1 the upshot of the analysis based on i.i.d. data extends ac?
 curately to the pragmatic setting whereby the observations are cor?
 related. Uniform boundedness of PWt (Yt) [cf. A2)] is natural in 
practice as it imposed by the data acquisition process. The bounded 
subspace requirement in A3) is a technical assumption that simpli?
 fies the analysis, and has been corroborated via extensive computer 
simulations [9]. 
4.1. Convergence 
The convergence of the iterates generated by Algorithm 1 is estab?
 lished first. Upon defining the function 
1 2 At 2 9t(L, q) := '2 IIPWt (Yt - Lq)112 + 211ql12 
in addition to et(L) := minq 9t(L, q), Algorithm 1 aims at mini?
 mizing the following average cost function at time t 
(8) 
Normalization (by t) ensures that the cost function does not grow un?
 bounded as time evolves. For any finite t, (8) is essentially identical 
to the batch estimator in (P2) up to a scaling, which does not affect 
the value of the minimizer. Note that as time evolves, minimiza?
 tion of Ct becomes increasingly complex computationally. Hence, 
at time t the subspace estimate L[t] is obtained by minimizing the 
approximate cost function 
in which q[t] is obtained based on the prior subspace estimate L[t -
 1] after solving [ef. (4)] q[t] = argminq9t(L[t - l], q). Obtain?
 ing q[t] this way resembles the projection approximation adopted 
in [14]. Since Ct(L) is a smooth convex function, the minimizer 
L[t] = arg minL Ct(L) is the solution of the quadratic equation 
\7Ct(L[t]) = Opxp. 
So far, it is apparent that the approximate cost function Ct (L [t]) 
overestimates the target cost Ct(L[t]), for t = 1, 2, .... However, 
it is not clear whether the subspace iterates {L[t]}?l converge, and 
most importantly, how well can they optimize the target cost func?
 tion Ct. The good news is that Ct(L[t]) asymptotically approaches 
Ct(L[t]), and the subspace iterates null \7Ct(L[t]) as well, both as 
t -+ 00. The latter result is summarized in the next proposition, 
whose proof is inspired by [11] and can be found in [9]. 
Proposition 2. Suppose At = A 'tit, and Amin[\72Ct(L)] 2': cfor 
some c > O. Then limt-texo \7Ct(L[t]) = Opxp almost surely (a.s.), 
i.e., the subspace iterates {L[t]}?l asymptotically coincide with the 
stationary points of the batch problem (P2). 
The sampling set Ot plays a key role towards satisfying the Hes?
 sian's positive semi-definiteness condition in Proposition 2. Intu?
 itively, if the missing entries are somehow uniformly spread across 
time, the likelihood that \72Ct(L) = fIpp+ t L?=l (q[T]q/[T])@ 
OT t cIpp holds is higher. 
4,2, Optimality 
In line with Proposition 1, one may be prompted to ponder whether 
the online estimator offers the performance guarantees of the nuclear?
 norm regularized estimator (PI), for which stable/exact recovery 
results are well documented e.g., in [2, 3]. Specifically, given the 
learned subspace L[t] and the corresponding Q[t] [obtained via (4)] 
5683 
over a time window of size t, is {X[t] L[tlQ'[t]} an optimal 
solution of (PI) when t ---+ oo? This in turn requires asymptotic 
analysis of the optimality conditions for (PI), and is established in 
the next proposition. Additionally, numerical tests in Section 5 cor?
 roborate that the online estimator attains the performance of (PI) 
after reasonable number of iterations. 
Proposition 3. For the iterates generated by Algorithm 1, if there ex?
 ists a subsequence {L[tk], Q[tk]} for which c1) limk-+= 'VCtk (L[tk]) 
= Opxp a.s., and c2) kamax[Prltk (Ytk - L[tklQ'[tk])l ::; )f,; 
hold, then the sequence {X[k] = L[tklQ'[tk]} satisfies the optimal?
 ity conditions for (P 1) [normalized by tkl as k ---+ 00 a.s. 
Regarding the convergence condition cl ), even though it holds 
for a time invariant rank-controlling parameter A as per Proposi?
 tion 2, numerical tests indicate that it also holds for the time-varying 
case (e.g., when At is chosen as suggested in Remark 2). Accord?
 ing to A2) and A3), amax[Prlt (Yt - L[t]Q'[t])] ? O( Vi), which 
implies that the quantity on the left-hand side of c2) cannot grow un?
 bounded. Moreover, upon choosing At ? O( Vi) as per Remark 2 
the term in the right-hand side of c2) will not vanish, which suggests 
that the qualification condition can indeed be satisfied. 
5. NUMERICAL TESTS 
The convergence and effectiveness of Algorithm I is assessed in this 
section via computer simulations. 
5,1. Synthetic data tests 
The signal Xt = UWt is generated from the low-dimensional sub?
 space U E lRpxr, with i.i.d. entries Up,i ? N(O, liP), and pro?
 jection coefficients Wi.t ? N(O, 1). The noise Vi.t ? N(O, a2) is 
i.i.d., and the entries of Yt are sampled uniformly at random with 
probability 1l' to form the diagonal sampling matrix nt. The obser?
 vations at time t are then generated as PWt (Yt) = nt(Xt + Vt). Fix 
r = 5 and p = 10, while different values of 1l' and a are examined. 
The evolution of the average cost Ct(L[t]) in (8) for different per?
 centages of missing data and noise variances is depicted in Fig. l (a). 
For validation purposes, the optimal cost [normalized by the window 
size t] of the batch estimator (PI) is also shown. It is apparent that 
Ct (L[t]) converges to its batch counterpart (PI), which corroborates 
that Algorithm I can attain the performance of (PI). This obser?
 vation together with the low-complexity of Algorithm l 's iterations 
[ef. Remark 1], make it a viable alternative for solving large-scale 
matrix completion problems. 
Next, Algorithm 1 is compared with two state-of-the-arte sub?
 space trackers, namely PETRELS [4] and GROUSE [I]. These two 
algorithms require and estimate of the dimensionality of the under?
 lying subspace, which is denoted by K. Set A = 0.1, f3 = 0.99, 
and consider an abrupt change in the subspace at t = 104 to evalu?
 ate the tracking performance of the algorithms. The figure of merit 
is the average estimation error et := t 2:::=1 IIXi - xi112/11xi112' 
which is depicted in Fig. I(b). It is observed that if the subspace 
dimensionality is chosen as K = p, Algorithm I attains a better esti?
 mation accuracy than PETRLES and GROUSE (a constant step size 
0.1 was adopted for the latter). Even though PETRELS works well 
for K = r, if one overestimates the rank PETRELS exhibiting erratic 
behaviors for 25% available observations. As expected, for the ideal 
choice of K = r all three schemes achieve similar estimation accu?
 racy. The smaller error exhibited by PETRELS (relative Algorithm 
I) might be due to a suboptimum choice of A. Still, Algorithm I is 
more stable numerically when the amount of missing observations 
(a) (b) 
Fig. 1. Performance of Algorithm 1. (a) Evolution of the average 
cost Ct(L[t]) versus the batch counterpart. (a) Relative estimation 
error for different schemes when 1l' = 0.25 and a2 = 10-3. 
?
 (a) (b) 
Fig. 2. Performance for Internet-2 data if K = P = 10 and f3 = 0.95. 
(a) Average estimation error for various amounts of missing data. (b) 
Estimated (red) versus true (blue) 00 flow traffic for 1l' = 0.25. 
is large, thanks to the regularization terms in (3). The price paid by 
Algorithm 1 is in terms of higher complexity per iteration. Note that 
the complexity for PETRELS is O(PK2), and only O(PK(l + 1l'K)) 
for the first-order algorithm GROUSE. 
5.2. Tracking Internet-2 network traffic 
In IP networks accurate estimation of the origin-to-destination (00) 
flow traffic is a task of paramount interest. Typically, the data avail?
 able is a measured traffic via NetFlow [7], for a small subset of 
00 flows. Several studies have demonstrated that 00 flow traf?
 fic exhibits a low-intrinsic dimensionality, which is mainly due to 
common temporal patterns across 00 flows and periodic behaviors 
across time [7]. In this example, OD-f1ow traffic-levels are col?
 lected from operation of the Internet-2 network (Internet backbone 
across USA) [7]. The measured 00 flows contain spikes (anoma?
 lies), which are removed as detailed in [9] to end up with an anomaly?
 free data stream {y,}. A subset of entries of Y t are then selected 
independently with probability 1l', to yield the input of Algorithm 1. 
The evolution of the average traffic estimation error (et) is depicted 
in Fig. 2(a), for different schemes and various amounts of miss?
 ing data. It is observed that the estimation accuracy and subspace 
learning speed degrades gracefully as the NetFlow sampling rate de?
 creases. Algorithm 1 outperforms competing alternatives when At is 
tuned adaptively as per Remark 2, for a2 = 0.1. When only 25% of 
the total OD flows are sampled by Netflow, Fig. 2(b) depicts how the 
representative 00 flows are accurately tracked using Algorithm 1. 
5684 
6. REFERENCES 
[1] L. Balzano. R. Nowak. and B. Recht. "Online identification and 
tracking of subspaces from highly incomplete information," in 
Proc. of Allerton Conference on Communication, Control. and 
Computing, Monticello, USA, Jun. 2010. 
[2] E. J. Candes and B. Recht, "Exact matrix completion via convex 
optimization," in Found. Comput. Math., vol. 9, no. 6, pp. 717-
 722,2009. 
[3] E. J. Candes and Y. Plan, "Matrix completion with noise," in 
Proceedings of the IEEE, vol. 98, pp. 925-936, 2009. 
[4] Y. Chi, Y. C. Eldar, and R. Calderbank, "PETRELS: Subspace 
estimation and tracking from partial observations," in Proc. of 
IEEE International Conference on Acoustics, Speech and Signal 
Processing, Kyoto, Japan, Mar. 2012. 
[5] W. Dai, O. Milenkovic, and E. Kerman, "Subspace evolu?
 tion and transfer (SET) for low-rank matrix completion," IEEE 
Trans. Signal Process., vol. 59, pp. 3120--3132, Jul. 2011. 
[6] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Sta?
 tistical Learning. Springer, Second edition, 2009. 
[7] A. Lakhina, K. Papagiannaki, M. Crovella, C. Diot, E. D. Ko?
 laczyk, and N. Taft, "Structural analysis of network traffic 
flows," in Proc. of ACM SIGMETRICS, NY, USA, Jul. 2004. 
[8] M. Mardani, G. Mateos, and G. B. Giannakis, "In-network 
sparsity regularized rank minimization: Applications and al?
 gorithms," IEEE Trans. Signal Process., 2012, see also 
arXiv:1203.1570vl [cs.MA]. 
[9] M. Mardani, G. Mateos, and G. B. Giannakis, "Dynamic 
anomalography: Tracking network anomalies via sparsity and 
low-rank," IEEE 1. Sel. Tpoics in Signal Process., 2012, see 
also arXiv:1208.4043vl [cs.NI]. 
[10] G. Mateos and G. B. Giannakis, "Robust PCA as bilinear de?
 composition with outlier-sparsity regularization," IEEE Trans. 
Signal Process., vol. 60, pp. 5176-5190, Oct. 2012. 
[11] J. Mairal, J. Bach, J. Ponce, and G. Sapiro, "Online learning 
for matrix factorization and sparse coding," in 1. of Machine 
Learning Research, vol. 11, pp. 19-60, Jan., 2010. 
[12] B. Recht, M. Fazel, and P. A. Parrilo, "Guaranteed Minimum?
 Rank Solutions of Linear Matrix Equations via Nuclear Norm 
Minimization," in SIAM Rev., vol. 52, pp. 471-501, 2010. 
[13] Y. Solo and X. Kong, Adaptive Signal Processing Algorithms: 
Stability and Performance, Prentice Hall, 1995. 
[14] M. B. Yang, "Projection approximation subspace tracking," 
IEEE Trans. Signal Process., vol. 43, pp. 95-107, Jan. 2012. 
[15] http://internet2. edu/observatory /archive/data-coll ecti ons. 
5685 

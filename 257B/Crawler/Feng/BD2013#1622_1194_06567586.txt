FPGA-Based HPC Application Design
 for Non-Experts
 David Uliana, Krzysztof Kepa, and Peter Athanas
 Virginia Polytechnic Institute and State University,
 Blacksburg, VA 24061, USA
 fduliana,kepa,athanasg@vt.edu
 Abstract—In the current era of big-data computing, most
 non-engineer domain experts lack the skills needed to design
 FPGA-based hardware accelerators to address big-data problems.
 This work presents bFlow, a development environment that
 facilitates the assembly of such accelerators, specifically those
 targeting FPGA-based hybrid computing platforms, such as
 the Convey HC series. This framework attempts to address
 the above problem by making use of an abstracted, graphical
 front-end more friendly to users without computer engineering
 backgrounds than traditional tools, as well as by accelerating
 bitstream compilation by means of incremental implementation
 techniques. bFlow’s performance, usability, and application to
 big-data life-science problems was tested by participants of
 an NSF-funded Summer Institute organized by the Virginia
 Bioinformatics Institute (VBI). In about one week, a group of four
 non-engineering participants made significant modifications to a
 reference Smith-Waterman implementation, adding functionality
 and scaling theoretical throughput by a factor of 32.
 I. INTRODUCTION
 Today’s world is one of “information everywhere,” in
 which the size and availability of valuable data sets is rising
 rapidly, especially in the life sciences. Consider the genome of
 a single human individual–processing this data set involves the
 analysis of three billion DNA base pairs. This fact is extremely
 relevant given the expectation that in the near future next-
 generation sequencing machines will produce whole human
 genomes in a matter of hours [1]. This flood of information
 creates the need for exceptional computing performance in
 order to process such data sets within tolerable times.
 At the forefront of big-data analysis efforts are domain
 experts, e.g. bioinformaticians who explore genome hypothe-
 ses by analyzing large quantities of genome data [2]. Such
 domain experts may be skilled programmers; however, their
 productivity–hypothesis discovery and verification rate–is lim-
 ited by the performance capacity of available computing re-
 sources. For example, the architecture of data-center class com-
 puting platforms is designed for speed over a general mix of
 problems, and efficiently applying such platforms to domain-
 specific data structures and algorithms such as DNA/protein
 sequence alignment is often not straightforward [3]. Heteroge-
 neous computing machines like the Convey Hybrid-Core (HC)
 servers have potential to address this gap, since they enable
 the creation of application-specific, FPGA- based accelerators
 tightly coupled with general-purpose processing resources (i.e.
 CPUs). This tight coupling is achieved through the use of
 custom instructions and cache-coherent, shared memory space
 [4]. Unfortunately, development flows for such accelerators
 require extensive computer engineering and digital design
 expertise, rendering the use of these platforms impractical for
 most experts in the life sciences community.
 In this work a development flow targeting hybrid-core
 systems such as the Convey HC servers is proposed with
 the purpose of aiding non-engineering domain experts in
 the assembly of big-data hardware accelerators. Such a flow
 should help such experts address ‘-omics’ problems on an
 unprecedented computational scale [2]. Since graphical design
 environments often provide a simpler, more intuitive mapping
 to parallel hardware than text-based languages, a graphical
 design entry tool, namely DataIO’s Azido, was selected as the
 flow’s front-end. Accelerators are described in Azido using
 a graphical, algorithmic language built on simple, canonincal
 primitives, and the tool facilitates development for heteroge-
 neous platforms within a single environment [5]. In this work,
 an Azido System Description (SD) was prepared to target the
 Convey HC architecture. This description acts as a plugin
 to Azido, encapsulating specifications of the HC platform
 organization and interface details. Most of these interfaces
 are abstracted to simpler, higher level structures within Azido
 to ease development. All abstractions are complemented with
 software helper routines (C/C++) that abstract the co-processor
 API into a framework more intuitive to the big-data user.
 II. BACKGROUND
 Design productivity for FPGA-based computing has suf-
 fered from the contemporary ASIC “design productivity gap”
 and has unique needs and opportunities not adequately ad-
 dressed by existing FPGA design tools. In [6] Nelson et
 al. proposed a productivity model that exposes three key
 contributors to high design productivity: multi-level design
 reuse, high-level design abstractions, and a more interactive
 verification environment that increases the number of devel-
 opment turns per day. All of these are necessary to improving
 design productivity, e.g. the use of high level (above HDL)
 languages would significantly reduce functional simulation
 time, thereby increasing turns per day. Also, describing designs
 in a hierarchical, high level language can promote reuse by
 making designs more portable.
 High-level synthesis is a very active area of research,
 and many high level design tools and approaches exist (e.g.
 ImpulseC, AutoESL, SysGen, catapultC). The primary focus
 of these tools is to reduce time to solution for designers
 who have considerable computer engineering expertise. While
 working from a high level of abstraction and guaranteeing
 functionally correct output, they require design input (e.g.
 pragma statements) concerning low-level hardware detail in
 order to produce efficient RTL output. Hence, such languages
 are generally inappropriate for use by those in the life-
 sciences community, which is comprised primarily of software
 programmers with no hardware design experience.
 978-1-4799-0493-8/13/$31.00 ? 2013 IEEE ASAP 2013261
A. Convey hybrid-core architecture and design flow
 The use of heterogeneous computing architectures like the
 Convey HC platform (CPU + FPGA) for fixed- point [7] and
 Nebula (CPU + GPU) for floating point algorithms [8], con-
 stitutes one approach to meeting big-data processing demands.
 The Convey HC-1 system, which is considered in this work,
 consists of a commodity Intel Xeon host server extended with
 a custom co- processor board. This board contains four large
 FPGAs (Xilinx part XC5VLX330) called Application Engines
 (AE), each augmented with cache-coherent, high bandwidth
 memory access (8 memory controllers per AE). Each config-
 uration of the Convey co-processor is called a “personality,”
 and makes application-specific functions available to the host
 server processor as custom instructions [9].
 Convey provides a Personality Development Kit (PDK) to
 enable the creation of custom personalities [10]. The PDK
 includes a comprehensive simulation environment with bus-
 functional models of all interfaces, enabling verification of
 the complete system–host CPU software with the four co-
 processor FPGAs–prior to bitstream compilation. The PDK
 supports HDL and synthesized netlists (e.g. EDIF) as design
 entry formats. Final personality compilation is performed by
 the Xilinx FPGA design flow, while software routines for the
 host CPU are compiled using Convey’s cnyCC compiler. The
 personality can execute on up to all four FPGAs, providing
 significant computational capability; however, the PDK does
 not address the needs and skills of big-data users, most of
 whom have little FPGA development experience.
 ? Local Machine
 ? Remote Shadowfax
 Compute
 Cluster
 Azido Desktop Environment
 -Convey Hybrid Core Platform
 Algorithm Design
 Synthesis (HC) Simulation (x86)
 Personality
 Compilation
 System
 Simulation
 Compilation
 (cnyCC)
 Host Executable
 EDIF to Verilog
 Conversion
 Verilog Netlist
 Run
 Verilog
 Wrap Logic
 Run-Time
 Interaction
 Host C/C++
 Application
 Host Application
 Development
 EDIF Netlist
 Personality Release
 (cae_fpga.tgz)
 Fig. 1. Movement of data within bFlow.
 B. Azido hardware design environment
 Azido is a graphical, object-oriented design environment
 based on the Implementation Independent Algorithm Descrip-
 tion Language (I2ADL). The tool abstracts low-level com-
 plexities of digital hardware design to intuitive, algorithmic
 objects built on its implementation-independent primitive li-
 brary, the CoreLib. Bitstream implementation is accomplished
 with Azido System Descriptions (SDs), which are script-based
 “plugins” to Azido. Among several existing graphical design
 environments, Azido was selected for three reasons: 1) it
 provides a flexible design environment and core library capable
 of servicing many application domains at multiple levels of
 abstraction, 2) the System Description-based implementation
 framework facilitates extention of the tool to many target
 platforms, and 3) beyond standard schematic-capture abilities,
 Azido provides some dynamic features, such as automatic
 data typing and graphical polymorphism. These characteristics
 distinguish Azido from tools such as LabVIEW [11] and
 Simulink [12]. The former was not considered because of
 its constraint to specific target platforms. The latter, though
 capable of producing platform independent HDL, lacks most
 of the dynamic features mentioned above.
 C. bFlow contributions
 This work presents bFlow, an approach that provides a
 simplified and portable accelerator development flow that
 supports rapid prototyping of big-data algorithms in hardware
 by non- engineers. Design productivity is improved with a
 high-level graphical front-end (i.e. Azido) and an accelerated
 compilation process employing incremental implementation
 strategies facilitated by qFlow [13] and Xilinx Hierarchical
 Design Flow [14]. Futhermore, this framework supports the
 growth of the third-party “personalities ecosystem” through
 the distribution of open- source accelerator implementations.
 Thus, no user is limited to closed-source personalities provided
 by the system vendor.
 III. RAPID BIO-ACCELERATOR DEVELOPMENT FLOW
 This approach is divided into three efforts: 1) configuring
 Azido to target the Convey HC-1 platform, 2) the acceleration
 of the back-end compilation process, and 3) a hands-on test
 of the complete flow. The full design framework is shown in
 Figure 1. The accelerator is designed and synthesized within
 Azido on the user’s local machine, compiled and packaged
 as a HC-1 personality on VBI’s Shadowfax cluster [15], and
 simulated and ultimately executed on the HC server. Local
 simulation of individual logical blocks may be performed using
 Azido’s built-in x86 system description, and very rough system
 simulation can also be done locally by instantiating custom
 Component Object Model (COM) objects in place of HC
 memory abstractions (e.g. stream characters from a file on the
 desktop instead of co-processor memory). Of course, cycle-
 accurate system simulation is possible on the HC server.
 A. HC-1 system description and software helper routines
 The Azido System Description (SD) for the HC platforms
 can be divided into three components: 1) a communication
 implementer for the transfer of probe results and stimuli
 between Azido and the personality at run-time, 2) an abstracted
 262
view of the co-processor dispatch interface, and 3) streaming
 abstractions for the memory interface.
 1) Azido communication implementer: The first component
 is invisible to the designer, and handles communication be-
 tween the Azido environment and a running personality in
 order to enable diagnostic probing and stimuli. This is achieved
 using a COM object within the Azido HC-1 SD, an SSH
 tunnel, and several utilities on the HC platform itself. The
 Convey HC management ring interface is used to probe and
 stimulate the design in a non-obtrusive manner.
 Fig. 2. Collection of Convey dispatch interface objects in Azido (left) and
 example usage (stream inverter) of the streaming memory abstractions (right).
 2) Convey dispatch interface abstraction: The second con-
 sists of the Azido abstraction of the AE dispatch interface
 (Figure 2). Logic external to the Azido-generated netlist de-
 codes incoming dispatch instructions and presents a simplified
 interface to the Azido designer. The AE general registers are
 exposed as simple read/write blocks in Azido, and the co-
 processor custom instructions are exposed as blocks with a
 “Start” output.
 3) Memory streaming abstraction: Lastly, to conceal the
 complexities of random memory access and address arith-
 metic, bFlow contains a simplified streaming abstraction of the
 memory controllers available to each AE. These “streamers”
 are comprised of “source” and “sink” modules, to be used to
 stream memory blocks in and out of memory (see Figure 2).
 In addition to the SD objects available to the designer
 within Azido, several software routines were developed to
 simplify the configuration and execution of a custom personal-
 ity. This is done by wrapping the Convey-provided, low-level
 assembly routines in higher-level, C/C++ routines contained in
 a C++ class.
 B. Extension of Smith-Waterman reference implementation
 To verify the usability and productivity that bFlow attempts
 to realize, it was placed in the hands of four non-engineering
 participants of the NSF-funded Summer Institute organized by
 VBI. The students were asked to review [16] and gain a basic
 understanding of the systolic array approach to implementing
 the Smith-Waterman (SW) local sequence alignment algorithm
 [17]. The students were then given a reference SW accelerator
 in Azido and tasked with accelerating the design and extending
 its functionality. As in [16], this accelerator consisted of a large
 systolic array to store the short (50-200 bases) query sequence
 and compute the scoring matrix values as the reference (mil-
 lions of bases) is streamed through. The output is the value
 and location of the highest-scoring cell.
 C. Partial implementation flows with qFlow and Partitions
 Acceleration of the bitstream compilation process is
 achieved by two methods, Xilinx Hierarchical Design Par-
 titions flow [14] and qFlow [13], both incremental, partial
 implementation frameworks that reduce build times through
 high-level management of the Xilinx ISE implementation
 process. Both methods exploit the Convey AE architecture,
 which contains static, unchanging interface logic that con-
 sumes roughly 25% of each AE, and is reimplemented on every
 run of Convey’s standard compilation flow.
 1) Partitions flow: The first approach taken makes use of
 the Xilinx Hierarchical Design Partitions flow [14]. Two par-
 titions were selected for this flow: (1) top partition containing
 the entire FPGA design and (2) the Azido-generated logic.
 The first is preserved while the second is constantly updated
 by the Azido designer. The implementation of this flow is very
 simple, consisting of some Makefile extensions and a couple
 constraint files (*.ucf).
 2) qFlow: This second utilizes a subset of the qFlow
 framework, a tool for acceleration back-end compilation. The
 tool assumes a hierarchical design methodology similar to that
 which the Convey PDK encourages. That is, the use of inter-
 face logic that is designed and configured once, experiencing
 few evolutions throughout the entire design process, during
 which the core, computational logic that is the focus of the
 design undergoes many evolutions. Compared to the partitions-
 based approach discussed above, qFlow provided generally
 faster compilations, but was unable to fit some of the larger
 designs that the partitions framework was able to.
 IV. RESULTS
 The participants were successful in their attempt to im-
 prove the performance and functionality of the reference SW
 implementation. Modifications included logic to maintain the
 index of the highest scoring alignment, and a transition from a
 single cell array to multiple arrays, in order to search multiple
 partitions of the reference sequence in parallel. The first mod-
 ification was simple, and included the use of Azido’s counter,
 maximum, multiplexer, and register objects; however, the sec-
 ond involved significant change to the high-level structure of
 the implementation. This resulted in a realized 4x bandwidth
 increase from 150 million to 600 million bases per second
 (bps), with a feasible speedup of 32x to 4.8 billion bps, given
 enough parallel cell arrays. These reported throughput rates
 are for co-processor performance only, and assume that the
 reference sequence(s) are available in co-processor memory.
 One difficulty experienced by the students during their use
 of the flow was synchronous design– especially synchronizing
 multiple flows of data. Azido’s CoreLib contains objects to ad-
 dress these difficulties; however, they lack elegance, consume
 excessive FPGA resources, and are visually ‘messy.’ Another
 complication of the design process was timing closure. Under
 the standard parameterization, the Convey PDK enforces a
 clock rate of 150 MHz. Such a tight constraint is easily and
 often broken by long chains of asynchronous operations, since
 Azido neither analyses the design nor enforces any timing
 restrictions at compile time.
 The performance of the alternative build flows is given in
 Table IV and visualized in Figure 3. Each Smith-Waterman
 263
TABLE I. BUILD TIMES (MEAN OF THREE RUNS) FOR CONVEY’S
 STANDARD FLOW, THE PARTITIONS FLOW, AND QFLOW. THE SPEEDUP
 OVER STANDARD FLOW IS GIVEN IN PARENTHESES. NOTE: DEVICE
 UTILIZATION LISTED IS THE UTILIZATION DUE ONLY TO USER LOGIC.
 Design Cells Device Util. (%) Mean Build Time (min)
 LUTs FFs Standard Partitions qFlow
 sw 1x8 8 1.83 2.16 89.10 65.60 (1.36) 26.58 (3.35)
 sw 1x12 12 2.43 2.44 89.90 54.16 (1.66) 26.12 (3.44)
 sw 1x16 16 3.02 2.73 104.09 57.92 (1.80) 32.94 (3.16)
 sw 1x24 24 4.22 3.30 98.80 76.85 (1.29) 34.75 (2.84)
 sw 1x32 32 5.41 3.87 102.20 72.89 (1.40) 38.27 (2.67)
 sw 4x8 32 5.62 4.10 96.90 61.58 (1.57) 39.20 (2.47)
 sw 1x48 48 7.79 5.01 128.50 82.95 (1.55) 43.02 (2.99)
 sw 1x64 64 10.18 6.15 129.73 87.92 (1.48) 53.75 (2.41)
 sw 4x16 64 10.36 6.34 130.08 84.74 (1.54) 53.94 (2.41)
 sw 4x32 128 19.85 10.81 165.99 173.62 (0.96) 97.33 (1.71)
 sw 4x48 192 29.34 15.29 173.22
 sw 4x64 256 38.83 19.76 208.89
 Ê Ê
 Ê Ê Ê Ê
 Ê Ê Ê
 Ê Ê
 Ê
 ‡
 ‡ ‡
 ‡ ‡
 ‡
 ‡ ‡ ‡
 ‡
 Ï Ï Ï Ï
 Ï Ï Ï
 Ï Ï
 Ï
 1x8 1x12 1x16 1x24 1x32 4x8 1x48 1x64 4x16 4x32 4x48 4x64
 0
 50
 100
 150
 200
 250
 Buil
 d
 Ti
 m
 e
 Hm
 in
 L Ï qFlow
 ‡ Partitions
 Ê Standard
 Fig. 3. Build times for Convey’s standard flow, the Partitions-based flow,
 and qFlow for twelve different variations of the Smith-Waterman Azido
 implementation.
 configuration is named with convention sw MxN, where M is
 the number of parallel systolic arrays and N is the length of
 each array. The median speedup over the standard, Convey-
 provided flow for the partitions-based approach 1.51, while
 that of qFlow was 2.76. The last two configurations tested,
 4 48 and 4 64, could not be placed into the dynamic region–
 the same dynamic region constraints were used for both flows.
 Note the jump in build time from configuration 4 16 to 4 32
 due to the increased utilization of the dynamic region.
 V. CONCLUSIONS AND FUTURE WORK
 This work presents bFlow, an FPGA-based big-data ac-
 celerator development environment significantly more usable
 by non-engineers than traditional design tools. Design pro-
 ductivity is increased by promoting object reuse, providing
 an high-level, abstracted graphical design environment, and
 reducing bitstream compile times. The framework was applied
 to the Convey HC-1 platform, and its usability and produc-
 tivity tested by participants of the NSF-funded bioinformatics
 Summer Institute at the Virginia Bioinformatics Institute. In a
 week, the participants successfully extended and accelerated
 a reference Smith-Waterman FPGA accelerator designed in
 Azido, achieving a theoretical throughput speedup of 32x and
 additional functionality. However, significant deficiencies in
 Azido’s graphical syntax, specifically the poor synchronization
 abstractions, proved to be major barriers to the participants’
 creation of complex logic.
 Future research efforts include the consideration of other
 design front-ends, such as LabVIEW [11], exploration of
 memory abstractions other than simple streaming objects, and
 gaining an understanding of the influence of domain-specificity
 on the usability and performance of accelerator design flows.
 ACKNOWLEDGMENT
 This work was supported in part by the I/UCRC Program
 of the National Science Foundation under Grant Nos. EEC-
 0642422 and IIP-1161022, and by NSF Award No. OCI-
 1124123, High Performance Computing in the Life/Medical
 Sciences.
 REFERENCES
 [1] L. Liu, Y. Li, S. Li, N. Hu, Y. He, R. Pong, D. Lin, L. Lu, and M. Law,
 “Comparison of next-generation sequencing systems,” BioMed Research
 International, vol. 2012, 2012.
 [2] L. J. McIver, J. W. Fondon III, M. A. Skinner, and H. R. Garner,
 “Evaluation of microsatellite variation in the 1000 genomes project
 pilot studies is indicative of the quality and utility of the raw
 data and alignments,” Genomics, vol. 97, no. 4, pp. 193–199, 4
 2011. [Online]. Available: http://www.sciencedirect.com/science/article/
 pii/S0888754311000024
 [3] M. S. Rosenberg, Sequence alignment : methods, models, concepts, and
 strategies. Berkeley: University of California Press, 2009.
 [4] Convey Computer, “The convey hc-1 computer architecture overview,”
 http://conveycomputer.com/Resources/ConveyArchitectureWhiteP.pdf.
 [5] Data I/O, “Azido beta,” http://azido.net/, 2012.
 [6] B. Nelson, M. Wirthlin, B. Hutchings, P. Athanas, and S. Bohner, “De-
 sign productivity for configurable computing,” in ERSA ’08: Proceed-
 ings of the International Conference on Engineering of Reconfigurable
 Systems and Algorithms, 2008, pp. 57–66.
 [7] K. Pereira, P. Athanas, H. Lin, and W. Feng, “Spectral method charac-
 terization on fpga and gpu accelerators,” Reconfigurable Computing and
 FPGAs (ReConFig), 2011 International Conference on, pp. 487–492,
 Nov. 30 2011-Dec. 2 2011.
 [8] W. chun Feng and K. W. Cameron, “The green500 list: Encouraging
 sustainable supercomputing,” Computer, vol. 40, no. 12, pp. 50–55,
 Dec. 2007.
 [9] J. D. Bakos, “High-performance heterogeneous computing with the
 convey hc-1,” Computing in Science & Engineering, vol. 12, no. 6,
 pp. 80–87, Nov.-Dec. 2010.
 [10] Convey Computer, “Personality development kit (pdk) for con-
 vey hybrid-core computers,” http://conveycomputer.com/Resources/
 PersonalityDevelopmentKit.pdf.
 [11] National Instruments, “Ni labview - improving the productivity of engi-
 neers and scientists - national instruments,” http://www.ni.com/labview/,
 2012.
 [12] MathWorks, Inc., “Simulink - simulation and model-based design,” http:
 //www.mathworks.com/products/simulink/, 2012.
 [13] T. Frangieh and P. Athanas, “A design assembly framework for fpga
 back-end acceleration,” in International Conference on Reconfigurable
 Computing and FPGAs (ReConFig 2012), to appear.
 [14] Xilinx, “Hierarchical design methodology guide,” http:
 //www.xilinx.com/support/documentation/sw manuals/xilinx13 1/
 Hierarchical Design Methodology Guide.pdf.
 [15] Virginia Bioinformatics Institute, “Partnership supercomputing pro-
 gram,” https://www.vbi.vt.edu/high performance computing/.
 [16] P. Zhang, “Implementation of the smith-waterman algorithm on a
 reconfigurable supercomputing platform,” Proceedings of the 1st in-
 ternational workshop on High-performance reconfigurable computing
 technology and applications held in conjunction with SC07 - HPRCTA
 ’07, p. 39, 2007.
 [17] T. F. Smith and M. S. Waterman, “Identification of common molecular
 subsequences.” J Mol Biol, vol. 147, no. 1, pp. 195–197, Mar 1981.
 264

Figure 1:  The Test Data Lifecycle
 Using Big Data and Predictive Machine Learning in 
Aerospace Test Environments
                                      Tom Armes                                                                                    Mark Refern                                        
           IntraStage, Inc.                                                                               UTC
                                  San Diego, USA                                                                             Birmingham, UK
                             tarmes@intrastage.com                                                             Mark.Redfern@utas.utc.com
 Abstract—It is estimated that in 2012 most mid-size 
companies in the USA generate the equivalent data of the US 
Library of Congress in 1 year. As a company, Wal-Mart
 creates the equivalent of 50 million filing cabinets worth of 
data every hour. While these numbers seem incredible, the 
trend for most companies is an increasing volume of data 
generation and storage. 
Test Data generated by Automatic Test Equipment (ATE) 
in R&D, manufacturing and Repair environments is no 
exception to this increased volume of data. The challenge of
 this enormous amount of Test Data is how to provide people 
with effective ways to make decisions from it. Data 
visualization through charts, graphs and reports has been,
 historically, one of the more effective ways to provide 
actionable intelligence because humans can readily make 
decisions based on patterns and comparisons. But as data 
volume goes up, even this method is reaching its limits. When 
one starts to combine large datasets like Manufacturing Test 
Data and Repair Data together, data visualization becomes 
problematic. More sophisticated algorithmic, machine 
learning and predictive approaches become critical. 
In this paper, we will explore the experiences of using 
predictive algorithms on "Big Data" from both Manufacturing 
Test and Repair Test environments in the complex mission 
critical aerospace industry. By effectively using datasets from 
different functional areas, we will be looking at applying SPC 
techniques to answer new questions about the correlation of 
Repair test data and manufacturing data with the end goal to 
predict number of returns in the future and minimize product 
escapes.
 I. INTRODUCTION 
The manufacturing of modern aerospace and medical 
device products generates enormous amounts of raw test data.  
Some of IntraStage’s customers produce more than 400 
Gigabytes of test data in one year. To put this amount of data 
in perspective, 25 of these types of companies can generate an 
amount of information equivalent to that which is currently 
stored in the US Library of Congress (10 TB).  And, today, 
there are over 10,000 aerospace manufacturing companies in 
the USA alone.  This is truly BIG Data, and begs the question: 
what does a company do with Big Data once it has been 
compiled?  How will engineers sift through the data to find the 
evidence to help them make critical business decisions?  
And, most importantly: how will Big Data be leveraged to 
provide a data model that predicts product failures before 
they’ve even happened?
 II. DEFINING BIG DATA IN TEST
 A. Volumes of data, Voluminous Sources
 Big Data in Test comes from all aspects from the products 
lifecycle.  This includes but is not limited to:
 ‚ Suppliers 
‚ R&D 
‚ NPI 
‚ Reliability 
‚ Manufacturing/Rework 
‚ RMA/Repair 
‚ Field 
There a number of interdependencies on these different 
types of data.  For example, an issue found in RMA (Returned 
Materials Authorization) can be reported back into R&D and 
Manufacturing.  This issue could be related to a bad 
component found years later in the field, necessitating long-
 term storage of data.  R&D and/or Manufacturing may need to 
go back to the supplier to gather test data on parts tested 10 
years earlier to fully understand a current issue.  As a product 
978-1-4673-5683-1/13/$31.00 ©2013 IEEE
Figure 2:  As products become more complex, data volume climbs
 Figure 1: Lotus 1-2-3 DOS Version
 evolves and moves through its life-cycle, it is vital for the 
stakeholders of the different relevant parties to see the life-
 cycle as a contiguous series of easily accessible and analyzed 
events rather than separate, distinct silos.
 B. The Profusion of Data Formats
 Data captured at any stage of the product life cycle can 
take on many different types and a virtually infinite number of 
formats.  This range of data includes but is not limited to:
 ‚ Manually Entered Data 
o Paper (including log books, paper 
travelers, certificates etc…) 
o Scanned hard-copy documents in PDF, 
TIF, JPG, etc. 
o MS Word, Excel files or other proprietary 
format 
‚ Automated Test Equipment (ATE) generated data 
o Parametric results  
? XML, Text, CSV, XLS, Binary or 
other custom formats 
? Databases (SQL Server, Oracle, 
Access, MySQL ….) 
o Waveform Data 
? 2D, 3D data waveform 
“snapshots” typically stored as 
images or CSV  
o Long term Logged data such as Reliability 
data 
? Typically stored in large text/CSV 
files based on cycle or period.   
Often contains multiple sensors 
with a time stamp associated 
with a number of measurements. 
o Binary attachments 
? High Resolution Images or Video 
? Sound or voice files 
? Signature images 
? Other support documents 
This list is by no means exhaustive but illustrates the many 
different types of data that can be and are commonly acquired 
at the many different sources or stages of a mission-critical 
product’s life.
 C. With Greater Product Complexity Comes Greater 
Demands for Data
 With the design of electronic components and products 
becoming more sophisticated and complex every day, there is 
a direct correlation on the amount of data that is acquired 
during test.  In addition, the cost for storage of data is getting 
steadily cheaper. With standard, reliable 2TB hard drives 
currently retailing for less than $100, engineers with vision 
and foresight feel encouraged to store as much data as 
possible, whether or not there is a current and clear plan to 
analyze and use the data.
 III. TYPES OF ANALYSIS: EARLY ANALYSIS TECHNIQUES
 Data visualization is relatively recent phenomenon.  Tables 
were first created during the first century for analyzing tabular 
data.  But it was not until the 18th and 19th centuries that the 
first graphs and charts (such as bar charts and histograms) were 
invented by William Playfair.  Now, in the information age, we 
have computers with amazing processing power combined 
with modern graphing capability to produce rich data 
visualization.
 A. The Evolution of Modern Analysis
 Before computers became commonplace appliances, 
desktop analysis was performed using hand written 
spreadsheets, with the calculations totaled using calculators.   
This technique dominated the way information was shared and 
analyzed.  Fortunately, the amount of data generated and 
available for analysis was, by and large, relatively limited.  As 
technology advanced, the ability (and need) to analyze data 
quickly evolved.  This was the genesis of software packages 
such as Lotus 1-2-3 (released in 1983) and MS Excel.  These 
products revolutionized the way data could be stored and 
analyzed.  
Soon, more advanced programs were published with the 
express purpose of analyzing data in many different ways.   
Many of these software packages took advantage of this data 
Figure 4:  Typical web-based reports
 now in digital format instead of relying on the information 
worker having to re-enter this data. Products such as Crystal 
Reports, Active Reports and so forth allowed users to connect 
to spreadsheets and do far more detailed and custom analysis 
of their data.  Information consumers all across organizations 
were able to use their desktop computers to access reports on 
their network to get to a ‘single version of the truth’.
 This evolution of reporting tools coincided with the 
storage of the data becoming far more efficiently stored as 
well as more readily accessible.  The modern database has 
been a major enabler of the reporting technologies.  Packages 
such as MS SQL Server, Oracle, DB2, Access and so forth are 
able to store far more normalized information than a 
spreadsheet could ever hope to.  Databases allowed companies 
to:
 ‚ Securely store their data in one repository 
‚ Normalize disparate datasets from different silos
 ‚ Store data more efficiently than flat files or 
spreadsheets
 ‚ Allow information workers access to this data based 
on permissions
 ‚ Drastically speed up data retrieval time
 These two technologies (Reporting and Database), 
combined with the rapid growth in internet usage and literacy, 
changed the way people interacted and consumed data.  Since 
the mid-1990s, web based reports have been adopted as the de 
facto medium of data dissemination for sophisticated 
organizations.  Information workers from C-Level executives, 
to Ops Managers, to Product Managers, to Test Managers and 
o have insisted that all of their corporate metrics be viewed 
using web based reporting.  This explosion in reports being 
viewed, modified, and shared from anywhere on any device 
started in the early 2000s and has continued at a feverish pace.
 This new paradigm of business analysis has brought an 
onslaught of new capabilities including:
 • Ad Hoc Web Based Reports
 • OLAP (Online Analytical Processing)
 • Dashboard technologies
 o KPI (Key Performance Indicators)
 o Self-service analysis
 • Automation features such as Subscriptions and Alerts
 These modern analysis systems, such as IntraStage, which 
can store Test Data across all stages of a products’ life cycle 
and allow information workers at any level to visualize the 
metrics that help them achieve their short- and long-term goals.  
To do this effectively software must take full advantage of 
these very recent technologies.
 IV. THE (PREDICTIVE) LINE ON THE HORIZON
 Now that technology has advanced enough to allow on-
 demand and automatic reporting on data generated mere 
milliseconds before, and alerts such as real-time SPC 
notifications are being distributed directly to critical users’ 
smart phones or email inboxes, one could assume that we must 
have solved nearly all of our problems in manufacturing, and 
the only time a product now gets returned is through an act of 
god.  This, unfortunately and obviously, is not quite the case.
 Although our tools and technologies are getting better and 
better for analyzing this data, Operations, Engineering and 
Management are and can count on being inundated with more 
and more data.   As we have seen, this data is streaming from 
multiple locations throughout the products’ lifecycle.  
Hundreds of product lines at many different stages in their 
lifecycle are being tested against thousands of different metrics 
at each station, results in millions and millions of attributes 
with billions and billions of data points.  
How can an organization keep up with all of this data?  
How do we need to adapt and reshape our current thinking?  
These modern database and reporting technologies do a 
fantastic job at showing us what is happening now, what 
happened in the past, and we can do some very limited 
forecasting into the future.  Humans can look at reports, 
dashboards in Excel or on the Web and, using our eyes, can 
look for outliers, seek root causes and correlate.
Figure 5
 Figure 6:  Variable Clustering
 However, what if we could model the future far more 
effectively?  What if we could use data from one process (like 
RMA) to determine the outcome of another process (like 
Manufacturing)?  
Let’s take, for example, an Engine Control Unit (ECU) on a 
modern jet engine.  Consider that this component undergoes an 
incredibly thorough amount of testing in Manufacturing.  The 
ECU is recognized as one of the most complicated and critical 
pieces of equipment on a modern airliner.  Like all man-made 
artifacts, failure can be expected during its lifecycle.  If and
 when it fails it will be sent to a RMA facility where it will 
typically undergo the same types of testing, such as ‘Final 
ATP,’ that was originally performed in manufacturing. 
Thus, for every part that fails, there will be at least two 
records of test-one record from manufacturing and one record 
from RMA.  Date and times of these tests can and often will be 
separated by months or years, particularly in high-cost, high-
 reliability aerospace products. The equipment used for test will 
also tend to differ in instrument composition and human 
operators.  In addition, some of the testing variables, limits, 
attributes, etc. may have changed over the years. However, 
most of the key measurements will still be present on both 
datasets.  And it is this commonality and correlation of key, 
critical measurements that allows the visionary engineer to 
derive the next level of analysis from the data-analysis that 
allows the organization to be proactive about potential quality 
issues instead of reacting to disasters.
 At IntraStage we are studying machine learning techniques 
to increase the accuracy of predicting failures in the field?  
What if we could say that certain serial numbers of ECUs that 
left the manufacturing facility after passing through Final ATP 
were five times more likely to fail in the field over the first 10 
years of life?  What if one of these ECUs was on Air Force 
One?
 Our process includes taking a grouping of 10 failures from 
RMA and automatically capturing all the test results for the 
RMA’d product.  Once we have identified and compiled the 
RMA data, our next step is to go back to the original 
Manufacturing data for those 10 failures.  
Now, we cluster the different attributes for our 
manufacturing data for those serial numbers that failed.  By 
looking for the biggest clusters (like Work Order or Station or 
Operator or Lot Number or Component Supplier ID) we can 
visually identify the most likely underlying causes of failure. 
While it’s easy for a human eye to look at one or a few 
attributes, the power of automated reporting and analytics 
allows data mining specialists to cluster off of multiple 
variables.
 Creating this predictive data model is dependent upon 
several variables.  One bedrock requirement is to have 
normalized, scrubbed data from serialized components, 
including a set of components that failed (of course, products 
that went through a formal RMA process would provide more 
robust data). In addition, if similar testing is done in both RMA 
and manufacturing (for instance, most RMA departments in 
Aerospace and Medical Devices have the same Automated 
Testing Equipment as the manufacturing line-and the products 
are run through that ATE as part of the RMA acceptance test), 
the data model is more robust. Having data from serialized 
products that failed in the field and having access to the 
original manufacturing data allows us to build a prototype 
predictive model. It’s also useful to have data that is as 
attribute-rich as possible.
 However, with more sets of failure data, the model will 
evolve and become more accurate.  Thus, it’s necessary to 
iterate the model multiple times. Each new iteration would 
include new RMA serial numbers and corresponding 
manufacturing data for those serial numbers.  As the data fed 
into the model grows, the data mining personnel will have to 
tweak the model to account for each RMA iteration’s failures. 
Start with control group of known failures. The goal then is to 
tune the model and you believe it to be fairly accurate for the 
failures you have, then you can take that model and run it 
against the other manufacturing data to see where the 
probabilities of failure lie.
 And once that data model has been tweaked and refined, 
significant value can be recognized for the organization. The 
analysts or engineers responsible for the predictive model can 
provide forecasts of future failures to business unit owners. By 
posing the question: ‘These certain products have not failed 
yet, but are at risk of failing’ and having a solid justification for 
that probability of failure, the engineer/analyst can affect real 
change and give executives the information necessary to 
prevent disastrous product failures in the field. 
The phrase ‘Big Data’ is relatively new, but the task of 
analyzing and utilizing ever-increasing amounts of information 
is a challenge that has been with the medical device and 
aerospace industries for years.  And dealing with that challenge 
will only grow in importance in the future.  Failures will 
always happen. Recalls will always happen. But developing an 
early-warning mechanism via machine learning that can 
ameliorate the risk of disaster will become of paramount 
importance.

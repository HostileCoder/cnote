Terabyte-scale image similarity search: experience and best practice
 Diana Moise
 INRIA Rennes, France
 Diana.Moise@inria.fr
 Denis Shestakov
 Aalto University, Finland
 INRIA Rennes, France
 Denis.Shestakov@aalto.fi
 Gylfi Gudmundsson
 INRIA Rennes, France
 Gylfi.Gudmundsson@inria.fr
 Laurent Amsaleg
 IRISA-CNRS, France
 Laurent.Amsaleg@irisa.fr
 Abstract—While the past decade has witnessed an unprece-
 dented growth of data generated and collected all over the
 world, existing data management approaches lack the ability to
 address the challenges of Big Data. One of the most promising
 tools for Big Data processing is the MapReduce paradigm.
 Although it has its limitations, the MapReduce programming
 model has laid the foundations for answering some of the Big
 Data challenges. In this paper, we focus on Hadoop, the open-
 source implementation of the MapReduce paradigm. Using as
 case-study a Hadoop-based application, i.e., image similarity
 search, we present our experiences with the Hadoop framework
 when processing terabytes of data. The scale of the data and the
 application workload allowed us to test the limits of Hadoop
 and the efficiency of the tools it provides. We present a wide
 collection of experiments and the practical lessons we have
 drawn from our experience with the Hadoop environment. Our
 findings can be shared as best practices and recommendations
 to the Big Data researchers and practioners.
 I. INTRODUCTION
 In the past decade, data has become ubiquitous. The
 scale at which data is generated has grown greater than
 ever before. According to the 2011 Digital Universe study
 of International Data Corporation (IDC), the information
 produced in 2011 all over the world, has gone beyond
 1.8 Zettabytes(ZB), marking an exponential growth by a
 factor of nine in just five years. This data explosion has
 enormously raised the bar for both industry and research:
 existing systems that acquire, store and process the data
 collections do not posses the hardware and software capaci-
 ties to accommodate the data. In this context, “Big Data”
 is becoming a hot term used to characterize the recent
 explosion of data. Big Data describes the unprecedented
 growth of data generated and collected from all kinds of data
 sources. This growth can be in the volume of data or in the
 speed of data moving in and out data-management systems.
 It can also be the growth in the number of different data
 formats in terms of structured or unstructured data.
 A massive amount of data produced all over the planet
 belongs to the multimedia type. Continuously growing mul-
 timedia collections come from social networks: in 2012,
 Facebook reported that 300 million images were uploaded
 every day, accumulating to 7 Petabytes of photo content
 every month [1]. These huge streams of images coming from
 all over the Web render multimedia content a significant
 instance of Big Data. Multimedia collections check most of
 the points in the “5V’s” list. Big Volume is clearly a feature.
 Usually, there are a limited number of standardized ways to
 represent multimedia content, so Variety is not much of an
 issue. There are scenarios in which Big Variability, Velocity
 and Value are important for multimedia processing. In a
 copyright detection scenario, obtaining the right result is as
 important as the time it takes to get it.
 Current state-of-the-art technologies developed for Big
 Data analytics can be employed to address the extremely
 large multimedia datasets now available on the Web. MapRe-
 duce [2] is probably the most efficient and popular tool that
 enables the processing of large amounts of information using
 commodity machines. Although it has its limitations, the
 MapReduce programming model has laid the foundations for
 higher-level data processing and is definitely a first step to
 answering many Big Data challenges. In this paper, we focus
 on the Hadoop [3] project, an open-source implementation of
 the MapReduce paradigm. By the means of a specific mul-
 timedia application, i.e., image similarity search, we present
 our experiences with Hadoop when processing terabytes of
 data. The scale of the data and the application workload have
 tested the limits of Hadoop and the efficiency of the tools it
 provides. Our findings can be shared as best practices and
 recommendations to the Big Data community.
 Our first contribution is adapting the Hadoop environment
 to multimedia retrieval application workloads, where default
 settings are no longer efficient. We list the parameters and
 values that delivered best performance for us. The second
 contribution is addressing the cluster heterogeneity problem
 by proposing a platform-aware configuration of the Hadoop
 environment. The third contribution is introducing the tools,
 provided by the Hadoop framework, useful for a large class
 of application workloads, in which a large-size auxiliary data
 structure is required for processing main dataset. Finally,
 we present a series of experiments conducted on very large
 image dataset (biggest among reported in the literature).
 In contrast to a number of Big Data reports describing
 experiments with either small or synthetic data, we report
 our findings and lessons obtained from a real-life use case,
 namely, finding images potentially violating copyrights in
 terabyte-sized collections.
 The paper is structured as follows. Section II briefly lists
 2013 IEEE International Conference on Big Data
 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 674
the challenges of Big Data analytics and presents current
 trends in this direction, with a focus on Hadoop. Section III
 gives an overview of MapReduce-based high-dimensional
 indexing scheme we designed and implemented for the
 Hadoop runtime environment. In Section IV we present
 our first results with running the application in the default
 Hadoop environment, while in Section V we describe further
 optimizations we achieved through various mechanisms.
 Section VI concludes.
 II. BIG-DATA PROCESSING
 The Big Data notion describes the unprecedented growth
 of data generated and collected from all kinds of data sources
 from various research and industry fields. Clearly inherent to
 the term is the volume of information involved in Big Data
 processing: its scale easily reaches terabytes and often goes
 beyond that. Apart from this big volume feature, there are
 several others that render Big Data processing a challenging
 field requiring complex and ground-breaking techniques. In
 this section, we focus on Big Data challenges and the current
 trends in Big Data analytics.
 A. Challenges
 Scale. The need for efficiently managing rapidly increas-
 ing volumes of data has always been there, and it was
 addressed for the most part. At the core of adjusting to
 the data size lies parallelism. Whether it is achieved on a
 single machine through multiple computing units, or across
 the machines of a distributed system, large data sets are
 nowadays efficiently handled. However, in the Big Data
 context, scalability is still an issue. Infrastructures such as
 clusters(notably HPC clusters), Grids, Clouds(Nimbus [4],
 Amazon Elastic Compute Cloud [5]), multi-core servers with
 large memory represent a major step forward, but still have
 their limitations when accommodating Big Data.
 Speed. In practice, the size of the data gives rise to
 real challenges. Performing actions such as data acquisition,
 storing, analyzing, visualizing, etc., becomes a problematic
 issue that needs dedicated approaches. It is obvious that
 the volume of data is the most immediate challenge for
 conventional data-management and processing frameworks.
 A fundamental change is required in the architecture of
 systems capable of delivering high throughput.
 Heterogeneity. Big data is also about the growth in the
 number of different data formats in terms of structured or
 unstructured data. Usually, a pre-processing step is required
 to bring the data to a form that is understood by the
 processing framework. Efficient representation, access, and
 analysis of semi-structured data require further work.
 B. Trends
 MapReduce [2] is probably the most efficient and popular
 tool that enables processing of large amounts of information
 using commodity machines. Although it has its limita-
 tions, this programming model has laid the foundations for
 higher-level data processing and is definitely a first step
 to answering some of the Big Data challenges. There are
 several MapReduce implementations (Apache’s Hadoop [6],
 Twister [7], Disco [8]) on top of which specialized higher-
 level frameworks were developed. Computer vision-wise, the
 Mahout [9] project uses Hadoop to provide an extensive list
 of machine learning algorithms. In the area of content-based
 image retrieval, there are systems that can handle several
 million images [10], [11], billions of descriptors [12], [13],
 or address web-scale problems [14], [15] and the Hadoop-
 based ImageTerrier platform [16].
 C. Hadoop
 The MapReduce programming model has been imple-
 mented by the open-source community through the Hadoop
 project [3]. Maintained by the Apache Foundation and
 supported by Yahoo!, the Hadoop project has rapidly gained
 popularity in the area of distributed data-intensive com-
 puting. The core of the Hadoop project consists of the
 MapReduce implementation and the Hadoop Distributed File
 System (HDFS).
 The Hadoop MapReduce framework [6] was designed
 following Google’s architectural model and has become the
 reference MapReduce implementation. The architecture is
 tailored in a master-slave manner, consisting of a single
 master jobtracker and multiple slave tasktrackers. The job-
 tracker’s main role is to act as the task scheduler of the
 system, by assigning work to the tasktrackers. Each task-
 tracker disposes of a number of available slots for running
 tasks. Every active map or reduce task takes up one slot, thus
 a tasktracker usually executes several tasks simultaneously.
 When dispatching “map” tasks to tasktrackers, the jobtracker
 strives at keeping the computation as close to the data as
 possible. This technique is enabled by the data-layout infor-
 mation previously acquired by the jobtracker. If the work
 cannot be hosted on the actual node where the data resides,
 priority is given to nodes closer to the data (belonging to the
 same network rack). The jobtracker first schedules “map”
 tasks, as the reducers must wait for the “map” execution to
 generate the intermediate data. Apart from data splitting and
 scheduling responsibilities, the jobtracker is also in charge
 of monitoring tasks and dealing with failures.
 The Hadoop Distributed File System(HDFS) [17] was
 built with the purpose of providing storage for huge files with
 streaming data access patterns, while running on clusters
 of commodity hardware. HDFS implements design concepts
 commonly used by distributed file systems: data is organized
 into files and directories, a file is split into fixed-size blocks
 that are distributed across the cluster nodes. The blocks
 are called chunks and are usually of 64 MB in size (this
 parameter specifying the chunk size is configurable). The
 architecture of HDFS consists of several datanodes and a
 675
single namenode. The nodes in a HDFS cluster that store
 the chunks are called datanodes. A centralized namenode
 is responsible for keeping the file metadata and the chunk
 location. HDFS handles failures through chunk-level repli-
 cation (3 replicas are kept by default). When distributing
 the replicas to the datanodes, HDFS employs a rack-aware
 policy: the first copy is always written locally, the second
 copy is stored on a datanode in the same rack as the first
 replica, and the third copy is shipped to a datanode belonging
 to a different rack (randomly chosen). The namenode decides
 and maintains the list of datanodes that store the replicas of
 each chunk.
 III. APPLICATION WORKLOAD (DISTRIBUTED INDEXING
 + SEARCHING)
 Our workload is based on the scalable, MapReduce ori-
 ented, high-dimensional indexing scheme initially presented
 in [18]. The indexing algorithm uses at its core a hier-
 archical unstructured quantization scheme, quite similar to
 the approach proposed by Nister and Stewenius [19]. In a
 nutshell, high-dimensional descriptors are clustered around
 randomly picked representative points that are hierarchically
 organized.
 A. Dataset preparation
 Our datasets, the collections of high-dimensional descrip-
 tors, were converted to Hadoop Sequence Files. Each raw
 descriptor becomes a sequence file record. A record in
 this file has a key (an image identifier), and a value (a
 high-dimensional descriptor). Hadoop scatters across many
 machines the blocks of the sequence file in its distributed file
 system, HDFS [17]. Thus, the entire descriptor collection is
 stored as a set of HDFS sequence files; this set of sequence
 files also represents the input data to the applications de-
 scribed further.
 B. Distributed index creation
 To be able to index a huge collection of descriptors
 using MapReduce, we first perform a phase of building
 auxiliary data required by the distributed process. This
 preliminary phase, which we execute on a single machine
 and usually only once, consists of building the index tree, a
 hierarchically-organized collection of cluster representatives.
 The tree is created by randomly picking from the descriptor
 collection C points, i.e., the representatives of the visual vo-
 cabulary dictionary eventually created. These representatives
 are organized in a hierarchy of L levels for efficiency. The
 index tree is serialized to a file on disk and sent to all the
 nodes involved in the distributed index creation.
 Creating an index from a very large collection of descrip-
 tors is a very costly process. Distributing it with MapReduce
 involves designing the map and reduce phase of the indexing
 application. Each map tasks receives at startup time (i) a
 block of input data (a chunk of a Hadoop sequence file)
 and (ii) the file containing the index tree. The mapper further
 assigns the descriptors in its data block to the correct rep-
 resentative discovered by traversing the tree. The resulting
 assignments are sent to reduce tasks that write to disks high-
 dimensional descriptors grouped by cluster identifier. This
 eventually creates one or more indexed files which contain
 clustered high-dimensional descriptors. The number of such
 files is defined by the number of reduce tasks.
 The indexing application is highly data-intensive, both in
 the map phase when reading the descriptors to assign them
 to cluster representatives, and also in the reduce phase when
 writing the clusters to disk. The application also generates
 a substantial amount of output data, slightly larger than
 the input set. The map stage is CPU-intensive as well,
 by performing distance calculations for every input record.
 The shuffling step intensively utilizes the network, as the
 intermediate data is sent from mappers to reducers. Finally,
 distributed indexing is a long-running application on account
 of its data- and CPU-intensive nature.
 C. Distributed search
 Search phase is geared toward throughput as it processes
 very efficiently large batches of queries, typically 104–107
 query descriptors. The search also requires a preliminary
 step, the creation of a lookup table, where all query de-
 scriptors of a batch are grouped according to their closest
 representative discovered from traversing the index tree. This
 lookup table is written to the local disk of all the nodes that
 will perform search. The MapReduce implementation of the
 search process is detailed further. Each map task receives
 (i) a block of data from one of the previously created index
 files, (ii) the file containing the lookup table. The mapper
 processes only the descriptors in its assigned chunk of data,
 that are relevant for the queries. Distance calculations are
 computed for those descriptors and queries assigned to the
 same cluster identifier. k-nn results are eventually emitted
 by the mappers, then aggregated by reducers to create the
 final result for the query batch.
 Our implementation of the distributed search [20] is data-
 intensive in the map phase where indexed files are read
 from disk. Unlike the indexing application, the distributed
 search is short-running since only a part of the indexed
 descriptors are compared to the queries. This also renders the
 search application unbalanced, the workload of each mapper
 depends on whether the chunk they process includes clusters
 required by the queries. Another aspect that differs in this
 application as opposed to the index creation, refers to the
 data shuffled by mappers and generated by reducers, which
 is insignificant in size, compared to the input data.
 Note that both applications require auxiliary data of
 substantial amount: the index tree/the lookup table have
 to be loaded by each mapper for every chunk of data.
 While our experiences pertain to these two applications, their
 676
Cluster #Nodes #CPU@Freq #Cores RAM Local/CPU Disk
 Cl1 64 2 Intel@2.50GHz 4 32GB 138GB
 Cl2 25 2 Intel@2.93GHz 4 24GB 433GB
 Cl3 40 2 AMD@1.70GHz 12 48GB 232GB
 Table I
 CLUSTER CONFIGURATIONS.
 characteristics are general enough so that our findings can
 be easily applied to big data processing.
 D. Dataset
 In our experiments we used the dataset provided by one
 of our partners in the Quaero project, http://quaero.org/.
 The dataset consists of around 30 billion SIFT descriptors
 (approximately four terabytes of data) extracted from 100
 million images harvested on the Web. Besides using this
 entire collection, we also used a subset of it containing
 20 million images, i.e., 7.8 billion descriptors (about one
 terabyte). To the best of our knowledge, these datasets are
 among the largest collections presented to the multimedia
 community. Details on evaluation of indexing quality and
 the quality results themselves can be found in [18].
 IV. HADOOP IN PRACTICE
 Although the Hadoop framework is now widely adopted,
 processing terabyte-sized datasets require to reconsider
 many of its design, implementation and execution aspects.
 The Hadoop has been shown to scale and efficiently process
 terabytes of data on thousands of machines: e.g., in Yahoo!,
 Facebook, etc., but a few research works actually reported
 Hadoop at such large scales, data- and platform-wise. In fact,
 one can hardly find real-life recommendations on Hadoop
 parameter configuration or suggestions on tools to be used
 for handling such amounts of data. The implementation and
 the experiments presented in this paper are real applications
 conducted on real datasets; in contrast to the usual Hadoop
 benchmarks, real applications raise complex problems and
 challenges. Luckily, the Hadoop framework contains a few
 tools to solve some of these problems. However, it is far
 from being obvious how and when to use them.
 We present here our experience with big data processing
 using Hadoop in a grid environment. Based on the challenges
 and problems we were faced with, we can recommend
 best practices (configuration tunning, tools) for dealing with
 them.
 The applications described in Section III were integrated
 into a framework that we developed to be able to: configure
 the compute environment, deploy a Hadoop cluster, transfer
 input data to HDFS from external storage, run indexing and
 searching jobs and optionally, retrieve the results and analyze
 logs.
 A. Experimental platform
 To reach the point where we could process terabytes of
 data on a Hadoop cluster instance, we have performed a large
 series of experiments on the Grid’5000 [21] platform. The
 Grid’5000 project is a widely-distributed infrastructure de-
 voted to providing an experimental platform for the research
 community. The platform is spread over 9 geographical sites
 located on French territory. We conducted our experiments
 on machines belonging to 3 clusters of the Rennes site
 of Grid’5000. The clusters configurations are shown in
 Table I. A Hadoop instance is deployed on these nodes as
 follows: the namenode and jobtracker are each launched on a
 dedicated node, while the datanodes and tasktrackers are co-
 deployed on the remaining nodes; we also allocate a separate
 machine to act as Hadoop client for job submissions.
 B. Experiments with the default Hadoop settings
 In a first round of experiments, we performed indexing
 with the default Hadoop configuration parameters. This gave
 us a baseline for assessing any improvements we could
 further obtain through various means.
 What we refer to as the default Hadoop deployment is
 the basic configuration shipped with Hadoop, to which we
 applied some adjustments, based on the guidelines provided
 for dealing with big data. The first obvious adjustment
 was to set the map and reduce slots to accommodate the
 platform capacities: 8 slots for mapping and 2 slots for
 reducing per node. We set these values based on our clusters
 computing power and following the recommendations in
 Hadoop’s tutorial. The second parameter we modified is the
 HDFS chunk size; the default value of 64 MB is not suitable
 for big data, as it entails larger pressure on the namenode
 (because of filesystem metadata) and a large number of map
 tasks be executed. After some experiments with varying
 chunk sizes, we discovered the right chunk size for our
 workload to be 512 MB. At the level of HDFS, we kept
 the default replication factor of 3 for the input data. The
 replication factor is a crucial parameter for performance, not
 only because it enables fault tolerance, but also because it
 favors local execution of map tasks, minimizing the amount
 of data read remotely. The output data however, was written
 only once, to avoid the substantial overhead of writing
 replicas remotely. Given the size of our dataset, this cost
 is considerably high.
 With these three basic configurations, indexing 1 TB of
 data on 106 nodes took 95 minutes, while 4 TB on 100 nodes
 were indexed in 10 hours in the same environment (Table II).
 There are several reasons to explain the significant difference
 in runtime between indexing 1 TB and 4 TB. In addition to
 the obviously larger number of tasks to be executed, there
 is also a larger computational work done by each map task.
 The tree of representatives built for indexing 4 TB is a lot
 larger than the one covering 1 TB: 1.8 GB as opposed to
 461 MB. The size of the index tree has two consequences.
 677
Nodes Default Hadoop Tuned Hadoop
 1 TB 50 202min 174.7min106 95min 75min
 4 TB 100 10h1min 8h27min
 Table II
 INDEXING TIME.
 Parameter Default value Tuned value
 #Map slots/tasktracker 2 8
 #Reduce slots/tasktracker 2 8
 Input data replication 3 3
 Output data replication 3 1
 Chunk size 64 MB 512 MB
 JVM reuse off on
 Map output compression off on
 Reduce parallel copies 5 50
 Sort factor 10 100
 Sort buffer 100 MB 200 MB
 Datanode max. receivers 256 4096
 Namenode handlers 10 40
 Table III
 HADOOP CONFIGURATION TUNING.
 First, each mapper has to perform more distance calculations
 to assign each point to the closest cluster. Second, the index
 tree takes up more RAM in each map slot, thus we had to
 reduce the map slots to 4 per node.
 V. LARGE-SCALE HADOOP
 A. Adjusting to the data size
 Understanding the application workload and accurately
 assessing the scale at which the application runs are key
 steps to achieving optimal performance. We present here a
 brief analysis of our workload, which helped us pinpoint the
 issues to further improve.
 Indexing 4 TB of data with Hadoop involves: storing
 16 TB of data (input replicated 3 times plus output writ-
 ten once), shuffling 4 TB of intermediate data, executing
 approximately 8000 map tasks. Platform-wise, we dispose
 of 100 nodes, i.e., 800 map slots (400 for 4 TB) and
 200 reduce slots, organized in 3 clusters connected to the
 same network switch. Based on these observations, several
 optimizations are in order. Since a substantial amount of data
 is shuffled from the mappers to the reducers, compressing
 the map output can be beneficial, as it also reduces network
 usage. Rack-awareness is not useful in this case, since the
 nodes connect to a single switch; however, if it is the
 case, configuring rack-awareness according to the network
 topology is necessary as it allows Hadoop to optimally place
 tasks on nodes for execution.
 Table III sums up these optimizations by listing the
 Hadoop parameters and the values we used in the indexing
 process. As mentioned above, enabling map output compres-
 sion reduced the amount of shuffled data by 30% (from 1 TB
 to 740 GB). Other low-level parameters refer to the shuffling
 phase, when the data is sorted on the mapper’s side, copied
 to the reducer and then merged and sorted on the reducer
 node. The last two lines of Table III show options configured
 on the HDFS nodes: the maximum number of chunk requests
 (read/write) each namenode can simultaneously handle, and
 the number of connections the namenode can serve at a time.
 Table II shows a comparison of the indexing time using
 the default Hadoop configuration as well as the optimized
 one. On the same testbed and with the same slots for
 mapping and reducing, the customized Hadoop parameters
 deliver a much faster indexing time. The results in Table II
 show that setting the right parameters for the Hadoop frame-
 work can drastically improve Hadoop’s performance. Tuning
 the parameters of the Hadoop framework is a rather complex
 task that requires good knowledge and understanding of
 both the workload and the framework itself. Given the
 performance gains obtained, tuning the parameters listed in
 Table III is worth-while; nevertheless, the values delivering
 best performance are highly dependent on the workload, and
 thus, tweaking them in order to discover the best values is
 advisable.
 B. Hadoop on heterogeneous clusters
 Large-scale processing of big data is most commonly
 performed in a heterogeneous environment. Resource het-
 erogeneity can impact the performance since most data-
 processing systems are designed for homogeneous plat-
 forms [22]. This is also the case for the Hadoop framework:
 the configuration settings can be specified only globally, not
 in a per-cluster manner. The consequence is that the Hadoop
 deployment is configured according to the least equipped
 node, at the expense of wasting resources on more out-fitted
 nodes. Parameters such as the number of map/reduce slots
 per tasktracker and the amount of RAM memory allocated
 to each task have to be set to the lowest available values.
 The advantage of having a single configuration for the
 whole platform is that it is easy to manage; the considerable
 downside however, is that the platform is under-utilized.
 To fully exploit the computing power of our three clus-
 ter environments, we developed a simple mechanism that
 configures tasktrackers parameters on each cluster based on
 the cluster’s capabilities. This mechanism consists of three
 steps:
 • deploy Hadoop on all the nodes using the global con-
 figuration settings, i.e., adjusting to the lowest available
 values;
 • create cluster-specific configuration files;
 • restart tasktrackers with the new configuration files.
 Specifically, considering the clusters description in Ta-
 ble I, we deployed Hadoop with 8 map slots and 2 reduce
 slots per node, and then restarted tasktracker processes on
 Cl3 to use 20 map and 4 reduce slots. With this platform-
 aware Hadoop deployment, indexing 1 TB of data on
 107 nodes finished in 65.4 minutes, 30 minutes faster than
 the default Hadoop run and 10 minutes faster than the tuned
 Hadoop run (see Table II).
 678
 0
  500
  1000
  1500
  2000
  2500
  3000
  3500
  0  500  1000  1500  2000  2500
 time(s
 )
 task id
 map
 (a) Default Hadoop
  0
  500
  1000
  1500
  2000
  2500
  3000
  3500
  0  500  1000  1500  2000  2500
 time(s
 )
 task id
 map
 (b) Platform-aware Hadoop
 Figure 1. Time progress of map tasks when indexing 1 TB on 107 nodes.
 Figure 1 shows the map tasks in time when indexing
 1 TB with the default Hadoop deployment (Figure 1(a))
 and when configuring Hadoop per-cluster (Figure 1(b)). The
 figures basically depict how the map phase progresses in
 time. There are about 2050 map tasks to be launched on the
 100 node platform. With the default Hadoop run, we could
 set 8 map slots per tasktracker, giving us a total of 856 slots
 for mapping at a time. As Figure 1(a) shows, the first wave
 on map tasks is launched in parallel. Since mappers run on
 nodes with different performance, there is some variance in
 map duration times which eventually leads to the degrada-
 tion of mapper waves. Also, the spikes show that some map
 tasks are significantly slower than the rest of the wave. This
 is an effect of using the default Hadoop settings that do
 not accommodate the large amount of data being processed.
 Some tasks have to spill their output to disk and wait for
 network to send the data to the reducers. Adjusting low-level
 Hadoop parameters (increasing the size of memory buffers,
 compressing the map output, etc.) can prevent this behavior,
 as can be seen in Figure 1(b). In addition, configuring each
 cluster according to its true capabilities provides 1192 slots
 for mapping in parallel, and finally, faster execution time.
 C. Dealing with large-size auxiliary data
 As discussed in Section III, both indexing and search-
 ing applications use auxiliary information to process the
 input data. In the case of the distributed index creation,
 the mappers load the tree of cluster representatives and
 traverse it for every input point to assign it to the closest
 cluster. Whereas the search application is concerned, the
 auxiliary data consists of the batch of query descriptors to
 be processed. In both cases, the additional information can
 amount to significant sizes: the index tree corresponding to
 1 TB of data is 461 MB in size, while for 4 TB of data,
 1.8 GB of auxiliary data are required. We focus here on
 tools we found useful in managing auxiliary data.
 1) Multi-threaded Mappers: Loading the same piece of
 information in every mapper is a requirement commonly
 encountered in MapReduce applications. The way Hadoop
 addresses this issue is to allow users to adjust the RAM
 dedicated to each mapper so that it accommodates the size
 of the resource to load. However, by doing so, one has
 to take into account the machine’s capacity, i.e., the total
 available memory: having more RAM per mapper means
 having less map slots per node, less map operations executed
 simultaneously, and finally, longer execution time. A way to
 avoid wasting CPU cores while still allocating more RAM,
 is to make use of multi-threaded mappers. Implemented in
 Hadoop, the multi-threaded mapper spans a configurable
 number of threads, each thread executing map tasks as a nor-
 mal mapper would. The threads of a mapper share the same
 RAM memory, thus auxiliary data can be loaded only once
 and read simultaneously by multiple threads during the map
 phase. The map computations are executed in parallel, each
 thread working on different (key, value) pairs. By setting the
 right number of threads per mapper, the CPU capacity can be
 fully utilized. The downside of this mechanism is that multi-
 threaded mappers use synchronization when both reading the
 input data and then when writing the map output: the threads
 synchronize when reading each (key, value) pair from the
 input chunk of data allocated to the mapper; running the map
 function on that pair is done in parallel by all the threads;
 however, writing the output of each map operation to the
 same map output buffer is again synchronized.
 The optimal configuration of map slots and number of
 threads per each slot strongly depends on the application
 workload. Obviously, the combination of map slots ?
 threads per slot has to fully utilize the machine’s processing
 power. The synchronization steps in the multi-threaded map-
 per implementation incur an inherent overhead when using
 679
Mapper slots Threads per mapper Time(min)
 1 8 87
 2 4 68
 4 2 41
 8 1 42
 Table IV
 SMALL-SCALE INDEXING WITH VARYING NUMBER OF MAP THREADS.
 this mechanism over the normal Hadoop mapper. Thus, the
 performance gain of using multi-threaded mappers is only
 partially dependent on the workload and the configuration.
 Our indexing application provides a suitable case for using
 multi-threaded mappers. For this approach to be beneficial
 performance-wise, we have to discover the configuration that
 allows us to overcome the synchronization overhead. In the
 normal mapper case, the index tree is loaded by each map
 task, while in the multi-threaded mapper, the tree is loaded
 once and then shared by all the threads. In both cases, there
 is an overhead that cannot be avoided: index tree loading
 vs. synchronization. To discover the optimal number of map
 slots ? threads per slot, we initially performed a small-scale
 experiment using 13 nodes belonging to the same cluster of
 the Rennes site. Disposing of 8 cores per node, Hadoop
 tasktrackers were configured with various combinations of
 map slots and threads. In each setup, we performed indexing
 on 45 GB of data using the multi-threaded mapper imple-
 mentation; the index in this experiment was 460 MB in size.
 The results are displayed in Table IV. In addition to
 these multi-threaded mapper runs, we also ran the indexing
 with the default mapper implementation on the same dataset
 and on the same platform; in this case, the indexing took
 40 minutes to complete. It is fair to assume this is the optimal
 execution time in the given setup, furthermore, we can use
 it as baseline to compare against. The slowest run is the
 configuration in which 8 threads are spawned within the
 same map slot; this is a clear effect of the reading/writing
 synchronization implemented in Hadoop’s multi-threaded
 mapper. As we increase the number of slots and reduce the
 number of threads per slot, the indexing time decreases.
 The best configuration that also achieves a running time
 comparable to the baseline is 4 map slots ? 2 threads per
 slot.
 The benefits of using multi-threaded mappers are revealed
 when processing the big dataset of 4 TB. When using the
 default mapper, the size of the index tree (1.8 GB) compelled
 us to increase the RAM size to 6 GB and limit the map slots
 per tasktracker to 4, under-utilizing the node’s CPU power.
 With multi-threaded mappers, we were able to take full
 advantage of all the CPU cores, while sharing the memory
 between the threads of a mapper. Based on the results of
 the small-scale experiment, we configured the tasktrackers
 with 4 mapper slots, each mapper running 2 threads. This
 setting delivered the best execution time for the distributed
 indexing: 6 hours and 8 minutes.
 Query batch(#images) Time(s)
 Complete loading 3,000 38212,000 521
 25,000 755
 MapFiles 3,000 34312,000 607
 25,000 932
 Table V
 SEARCH TIMES OVER 1 TB ON 100 NODES.
 2) MapFiles: In some applications, the mappers require
 auxiliary data related only to the data chunks they process.
 In this case, we can avoid loading all the auxiliary data
 by loading only the necessary part. This partial loading of
 auxiliary information can be achieved by the means of Map-
 Files. Hadoop’s MapFiles can be regarded as common map
 structures, sets of (key, value) associations that persistently
 store the data in binary format, as HDFS SequenceFiles.
 In addition to the data file that contains the (key, value)
 mappings in order, a MapFile also has an index file storing
 keys and for each key, its offset in the data file. The index
 file can be configured to store only a fraction of the key set.
 MapFiles are an efficient tool to load auxiliary data and
 reduce RAM usage, since only the index file is loaded into
 memory, while the data file is read from disk whenever
 necessary. Accessing a MapFile, i.e., getting the value as-
 sociated to a key, consists of 1) searching the key in the
 in-memory index and getting the offset of the key on disk,
 and 2) seeking in the data file to the key’s offset and reading
 the corresponding value.
 We used MapFiles to store the batches of queries in the
 searching application. We performed searches on 1 TB of
 data in two scenarios: when the batch of queries is entirely
 loaded from disk into memory by each mapper, and when
 the batch is stored as a MapFile and only its index is kept
 in the main memory. Using a MapFile key interval of 1, we
 searched over 1 TB on 100 nodes, using three batches of
 different sizes. The search times shown in Table V prove that
 MapFiles are an efficient tool for managing auxiliary data.
 Apart from delivering comparable performance, MapFiles
 also reduce RAM usage. For instance, instead of loading
 approximately 500 MB of queries, each mapper keeps in
 memory a 4.6 MB index of the keys. This can make a
 significant difference when the query batch is very large. If
 the query batch is of the order of gigabytes in size, loading
 it into memory by each mapper is inefficient and sometimes
 unfeasible. Even if the total available RAM memory is able
 to accommodate the size of the batch, this would impact the
 number of mappers running simultaneously. Although using
 MapFiles takes longer to process queries, it still allows all
 the mappers to run at the same time; thus the overhead of
 reading from disk is eventually outweighed.
 D. Observations
 Among other tools that Hadoop provides to facilitate data
 distribution, we found particularly useful the distributed
 680
cache, which transparently transfers auxiliary data to the
 local disks of all the nodes.
 Overall, Hadoop is helpful for achieving scalability. How-
 ever, the size of the data puts a significant load on the
 framework. We observed that tasks often fail because of
 insufficient disk space, communication timeouts, etc. These
 failures can be avoided or at least minimized through pa-
 rameter configuration. On the other hand, we encountered a
 few Hadoop errors that we were hard to localize and fix. At
 this point, the framework’s fault tolerance mechanisms are
 crucial for completing the job, which Hadoop manages to
 achieve, on average.
 VI. CONCLUSION
 This paper has presented our experience with processing
 terabytes of multimedia data through the Hadoop MapRe-
 duce framework. We described a wide collection of exper-
 iments and the practical lessons we have drawn from our
 experience with the Hadoop environment. We have shown
 the performance benefits of understanding the application
 workload and of tuning Hadoop configuration parameters
 accordingly. In addition, we addressed issues commonly
 found in Big Data processing: resource heterogeneity and
 management of large auxiliary data. The tools we employed
 are existing tools implemented in Hadoop, that despite the
 lack of documentation, prove to be very efficient once one
 understands how and when to use them. Our experiments
 have shown the benefits of using multi-threaded mappers
 and MapFiles in reducing memory usage while maintaining
 the same degree of parallelism.
 Although the tools provided by the Hadoop to accom-
 modate great data scales proved to be essential for the
 performance of our applications, there is still a room for
 improvement. For example, the multi-threaded mappers syn-
 chronize when reading and writing a single key/value pair,
 negatively affecting the performance. One way to minimize
 the number of synchronization steps is to have each thread
 use a separate buffer for reading and writing. While this
 mechanism is easy to implement for writing, for reading it
 is less trivial, since it requires modifications in the Hadoop’s
 input data splitting process. Another interesting direction for
 future work is to implement our applications using other
 distributed frameworks and compare them to the Hadoop’s
 performance. Since our experimental platform can accom-
 modate our dataset into main memory, we plan to look into
 in-memory distributed frameworks, such as Main Memory
 Map Reduce (M3R) [23], a Hadoop variant that maintains
 the data in the cluster’s main memory.
 The implementation and the experiments presented in this
 paper are real applications conducted on real datasets. The
 lessons drawn from our work can be shared as best practices
 and recommendations in the unceasing task of rising to the
 Big Data challenge.
 ACKNOWLEDGEMENTS
 This work was partly achieved as part of the Quaero
 Project, funded by OSEO, French State agency for inno-
 vation (see http://www.quaero.org/). The experiments pre-
 sented in this paper were carried out using the Grid’5000/
 ALADDIN-G5K experimental testbed (see http://www.
 grid5000.fr/).
 REFERENCES
 [1] http://gigaom.com/2012/10/17/
 facebook-has-220-billion-of-your-photos-to-put-on-ice/.
 [2] J. Dean and S. Ghemawat, “MapReduce: simplified data
 processing on large clusters,” Commun. ACM, vol. 51, no. 1,
 pp. 107–113, 2008.
 [3] “The Apache Hadoop Project,” http://www.hadoop.org.
 [4] K. Keahey and T. Freeman, “Science Clouds: Early expe-
 riences in Cloud computing for scientific applications,” in
 Cloud Computing and Its Applications CCA, 2008.
 [5] D. Robinson, Amazon Web Services Made Simple: Learn how
 Amazon EC2, S3, SimpleDB and SQS Web Services enables
 you to reach business goals faster. Emereo Publishing, 2008.
 [6] “The Hadoop MapReduce Framework,” http://hadoop.apache.
 org/mapreduce/.
 [7] “Twister - Iterative MapReduce,” http://www.
 iterativeMapReduce.org/.
 [8] P. Mundkur, V. Tuulos, and J. Flatow, “Disco: a computing
 platform for large-scale data analytics,” in Proc. of the 10th
 ACM SIGPLAN workshop on Erlang, 2011, pp. 84–89.
 [9] S. Owen, R. Anil, T. Dunning, and E. Friedman, Mahout in
 Action. Manning Publications, 2011.
 [10] H. Lejsek, F. H. Amundsson, B. T. Jo´nsson, and L. Amsaleg,
 “NV-Tree: An efficient disk-based index for approximate
 search in very large high-dimensional collections,” IEEE
 Trans. Pattern Analysis and Machine Intelligence PAMI,
 vol. 31, pp. 869–883, 2009.
 [11] H. Je´gou, F. Perronnin, M. Douze, J. Sa´nchez, P. Pe´rez,
 and C. Schmid, “Aggregating local image descriptors into
 compact codes,” IEEE Trans. Pattern Analysis and Machine
 Intelligence PAMI, vol. 34, no. 9, pp. 1704–1716, 2012.
 [12] H. Lejsek, B. T. Jo´nsson, and L. Amsaleg, “NV-Tree: nearest
 neighbors at the billion scale,” in ACM Proc. International
 Conference on Multimedia Retrieval ICMR, 2011, pp. 54:1–
 54:8.
 [13] H. Jegou, R. Tavenard, M. Douze, and L. Amsaleg, “Search-
 ing in one billion vectors: Re-rank with source coding,” in
 IEEE International Conference on Acoustics, Speech, and
 Signal Processing, ICASSP, 2011, pp. 861–864.
 [14] M. Douze, H. Je´gou, H. Sandhawalia, L. Amsaleg, and
 C. Schmid, “Evaluation of GIST descriptors for web-scale
 image search,” in ACM Proc. International Conference on
 Image and Video Retrieval CIVR, 2009, pp. 19:1–19:8.
 681
[15] M. Batko, F. Falchi, C. Lucchese, D. Novak, R. Perego, F. Ra-
 bitti, J. Sedmidubsky´, and P. Zezula, “Building a web-scale
 image similarity search system,” Multimedia Tools Appl.,
 vol. 47, no. 3, pp. 599–629, 2010.
 [16] J. S. Hare, S. Samangooei, D. P. Dupplaw, and P. H. Lewis,
 “ImageTerrier: an extensible platform for scalable high-
 performance image retrieval,” in ACM Proc. International
 Conference on Multimedia Retrieval ICMR, 2012, pp. 40:1–
 40:8.
 [17] K. Shvachko, H. Kuang, S. Radia, and R. Chansler, “The
 Hadoop Distributed File System,” in IEEE Proc. Mass Stor-
 age Systems and Technologies MSST, 2010, pp. 1–10.
 [18] D. Moise, D. Shestakov, G. Gudmundsson, and L. Amsaleg,
 “Indexing and Searching 100M Images with Map-Reduce,”
 in International Conference on Multimedia Retrieval ICMR,
 2013, pp. 17–24.
 [19] D. Nister and H. Stewenius, “Scalable recognition with a
 vocabulary tree,” in IEEE Proc. Computer Society Conference
 on Computer Vision and Pattern Recognition CVPR, 2006, pp.
 2161–2168.
 [20] D. Shestakov, D. Moise, G. Gudmundsson, and L. Amsa-
 leg, “Scalable high-dimensional indexing with Hadoop,” in
 Content-Based Multimedia Indexing CBMI, 2013, pp. 207–
 212.
 [21] Y. Je´gou, S. Lante´ri, J. Leduc, and all, “Grid’5000: a large
 scale and highly reconfigurable experimental Grid testbed.”
 Intl. Journal of HPC Applications, vol. 20, no. 4, pp. 481–
 494, 2006.
 [22] F. Ahmad, S. T. Chakradhar, A. Raghunathan, and T. N. Vi-
 jaykumar, “Tarazu: optimizing mapreduce on heterogeneous
 clusters,” SIGARCH Comput. Archit. News, vol. 40, no. 1, pp.
 61–74, 2012.
 [23] A. Shinnar, D. Cunningham, V. Saraswat, and B. Herta,
 “M3R: increased performance for in-memory Hadoop jobs,”
 Proc. VLDB Endow., vol. 5, no. 12, pp. 1736–1747, 2012.
 682

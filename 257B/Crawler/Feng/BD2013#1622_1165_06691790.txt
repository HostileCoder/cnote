Risk Adjustment of Patient Expenditures:
 A Big Data Analytics Approach
 Lin Li, Saeed Bagheri, Helena Goote, Asif Hasan, Gregg Hazard
 Philips Research North America
 Briarcliff Manor, US
 Lin-Li@philips.com, Saeed.Bagheri@philips.com,
 Helena.Goote@philips.com, Asif.Hasan@philips.com, Gregg.Hazard@philips.com
 Abstract—For healthcare applications, voluminous patient
 data contain rich and meaningful insights that can be revealed
 using advanced machine learning algorithms. However, the
 volume and velocity of such high dimensional data requires
 new big data analytics framework where traditional machine
 learning tools cannot be applied directly. In this paper, we
 introduce our proof-of-concept big data analytics framework
 for developing risk adjustment model of patient expenditures,
 which uses the ”divide and conquer” strategy to exploit the
 big-yet-rich data to improve the model accuracy. We leverage
 the distributed computing platform, e.g., MapReduce, to im-
 plement advanced machine learning algorithms on our data
 set. In specific, random forest regression algorithm, which is
 suitable for high dimensional healthcare data, is applied to
 improve the accuracy of our predictive model. Our proof-of-
 concept framework demonstrates the effectiveness of predictive
 analytics using random forest algorithm as well as the efficiency
 of the distributed computing platform.
 Keywords-Healthcare Big Data, Risk Adjustment, Dis-
 tributed Computing, Random Forest, Patient Expenditure
 I. INTRODUCTION
 Risk adjustment model [1] performs as an actuarial tool
 to predict health costs based on the relative actuarial risks
 of a patient, which is becoming increasingly important
 due to their value towards (a) Identification of high-risk
 (future high cost or high utilization) individuals for program
 management, (b)Normalization of population to evaluate the
 provider effectiveness and efficiency in terms of manag-
 ing resources among different types of patient, (c) Pricing
 health plan or predicting future claims cost trends, etc..
 There are many risk-adjustment models that have been pub-
 lished [2], where the CMS hierarchical condition categories
 (HCC) model has been adopted by CMS for Medicare
 risk-adjustment because of its transparency, simplicity of
 modeling, and clinical coherence [2]. CMS/HCC risk ad-
 justment model represents the patient diagnosis information
 with hierarchical condition categories (HCC) that effectively
 reduce the number of the diagnostic categories by imposing
 the hierarchies among diseases and use linear regression
 algorithm to train the predictive model mainly because of its
 computational efficiency and the robustness to limited data.
 However, the nonlinear functional relationship between the
 patient risk factors and the health costs cannot be sufficiently
 captured by linear regression model.
 Health care industry is experiencing the impact of Big
 Data, which brings the opportunity to improve the accuracy
 of the risk adjustment model. The high volume of data
 can overcome the data noise and bias. On the other hand,
 Big data also enable the effectiveness of the advanced
 machine learning algorithms. In general, advanced machine
 learning algorithms have more parameters to be optimized
 than simple methods. Therefore, a limited supply of data
 may introduce the over-fitting problem and thus reduce its
 effectiveness, which is not an issue for big data applications.
 Therefore, we aim to leverage advanced machine learning
 algorithms and healthcare big data to improve the prediction
 performance of risk adjustment model. For patient big data,
 the new processing environment is required to store, transfer
 and analyze voluminous data, which is beyond the ability
 of conventional data processing tools, for example Matlab.
 Therefore, a distributed computing platform with the Apache
 Hadoop infrastructure [3] is employed in our study to
 support our data-intensive application. Hadoop derived from
 MapReduce [4] provides a distributed, scalable, and portable
 file system to hold vast amounts of data on a cluster of
 machines with high aggregate bandwidth across the clusters
 and uses ”divide and conquer” strategy to run an application
 in parallel across the cluster against parts of the stored data.
 In addition, the large amount of risk factors of the patient
 expenditure including demographics, diagnoses, comorbidi-
 ties, etc. induces the data high-dimensionality, which also
 leads to the difficulty of modeling. Random Forests [5],
 as one type of ensemble machine learning algorithms, are
 particularly suitable to modeling the complex functional
 relationship in high-dimensional data, which can achieve
 increased performance by generating multiple prediction
 models each with a different feature subset. Therefore,
 we proposes a random-forest-based risk adjustment model
 running on the MapReduce platform to fight the “curse
 of dimensionality”. This paper demonstrates the promising
 results of our prototype framework with respect to both
 prediction accuracy and computation efficiency.
 2013 IEEE International Conference on Big Data
 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 12
II. PATIENT DATA
 This paper utilized inpatient databases collected from
 New York State to test the proposed framework of the
 risk adjustment model for Healthcare big data. It contains
 a core set of clinical and nonclinical information on all
 patients,which allows estimation of inpatient expenditure
 based on various patient risk factors including demographics,
 principal and secondary diagnosis, and comorbidities.
 A. Patient Feature Extraction
 In this paper, we use CMS hierarchical condition cate-
 gories (HCC) to extract patient features, which is adopted
 by CMS for Medicare risk adjustment because of its trans-
 parency, ease of modification, and good clinical coherence.
 To be specific, demographic risk factors included in the
 model are 24 mutually exclusive age/sex cells. For diagnosis
 information, CMS/HCC model first maps the raw diagnosis
 data coded with ICD9 code [6] to Condition Categories
 (CCs) and then map CCs to 70 hierarchical condition
 categories (HCC) by imposing hierarchies [7], as shown in
 Figure 1. In addition, multiple comorbidities, such as alcohol
 abuse, congestive heart failure, depression and diabetes are
 also incorporated to enrich the patient representation. Over-
 all the risk factors for patient expenditure are represented by
 125 dimension features.
 Figure 1. Diagnostic feature extraction using CMS-HCC model
 III. PREDICTIVE MODEL IN MAPREDUCE PARADIGM
 For predictive model, linear regression model is widely
 used as a standard one for risk adjustment [2], partly because
 of its robustness to limited data with high-dimensionality.
 The availability of enormous patient data enables the ef-
 fective application of the advanced regression algorithms,
 which are better qualified to identify the nonlinear complex
 functional relationship in patient data. In this paper, Leo
 Breiman‘s random forests algorithm as one type of advanced
 regression model is chosen to build the predictive model [5],
 because of its suitability for high-dimensional data. Given
 the high-dimensionality of the patient risk factors, The ran-
 dom forest methodology use ”divide and conquer” strategy
 to achieve enhanced prediction performance, by training an
 ensemble of decision trees each with a different feature
 subset and using the vote scheme to combine the results.
 Random forest also holds some other desirable advantages
 for this application such as robustness to noise and outliers,
 suitability for parallelization, and lack of dependence upon
 tuning parameters [5], [8]. The random forest regression
 model can be generally described by three steps:
 • Draw n bootstrap sample from the original patient data.
 • For each of the bootstrap samples, grow a regression
 tree. At each node randomly sample m risk factors and
 choose the best split among those variables.
 • Predict new sample by aggregating the prediction of
 the n trees.
 where the optimal value of free parameters (n and m) are
 determined by the best results of cross-validation dataset
 after scanning these parameters. Apache Hadoop is used
 to build a distributed computing cluster, which contains
 one master server and three slave servers for the initial
 prototype framework. We use Apache Mahout’s random
 forest implementation [9], [10] to train the risk adjustment
 model on the cluster, which builds enormous number of
 decision trees in the model in parallel.
 IV. RESULTS
 In this paper, we would like to evaluate the effectiveness
 of the random forest-based risk adjustment model and the
 efficiency of the distributed computing framework. The
 linear regression model used in Medicare risk adjustment is
 adopted as the baseline. Five-fold cross-validation is imple-
 mented on one hospital data (100,000 samples) to compare
 the performance between the linear regression and random
 forest regression model, where R2 is calculated to measure
 the prediction accuracy. The results show that the ran-
 dom forest (R2 = 0.38/0.008 (mean/ standard derivation))
 significantly outperforms linear regression model (R2 =
 0.31/0.01 (mean/standard derivation)), which indicates the
 effectiveness of the random forest to identify the complex
 patterns in high dimensional patient data and thus illustrates
 its capability of enhancing the risk adjustment model per-
 formance. The comparison of 2D histogram between linear
 regression and random forest for test data is shown in
 Figure 2, where x axis and y axis present the predicted
 inpatient expenditure (log10) and the actual/target inpatient
 expenditure (log10), respectively. The color is labeled by
 the number of test samples located at (x, y). If the inpatient
 expenditure of a sample is able to be correctly predicted,
 then the sample will lie along the diagonal. It is observed
 that there are more samples located close to diagonal for
 random forest model than linear regression model.
 In addition, we evaluate the enhancement by distributed
 computing on multiple servers to address the computation
 13
Target log10( expenditure )
 Predicted lo
 g 1
 0(expenditure
 )
  
 
Random Forest
 3 4 5 6
 3
 3.5
 4
 4.5
 5
 5.5
 6 0
 20
 40
 60
 80
 100
 Target log10 ( expenditure )
 Predicted lo
 g 1
 0( expenditure 
)
  
 
Linear Regression 
3 4 5 6
 3
 3.5
 4
 4.5
 5
 5.5
 6
 Figure 2. Comparison of 2D histograms between linear regression and
 random forest for test data
 burden induced by both model complexity and high volume
 of data. All New York inpatient data (3, 400, 000 samples)
 are used to build the random forest model (200 trees and
 10 features per tree). Compared to running on a single
 server, we achieve a 2.32 times speedup by using Hadoop
 infrastructure to parallel the computation on 3 slave servers,
 where computation time on a single server is 299 sec and
 computation time on 3 slave servers is 121 sec.
 V. DISCUSSION
 This paper describes our ongoing research that uses ”di-
 vide and conquer” strategy to leverage health care big data
 and advanced machine learning technology to improve the
 risk adjustment model. Our results show that the random-
 forest-based model significantly outperform the linear re-
 gression model employed by CMS, which illustrates the
 effectiveness of the random forest algorithm to identify the
 complex relationship in high-dimensional patient big data. It
 also indicates its potentials to incorporate more patient risk
 factors, such as disease history, insurance, income level, sec-
 ondary diagnoses to enrich the risk adjustment model. In ad-
 dition, it is evident that our distributed-computing platform
 successfully supports the data-intensive and computation-
 intensive application. The distributed-computing cluster can
 be expanded to enable learning the model from national-
 wide data set (billions of samples), and thus to exploit the
 power of big data in risk adjustment and other health care
 service areas.
 REFERENCES
 [1] I. Duncan, Healthcare Risk Adjustment and Predictive Mod-
 eling. Actex Publications, 2011.
 [2] G. Pope, J. Kautter, R. Ellis, A. Ash, J. Ayanian, L. Iezzoni,
 M. Ingber, J. Levy, and J. Robst, “Risk adjustment of medi-
 care capitation payments using the cms-hcc model,” Health
 Care Financing Review, vol. 25, pp. 119 – 141, 2004.
 [3] [Online]. Available: http://hadoop.apache.org/
 [4] J. Dean and S. Ghemawat, “Mapreduce: simplified data
 processing on large clusters,” Commun. ACM, vol. 51, no. 1,
 pp. 107–113, Jan. 2008.
 [5] L. Breiman, “Random forests,” Machine Learning, vol. 45,
 no. 1, pp. 5–32, 2001.
 [6] ICD-9-CM Official Guidelines for Coding and Reporting.
 [7] G. Pope, R. Ellis, A. Ash, J. Ayanian, D. Bates, H. Burstin,
 L. Iezzoni, E. Marcantonio, and B. Wu, “Diagnostic cost
 group hierarchical condition category models for medicare
 risk adjustment,” Health Economics Research, Inc. Waltham,
 MA, 2000.
 [8] A. Liaw and M. Wiener, “Classification and regression by
 randomforest,” R News, vol. 2, no. 3, pp. 18–22, 2002.
 [9] [Online]. Available: mahout.apache.org
 [10] S. Owen, R. Anil, T. Dunning, and E. Friedman, Mahout in
 action. Manning, 2011.
 14

Scaling Concurrency of Personalized Semantic
 Search over Large RDF Data
 Haizhou Fu, HyeongSik Kim, Kemafor Anyanwu
 North Carolina State University, Raleigh, NC, USA
 {hfu, hkim22, kogan}@ncsu.edu
 Abstract—Recent keyword search techniques on Semantic Web
 are moving away from shallow, information retrieval-style ap-
 proaches that merely find “keyword matches” towards more
 interpretive approaches that attempt to induce structure from
 keyword queries. The process of query interpretation is usually
 guided by structures in data, and schema and is often supported
 by a graph exploration procedure. However, graph exploration-
 based interpretive techniques are impractical for multi-tenant
 scenarios for large databases because separate expensive graph
 exploration states need to be maintained for different user
 queries. This leads to significant memory overhead in situations
 of large numbers of concurrent requests. This limitation could
 negatively impact the possibility of achieving the ultimate goal
 of personalizing search. In this paper, we propose a light-
 weight interpretation approach that employs indexing to improve
 throughput and concurrency with much less memory overhead.
 It is also more amenable to distributed or partitioned execution.
 The approach is implemented in a system called “SKI” and an
 experimental evaluation of SKI’s performance on the DBPedia
 and Billion Triple Challenge datasets shows orders-of-magnitude
 performance improvement over existing techniques.
 Keywords—Scalability; Concurrency; Keyword Query Interpreta-
 tion; Personalization; Big RDF data; Semantic Web;
 I. INTRODUCTION
 There is increasing interest in exploiting the growing amount
 of structured, semantic data on the Web to improve the quality
 of keyword search. The idea is to be able to better identify a
 user’s intended semantics for a query so as to provide the
 appropriate set of results. To achieve this, techniques must go
 beyond the IR or “meaning as match” interpretation styles used
 by traditional search engines. Most techniques proposed for
 keyword queries over structured/semi-structured data are also
 based on such interpretation styles with relational databases
 ([1], [2], [3], [4], [5], [6]) and XML databases ([7], [8]). The
 limitation of these approaches are well known: i) result lists
 that are semantically incongruent so that users have to manu-
 ally filter out results that are irrelevant to the user’s querying
 context; ii) inability to deal with moderately complex queries.
 One example is the class of “list queries” that describes a set
 of entities rather than a specific entity. For example, the query
 “patents big data management” is likely intended to return a
 list of patents on topics related to big data management. For
 example, a query for “patent big data management” may miss
 patents that do not contain the phrase “big data management”
 but contain keywords like “scalable”, “MapReduce” and so on.
 Processing such queries with current IR techniques presents
 challenges due to the length of such queries. In addition, some
 keywords in the queries are descriptive, which should not be
 considered as data explicitly associated with relevant results
 but as metadata.
 To address this limitation, semantic search techniques have
 been proposed ([9], [10], [11]) where an “interpretation” phase
 is inserted prior to query evaluation. The goal of interpretation
 is to map a keyword query to a structured query, thereby
 giving the query a fixed semantics for which semantically
 congruent results can then be computed. However, identifying
 a unique semantics is often difficult because the terse nature of
 keyword queries makes them ambiguous. Consequently, most
 techniques address this as a top-K problem producing a list of
 K structured queries representing the K most likely intended se-
 mantics for a given keyword query. The structurization process
 involves computing connected subgraphs of keyword hits on a
 summarized representation of schema and data graphs. These
 graphs are mapped to structured queries by distinguishing
 the roles of keywords in a query i.e. metadata keywords
 vs. data keywords. For example, for the query “conferences
 tutorial big data” depending on the roles assigned to the
 word “conferences”, the interpretation could be “the list of
 conferences with tutorials on big data” or could be the “list
 of tutorials at conferences on big data”.
 Heuristics for ranking connected subgraphs typically assign
 the intended semantics of a query to the probability of the
 query, defined in terms of the frequency or occurrence of
 subgraph structures. This is similar in spirit to search engines
 determining most relevant results based on click patterns in
 query and click-through logs. However, such techniques are
 impersonal and fail to customize interpretation of a query to
 an individual user’s needs. As an example, while the most
 frequent intention associated with the query “magic sets” may
 be toys because the shopping context is very general and has
 a large population of users. However, the same query could be
 intended to mean the query optimization technique which is
 well known but in a much smaller community. Unfortunately,
 such a relatively unpopular context would not be considered
 even if intended. It is important to note that this is not simply
 a question of profiling users and determining interests of a
 user because users have different interests and interact with the
 Web in different contexts. For example, a database researcher
 issuing the query “magic sets” may intend the context of toys
 when shopping for some gifts. Another example is “Jaguar
 Speed”; a computer science researcher may be investigating the
 supercomputer, then at a different time be buying a car, and yet
 another time be helping a middle school child with a science
 homework. Therefore, the ideal situation is to personalize
 query interpretation by identifying the intended interpretation
 for a specific user at a specific time.
 2013 IEEE International Conference on Big Data
 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 556
In our previous work ([9], [10]), we addressed the problem
 of “personalizing” keyword query interpretation by capturing
 and exploiting a user’s “ambient” query context in terms of the
 queries preceding a query being considered. The underlying
 hypothesis is that users often pose multiple queries related to
 a specific need or task so that earlier queries can provide some
 contextual basis for future queries. The approach proposed
 modeled this context-aware query interpretation as computing
 top-K connected subgraphs over a context-aware summary
 graph. The context-aware summary graph was modeled as a
 dynamic weighted graph model in which keywords in a query
 ascribe weights transfer summary graph and the impact. The
 result is that for a given query being issued at a given time,
 connection subgraphs involving concepts and relationships
 with lower costs (higher weights) are ranked higher in the top-
 K list (decay model). A key challenge addressed in previous
 work is the indexing and management of the dynamic weight
 graph model and efficient algorithm for computing the Steiner
 Trees.
 Motivation. Like the non-personalized techniques for query
 interpretation, our earlier proposed context-aware interpreta-
 tion technique is heavy-weight because it is based on graph
 exploration on memory resident summary graphs. For big data,
 the number of hits per keywords and the degree of connectivity
 between nodes rises, increasing the exploration search space
 for connection subgraphs and thus memory requirements. For
 example, the keyword “conference” has 19 occurrences in
 DBPedia 3.61, which includes 272 concept nodes and around
 3000 properties (edges). Further, it has 1785 hits in the Billion
 Triple Challenge datasets (BTC 2009)2, which contains 150232
 class nodes and around 1.3 million edges (BTC is a superset
 of DBpedia). In personalized query interpretation, each user
 has a different dynamic weighted graph because of differences
 in their query contexts. This could imply a separate graph
 exploration process for each user. For example, the memory
 requirements for interpreting a single query on the BTC dataset
 is around 500MB, which leads that a server with 8GB RAM
 can handle only 17 user queries concurrently. Consequently,
 we are faced with the limitation of only being able to meet
 the goal of personalized and contextual search under low
 concurrency/throughput, which is impractical for real multiuser
 search environments.
 Contributions and Scope. In this paper, we focus on in-
 creasing the practicability of personalized semantic search
 in big data contexts by introducing lightweight interpretation
 techniques. Specifically, we propose an approach called “SKI
 that uses a dual indexing scheme to maximize the concurrency
 and throughput of the query interpretation process while min-
 imizing the latency. SKI indexes user-specific query context
 information separately from the more general user-independent
 information about concepts and their relationships. The former
 is captured in an index – personalized query context map
 (PCM) and the latter in dense path index (DPI) – an index of
 subgraph structures. The data-specific indexes inform a graph
 exploration-free interpretation algorithm’s (GeFree) decisions
 about which substructures to prune and how to assemble
 substructures into complete interpretations. GeFree avoids the
 need for graph exploration and is fast and memory-efficient
 reducing both latency and memory requirements of query
 1http://blog.dbpedia.org/2011/01/17/dbpedia-36-released/
 2http://km.aifb.kit.edu/projects/btc-2009/
 interpretation.
 Impact. We posit that integrating our proposed scalable query
 interpretation approach with many ongoing-efforts on scalable
 query-answering techniques, will result in scalable Semantic
 Web search architectures.
 The rest of the paper is organized as follows: Section 2
 includes the preliminaries of the keyword query interpretations
 and the problem definition. Section 3 describes the overview
 of the SKI architecture. Section 4 discusses the details of the
 index structures and algorithms used in the SKI. Section 5 and
 6 present performance evaluation results and related work.
 II. PROBLEM DEFINITION
 Let ? be a universe of words. An RDF schema graph
 is a labeled graph GS = (VS , ES , ?S) where the nodes
 represents classes and edges represent properties. ?S is a
 labeling function that maps a graph element (node or edge)
 to l ? ? that contains words that make up the label of
 the class or property. An RDF data graph is also a labeled
 graph GD = (VD, ED, ?D, piD) that has VD, ED and ?D are
 similarly defined with ?D mapping a graph element to set
 of words contained in the string literal associated with that
 element. piD maps each data graph element (node or edge)
 gD to a set of schema graph elements: those to which gD is
 connected by a “rdf:type” relation.
 Given a schema graph GS and data graph GD, an annotated
 schema graph GA is a tuple (GS , ?A) where ?A maps
 each graph element gS such that ?A(gS) = { ?S(gS) ?
 ?D(gD) |gS ? piD (gD)}. In other words, the function ?A lifts
 labels on elements of data graph to the schema elements that
 they are instances of, thereby annotating the schema graph
 with keywords from the data graph.
 A keyword query Q is a set of words {k1, k2, ..., k|Q|} from the
 universe ?. An element gi of an annotated schema graph GA
 is called a hit for keyword k if k ? ?A(gi). The set of all hits
 for a keyword k is denoted as hit(k). An interpretation [[Q]]
 of a keyword query Q is a subtree of GA, that contains one
 hit for each keyword in Q. Since there are potentially multiple
 interpretations of a keyword query (due to different possible
 combinations of hits for all keywords), we focus on a specific
 (top) interpretation for Q which we denote [[Q]]?, but ignore
 for now, how to choose [[Q]]? from the set of alternatives for
 Q.
 DEFINITION 2.1: A context-aware annotated graph or CAG
 for short, associates weights with an annotated schema graph
 based on the sequence of queries that have been posed on that
 graph. A sequence of queries Q1, ..., Qm produces a sequence
 of weighted graphs G0 ? G1 ...? Gm where Gt (0 ≤ t ≤ m)
 is the weighted annotated schema graph produced after Qt.
 Gt is defined as (GA, ?t) where ?t ? W , W is a family of
 weighting functions s.t:
 • ?0 ? W is an init. function that assigns weights to all
 elements of the annotated schema graph prior to Q1;
 • ?t ? W is defined s.t. for g ? [[Qt]]?, i.e. in the top
 interpretation for Qt, ?t(g) = ?(?t?1(g))).
 In other words, after Qt, the weights of a graph elements in
 the top interpretation of Qt, is a function of their weights after
 557
 !""!""!##!
 $!%&'()*+,
 )*+,
  -'./*/&
 0-*+
 123
  !"
 #!"
 $*.&
 #!"
  !$
  !"  !"
  
 !""!""!##!
 $!%&'()*+,
 )*+,
  -'./*/&
 0-*+
 #!"
 $*.&
  !$
 423 423
  
 !"
 5!"##"##"$$"%
 &"'()%*+,-6
  #"
 5!.)/0+0(
 &+/(6
  !""!""!##!
 $!%&'
 $!%&' $!%&'()*+,
 78.!%!.9 :!";!+/
 123
 123
 #!" #!$
 #!"  !" #!"  !"
  $"
 12"#3",0%
 45/"'"/67
  
 !""!""!##!
 $!%&'
 $!%&' $!%&'()*+,
 78.!%!.9 :!";!+/
 #!"
 #!"
 #!$
 423 423
   %"
 5!"##"##"$$"%
 &"'()%*+,-6
  !""!""!##!
 $!%&'()*+,  !""!""!##!
 )*+,
  !""!""!##!
 $!%&'
 <.*.& $!%&' $!%&'()*+,
  -'./*/&
 0-*+ 78.!%!.9 :!";!+/
 =*+,(->
 .;&('!%&'
 ;*"
 *8.!%!.9?+*@&
 A-8*.&B?!+A-8*.&B?!+
 ".*.&?+*@&
 ;*"?#'-BC8.
 =*+,?+*@& '!%&'?+*@&
 #!"
  !" #!"#!"
 #!"
 D-.*A(E&!/;."F(%!"D-.*A(E&!/;."F(%!"
 123
 123
 123
 123 123
 $*.&;*"
 123
 G*H
 G=H G8H
 GBH G&H
 I"&'&'
 I"&'((
 Fig. 1. An example context-aware annotated graph (CAG) and its interpretation processes. The example CAG contains the two possible interpretations on the
 the keyword query “Mississippi River Bank”: the bank of the Missippippi River (a financial institute) or the Missippippi River (a large natural stream of water).
 Qt?1. This leads to the definition of an annotated schema
 graph as a dynamically weighted graph whose elements’
 weights change with queries.
 As mentioned earlier, the details of how to select [[Q]]? –
 the top interpretation for a keyword query Q, are omitted.
 However, this can be thought of as the lowest weighted
 interpretation when the weights of all graph elements in that
 interpretation are combined. The combination cost function
 and ? are discussed in [9], but we illustrate the concept of
 a CAG with an example.
 Context-Aware Interpretation Example Fig. 1(a) shows the
 example CAG with some classes and literal nodes which are
 annotated with literals from the data graph. Given a keyword
 “Mississippi River Bank”, we find matches in this graph and
 select the “highest” weighted connecting subtree as the “top”
 interpretation. In Fig. 1(a), the graph has been initialized
 with some weights, e.g., the class nodes “Mississippi River
 Bank” and “Bank” are initialized with the weight 1.0 and
 2.0 respectively. For example, the green subtree involving the
 class nodes Mississippi River Bank and Bank represents the
 interpretation of the query as the bank of the Missippippi River
 while the purple subtree involving nodes Mississippi River
 and River represent the interpretation the Missippippi River.
 However, based on the weight assignment, it is difficult to
 determine which interpretation should be selected as a highest
 weighted one since the total weight of subgraphs are the same,
 i.e. 3.0 and 3.0 for both subtrees. Note that the method for
 computing the initial weights can vary and is not our primary
 concern here. We calculate the total weights as the sum of the
 weights of nodes and edges, not the sum of the weights of all
 paths represented in the subgraph.
 Fig. 1(b-d) show how context-aware interpretations are pro-
 cessed with two paths originating from the initial graph. Each
 path represents a sequence of queries performed by a user:
 the top path (Q1 and Q2) by the user X and the bottom
 path (Q3 and Q4) by the user Y . In the top path, Fig. 1(b)
 shows that the keyword query Q1 “Mortgage Rate” results in
 increasing the weight of the nodes Mortgage Loan and Rate
 from 1.0 to 2.0, and the weight of its neighboring node Bank
 from 2.0 to 2.5. These changes make the top interpretation of
 the keyword query Q2 becomes the subtree consisting of the
 nodes Mississippi River Bank and Bank in Fig. 1(c) because
 the sum of the weights of those nodes are the highest one, i.e.
 2.5+1.0 = 3.5. Similarly, in the bottom path, the weight of the
 node River is increased from 1.0 to 1.5 because of the keyword
 query Q3 “Fishing Activity” in Fig. 1(d). This increment leads
 to the subtree with the nodes Mississippi River, River, and
 River Bank as the top interpretation of the subsequent query
 Q4 because its weight now becomes the highest one, i.e.,
 1.0 + 1.5 + 1.0 = 3.5. Note that Q2 from the user X and Q4
 from the user Y are the same queries but produce different top
 interpretations.
 Although our example illustrates the changes in weights as
 increasing weights, the real situation is the inverse (higher
 weight leads to lower cost). Since we are solving this as a
 translation of the Steiner Tree problem which is a minimization
 problem, the “top” interpretations should actually be the small-
 est weighted ones. Consequently, our concept of increasing
 weights in example to reflect importance, is actually done as
 decreasing weights.
 The problem we address in this paper is supporting personal-
 ized CAGs, i.e. for two users X and Y with query sequences
 QSX and QSY resp., CAGX and CAGY are two (possibly
 different) CAGs associated with X and Y resp. The conceptual
 model of CAGs extends very naturally to multiple users.
 However, the key challenge is how to scale up the number
 of concurrent users, since a different CAG has to be managed
 for each user? Particularly, in the presence of big annotated
 schema graphs. The following section discusses our approach.
 III. THE SKI APPROACH
 A. Architecture Overview
 In this section, we present the SKI strategy for scalable
 personalized query interpretation that consists of three main
 components:
 558
1) a distributed master-slave architecture for scalability
 2) an index for user query-context information called a per-
 sonalized query context map (PCM), which is separate
 from a more heavy-weight index of user-independent
 information about graph structures, i.e., concepts and
 relationships in the schema graph 3 captured in a couple
 of other indexes. After each query has been interpreted,
 PCM is updated to reflect this new information about
 query context i.e. ?Uit ? ?
 Ui
 t+1 for user Ui. To avoid
 the overhead of maintaining all dynamic weighted graphs
 for supporting multiple concurrent interpretations, PCM
 maintains only node-weight maps for each user.
 3) an algorithm GeFree for computing top-k connection
 subgraphs for each user query in which user-independent
 indexes (i.e., dense path index, DPI) are shared across
 interpretation instances for each user and are then com-
 bined with their user-specific context information in the
 PCM. GeFree is graph exploration free making it more ef-
 ficient and allowing higher throughput and the sharing of
 user-independent indexes minimizes amount of memory
 required for interpretation to allow increased concurrency.
 4) form the query interpretation layer which is interfaced
 with the query answering layer that uses standard RDF
 query processing engines such as SESAME.
 In this paper, we focus on the interpretation layer. The slave
 nodes are grouped into clusters, or interpretation clusters for
 and the master node is responsible for scheduling user query
 requests to interpretation clusters and exchanging information
 such as resulting interpretations and query answers between
 users and interpretation clusters. Each interpretation cluster is
 responsible for processing a specific group of users’ query
 requests. Indexes are replicated across all slave nodes with one
 exception where, a partitioning of PCM in an interpretation
 cluster contains only the context map for the group of users
 maintained by this cluster, it is replicated across slave nodes
 within this cluster. Each slave node in an interpretation cluster
 runs a separate thread for each interpretation instance. Each
 interpretation instance has 3 key steps depicted in Fig. 2:
 1) Step 1.1 Initialize Keywords Hits: by looking up the
 KS-Map (inverted index) that maps keywords to schema
 graph elements;
 2) Step 1.2 Identify Candidate Roots: identifies all possible
 roots of candidate connecting trees linking at least one
 keyword hit for all keywords in a query. This process is
 supported by Rabit Index: a group reachability index for
 rapid identification of root nodes of connecting trees;
 3) Step 1.3 Top-K Graph Patterns Generation: uses the
 GeFree algorithm to assemble and generate top-K graph
 patterns representing top-K interpretations based on can-
 didate roots computed in 1.2. GeFree is a “graph explo-
 ration free” algorithm that uses DPI for fast construction
 of interpretations. It uses materialized path information
 stored in DPI and Rabit to enable rapid assembly of the
 top-K minimum connecting trees representing the top-k
 interpretations. To personalize the interpretation process,
 it uses the user’s information in the PCM partition as-
 signed to that slave node.
 3In the rest of the paper, when we say “schema graph”, we actually mean
 “annotated schema graph”
 Fig. 2. Architecture and workflow for a single slave node
 After the personalized top-K candidate graph patterns are
 generated, Steps 2 to 4 take care of sending them to the
 user layer and sending the highest ranked queries to the query
 answering layer for processing. Since the ranking is based on
 heuristics and may not be perfect, sending the top-K list of
 graph patterns allows a user to select a different top-1 pattern
 if the highest ranked pattern is not their intended interpretation.
 When this happens, the processing of the originally dispatched
 top-1 query is halted and query processing for the newly
 selected graph pattern begins.
 Elaborating on the algorithms requires the introduction of some
 concepts: ∆-path covers and ∆-graph exploration graphs
 which define structural information maintained by DPI and
 GeFree. Candidate root is a key concepts for Rabit index and
 GeFree.
 B. Keyword Query Interpretation in SKI
 1) Foundations:
 DEFINITION 3.1: A ∆-path cover of a node r in graph G is
 denoted by pi(∆, r, G) = {pi}, where pi is a path such that
 ?pi, pi ? pi(∆, r, G) if and only if r ? G is the source node
 of pi and |pi| ≤ ∆, i.e. the path cover pi(∆, r, G) contains all
 reachable paths of length less than or equal to ∆ from r. For
 example, a 2-path cover of node D in the graph shown in Fig. 3
 is {D, D ? A, D ? B, D ? E, D ? X , D ? X ? N ,
 D ? X ? I , D ? H ? M , D ? H ? I , D ? H ? E,
 D ? E ? H , D ? E ? I , D ? E ? F}.
 DEFINITION 3.2: A ∆-Exploration Graph of a node r in
 graph G (denoted by g∆r ) is a rooted subgraph of G, which
 contains all edges in pi(∆, r, G). For example in Fig. 3, let GS
 be the whole graph, the subgraph in the box is a 2-exploration
 graph rooted at node Z : g2Z , which contains all edges in
 pi(2, Z, GS) = {Z ? Q, Z ? Q ? M, Z ? Q ? S, Z ?
 Q ? R}.
 DEFINITION 3.3: Given a keyword query Q and a schema
 graph GS , a candidate root of Q is the root node r of a ∆-
 exploration graph g∆r , which contains at least one hit of each
 559
Fig. 3. Example for the definitions of ∆-path cover and ∆-Exploration
 Graph. Arrows represents exploration states on an undirected schema graph
 keyword in Q. For example, in Fig. 3, I is a candidate root for
 keyword query k1, k2, k3, because the 2-exploration graph g2I
 contains a hit P (matches k1), K (matches k2) and Y (matches
 k3). Those hits (such as P, K, Y in g2I ) that are covered by the
 ∆-exploration graph g∆r rooted at a candidate root r are called
 productive hits. Productive hits can always reach a candidate
 root following a path of length less than or equal to ∆.
 2) Identifying Candidate Roots: Different from the graph
 exploration based approaches, where roots are identified by
 exploring paths originated from each hits, in this section, we
 introduce the idea of fast candidate roots and productive hits
 identification.
 Group Reachability Bitmap Index. Given a keyword query
 Q = {ki} , in order to identify a group of candidate roots,
 a “group reachability” question that needs to be answered is:
 Given a set of groups of nodes, whether every node in a group
 of nodes CR can reach at least one node in ALL the other
 groups within∆ hops. We propose a group reachability bitmap
 index (Rabit) to identify candidate roots and productive hits
 more efficiently. For each node ui in the graph, we propose
 to use a bit vector denoted by ?i =< bi1, bi2, ...bi|V (GS)| > to
 represent the nodes that can reach ui within ∆ hops. Notice
 that, in Fig. 3, each bit vector for each node is 26 bits long
 because there are 26 nodes in that graph, which can then be
 represented using a 32-bit integer, such that we can use one
 integer to represent the ∆ exploration graph for each node.
 Let ? be a function that maps a set of nodes U = {u1, ...um}
 to bit vector such that only bui = 1 iff ui ? U . Now, we can
 check the reachability of two nodes using bitwise AND (i.e.,
 &) in the following way: we can check check if node H can
 reach A in 2 hops by calculating ?A & ?({H}) = 56885252
 & 262148 = 262148, where ?({H}) = 262148.
 Using the Rabit index, we can identify for each group of
 keyword hits, a set of neighboring nodes that can reach at
 least one hit in that group by using bitwise OR operation.
 For example, in Fig. 3, x = GetNeighbors({A, C}, 2) =
 ?A | ?C = 56885252 | 12255232 = 67043332 =
 111111111100000000000001002, which means the set of node{A, B, C, D, E, F, G, H, I, J, X} is the union of neighbors
 of {A, C}. Then, we can identify a bit vector that repre-
 sents the common nodes in all groups of neighboring nodes.
 This can be achieved by bitwise AND operation, and those
 nodes are candidate roots. For example, for keyword query
 ”k1,k2,k3” in Fig. 3, the candidate roots can be calculated
 in the following steps: the 3 groups of hits for each key-
 word are G1 = {A, P}, G2 = {K, T}, G3 = {Y, R}.
 For k1, x1 = GetNeighbor(G1, 2) = 57023492; for k2,
 x2 = GetNeighbor(G2, 2) = 1295074; for k3, x3 =
 GetNeighbor(G3, 2) = 1303427. Then x1 & x2 & x3
 = 131072 = 000000001000000000000000002, which means,
 only the node I is the candidate root. Those hits that can NOT
 reach any candidate roots are un-productive hits and therefore,
 should be pruned. Those un-productive hits can be identified
 using bitwise AND operation in the similar way.
 3) GeFree: A Graph Exploration Free Approach: After iden-
 tifying candidate roots, we present another important index
 called dense path index (DPI). Having unproductive hits
 pruned, given a set of candidate roots CR = {ri}, for
 keyword query Q, and each root ri, there are many possible
 interpretations rooted at ri. However, only knowing the roots
 is not enough to identify the top-K interpretations among all
 possible interpretations that rooted at each root in CR. The
 paths from a root node to all hits need to be computed and
 assembled to construct an interpretation.
 Dense Path Index. The dense path index is a two-level
 hierarchical index. The first layer is called node-exploration
 graph map, or NE map, which is organized as a hash table
 that maps any node r in GS to a ∆-exploration graph g∆r .
 NE(r) = g∆r denotes such mapping. The second layer of
 the dense path index is called destination-path map, or DP
 map that maps a destination node to a set of paths: For each
 ∆-exploration graph, the ∆-path cover of r, i.e., pi(∆, r, g∆r )
 is pre-computed and those paths in pi(∆, r, g∆r ) are grouped
 by destination nodes pi(∆, r, g∆r ) = {Pv0 ? Pv1 ? ... ? Pvm},
 where Pvi is called a path group of vi, where vi ? V (g∆r ) is
 a destination node, and ?i, j, Pvi ?Pvj = ?. DP (v, g∆r ) = Pv
 denotes the mapping for g∆r from a destination node v to the
 path group Pv of v. The ∆-path cover can be organized as a
 hash table that maps any destination node vj ? V (g∆r ) to the
 path group of vj , i.e., Pvj .
 For each root-keyword pair, they are updated and stored in
 PCM after each query generation and retrieved from PCM
 during query time ) at most the top-K minimum weighted
 paths are necessary for computing top-K interpretations. For
 example, given a keyword query Q =< w1, w2 >, considering
 a candidate root r, assuming that we are only interested in
 top-2 interpretations, let P1 =< p11, p12, p13 > be the top-3
 paths from r to any hits of w1; P2 =< p21, p22 > be the
 top-2 paths from r to any hits of w2, any combinations of
 paths including p13 will not form a top-2 interpretation and
 should not be investigated during the processing of generating
 top-K interpretations. The GeFree algorithm is illustrated in
 Algorithm 1.
 GeFree Algorithm. For each keyword wi, only productive
 hits are left and stored in H[i] for wi (line 4). For each
 candidate root ri, the corresponding ∆-exploration graph is
 returned using NE map (line 7). Then, for each keyword
 wj , H[j] contains a set of productive hits for wj ; for each
 productive hit vjk, DP map returns a path group Pk (line 11).
 For a candidate root ri and a keyword wj , Top-K root-
 keyword paths are computed by merging topK-heaps from
 each path groups of all productive hits of wj into a priority
 queue P Qj of size K (line 12 and line 13). In line 14,
 for the specific root ri, a sorted list Sj containing K paths
 560
Algorithm 1 GeFree
 1: Input: Q = {wi};CR = {ri}; PKE = {ui}.
 2: TOPK = ? is a priority queue for maintaining top-K interpretations
 3: for all wi ? Q do
 4: H[i] = hit(wi) ? PKE;
 5: end for
 6: for all ri ? CR do
 7: g∆ri = NE(ri);8: for all wj ? Q do
 9: PQj ; // Initialize a priority queue for maintaining top-K paths
 10: for all Hit vjk ? H[j] do
 11: PG = DP (vjk, g∆ri );
 12: Calculate Pk : TopK paths in PG as a heap;
 13: Pk ? PQj ;
 14: Sorted Path Group Sj = PQj .HeapSort();
 15: end for
 16: end for
 17: TOPK = TopKCombinations({Sj});
 18: end for
 19: return TOPK;
 is computed for each keyword. |Q| lists are computed in
 total. T opKCombinations({Sj}) is an algorithm to quickly
 generate top-K combinations of paths from |Q| sorted path
 lists. The algorithm is based on the T opKCombinations
 algorithm proposed in [9]. This algorithm suggests an early
 termination strategy such that top-K combinations will be
 generated without enumerating all combinations of paths and
 has guaranteed O(K |Q|) time complexity. The time complexity
 of GeFree is O(|CR|(K |Q|+K|P KE| lg(K))), where CR is
 the set of candidate roots, and P KE is the set of all productive
 hits, K is the number of top-K interpretations generated.
 Because in the worst case, CR may contain all nodes in the
 graph and P KE is equal to a set of all possible hits, the
 worst case time complexity is O(n(K |Q|+Km lg(K))), where
 n = |V (GS)| and m = ∑ |hit(wi)| is the total number of
 hits. In comparison, the time complexity for graph exploration
 based algorithm as reported in CoSi [9] is O(nd∆K |Q|), where
 d is the degree of the graph.
 IV. EVALUATION
 We evaluated the scalability of the proposed multi-tenant query
 interpretation system (SKI) by comparing its performance with
 other systems (CoSi [9] and TKQ2S [11]). In this evaluation,
 we mainly focused on measuring the efficiency improvement
 and scalability in terms of concurrency because the effective-
 ness of the personalized techniques proposed in CoSi has been
 studied in our prior work [9].
 Datasets and Testbed: Two real-life datasets were employed:
 (a) DBPedia 3.6 (approx. 300 classes, 3K property types, 3.5
 million entities – 678 million triples) and (b) the “Billion Triple
 Challenge 2009” or BTC (1.19 billion triples). For the BTC,
 we created the schema graph from its dataset, which includes
 150232 classes (nodes) and around 1.3M properties (edges).
 For each experiment, we first initialized random weights
 and used the same set of initial weighs for all personalized
 algorithms to represent the bootstrapping context of user query
 history. The evaluation was conducted on a cluster of up to 30
 slave nodes, each of which is equipped with Intel dual core 2.0
 GHz CPU and 8GB RAM. All algorithms were implemented
 with C#, and all results were averaged over at least 5 trials.
 Tasks and Metrics: We designed two tasks to evaluate the
 performance of each approach. The first task measured the
 performance metrics such as execution time and DOTA for
 each query and dataset. We introduced DOTA or Degree Of
 Data/Appr. Graph PCM. KS-MAP DPI RABIT EstMaxU
 BTC-S 337 2.8 221.94 155.66 25.6 2857
 BTC-C/T 445 — — — — 17
 DBPedia-S 1.1 0.07 38.83 6.75 1.33 114286
 DBPedia-C/T 1.4 — — — — 5714
 TABLE I. MEMORY CONSUMPTION AND CONCURRENCY (MB) –
 SKI(S), COSI/TKQ2S(C/T)
 Term Ambiguity in our previous work [9] that characterizes
 the ambiguity of a query as a reflection of how much work
 (i.e. exploration search space) is needed for its interpretation.
 DOTA is defined as a function of the number of combinations
 of all keyword hits. The second tasks measured the latency of
 query responses while varying the MPL (Multi-Programming
 Level), i.e. the number of concurrent running programs.
 Queries: We employed 8 real life queries from Semantic
 Search Challenge 20114. The additional details can be found
 in the project page5.
 A. Task 1. Measuring Execution Time and DOTA.
 We recorded the execution time and DOTA of the 8 queries
 (Q1-Q8) in the query set. Fig. 4(a) shows that SKI generally
 outperformed CoSi for both datasets. Fig. 4(b) shows that
 the queries tend to have much higher DOTA in the BTC
 dataset than in DBPedia because all keywords appeared in
 both datasets but with varying frequencies because BTC was
 a superset of the DBPedia dataset. (Note that the queries are
 ordered by the DOTAs of queries over DBPedia, not BTC.) The
 performances of both CoSi and SKI dropped when interpreting
 the same queries on a dataset with larger scale since the
 degree of ambiguity increases for the same keyword query.
 For queries Q6 to Q8, the DOTA increased significantly; SKI
 performed much stable on both BTC and DBPedia while the
 performance of CoSi dropped considerably because the search
 space increased significantly with the growth of DOTA. In
 particular, the DOTA of Q2 over BTC was larger than the
 DOTAs of Q1 and Q3; therefore, the time spent by CoSi for
 Q2 on BTC was higher than that for Q1 and Q3.
 B. Task 2. Measuring Query Latency.
 In this task, we increased the MPL and measured the cor-
 responding response latency of concurrently running queries
 to compare the degree of concurrency. Due to the resource
 contention between multiple programs, the performance and
 memory consumption of the personalized interpretation al-
 gorithm impact the response latency of concurrently running
 queries for different users. Fig. 4(c) shows the impact of MPL
 on the latency (in milliseconds) for interpretation algorithms.
 Although all approaches experience increased latency as MPL
 increases, the rate of increase is higher with the exploration-
 based approaches than the index approach of SKI. This sug-
 gests that the concurrency overhead for CoSi and TKQ2S is
 higher than SKI. Note that SKI supports much higher multi-
 programming level while keeping low latency. CoSi or TKQ2S
 were not able to maintain reasonable latency for MPL greater
 than 5 due to high memory consumption and CPU overhead.
 As an explanation for this behavior, we report the amount of
 memory consumption of SKI, CoSi and TKQ2S in Table I for
 each user. Note that the reported memory usage of graph for
 graph exploration-based algorithms includes the overhead of
 4https://km.aifb.kit.edu/ws/semsearch11/
 5http://research.csc.ncsu.edu/coul/SKI/experiments.xlsx
 561
Fig. 4. (a) The execution time of the three approaches (SKI, CoSi, and TKQ2S) and (b) The DOTAs for the query set QS, (c) The latency of the three
 approaches while varying the MPL.
 SC5 SC10 SC15 SC20 SC25 SC30
 1000 54.67 78.88 89.24 107.57 150.37 166.39
 5000 81.52 129.69 178.43 229.45 278.85 265.45
 10000 80.45 152.59 215.47 268.22 307.46 353.27
 TABLE II. THROUGHPUT
 maintaining intermediate graph exploration states, e.g., paths
 explored and cursors generated. The column EstMaxU shows
 the estimation of the maximum number of user queries that can
 be running concurrently based on the memory consumption
 of each approach. In case of BTC dataset, each user in
 CoSi/TKQ2S requires around 445MB for maintaining graph
 exploration states associated with the summarized schema
 graph while the SKI consumes only 2.8MB for PCM and other
 indexes for each user. Thus, CoSi/TKQ2S can only support up
 to 17 users using 8GB memory, but SKI can handle up to 2857
 users concurrently.
 Additionally, we conducted experiments on the interpretation
 slave clusters (SC) with SKI deployed on them and report
 the throughput in Table II. Six groups of experiments were
 conducted with varying sizes of an interpretation cluster (up
 to 30 slave nodes) and different sizes of the query mixes (1k
 to 10k queries). The throughput (queries per second) of the
 larger cluster was higher.
 V. RELATED WORK
 For approaches that do not require graph exploration,
 BLINKS [3] proposes a similar idea to pre-compute i) node
 to keyword index and ii) keyword to node index. However,
 BLINKS employs a distinct root semantics where a root
 node identifies a unique sub-graph that connects all keywords
 by shortest paths. In this paper, we propose to save and
 manage richer path information (not only shortest path) and
 generate answers with Steiner tree semantics, which implies
 larger search space. In addition, BLINKS maintains mappings
 from keyword to lists of nodes ordered by costs; EASE [12]
 proposes EI-index, where ordered weighted r-radius graphs for
 pair wise tokens are pre-computed. However, adapting these
 kinds of indexes to personalized query interpretation such as
 CoSi [9] has some challenges. Because after every query, the
 classes and properties associated with the query, as well as
 related concepts and properties, will need to have their weights
 increased to reflect the impact of the query on the context
 model, the ordering of all indexes containing such classes and
 properties will need to be updated. For existing techniques
 this may require a complete rebuild after each query which is
 impractical.
 VI. CONCLUSION
 This paper presents an approach for “interpretive” search that
 scales well under high concurrent workloads. The approach
 achieves this by relying on a suitable set of indexes rather than
 expensive graph exploration based algorithms. Comprehensive
 evaluation over real world datasets showed very promising
 results.
 VII. ACKNOWLEDGMENT
 This work was partially funded by NSF grant IIS-0915865.
 REFERENCES
 [1] B. Aditya et al., “BANKS: Browsing and Keyword Searching in
 Relational Databases,” in Proc. VLDB, 2002, pp. 1083–1086.
 [2] S. Agrawal et al., “DBXplorer: Enabling Keyword Search over Rela-
 tional Databases,” in Proc. SIGMOD, 2002, p. 627.
 [3] H. He et al., “BLINKS: Ranked Keyword Searches on Graphs,” in Proc.
 SIGMOD, 2007, pp. 305–316.
 [4] V. Hristidis and Y. Papakonstantinou, “DISCOVER: Keyword Search
 in Relational Databases,” in Proc. VLDB, 2002, pp. 670–681.
 [5] Y. Luo et al., “SPARK: Top-k Keyword Query in Relational Databases,”
 in Proc. SIGMOD, 2007, pp. 115–126.
 [6] Y. Luo et al., “SPARK2: Top-k Keyword Query in Relational
 Databases,” IEEE Trans. Knowl. Data Eng., vol. 23, no. 12, pp. 1763–
 1780, 2011.
 [7] Z. Liu and Y. Chen, “Answering Keyword Queries on XML Using
 Materialized Views,” in Proc. ICDE, 2008, pp. 1501–1503.
 [8] Z. Liu and Y. Chen, “Identifying Meaningful Return Information for
 XML keyword Search,” in Proc. SIGMOD, 2007, pp. 329–340.
 [9] H. Fu and K. Anyanwu, “Effectively Interpreting Keyword Queries on
 RDF Databases with a Rear View,” in Proc. ISWC, 2011, pp. 193–208.
 [10] H. Fu et al., “CoSi: Context-Sensitive Keyword Query Interpretation
 on RDF databases,” in Proc. WWW, 2011, pp. 209–212.
 [11] T. Tran et al., “Top-k Exploration of Query Candidates for Efficient
 Keyword Search on Graph-Shaped (RDF) Data,” in Proc. ICDE, 2009,
 pp. 405–416.
 [12] G. Li et al., “EASE: An Effective 3-in-1 Keyword Search Method for
 Unstructured, Semi-structured and Structured Data,” in Proc. SIGMOD,
 2008, pp. 903–914.
 562

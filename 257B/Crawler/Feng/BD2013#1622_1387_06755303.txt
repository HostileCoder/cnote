A Clustering-based Collaborative Filtering Approach for Mashups 
Recommendation over Big Data
 Rong Hua,c Wanchun Dou Jianxun Liuc
 a State Key Laboratory for Novel 
Software Technology
 Nanjing University
 Nanjing, Jiangsu, China
 State Key Laboratory for Novel Software 
Technology
 Nanjing University,
 Nanjing, Jiangsu, China
 c Key Laboratory of Knowledge 
Processing & Networked Manufacturing 
Hunan University of Sci & Tec
 Xiangtan, Hunan, China
 ronghu@126.com douwc@nju.edu.cn ljx529@gmail.com
 AbstractÑSpurred by services computing and Web 2.0, more 
and more mashups are emerging on the Internet. The 
overwhelming mashups become too large to be effectively 
recommended by traditional methods. In view of this challenge, 
we propose a clustering-based collaborative filtering approach 
for mashup recommendation over big data. This approach 
mainly divided into two phases: clustering and collaborative 
filtering. By using clustering techniques, the data size is 
reduced so that the computation time of collaborative filtering 
algorithm is decreased significantly. Several experiments are 
done to verify the efficient of the proposed approach at the end 
of this paper.  
Keywords- clustering, collaborative filtering, mashup, API, 
tag. 
I. INTRODUCTION
 The Web has undergone a major change from a 
primarily publication platform towards a participatory and 
ÒprogrammableÓ platform, where numerous heterogeneous 
Web-delivered services are emerging, resulting in the 
creation of Web mashups [1]. The term ÒmashupÓ implies 
easy, fast integration, frequently using open application 
programming interfaces (API) and data sources to produce 
enriched results that were not necessarily the original reason 
for producing the raw source data [2]. Many companies and 
institutions provide various mashup solutions or re-label 
existing integration solutions as mashup tools. For example, 
HousingMaps (http://www.housingmaps.com) combines 
property listings from Craigslist ( http://www.craigslist.org/) 
with map data from Google Maps ( http://maps.google.com/) 
in order to assist people moving from one city to another 
and searching for housing offers [ 3 ]. More interesting 
mashups include Zillow (http://www.zillow.com/) and 
SkiBonk ( http://www.skibonk.com/).  
To speed up the mashup development process and to 
enable end users more easily to mash up their web 
applications, numerous mashup-specific development tools 
and frameworks have emerged, e.g., Google mashup Editor
 (http://code.google.com/gme/) provides a template-based 
environment for mashup development and Yahoo! Pipes
 (http://pipes.yahoo.com/pipes/) allows the mixing of 
popular data feeds to create data mashups via a visual editor.
 As the number of mashups increases continuously, how to 
create and find values from the mashup-oriented big data 
become an important research problem.  
Recommender systems (RSs) have been proved to be a 
valuable means for dealing with the information 
overwhelming problem by pointing a user towards new not-
 yet-experienced items that may be relevant to the active 
userÕs current task. ÒItemÓ is the general term used to denote 
what the RSs recommend to users and Òthe active userÓ is 
the user looking for suggestions. Collaborative filtering (CF) 
such as item- and user-based methods are the dominant 
techniques applied in RS algorithms [4]. Given an unknown 
rating of a target item by the active user to be estimated, 
user-based CF first measures rating similarities between the 
active user and other users, then predicts the unknown rating 
by averaging the (weighted) known ratings of the test item 
by rating similar users, while item-based CF first computes 
similarity between the target item and other items, then 
predicts the unknown rating by averaging the (weighted) 
known ratings of similar items by the active user. Since the 
relationships between items are relatively static, item-based 
algorithms may be able to provide the same quality as the 
user-based algorithms with less online computation. 
However, in terms of item-based CF for mashup 
recommendation over big data, the main challenges include: 
1) how to effectively process large quantities of service 
within acceptable processing time; and 2) how to improve 
the accuracy of CF algorithm. 
In view of these challenges, we propose a cluster-based 
collaborative filtering approach (CbCF), which consists of 
two phases: clustering and collaborative filtering. In fact, 
clustering is used as a preprocessing step to separate big 
data into manageable parts. Since the number of services in 
a cluster is much less than the total number of services, the 
computation time of CF algorithm can be reduced 
significantly. Besides, the ratings of similar services within 
a cluster are more relevant to each other than those of 
dissimilar services, therefore the recommendation accuracy 
based on usersÕ ratings may be enhanced. 
The rest of this paper is organized as follows. In Section 
II, CbCF approach is described in detail step by step. Then, 
a case is studied in Section III. In Section IV, several 
experiments are conducted on real world data which is 
extracted from Programmableweb.com. Related work is 
2013 IEEE 16th International Conference on Computational Science and Engineering
 978-0-7695-5096-1/13 $31.00 © 2013 IEEE
 DOI 10.1109/CSE.2013.123
 810
analyzed in Section V. At last, we draw some conclusions in 
Section VI.  
II. CBCF APPROACH
 CbCF approach operates on a user-cluster matrix instead 
of on the whole user-item matrix. It can be divided into four 
steps as follows. 
Step 1: Compute tag-based similarity and APIs-based 
similarity between mashups, which are then combined into a 
content similarity. 
Step 2: Propagate similarity according to mashups similarity 
network. Many mashups those are originally dissimilar 
establish similar relationship via propagation. 
Step 3: Cluster mashups based on their similarities. By 
using K-means cluster algorithm, mashups are partitioned 
into K clusters. 
Step 4: Recommend mashups according to their predicted 
ratings. A mashupÕs predicted rating is measured by the 
ratings of its neighbors in the same cluster. 
These steps are described in detail in the next 
subsections. 
A. Compute Content Similarity  
Since a mashup is labeled with some tags, the similarity 
between every two mashups can be measured according to 
the number of co-labeled tags [5]. That is to say, while two 
mashups are labeled with one or more than one same tag,
 they are considered similar.  
Although tags are a flexible way of categorizing data, 
they are prone to syntactic variations due to the amount of 
freedom users have, which results in different tags with the 
similar meanings. And, tagging is related to the use of 
synonyms and homonyms [????????? ].
 Therefore, more information is required for the similarity 
computation of mashups. As a mashup often uses several 
APIs to complete its functions, the co-used APIs also denote 
the similarity between mashups. 
Mathematically, tags-based similarity and APIs-based 
similarity are both computed by Jaccard similarity 
coefficient (JSC) which is a statistical measure of similarity 
between sample sets. For two sets, JSC is defined as the 
cardinality of their intersection divided by the cardinality of 
their union [6]. Concretely, tags-based similarity between 
two mashups is computed by formula (1) [7,8]:
 ,   = 	
 		 	        (1) 
Here,   and   are two mashups,   is a set of tags labeled 
to   and   are a set of tags labeled to  . 
Similarly, APIs-based similarity between two mashups 
is computed by formula (2):  
,   = 	
 		 	        (2) 
Here,   and   are two mashups,  is a set of APIs used 
by   and  is a set of APIs used by  .
 Content similarity between two mashups is computed by 
weighted sum of tags-based similarity and APIs-based 
similarity, which is expressed as follow: 
,   =  ? ,   + (1 ? ) ? ,  
 In this formula,   and   are two mashups,  ? [0,1]  is 
the weight of tags-based similarity and (1 ? )  is the 
weight of APIs-based similarity. The weights express 
relative importance between these two. For definiteness and 
without loss of generality, the weights are set to the same 
value in this paper, i.e.,  =  = 0.5. In this way, direct 
similarities between mashups are generated.  
B. Propagate Content Similarity 
For those mashups that have no direct content similarity 
relationships, a similarity propagation procedure based on 
structure information is performed to find more mappings.
 Similarity propagation can provide additional information 
and leads to extra information accessible for 
recommendation purposes and particularly relaxes the 
sparsity and the cold start problems.  
Since the content similarity relationship is symmetrical, 
we generate an undirected mashup graph  = (, ), with 
 represents a set of vertices and  represents a set of edges. 
Let  ?   denote a vertex labeled by the mashup  .
 Add an edge from a vertex  ?  to a vertex  ?  if 
they have a content similarity relationship.  
 Let   ,   ( ³ 1,  ³ 1,  > ) denote the kth 
possible path from a vertex  ?  to a vertex  ?  .
 Each path is defined as a sequence of vertices 
 , É  + , É ,  , where 0 <  ²  ?  . Let 
  ,   represents the length of   ,  . After 
set a propagation path length threshold L which is an integer 
greater than 1, the content similarity is propagated according 
to one of the three rules defined as follows: 
Rule 1: If there is only one path from vertex  to  , and 
the length of this path 1 <   ,   ²  , then the 
propagated content similarity between mashup   and 
 is the minimum content similarity of mashups labeled to 
adjacent vertices on this path multiply a path length decay 
factor , i.e.,  
 ,   = !" , +1, É , ?1,  # ? 
 where  = ?
 1  ,  +1
 
 . 
Here,   and  +1  are the first two adjacent vertices on
 the path from    to  ,   and  +1  are the last two 
adjacent vertices on the path from  to  ,  , +1
 is the content similarity between    and +1 ,
 811
?1,    is the content similarity between  ?1 and
  , 1  ,   is length of the only path from  to  . 
Rule 2: If there are n paths ($ > 1) from vertex  to  ,
 and for any path  (1 <  ² $) ,  1 <   ,   ²  , 
then the propagated content similarity between mashup 
 and   is the average value of the propagated content 
similarities via different paths, i.e., 
 ,   = · 
   ,  $=1
 $
 Here,   ,    is the propagated content similarity 
between two mashups   and   via the kth path from 
vertex  to  .
 Rule 3: If there is no path from vertex  to  , or for any 
path  (1 <  ² $)  from vertex  to  ,
   ,   > , then  ,   = 0.
 After similarity propagation, indirect similarities 
between mashups are obtained. A content similarity matrix 
S is then built. Each entry of S is the corresponding content 
similarity or propagated content similarity between two 
mashups. 
C. Cluster Mashups  
The goal of cluster analysis is that the objects within a 
group be similar to one another and different from the 
objects in other groups. The greater the similarity within a 
group and the greater difference between groups, the better 
or more distinct the clustering. Clustering methods can be 
classified into three categories: partitioning methods, 
density-based methods [9,10], and hierarchical methods [11,
 12  ]. A commonly-used partitioning method is K-means 
[13], which has two main advantages: relative efficiency 
and easy implementation. Therefore, we use K-means 
algorithm for mashups clustering. By this algorithm, the set 
of mashups M is divided into clusters 1, 2, É , & , where 
 ?  = * and  =  1 ? 2 É ? & .
 Many clustering algorithms make soft rather than hard 
assignments. With hard assignments, every mashup is a 
member of one and only one cluster. Soft assignments allow 
for degrees of membership and membership in multiple 
clusters. By k-means algorithm, mashups are clustered into 
one of k groups by iteratively reassigning each mashup to its 
nearest cluster. The distance of a mashup to a cluster is 
defined as the distance of that mashup to the centroid of the 
mashups currently assigned to that cluster. Choosing the 
proper initial centroids is the key step of this algorithm. A 
simple approach is to choose the initial centroids randomly, 
but the resulting clusters may be poor. Another commonly 
used technique is to perform multiple runs, each with a 
different set of randomly chosen initial centroids, and select 
the set of clusters with the minimum sum of the square error.
 We use a basic k-means algorithm as shown in Figure 1 to 
cluster mashups.
 Input: A target number of clusters k, a set of mashups M, a 
content similarity matrix S.
 Output: An assignment of mashups to clusters. The 
assignment is represented as a mapping from each 
mashup to a particular cluster  ( = 1, 2, É , ). 
1: Select k points as initial centroids.
 2: Repeat
 3: Form k clusters by assigning each mashup to its closest 
centroid.
 4:  Re-computes the centroid of each cluster.
 5: until Centroid do not change.
 Figure 1. k-means algorithm for mashups clustering 
To improve the efficiency of CbCF approach, clustering 
algorithm is implemented offline.  
D. Recommend Mashups  
(1) Compute rating similarity 
Common ÒsimilarityÓ measures include the Pearson 
correlation coefficient (PCC) [14] and the cosine similarity 
between ratings vectors. PCC was found to perform better 
than cosine vector similarity [15]. Therefore, PCC is applied 
in this paper. Based on a PCC formula proposed in [16], the 
rating similarity between the target mashup /  and a 
mashup   is computed as follow:   
3 /,   = · 4,/ ? 4/ ? 4, ? 4 6?7 / 
7 
 8 · 4,/ ? 4/26?7 / 
7  ? 8 · 4, ? 4 26?7 / 
7 
 (3)
 Here, 7/ is a set of users who rated /  while 7 is a set of 
users who rated  , 6  is a user who both rated /  and 
 , 4,/  is the rating that 6  gave to / , 4,  is the rating that 
6  gave to  , 4  is the average rating of mashup  , and 4
 is the average rating of mashup  . It should be noted that 
there is a precondition before compute the rating similarity 
between every two services in CbCF. That is, service :/  and 
:  must both belong to a same cluster / , i.e., :/ ? / ; : ?
 / . Furthermore, it should be noted that if the denominator of 
formula (4) is zero, we make 0.  
(2) Select neighbors 
Neighborhood has an impact on the recommendation 
quality [ 17 ]. In traditional IbCF approach [ 18 ], the 
neighbors of a mashup  are determined according to 
constraint formula (4).  
!()  =  "   	 3 /,   > ,  ­   # (4) 
Here, :,   is the rating similarity between mashup 
  and mashup  . It means that a mashup whose similarity 
exceeds a pre-defined threshold can be chosen as a neighbor 
of a distinct mashup. All the neighbors of the mashup 
 are put into the neighbor set !(). 
812
(3) Compute Predicted Rating 
The predicted rating is typically an average of ratings of
 training mashups that have been determined to be similar to 
the test mashup, weighted according to the degree of 
similarity between the training and test mashups. As 
proposed by Zheng et al. [19], the predicted rating of a
 mashup is computed by Eq. (5): 
(6A , / )  =  4?/ +
 ·
  4A, ? 4?  ? 3 /,    ?!( / )
 ·
  3 /,    ?!( / ) (5) 
Here, 6A  is a active user who is looking for suggestions, /  is a 
target mashup which is relevant to the current task of the active 
user, 4?/  is the average rating of mashup / , !(/ ) is the 
neighbor set of the mashup / , and 3 /,   is the rating 
similarity between / and   which is computed by Eq. (3).  
If the predicted rating of a mashup exceeds a threshold,
 it will be recommended to the active user. 
III. A CASE STUDY
 As shown in Table I, seven 
mashups (1, 2, 3, 4, 5, 6 and 7)  and the 
corresponding APIs and tags are listed. For example, 
mashup 1  is labeled by ÒnewsÓ, ÒphotoÓ, ÒsearchÓ and 
ÒsemanticÓ. Mashup 2  is labeled by ÒautoÓ, ÒcarsÓ,
 ÒsearchÓ, and ÒshoppingÓ. That is to say, there is one same 
tag (i.e., ÒsearchÓ) among the total seven tags. Therefore, 
the tags-based similarity between 1 and 2 is computed as 
(1, 2) = |1
2||12| =
 1
 7
 . And since there is only one API 
used by both 1 and 2, the APIs-based similarity between 
1 and 2 is computed as (1, 2) = |1
2||12| = 1. If the 
weight of tags-based similarity  is set to 0.5, the content 
similarity between 1 and 2is computed as (1, 2) =
  ? (1, 2) + (1 ? ) ? (1, 2) = 0.57. 
TABLE I. CASE OF MAHSUPS
 No. Mashup Name Used API Labeled Tags
 m1 SemanticSearcher.com Google Base news, photo, 
search, semantic
 m2 AllAutoSites Google Base auto, cars, search, shopping
 m3 360 Cities Google Maps 3d, mapping, panoramic, photo
 m4
 Anthems on Google 
Maps
 Google Maps + 
YouTube
 anthems, 
deadpool, 
mapping, video
 m5 Audio Disco MTV + YouTube deadpool, music, video
 m6 Favmvs
 Google Search + 
MTV
 deadpool, MTV, 
music, video
 m7 MTV Billboard charts 411Sync mobile, MTV, music, WAP
 Content similarities between every pair of mashups are 
computed in the same way and the results are put into Table 
II.
 TABLE II. CONTENT SIMILARITY MATRIX
 m1 m2 m3 m4 m5 m6 m7
 m1 / 0.571 0.071 0 0 0 0
 m2 0.571 / 0 0 0 0 0
 m3 0.071 0 / 0.321 0 0 0
 m4 0 0 0.321 / 0.367 0.167 0
 m5 0 0 0 0.367 / 0.542 0.083
 m6 0 0 0 0.167 0.542 / 0.167
 m7 0 0 0 0 0.083 0.167 /
 Each mashup is represented by a vertex. Every content 
similarity relationship is represented by an undirected edge. 
And the weight of each edge is the related content similarity. 
We then get an undirected weighted graph as Figure 2. 
Figure 2. Content similarity propagation 
Based on the three rules in Section II-B, the 
propagations of mashupsÕ content similarities are discussed 
in three cases as follows (L=4): 
(1) There are only one path from 1  to 4  (i.e., 1 ?
 3 ? 4 ), and 1 , 4  = 2 , which meets Rule
 1.Therefore, the propagated content similarity between 1
 and 4 is computed as: 
(1, 4) = !{(1, 3), (3, 4)} ?  = 0.053 ,
 where  = ? 1 , 4 +1
 
 =
 3
 4
 .  
Similarly, (2, 3) = 0.053? (2, 4) = 0.036 ?
 (2, 5) = 0.018?(2, 6) = 0.018.
 (2) There are three paths from 1  to 5  (i.e., 1 ? 3 ?
 4 ? 5 , 1 ? 3 ? 4 ? 6 ? 5 and 1 ?
 3 ? 4 ? 6 ? 7 ? 5 ). Their lengths are  
11 , 5  = 3 , 21 , 5  = 4  and 31 , 5  = 5 ,
 respectively. According to Rule 2, the propagated content 
similarity betweem 1 and 5 is computed as: 
m3
 m1
 m4 m5
 m2 m6
 m7
 0.571
 0.071
 0.321 0.367 0.083
 0.1670.167
 0.542
 813
(1, 5) = 
 1( 1, 5)+2( 1, 5)
 2
 = 0.027,
 where 
1(1, 5) = !{(1, 3), (3, 4), (4, 5)} ?  , 
 = ?
 1 1 , 5 +1
 
 =
 2
 4
 , and 
2(1, 5) =
 !{(1, 3), (3, 4), (4, 6), (6, 5)} ? , 
 = ?
 2 1 , 5 +1
 
 =
 1
 4
 . 
Similarly, (1, 6) = 0.027 ? (1, 7) = 0.018 ?
 (3, 5) = 0.162 ? (3, 6) = 0.143 ?
 (3, 7) = 0.056?(4, 7) = 0.090.
 (3) There are three paths from 2 to 7  (i.e., 2 ? 1 ?
 3 ? 4 ? 5 ? 7?2 ? 1 ? 3 ? 4 ?
 6 ? 7  and 2 ? 1 ? 3 ? 4 ? 6 ? 5 ?
 5 ), but 12 , 7  = 5 , 22 , 7  = 5 and 
32 , 7  = 6. That is to say, the lengths of these paths 
are all greater than the propagation path length threshold .
 Therefore, according to Rule 3, (2, 7) = 0. 
The content similarity or propagated content similarity 
between   and   is put into Table III. 
TABLE III. CONTENT SIMILARITY MATRIX
 m1 m2 m3 m4 m5 m6 m7
 m1 / 0.571 0.071 0.053 0.027 0.027 0.018
 m2 0.571 / 0.053 0.036 0.018 0.018 0
 m3 0.071 0.053 / 0.321 0.162 0.143 0.056
 m4 0.053 0.036 0.321 / 0.367 0.167 0.090
 m5 0.027 0.018 0.162 0.367 / 0.542 0.083
 m6 0.027 0.018 0.143 0.167 0.542 / 0.167
 m7 0.018 0 0.056 0.090 0.083 0.167 /
 Based on k-means algorithm (k=2), these mashups are 
partitioned into two clusters, i.e., 1  and 2  are clustered 
into a group (C1) and 3, 4, 5, 6 A$H 7 are clustered 
into a group (C2).
 Suppose there are four users have rated these mashups 
(as shown in Table IV). The ratings are on 5-point scales 
and 0 means the user did not rate the mashup. As 63 does 
not rate 5 , 63 is regarded as an active user and 5  is 
looked as a target mashup in this case. Furthermore, in order 
to verify this method, 3  is selected as a reference target 
mashup for 63. The target cluster is 2. 
TABLE IV. USER-MASHUP RATING MATRIX
 C1 C2
 m1 m2 m3 m4 m5 m6 m7
 u1 5 4 4 3 3 1 0
 u2 1 1 4 5 4 4 2
 u3 4 4 2 2 0 2 3
 u4 5 4 5 5 5 1 5
 Using formula (3), rating similarity between 5 and every 
other mashup in the target cluster are calculated and listed in 
Table V. And rating similarity and enhanced rating 
similarity between 3 and every other mashup in the target 
cluster are calculated and listed in Table VI. 
TABLE V. RATING SIMILARITIES WITH 5
 Mashup pair Rating similarity
 (3, 5) 0.544
 (4, 5) 0.736
 (6, 5) 0
 (7, 5) 0.781
 TABLE VI. RATING SIMILARITIES AND ENHANCED RATING 
SIMILARITIES WITH 3
 Mashup pair Rating similarity
 (3, 4) 0.839
 (3, 5) 0.544
 (3, 6) -0.187
 (3, 7) 0.499
 The Pearson correlation coefficient ranges in value from -1
 to +1, indicating perfect negative correlation at -1, absence 
of correlation at zero, and perfect positive correlation at +1.  
If set the rating similarity threshold I to 0.4, !(5) ={3, 4, 7} and !(3) = {4, 5, 7}. 
Then,  (463,5 )  = 2.25 and  (463,3 )  = 2
 As for 5, if the recommending threshold is set to 3, it 
will not be recommended to 63 . In addition, as the real 
rating of 3 by 63 is 2 (see Table IV), it can be inferred that 
CbCF has good prediction accuracy.  
IV. EXPERIMENTS AND ANALYSIS
 A. Experiment Context 
1) Experiment Platform 
All Experiments are implemented using Visual C++ 6.0 
running on an HP desktop with an Intel Core i3 3.20GHz 
CPU, 2GB RAM, and Windows 7 (32bit) operating system. 
2) Experiment Dataset 
814
Numerous of mashups, APIs and user profiles are listed 
on ProgrammableWeb.com and the number is still on the 
increase [20]. Up to November 2012, 6,226 mashups and 
related information are crawled from this site. The mashups 
are labeled with 20,936 tags among which 3,520 tags are 
different. And, 12,200 APIs are used by these mashups 
among which 4,506 APIs are different.  
Since rating is a new feature appearing on 
ProgrammableWeb.com, there are very few ratings 
available. Therefore, we generate pseudorandom integer 
integers in the range 0 to 5 as the ratings of mashups. Each 
rating represents an entry in a $ ?  user-mashup matrix. 
Assume there are 500 users that have rated some mashups 
published on the website. Then the matrix consists of 500 
rows and 6226 columns. In total, 50,000 non-zero ratings 
are generated. The sparsity level of the dataset is 98.39%
 (sparsity level =1-50000/500*6226=0.9839).
 We add an empirical evaluation based on a well known 
statistical test, namely the k-fold cross validation [21]. The 
ratings records are split into k mutually exclusive subsets 
(the folds) M1, M2, É, Mk of equal size. During each fold 
 ? {1, 2, É , }, it is tested on Mi and trained on the rest. The 
cross-validation process is then repeated k times, with each 
of the k subsets used exactly once as the validation data. We 
generate five distinct splits of training and test data from the 
rating dataset, i.e., k=5. For each data split, 80% of the 
original set was included in the training data and 20% of it 
was included in the test data. The test sets in all cases were 
disjoint [22].  
3) Cluster of Dataset 
Using K-means algorithm, the mashups are partitioned 
into K clusters. Set the values of K to 3 and 4 respectively, 
the centroids and the number of mashups in each cluster are 
shown in Table VII and Table VIII.  
TABLE VII. THE CENTROID AND THE NUMBER OF MASHUPS OF EACH
 CLUSTER (K=3) 
Direct path Centroid1 Centroid2 Centroid30.886 0.752 0.701
 Number of mashups 857 2482 1799
 2-path Centroid1 Centroid2 Centroid30.756 0.529 0.567
 Number of mashups 5179 9425 9912
 3-path Centroid1 Centroid2 Centroid30.758 0.361 0.546
 Number of mashups 5253 748521 20289
 TABLE VIII. THE CENTROID AND THE NUMBER OF MASHUPS OF EACH
 CLUSTER (K=4) 
Direct path Centroid1 Centroid2 Centroid3 Centroid40.886 0.752 0.713 0.700
 Number of mashups 857 2482 100 1699
 2-path Centroid1 Centroid2 Centroid3 Centroid40.756 0.614 0.563 0.529
 Number of mashups 5177 776 9239 9324
 3-path Centroid1 Centroid2 Centroid3 Centroid40.756 0.526 0.567 0.361
 Number of mashups 5180 10371 9992 748520
 The value of k in k-means algorithm has effect on the 
computation time. A big value of k may reduce the scale of 
cluster and thus decrease the computation time. However, if 
k is set to a big value optionally, the value of centroids is 
very close, which would exclude the possibility of some 
mashups to be the neighbors of the target mashup. As a 
trade-off, we set the value of k to 3.  
B. Experiment Evaluation 
For each test mashup in each fold, its predicted rating is 
calculated based on IbCF and CbCF approach separately.
 We then use a widely popular statistical accuracy metric 
named Mean Absolute Error (MAE), which is a measure of 
the deviation of recommendations from their true user-
 specified ratings [23].  
The threshold   in formula (4) affects the number of 
neighbors. We create a threshold vector with values ranging 
from 0 to 0.1 (with a step size of 0.01) and compute the 
predicted rating of each mashup in the test data. The average 
MAE of the test mashups in each fold is calculated and 
listed in the Figure 3. It can be observed that CbCF yields 
lower MAE values than IbCF, which means CbCF may 
generate better recommendation to users than IbCF. 
Figure 3. Comparison of MAE  
0 0.02 0.04 0.06 0.08 0.1
 0.65
 0.7
 0.75
 Alpha
 M
 AE
 Fold 1
 0 0.02 0.04 0.06 0.08 0.1
 0.65
 0.7
 0.75
 Alpha
 M
 AE
 Fold 2
 0 0.02 0.04 0.06 0.08 0.1
 0.65
 0.7
 0.75
 Alpha
 M
 AE
 Fold 3
 0 0.02 0.04 0.06 0.08 0.1
 0.65
 0.7
 0.75
 Alpha
 M
 AE
 Fold 4
 0 0.02 0.04 0.06 0.08 0.1
 0.65
 0.7
 0.75
 Alpha
 M
 AE
 Fold 5
 IbCF
 CbCF
 815
To verify the efficiency of CbCF, the computation time 
of predicted ratings is compared with the computation time 
of IbCF. It can be seen from Figure 4, CbCF spends less 
time than IbCF because the number of neighbors of a 
mashup is smaller in most cases. 
Figure 4. Comparison of run time 
V. RELATED WORK AND COMPARATIVE ANALYSIS
 Clustering methods for CF have been extensively 
studied by some researchers. Ungar et al. [24] clustered
 users and items separately using variations of k-means and 
Gibbs sampling. Users are clustered based on the items they 
rated and items are clustered based on the users that rated 
them. Users can be re-clustered based on the number of 
items they rated, and items can be re-clustered based on the 
number of users rated them. Each user is assigned to a class 
with a degree of membership proportional to the similarity 
between the user and the mean of the class. The 
performance of CF on synthetic data is good, but not good 
on real data. OÕConnor et al. [25] and Sarwar et al. [23] used 
clustering techniques to partition the data into clusters and 
use a memory-based CF algorithm to make predictions for 
CF tasks within each cluster. Xue et al. [26] combined the 
strengths of memory-based CF approaches and model-based 
CF approaches in order to enable recommendation by 
groups of closely related individuals. By using the rating 
information from a group of closely related users, unrated 
items of the individual user in a group can be predicted. 
Quan et al. [27] proposed a CF algorithm that adds item 
clusters to a user-based approach. When predicting an 
itemÕs rating for a user, it looks at similar users with respect 
to ratings within that item's cluster. There are also other 
studies on clustering techniques for collaborative filtering 
[28,29,30,31]. In the above research work, users and items 
are clustered using the rating data while the additional 
information, for example the relationship between items, is 
ignored. Additional information has been proved to be very 
useful in certain application domains [ 32 ]. In proposed 
CbCF approach, mashups are first clustered based on the 
synthetic similarity distance, which is the weighted sum of 
tags-based similarity and APIs-based similarity. All 
mashups are partitioned into k clusters and a traditional CF 
algorithm is operated on a user-cluster matrix instead of on
 the whole user-item matrix. By reducing the dimensions of 
user-item rating matrix and excluding some irrelevant 
mashups, this approach can improve the quality of 
recommendation and the online performance of CF 
algorithms. 
VI. CONCLUSION
 Big data require new forms of processing to enable 
enhanced decision making, insight discovery and process
 optimization. We present a cluster-based collaborative 
filtering approach for mashups recommendation over big 
data. Besides using ratings data, similar relationship 
between mashups is proposed to identify their 
neighborhoods. A clustering technique is applied to find the 
groups of similar mashups. After that, a traditional CF 
algorithm is used to generate the recommendations. The 
results of experiments with real-world data set show that the 
CbCF approach outperforms traditional item-based 
collaborative filtering algorithms in terms of MAE and the 
computation time of predicted ratings. 
ACKNOWLEDGMENT
 This work was supported by Natural Science Fund of 
China under Grant No. 61021062, 61073032, Scientific and 
Technical Support Program of Jiangsu Province under Grant 
No. BK2011171, Excellent Youth Found of Hunan Scientific 
Committee of China under Grant No. 11JJ1011. 
REFERENCES
 [ 1 ] Liu X Z, Huang G, Zhao Q, et al., ÒiMashup: a mashup-based 
framework for service composition,Ó Science China Information 
Sciences, 2013, pp. 1-20. 
[ 2 ] Agarwal S and Nath A, ÒA study on implementing Green IT in 
Enterprise 2.0,Ó International Journal of Advanced Computer 
Research, 2013, 3(1), pp. 43-49. 
[3] Jin Y, Boualem B, Fabio C, and Florian D, ÒUnderstanding mashup 
0 0.02 0.04 0.06 0.08 0.1
 200
 300
 400
 Alpha
 Ti
 m
 e(s
 )
 Fold 1
 0 0.02 0.04 0.06 0.08 0.1
 0
 200
 400
 Alpha
 Ti
 m
 e(s
 )
 Fold 2
 0 0.02 0.04 0.06 0.08 0.1
 200
 300
 400
 Alpha
 Ti
 m
 e(s
 )
 Fold 3
 0 0.02 0.04 0.06 0.08 0.1
 200
 300
 400
 Alpha
 Ti
 m
 e(s
 )
 Fold 4
 0 0.02 0.04 0.06 0.08 0.1
 0
 200
 400
 Alpha
 Ti
 m
 e(s
 )
 Fold 5
 IbCF
 CbCF
 816
Development and its Differences with Traditional Integration,Ó
 Internet Computing, 2008, 12(5), pp. 44-52. 
[ 4 ] Tso-Sutter K H L, Marinho L B, Schmidt-Thieme L. Tag-aware 
recommender systems by fusion of collaborative filtering 
algorithms[C]//Proceedings of the 2008 ACM symposium on Applied 
computing. ACM, 2008: 1995-1999. 
[5] Giannakidou E, Koutsonikola V, Vakali A, et al., ÒCo-clustering tags 
and social data sources,Ó in proceedings of the Ninth International 
Conference on Web-Age Information Management  (WAIM'08), 2008,
 pp. 317-324. 
[6] Bank J and Cole B, ÒCalculating the jaccard similarity coefficient with 
map reduce for entity pairs in Wikipedia,Ó.Wikipedia Similarity Team, 
2008. 
[ 7 ] Yusef H M and Vctor H S, ÒImproving Tag-Clouds as Visual 
Information Retrieval Interfaces,Ó in proceedings of International 
Conference on Multidisciplinary Information Sciences and  
Technologies, 2006.
 [ 8 ] Toine B and Antal van den B, ÒCollaborative and Content-based 
Filtering for Item Recommendation on Social Bookmarking Websites.
 In Proceedings of ACM RecSys Õ09 Workshop on Recommender 
Systems and the Social Web, 2009. 
[9] Ester M, Kriegel H P, Sander J, and Xu X, ÒA density-based algorithm 
for discovering clusters in large spatial databases with noise,Ó in
 Proceedings of the International Conference on Knowledge Discovery 
and Data Mining (KDD Õ96), 1996.
 [10] Ankerst M,  Breunig M M, Kriegel H P, and Sander J, ÒOPTICS: 
ordering points to identify the clustering structure,Ó in proceedings of 
ACM SIGMOD Conference, 1999, pp. 49-60. 
[11] Han J and Kamber M,  ÒData Mining: Conce pts and Techniques,Ó
 Morgan Kaufmann, San Francisco, Calif, USA, 2001. 
[12]Su X, Kubat M, Tapia M A, and Hu C, ÒQuerysize estimation using 
clustering techniques,Ó in proceedings of the 17th International 
Conference on Tools with Artificial Intelligence (ICTAIÕ05), 2005, pp. 
185-189. 
[13] Tapas K, David M M, Nathan S N, et al., ÒAn Efficient k-Means 
Clustering Algorithm: Analysis and Implementation,Ó IEEE 
Transactions on Pattern Analysis and Machine Intelligence, 2002, 
24(7), pp. 881-892. 
[ 14 ] Resnick P, Iakovou N, Sushak M, Bergstrom P, and Riedl J,
 ÒGroupLens: An Open Architecture for Collaborative Filtering of 
Netnews,Ó in proceedings of Computer Supported Cooperative Work 
Conference, 1994. 
[15] Herlocker J, Konstan J A, and Riedl J, ÒAn empirical analysis of 
design choices in neighborhood-based collaborative filtering 
algorithms,Ó Information retrieval, 2002, 5(4), pp. 287-310. 
[ 16 ] Zheng Z, Ma H, Lyu M R, et al., ÒQoS-aware Web service 
recommendation by collaborative filtering,Ó IEEE Transactions on 
Services Computing, 2011, 4(2), pp. 140-152. 
[17] Pham M C, Cao Y, Klamma R, et al., ÒA clustering approach for 
collaborative filtering recommendation using social network analysis,Ó
 Journal of Universal Computer Science, 2011, 17(4), pp.  583-604. 
[18] Herlocker J, Konstan J A, Riedl J, ÒAn empirical analysis of design 
choices in neighborhood-based collaborative filtering algorithms,Ó
 Information retrieval, 2002, 5(4), pp. 287-310. 
[ 19 ] Zheng Z, Ma H, Lyu M R, et al., ÒQoS-aware Web service 
recommendation by collaborative filtering,Ó IEEE Transactions on 
Services Computing, 2011, 4(2): 140-152. 
[20] Wang J, Chen H, Zhang Y, ÒMining user behavior pattern in mashup 
community,Ó in proceedings of IEEE International Conference on 
Information Reuse & Integration, 2009, pp. 126-131.  
[21] Kohavi R, "A Study of Cross-Validation and Bootstrap for Accuracy 
Estimation and Model Selection,Ó in proceedings of International 
Joint Conference on Artificial Intelligence (IJCAI), 1995. 
[22] Acilar A M and Arslan A, ÒA collaborative filtering method based on 
artificial immune network,Ó Expert Systems with Applications, 2009, 
36, pp. 8324-8332. 
[23] Sarwar B M, Karypis G, Konstan J A, and Riedl J, ÒRecommender 
systems for large-scale E-commerce: scalable neighborhood 
formation using clustering,Ó in proceedings of the 5th International 
Conference on Computer and Information Technology (ICCIT Õ02), 
2002,1. 
[24] Ungar L H and Foster D P, ÒClustering methods for collaborative 
filtering,Ó in proceedings of the Workshop on Recommendation 
Systems, AAAI Press, 1998. 
[25] OÕConnor M  and Herlocker J, ÒClustering items for collaborative 
filtering,Ó in proceedings of the ACM SIGIR Workshop on 
Recommender Systems (SIGIR Õ99) , 1999. 
[26] Xue G R, Lin C, Yang Q, et al., ÒScalable collaborative filtering using 
cluster-based smoothing,Ó in proceedings of the 28th annual 
international ACM SIGIR conference on Research and development 
in information retrieval,  2005, pp. 114-121. 
[ 27 ] Quan T K, Fuyuki I, Shinichi H, ÒImproving accuracy of 
recommender system by clustering items based on stability of user 
similarity,Ó in proceedings of International Conference on 
Computational Intelligence for Modeling Control and Automation and 
International Conference on Intelligent Agents Web Technologies and 
International Commerce, 2006, pp. 61-61. 
[28] George T and Merugu S, ÒA scalable collaborative filtering framework 
based on co-clustering,Ó in proceedings of the Fifth IEEE 
International Conference on Data Mining (ICDM Õ05), 2005, pp. 625-
 628. 
[29 ] Truong K, Ishikawa F, and Honiden S, ÒImproving accuracy of 
recommender system by item clustering,Ó IEICE - Trans. Inf. Syst., 
E90-D(9), 2007, pp. 1363-1373. 
[30] Rashid A M, Shyong, Karypis G, and Riedl J, ÒClustknn: A highly 
scalable hybrid model- & memory-based cf algorithm,Ó In WebKDD, 
2006. 
[31] Zhang M and Hurley N, ÒNovel item recommendation by user profile 
partitioning,Ó in proceedings of the 2009 IEEE/WIC/ACM 
International Joint Conference on Web Intelligence and Intelligent 
Agent Technology, 2009, pp. 508-515. 
[32] Manh C P, Yiwei C, Ralf K, Matthias J, ÒA Clustering Approach for 
Collaborative Filtering Recommendation Using Social Network 
Analysis,Ó Journal of Universal Computer Science, 2011, 17(4), pp. 
583-604. 
817

Correlation-based Performance Analysis for Full-System MapReduce Optimization
 Qi Guo1, Yan Li1, Tao Liu1, Kun Wang1, Guancheng Chen1, Xiaoming Bao2, Wentao Tang2
 1. IBM Research - China, Beijing, China
 2. IBM Systems & Technology Group, Beijing, China
 Abstract—Big Data is changing this world at a surprising
 speed, and MapReduce plays a critical role in finding insights
 in Big Data. However, to efficiently extract insights from Big
 Data, performance optimization of MapReduce applications is
 a challenging task. To facilitate the full-system optimization
 of MapReduce applications, we propose a correlation-based
 performance analysis approach to efficiently identify critical
 outliers. The basic intuition is that critical outliers are key
 to the overall performance and they can only be accurately
 identified by correlating different phases, tasks and resources.
 Based on the proposed approach, we further implement a
 correlation-based performance analysis tool, called SONATA. It
 can efficiently identify critical outliers, and then, recommend
 optimization suggestions for practitioners based on embedded
 rules. Since the performance overhead is key to the applicability
 of a performance tool, we conduct experiments to demonstrate
 that SONATA is a practical tool with less than 5% overhead and
 good scalability. To demonstrate the effectiveness of SONATA,
 we share several cases during the performance tuning of IBM
 Platform SymphonyTMwith the help of SONATA.
 Keywords-Big Data, MapReduce, Performance Analysis, Op-
 timization
 I. INTRODUCTION
 The ever-increasing volumes of data pose significant
 challenges on distributed computing technology. As one
 of the most popular programming paradigms to address
 these challenges, MapReduce [1] has been widely used in
 industry. Actually, following Google’s MapReduce, there
 also exists many varieties, such as, Apache Hadoop [2],
 Cloudera Enterprise [3], IBM InfoSphere BigInsightsTM [4],
 IBM Platform SymphonyTM [5], and so on.
 Despite various underlying implementations of the
 MapReduce runtime, the performance optimization of
 MapReduce applications is still a great challenge since
 there exists numerous adjustable parameters in MapReduce
 framework. For example, Hadoop MapReduce contains more
 than 150 configuration parameters that may have non-
 trivial impacts on the performance. To make the tuning of
 MapReduce framework tractable, several approaches have
 been proposed to automatically determine the near-optimal
 MapReduce parameters with small overheads [6], [7].
 In addition to the parameters of MapReduce framework,
 the performance of MapReduce execution is also heavily
 determined by configurations from other layers, such as,
 JVM, operating system, virtual machine and hardware. Thus,
 it is expected that a fine-grained full-system co-optimization
 technique could have potential to significantly improve the
 performance. As an illustrative example, during the prac-
 tice of tuning Terasort to be executed on a 10-node IBM
 PowerLinuxTM7R2 cluster, we found that the performance
 improvement comes from different sources. In more detail,
 about 30% and 20% performance improvements stem from
 system software level (JVM, Garbage Collection (GC) and
 Huge Page, etc.) tuning and underlying hardware level
 tuning (Turbo mode, SMT (Simultaneous multithreading),
 and hardware prefetching, etc.), respectively. Nevertheless,
 due to the involvement of many parameters from different
 layers, the tuning process is much more complicated and
 always requires specialized expertise.
 To ease the burden of full-system optimization, in this
 paper, we propose a novel full-system MapReduce opti-
 mization approach based on correlation-based performance
 analysis. The intuition is that the overall performance of
 MapReduce applications is always determined by the critical
 outliers, for example, the slowest map or reduce tasks that
 determine the overall execution. Thus, identifying critical
 outliers is most important for performance optimization
 of MapReduce applications. Actually, the critical outliers
 can only be accurately detected by correlating different
 subphases (of a task), tasks (e.g., map/reduce tasks) and
 resources (e.g., CPU, Memory and Disk). In other words, it
 is necessary to conduct correlation-based analysis to identify
 the real performance issues. Once performance issues are
 identified, some empirical rules could be applied to guide
 the practitioners to optimize the overall performance.
 Based on the proposed correlation-based analysis ap-
 proach, we implement a full-system MapReduce perfor-
 mance analysis tool, which is called as SONATA, on IB-
 M Platform SymphonyTM. Apparently, to make such an
 instrumentation-based performance tool viable, the addition-
 al performance overhead should be low enough. To evaluate
 the performance overhead of SONATA, we conduct exper-
 iments with several widely-used MapReduce benchmarks.
 The experimental results show that SONATA only causes less
 than 5% additional performance overhead for the evaluat-
 ed benchmarks, which well demonstrates the efficiency of
 SONATA.
 To demonstrate the effectiveness of SONATA, we present
 several cases during the tuning of Terasort on IB-
 M PowerLinuxTM7R2 systems running IBM Platform
 SymphonyTM. Our experiences show that correlation-based
 performance analysis is indeed very effective and efficient
 2013 IEEE International Conference on Big Data
 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 753
for full-system performance optimization, and we manage to
 sort 1TB data in 7 minutes on 10-node PowerLinuxTM7R2
 systems running IBM Platform SymphonyTM.
 The rest of the paper proceeds as follows. Section II
 introduces background of MapReduce programming model
 and the importance of critical outliers for performance
 tuning. Section III elaborates the proposed correlation-based
 performance analysis. Section IV details the implementation
 of SONATA. Section V evaluates the performance overhead
 and scalability of SONATA. Section VI presents several
 case studies of SONATA. Section VII compares SONATA
 with other MapReduce optimization approaches. Finally,
 section VIII concludes this paper.
 II. BACKGROUND
 A. MapReduce Background
 The MapReduce programming paradigm executes a job in
 two phases: Map and Reduce. In the map phase, several map
 tasks are executed in parallel to process the corresponding
 input data. After all map tasks finished, the intermediate
 results are transferred to the reduce tasks for further pro-
 cessing. The map and reduce tasks communicate with a
 master node which dominates the MapReduce process in the
 centralized control system. In a typical implementation of
 MapReduce runtime, such as, Apache Hadoop, the primary
 controlling component on the master node is referred as Job
 Tracker (JT) while the secondary controlling threads on slave
 nodes are referred as Task Trackers (TT). The functions of
 the map and reduce tasks are usually defined and rely on
 the particular missions required by applications. Under this
 framework, the map and reduce tasks can implement massive
 data parallelism for large-scale data processing .
 B. Critical Outliers
 The execution of one MapReduce job consists of several
 parallel tasks, and outlier tasks refer to the tasks that take
 longer to finish than other similar tasks. In addition to these
 task-level outliers, there also exists outliers in different level,
 such as phase-level and resource-level outliers. The phase-
 level outliers are phases that dominate the entire execution of
 a task. The resource-level outliers are abnormal behaviors of
 resource usages, for example, CPU is not fully utilized, disk
 access is much slower than that of other nodes, and memory
 usage is over-used compared with that of other nodes.
 Although outliers exhibit different behaviors compared
 with their counterparts, they may not have significant im-
 pacts on the overall performance. For example, the outlier
 tasks do not lie in the critical path will not postpone all sub-
 sequent execution. Therefore, only critical outliers should
 be identified for performance diagnosis and optimization.
 However, as the size of clusters (e.g., a cluster contains
 more than 2000 nodes has been deployed at Taobao [8])
 and the parallelism of jobs (i.e., tens of thousands of tasks
 are executed in parallel) continue to grow, the identifica-
 tion of critical outliers is extremely complicated and time-
 consuming for engineers in industrial practice.
 III. CORRELATION-BASED PERFORMANCE ANALYSIS
 In this section, we introduce the proposed correlation-
 based performance analysis approach to effectively and
 efficiently identify critical outliers.
 The basic intuition of correlation-based performance anal-
 ysis is to correlate different phases, tasks and resources to
 effectively and efficiently identify critical outliers. As shown
 in Figure 1, all those correlations can be roughly grouped in-
 to 4 classes, that is, phase-phase correlations, task-task cor-
 relations, task-resource correlations, and resource-resource
 correlations.
 Figure 1. All correlations can be grouped into four types, that is, phase-
 phase, task-task, task-resource and resource-resource correlations.
 A. Phase-Phase Correlations
 The correlation between different phases is utilized to
 identify the critical phases of one task’s execution. This
 correlation is especially useful for the optimization of
 MapReduce runtime engine. By dividing the execution of
 one task into different important subphases, and then instru-
 menting them during execution, the designers of MapReduce
 engines can easily identify the critical paths for potential
 optimization. For example, during the performance opti-
 mization of a MapReduce engine, when encountering long-
 tail reduce tasks, multi-threaded processing is a potential
 solution to accelerate these tasks. In addition to the design-
 ers of MapReduce engines, the end-users can also benefit
 from such correlations. For example, they can quantitatively
 compare the execution details of different implementations
 of user-defined routines, and then improve the efficiency
 accordingly.
 B. Task-Task Correlations
 The correlations between tasks mainly target to identify
 the slow map or reduce tasks. Although the identification
 of slow tasks is straightforward, it is non-trivial to reveal
 the underlying reasons of such phenomenons. To offer more
 insights of such slow tasks, we further consider two types
 of task-task correlations, that is, inter-node and intra-node
 task-task correlations.
 754
During the performance analysis, inter-node correlations
 help to easily identify the slow tasks among all nodes. Once
 a number of tasks are recognized as the slow tasks, we
 should further verify whether such tasks are on the same
 node. Once such tasks are executed on different nodes, we
 should mainly examine the data input size of each task to
 determine unbalanced data partition. Otherwise, we should
 check the system configuration of the node where the slow
 tasks are running. Moreover, for an abnormal node, the intra-
 node correlations can identify the slow tasks running on
 it. Since the system configuration is the same for all tasks
 on this node, the unbalanced data partition and scheduling
 issues (e.g., inappropriate setting of task slots) could be
 considered as the main reasons for the slow tasks.
 C. Task-Resource Correlations
 Once critical outliers of tasks are identified, the correla-
 tions between such tasks and the corresponding resources are
 used to identify the critical outliers of resources, e.g., the low
 usage of resources that may impair the overall performance.
 As shown in Figure 2, task 1469 is first identified as a
 critical outlier since it delays the overall execution of the
 map phase. Since task 1469 is executed on node JUNO2, we
 correlate it with the resources on node JUNO2. With respect
 to the CPU utilization, we can see that during the execution
 of task 1469, the CPU utilization drops significantly. To
 improve the CPU utilization in the transition from the map
 tasks to the reduce tasks, we could start the shuffle phases
 earlier or schedule more reduce tasks to this node to occupy
 the CPU.
 D. Resource-Resource Correlations
 The correlations between different resources are also
 useful for performance diagnosis and optimization. Similar
 to task-task correlations, there are two kinds of resource-
 resource correlations, that is, inter-node and intra-node
 correlations.
 The inter-node correlations are conducted by correlating
 the same resources (e.g., network) between different nodes.
 As a result, the abnormal resource usage can be easily
 identified. For example, a slow network adapter may delay
 the shuffle phases to all nodes. In this case, the critical tasks
 are not easy to identify since the shuffle phases of all tasks
 are postponed, this performance issue can only be detected
 by conducting inter-node resource-resource correlation.
 The intra-node correlation is conducted by correlating
 different resources on the same node, and potential perfor-
 mance optimization might be adopted accordingly. As shown
 in Figure 3, the total network bandwidth is about 1.3GB/s,
 while the corresponding CPU usage is more than 95%. In
 this case, since two 10GbE network cards are used for inter-
 node communication, the main bottleneck is the CPU usage.
 Actually, the large data transfer rate also aggravates the load
 of CPU usage. To ease this burden, RDMA (Remote Direct
 Figure 2. The correlation between task and resource to identify tasks or
 phases that are bottlenecks.
 Memory Access) would be a very promising technique to
 release CPU from costly data copy operations [9] to further
 improve the overall performance.
 Figure 3. The correlation of resource and resource to identify potential
 performance optimization.
 E. Workflow of correlation-based performance analysis
 Based on four types of correlations, Algorithm 1 shows
 the basic workflow to conduct correlation-based perfor-
 mance analysis to effectively and efficiently identify crit-
 ical outliers. The first step is to utilize inter-node task-
 task correlations to identify a task set T contains critical
 outliers of tasks. Then, intra-node task-task correlations can
 755
determine whether all tasks or a subset of tasks in T run
 on the same node. For each task ti in task set T , task-
 resource correlations should be leveraged to identify whether
 the corresponding resources are outliers. Besides, phase-
 phase correlations can be used to determine the most time-
 consuming phases for each outlier task. Even when there
 is no critical task, in other words, all tasks are completed
 very close, inter-node resource-resource correlations could
 be used to diagnose whether resource issues exist. Moreover,
 intra-node resource-resource correlations could offer more
 insights to determine potential performance optimization.
 Finally, phase-phase correlations on the task with averaged
 statistics could provide suggestions for the optimization of
 user-defined routines or runtime engines.
 Algorithm 1: Workflow of correlation-based perfor-
 mance analysis
 T = critical outliers of tasks determined by inter-node task-task
 correlations;
 intra-node task-task correlations to determine whether subsets
 Ts ? T are on the same node;
 for ti ? T do
 task-resource correlations on ti to find outliers of resources;
 phase-phase correlations on ti to find critical outliers of phases;
 end
 inter-node resource-resource correlations to diagnose resource issues;
 intra-node resource-resource correlations to determine potential
 optimization;
 phase-phase correlations on tavg find critical outliers of phases;
 IV. SONATA: FULL-SYSTEM MAPREDUCE
 PERFORMANCE ANALYZER
 Based on the proposed correlation-based performance
 analysis, we further implement SONATA, a full-system per-
 formance analyzer that first effectively and efficiently detects
 critical outliers, and then recommends suggestions for po-
 tential optimization. In this section, we elaborate the detailed
 implementation of SONATA.
 A. Framework
 SONATA consists of four main phases, that is, data collec-
 tion, data loading, performance visualization and optimiza-
 tion recommendation, which can be illustrated in Figure 4. In
 data collection, two types of runtime statistics, i.e., execution
 details and resource usage, on each node are collected, and
 then they are aggregated at a specified master node. In
 data loading, a data loader on the master node periodically
 retrieves data from aggregated statistics and writes them
 into the database. In performance visualization, the collect-
 ed runtime statistics are obtained from the database and
 displayed in a correlated mode via web-based performance
 visualizer. Meanwhile, when end-users using the visualizer
 to get a deep understanding of the MapReduce’s behaviors,
 the potential critical outliers could be efficiently identified
 and optimization suggestions are recommended by optimiza-
 tion recommendation. In the following sections, we present
 details of each phases.
 Figure 4. The framework of SONATA for full-system MapReduce perfor-
 mance analysis.
 B. Data Collection
 As shown in Figure 5, there are two main components,
 that is, monitor and aggregator, for data collection.
 The monitor is distributed on each node for gathering both
 the execution details of programs and hardware resource
 usage information. The basic idea to collect the execution
 details of programs is based on the MapReduce’s counters,
 which are useful mechanism to gather execution statistics
 and supported by most MapReduce implementations (e.g.,
 Apache Hadoop and IBM Platform SymphonyTM). In more
 detail, we define several new counters for the interested
 execution phases, for example, the potential critical phases
 of MapReduce execution, and then add specific codes on
 the source codes of runtime engine to update the corre-
 sponding counters during execution. On the other hand,
 to collect the hardware resource usage information, such
 as CPU/memory usage, disk and network bandwidth, we
 leverage a lightweight performance monitoring tool, i.e.,
 Ganglia [10]. More specifically, the monitor first reads the
 raw data from Ganglia and then generates organized data for
 processing by the aggregator.
 Figure 5. The components for data collection.
 The aggregator on the master node periodically collects
 the data from the monitors on all slave nodes. Then, the data
 are organized in XML format and written into a history file
 in order. Figure 6 illustrates an XML item of the collected
 execution details in the history file. To avoid a large size of
 the history files, the aggregator also periodically flushes the
 data in current history files into out-of-date files.
 756
Figure 6. The counter MAP COMPLETE TIME is stored in XML format
 for Task 2 in Job 24002.
 C. Data Loading
 Since the data in current history file are temporary and it is
 non-trivial to efficiently manipulate file data for performance
 analysis, we consider to store the collected data into the
 database, which is implemented by the data loader. The
 data loader, which runs as a daemon on the master node,
 periodically reads the history file and writes the XML items
 into the database. The data loader can remember the last
 reading offset of the history file to avoid writing duplicate
 records into the database.
 In addition to loading the history file into the database,
 the data loader also intercepts the data generated by the
 aggregator, and then the intercepted information is also
 written into the database. More specifically, once the data
 loader is invoked at a scheduled time, a new record, along
 with the current timestamp, will also be inserted into the
 database. Apparently, such dynamically updated information
 of running jobs is the basis of on-the-fly visualization and
 monitoring.
 D. Performance Visualization
 Currently, in performance visualization, four analysis
 views can be generated to present more insights of the
 execution of programs. The first one is the overall view,
 where the execution timelines of all map and reduce tasks
 are depicted. Thus, the users can easily observe information
 like how many tasks were running at any time point and
 how many map/reduce task waves. The second one is the
 resource view, where the CPU/memory usage, disk and net-
 work bandwidth are depicted. The users can easily identify
 abnormal usage curves of specific nodes from this view.
 The third one is the breakdown view of the execution of
 one specific map or reduce task, and the users can easily
 identify the critical phases of one task. Actually, the above
 three views are correlated to facilitate the identification of
 critical outliers. For example, once a task in the overall view
 is selected, both the tasks running on the same node in the
 overall view and the resource usage curves in the resource
 view will be highlighted. Besides, the breakdown view will
 also display the execution details of the selected task. The
 fourth view is a table to list the detailed execution statistics
 of the entire jobs, for example, the status of the job, the
 average execution time of all map tasks, and the HDFS
 written/read bytes, etc.
 E. Optimization Recommendation
 Figure 7. An illustrate example to automatically diagnose the hardware
 configurations.
 Although end-users could only rely on the visualizer to
 conduct performance analysis, it would be very helpful to
 automatically determine critical outliers and thus provide
 optimization recommendations for inexperience users. Thus,
 in the optimization engine, we embedded several empirical
 rules to recommend tuning guidelines for potential perfor-
 mance improvements. Figure 7 shows an illustrative example
 to automatically diagnose the hardware configurations and
 present the optimization recommendations. In this example,
 by conducting task-task correlation, it can be easily found
 that only tasks on node JUNO2 (e.g., task 2584) are the crit-
 ical outliers, which indicates that there exists performance
 issues on JUNO2. To identify the root cause of this observa-
 tion, we further perform task-resource and resource-resource
 correlations. We found that on JUNO2, the CPU usage is
 overutilized (> 99%) but the network access is underutilized
 (the maximal disk write speed is only 170MB/s) compared
 with other nodes, which implies that CPU of JUNO2 is
 one of critical outliers of this execution. Thus, SONATA
 recommends to check the CPU-related configurations, for
 example, the SMT setting, hardware prefetching, and CPU
 757
frequency. Finally, we identify that this problem is caused
 by setting the SMT as 2 on JUNO2 while on other nodes the
 SMT is set as 4. Actually, such domain knowledge could be
 easily summarized as an empirical rule as shown in Figure 8.
 Figure 8. The example in Figure 7 can be summarized as an empirical
 rule for automatic optimization recommendation.
 V. PERFORMANCE EVALUATION
 In this section, we evaluate the performance overhead and
 scalability of SONATA, which has been deployed on IBM
 Platform SymphonyTM.
 A. Experimental Methodology
 The MapReduce benchmarks for evaluation are widely-
 used Terasort and Wordcount, and the corresponding data
 sets are 1TB data from TeraGen and 210GB data from
 Bible.txt, respectively. The cluster we used for evaluation
 consists of up to 10 POWER7TMR2 servers, each contains 2
 POWER7TMchips, and each chip has 32 hardware threads.
 The detailed information of the evaluated cluster is listed in
 Table I.
 Table I
 EVALUATED IBM POWER7 CLUSTERS
 Cluster 10 PowerLinuxTM7R2 Servers
 CPU 16 processor cores per server (160 total)
 Memory 128GB per server (1280GB total)
 Internal 6 600GB internal SAS drives
 Storage per server (36TB total)
 Expansion 24 600GB SAS drives in IBM EXP24S SFF
 Storage Gen2-bay Drawer per server(144TB total)
 Network 2 10GBe connections per server
 Switch BNT BLADE RackSwitch G8264
 The performance of MapReduce benchmarks is measured
 by the end-to-end execution time of a job. To resist stochastic
 effects, we run each benchmark five times and only the
 minimal value is treated as the measured performance. By
 comparing the performance of benchmarks running on the
 original IBM Platform SymphonyTMand the version attached
 with SONATA, we can quantify the additional performance
 overhead caused by SONATA.
 B. Runtime Overheads
 Figure 9 shows the performance overhead of SONATA
 (with respect to the original IBM Platform SymphonyTM)
 on 5- and 10-node clusters. The first observation is that the
 additional overhead caused by SONATA is very low for the
 evaluated benchmarks. For example, even for benchmark
 Terasort running on the 10-node cluster, the performance
 overhead is only 3.5%. The second observation is that
 the additional performance overhead may vary for different
 benchmarks. For benchmark Wordcount, the performance
 overhead is only 1.5% when running on 10-node cluster.
 The third observation is that even increasing the number of
 nodes, the performance overhead does not increase signifi-
 cantly for the evaluated benchmarks.
 Figure 9. The execution overhead of SONATA compared with the original
 execution time without instrumentation.
 The instrumented counters are collected for each task,
 and the number of tasks typically increases along with the
 data sizes. For example, when sorting 512GB data, the
 number of map tasks is only 1432, while sorting 1.5TB data,
 the number of map tasks increases to 2796. Therefore, the
 performance overhead may vary given different number of
 tasks. To evaluate the scalability of SONATA, we measure
 the performance of Terasort given 512GB, 1TB, and 1.5TB
 data. As shown in Figure 10, we can see that given different
 number of tasks, the performance overhead does not vary
 significantly, and the maximal execution overhead is still less
 than 5%. Thus, the results from our test cases suggest that
 SONATA will a practical tool when deployed in commercial
 environments.
 Figure 10. The scalability of SONATA for different size of tasks.
 VI. CASE STUDIES
 In this section, we share our experiences on tuning the
 full-system performance of Terasort running against IBM
 Platform SymphonyTMon a IBM POWER7TMcluster with the
 help of SONATA. During the tuning process, the performance
 issues we encountered can be roughly grouped into three
 758
main categories, that is, system problems, inappropriate
 runtime configurations, and runtime inefficiency.
 A. Diagnosing System Problems
 In addition to the example shown in Figure 7 where
 SONATA can efficiently facilitate the identification of inap-
 propriate configurations of CPU, e.g., SMT, prefetching and
 frequency setting, we further present an example to illustrate
 that an abnormal disk behavior can also be detected by
 SONATA. As shown in Figure 11, several reduce tasks on
 JUNO1 are critical outliers that postpone the overall execu-
 tion time. By correlating the tasks with the disk resources of
 the corresponding nodes, we found that the total disk loads
 on JUNO1 (< 1.72GB/s) are much more heavier than that
 of other nodes (e.g., < 864MB/s on node JUNO9).
 Figure 11. Diagnosing disk problem via SONATA. Disk loads of node
 JUNO1 are much more heavier than that of other nodes (e.g., node JUNO9).
 By examining the operations during the execution of this
 job, we finally identify that other users have run a disk
 benchmark on JUNO1 in the same time slot since this cluster
 was shared by several teams. After that, we obtain exclusive
 access to this cluster to avoid inferences from other teams.
 B. Tuning Runtime Configurations
 Similar to Hadoop MapReduce, there also exists many
 adjustable configurations in IBM Platform SymphonyTMfor
 MapReduce practitioners. Here we list three cases to illus-
 trate how SONATA can help optimize the runtime configu-
 rations.
 The first tuning case is about the buffer usage in map
 tasks, as shown in Figure 12. In this example, it can be easily
 identified the critical tasks, such as task 402, by conducting
 task-task correlation. Actually, during the execution of this
 task, we can see that multiple sorts and spills (i.e., 2 sorts and
 spills) occur. Due to multiple sorts and spills, the resultant
 overhead is more than 49% of the overall map time. By
 identifying that the overhead of sorts and spills is larger than
 a predefined threshold (e.g., 20%), SONATA recommends to
 adjust the buffer-related parameters, for example, increase
 the parameter such as io.sort.mb. By increasing this param-
 eter, multiple sorts and spills could be completely eliminated
 and the overall performance is improved accordingly.
 Figure 12. By correlating phase-phase correlation for critical tasks,
 potential optimization could be recommended accordingly. In this example,
 there are multiple costly sorts and spills in the critical map task. They could
 be eliminated by enlarging the parameter such as io.sort.mb.
 The second example is about the tuning of another critical
 parameter, slowstart, for IBM Platform SymphonyTM, which
 determines the fraction of the number of map tasks that
 should be complete before reduce tasks are scheduled. As
 shown in Figure 13 (the bottom figure), by correlating
 different phases in a critical reduce task, the WaitInFetch
 phase is most time-consuming in shuffle, which occupies
 more than 99% of the overall execution time of the reduce
 task. Moreover, by correlating tasks with the resource usage
 (the middle figure), the corresponding CPU is not 100%
 fully utilized (i.e., 89%) in shuffle. Thus, we could consider
 to increase the value of slowstart to release more reduce
 slots to map slots before the completion of all map tasks.
 The third tuning case is about the selection of appropriate
 compressors for data output. When sorting 1TB data via
 Terasort, by conducting phase-phase correlation for one
 critical map task, the shuffle phase, which transfers data
 from the map tasks to the reduce tasks, dominates the
 entire execution time (e.g., more than 70%). It is intuitive
 to compress the data to reduce the IO (disk and network)
 overheads, which is well supported by most MapReduce
 runtime engines. Since the efficiencies of different compres-
 sors vary significantly, SONATA can efficiently compare the
 performance of different compressors and recommend the
 best for further usage. By comparing the performance of
 DefaultCodec (zip) and Lz4Codec compressors, we found
 that the overall execution time of Default compressor is
 about 1.5 times longer than that of Lz4 compressor. Besides,
 759
Figure 13. By correlating the slow reduce tasks with their subphases,
 the WaitInFetch phase is more than 99% of the overall execution time.
 Besides, the CPU is not fully utilized during the shuffle phase. Thus, we
 could increase slowstart to let more map tasks scheduled for execution.
 by utilizing Lz4 compressor, the shuffle ratio (with respect
 to the execution time of reduce tasks) can be reduced by
 118%.
 C. Improving Runtime Efficiency
 In addition to the MapReduce practitioners, the perfor-
 mance tool is also very helpful for the runtime designers
 to optimize the implementation of MapReduce engines. In
 Apache Hadoop, the map and the reduce slots can only
 be used for the map and the reduce tasks, respectively. In
 IBM Platform SymphonyTM, the default configuration also
 follows this guideline that slots cannot be shared by the map
 and the reduce tasks. Figure 14 shows the execution behav-
 iors of Terasort when setting 32 map slots and 32 reduce
 slots on each PowerLinuxTM7R2 node (which contains 64
 hardware threads). Apparently, we can see that during the
 first wave of map execution, the CPU utilization is relatively
 low on this cluster, e.g., the CPU utilization of node JUNO0
 is less than 37%.
 To improve the utilization of CPU, IBM Platform
 SymphonyTMoffers generic slots that can be shared between
 the map tasks and the reduce tasks. Figure 15 shows that
 given the same MapReduce parameters, by sharing slots
 between the map and the reduce tasks, the CPU utilization
 can be significantly improved to more than 50% for node
 JUNO0. In this case, the generic slots can improve the
 overall performance by 12.99% compared with the separated
 Figure 14. Terasort running with separated map and reduce task slots.
 slots (485s vs 422s).
 Figure 15. Terasort running with shared map and reduce task slots.
 VII. RELATED WORK
 There exists a number of research efforts to propose
 performance analysis approaches and tools for efficient
 MapReduce processing.
 Mantri is a system that monitors tasks and culls outliers
 using cause- and resource-aware techniques [11]. Although
 Mantri also focused on optimizing outliers, it can only ad-
 dress task-level outliers. In contrast, SONATA facilitates the
 full-system performance optimization by identifying phase-
 level, task-level and resource-level outliers.
 HiTune is a scalable, lightweight and extensible perfor-
 mance analyzer for Hadoop MapReduce based on dataflow-
 based performance analysis [12]. Although HiTune also
 shows the execution details of tasks and resource usages as
 SONATA does, it cannot directly provide the correlations a-
 mong phases, tasks, and resources, which are most important
 760
to efficiently identify critical outliers. Moreover, SONATA
 can monitor the behaviors of a job’s execution on-the-fly
 to facilitate online optimization, while HiTune only offers
 post-execution performance analysis.
 Starfish is a self-tuning system that enables Hadoop
 users to get good performance automatically without deep
 understanding of many tuning knobs available [7]. One of
 the key features of Starfish is that it can automatically set the
 near-optimal parameters for Hadoop jobs. However, Starfish
 mainly focuses on the tuning of the parameters of Hadoop
 framework, which cannot be used to optimize the runtime
 engines and diagnose hardware problems. In other words,
 Starfish cannot facilitate full-system optimization.
 Theia [13] is a visualization tool to generate visual signa-
 tures of each job’s performance by analyzing the application-
 level logs of a Hadoop cluster. The visual signatures facil-
 itate troubleshooting the performance problems (e.g., hard-
 ware failure, data skew, and software bugs, etc.) on the
 cluster. However, Theia cannot tackle with misconfigurations
 of runtime engine and underlying hardware.
 Hadoop vaidya is a rule-based performance diagnostic
 tool for MapReduce jobs [14]. Although it can provide
 recommendations based on the analysis of runtime statistics,
 it cannot facilitate full-system optimization. PerfXPlain is
 a system enables users to ask questions about the relative
 performances (i.e., runtimes) of pairs of MapReduce job-
 s [15]. However, this system mainly focuses on explaining
 the performance similarity or difference between pairs of
 MapReduce job or task executions.
 In summary, current approaches mainly target to the
 optimization of MapReduce parameters, which cannot be
 helpful for full-system optimization. In contrast, by cor-
 relating different phases, tasks and resources, SONATA is
 able to efficiently identify critical outliers for full-system
 optimization. Besides, the monitoring capability of SONATA
 can facilitate the online optimization of MapReduce appli-
 cations.
 VIII. CONCLUSION
 In this paper, we present a correlation-based performance
 analysis approach to efficiently identify the critical outliers
 for full-system MapReduce optimization. The key intuition
 is that the critical outliers can only be accurately and
 efficiently identified by correlating different phases, tasks,
 and hardware resources. Once critical outliers are identified,
 several empirical optimization rules can be deployed to tame
 them. The proposed approach is further implemented as a
 tool, named as SONATA. SONATA has been deployed on IBM
 Platform SymphonyTM, and experimental results show that
 it only causes less than 5% performance overhead for more
 than 2000 tasks on a 10-node PowerLinuxTM7R2 system.
 Finally, with the help of SONATA, we manage to sort 1TB
 data in 7 minutes on this system running IBM Platform
 SymphonyTM.
 ACKNOWLEDGEMENT
 We would like to thank H. Peter Hofstee and Jian Li for
 sharing the cluster with us. We also would like to thank Zane
 Hu, Alicia Chin and Yonggang Hu for providing product
 supports.
 REFERENCES
 [1] J. Dean and S. Ghemawat, “Mapreduce: simplified data
 processing on large clusters,” in Proceedings of OSDI’04,
 2004, pp. 137–150.
 [2] “Apache hadoop,” http://hadoop.apache.org/.
 [3] “Cloudera enterprise core,” http://www.cloudera.com/content/
 cloudera/en/products/cloudera-enterprise-core.html.
 [4] “Ibm infosphere biginsights,” http://www-01.ibm.com/
 software/data/infosphere/biginsights/.
 [5] “Ibm platform symphony,” http://www-03.ibm.com/systems/
 technicalcomputing/platformcomputing/products/symphony/.
 [6] K. Kambatla, A. Pathak, and H. Pucha, “Towards optimizing
 hadoop provisioning in the cloud,” in Proceedings of Hot-
 Cloud’09, 2009.
 [7] H. Herodotou, H. Lim, G. Luo, N. Borisov, L. Dong, F. B.
 Cetin, and S. Babu, “Starfish: A self-tuning system for big
 data analytics,” in Proceedings of CIDR’11, 2011, pp. 261–
 272.
 [8] Z. Ren, X. Xu, J. Wan, W. Shi, and M. Zhou, “Workload
 characterization on a production hadoop cluster: A case study
 on taobao,” in Proceedings of IISWC’12, 2012, pp. 3–13.
 [9] P. W. Frey and G. Alonso, “Minimizing the hidden cost of
 rdma,” in Proceedings of ICDCS ’09, 2009, pp. 553–560.
 [10] “Ganglia monitoring system,” http://ganglia.sourceforge.net.
 [11] G. Ananthanarayanan, S. Kandula, A. Greenberg, I. Stoica,
 Y. Lu, B. Saha, and E. Harris, “Reining in the outliers in map-
 reduce clusters using mantri,” in Proceedings of OSDI’10,
 2010, pp. 1–16.
 [12] J. Dai, J. Huang, S. Huang, B. Huang, and Y. Liu, “Hitune:
 dataflow-based performance analysis for big data cloud,” in
 Proceedings of ATC’11, 2011, pp. 7–20.
 [13] E. Garduno, S. P. Kavulya, J. Tan, R. Gandhi, and
 P. Narasimhan, “Theia: visual signatures for problem diag-
 nosis in large hadoop clusters,” in Proceedings of LISA’12,
 2012, pp. 33–42.
 [14] “Hadoop vaidya,” http://hadoop.apache.org/docs/stable/vaidya.html.
 [15] N. Khoussainova, M. Balazinska, and D. Suciu, “Perfxplain:
 debugging mapreduce job performance,” Proc. VLDB Endow.,
 vol. 5, no. 7, pp. 598–609, 2012.
 761

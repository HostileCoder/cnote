An NML-based Model Selection Criterion for General Relational Data Modeling
 Yoshiki Sakai, Kenji Yamanishi
 Graduate School of Information Science and Technology
 The University of Tokyo
 Tokyo, Japan
 Email: {yoshiki sakai, yamanishi}@mist.i.u-tokyo.ac.jp
 Abstract—Whereas the main interest in most existing data
 mining approaches has been sequence data on a single type of
 object, namely attribute data, real-world databases store infor-
 mation about multiple relationships between various classes of
 objects. The modeling of these general relational data (GRD)
 plays an important role in eliciting knowledge across multiple
 relations. It is not reasonable to directly apply existing modeling
 methods to GRD, because GRD have statistical properties that
 distinguish them from attribute data. In this paper, we address
 the issue of statistical model selection in GRD modeling. From
 the viewpoint of the minimum description length principle,
 we propose a new model selection criterion by considering
 the statistical properties of GRD. We employ the normalized
 maximum likelihood code-length as a model selection criterion,
 and provide an asymptotic expansion theorem for its applica-
 tion to GRD modeling. To demonstrate its use in a critical
 application, we apply our proposed criterion to the issue of
 model selection in relational data clustering. An experiment
 using artificial datasets demonstrates the effectiveness of our
 technique compared to other criteria, and we also present a
 brand analysis using real beer-purchase data.
 Keywords-relational data; model selection; normalized max-
 imum likelihood code-length; stochastic block model
 I. INTRODUCTION
 We are concerned with the modeling of general relational
 data. In the fields of statistics, machine learning, and data
 mining, interest is typically focused on sequence data for
 a single type of object, known as attribute data. How-
 ever, real-world databases store information about multiple
 relationships between various classes of objects. Let us
 consider the example of a purchase records database (Fig.
 1). We are interested in two classes of objects: customers
 and items. The database may not only store the profiles of
 customers or attributes of items, but also data about the
 purchase relationship between customers and items. This
 may sometimes include information regarding friendships
 among customers. Data that consist of multiple tables on
 multiple classes of objects are called general relational data
 (GRD) [1]. A certain customer profile may be strongly
 correlated with certain attributes of the items they buy.
 Customers or items could be divided into a number of
 clusters to help us understand the purchase behavior of the
 global customer base. The mining of GRD aims to reveal
 such hidden patterns, eliciting knowledge across multiple
 tables.
 g1006 g1007 g69
 g18313g18313g18313
 g18445
 g39g286g374g282g286g396 g68g258g367g286 g38g286g373g258g367g286 g38g286g373g258g367g286 g18313g18313g18313 g68g258g367g286
 g4g336g286 g1009g1008 g1006g1009 g1008g1010 g18313g18313g18313 g1011g1004
 g94g258g367g286g400 g1009g1004g1004 g1005g1006g1004 g1010g1012g1004 g18313g18313g18313 g1006g1004g1004
 g89g437g258g367g349g410g455 g44g349g336g346 g62g381g449 g68g349g282g282g367g286 g18313g18313g18313 g44g349g336g346
 g87g396g349g272g286 g1006g1004 g1005g1004 g1005g1009 g18313g18313g18313 g1005g1012
 g68g258g374g437g296g856 g4 g17 g4 g18313g18313g18313 g127
 g1005 g1006 g68g1007 g18313g18313g18313
 g1006 g1007
 g69
 g18313g18313g18313
 g18445
 g68
 g1006
 g1005
 g18313
 g18313
 g18313
 g17791 g16226 g18441 g17791
 g16226 g17791 g18441 g18314
 g17791 g18441 g17791 g16226
 g18313g18313g18313
 g18313
 g18313
 g18313
 g1006 g1007 g69
 g18313g18313g18313
 g18445
 g1006
 g1007
 g69
 g18313
 g18313
 g18313
 g18445
 g18441 g16226 g17791 g17791
 g16226 g18441 g17791 g18313g18313g18313 g16226
 g17791 g17791 g18441 g16226
 g17791 g16226 g16226 g18441
 g18313
 g18313
 g18313
 Figure 1. Example of general relational data
 Statistical modeling plays an important role in GRD min-
 ing. The central issue in GRD modeling is the selection of
 an appropriate statistical model. Statistical model selection
 determines the optimal probabilistic model for representing
 the given data from a set of candidate models. Simple
 models with few degrees of freedom may fail to capture the
 nature of the data, whereas complex models with many de-
 grees of freedom may overfit the data, losing their structural
 significance. To justify the statistical meaning of the selected
 probabilistic model, it is important to balance this trade-off.
 The problem of model selection arises frequently in several
 practical data mining tasks, such as structure learning in
 Bayesian networks (e.g., [2]), determination of the number
 of clusters in clustering (e.g., [3]), and rank determination
 in matrix factorization (e.g., [4]).
 The purpose of this paper is to propose a new model
 selection method that considers the statistical properties of
 GRD models, and to empirically demonstrate its advantages
 over other possible criteria.
 In attribute data modeling, many criteria for model selec-
 tion have been established. Akaike [5] proposed Akaike’s in-
 formation criterion (AIC), which is founded on information
 entropy; later, Schwarz [6] stated the Bayesian information
 criterion (BIC) on the basis of Bayesian statistics. Following
 these, a number of other criteria have been proposed. Some
 criteria, such as BIC, have been common ways to deal with
 the model selection of relational data (e.g., [7]). However,
 2013 IEEE International Conference on Big Data
 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 421
few model selection criteria suitable for GRD modeling have
 been studied. Most existing criteria have been derived under
 the condition that the central limit theorem (CLT) holds
 uniformly over the attribute data. Such criteria cannot be
 directly applied to GRD models, because GRD are a mixture
 of attribute and relational data of various sizes, meaning that
 the CLT does not hold uniformly over them. It is therefore
 important to develop criteria that account for the variously
 sized data and asymptotic properties of GRD.
 In this paper, we consider model selection on the basis
 of the minimum description length (MDL) principle [8],
 which offers a general model selection framework from
 an information-theoretical approach. The underlying idea
 of the MDL is that learning can be equated with data
 compression. MDL theory asserts that the best model is that
 which minimizes the total code-length required to encode
 the given data and the model itself.
 In applying MDL-based model selection, the normal-
 ized maximum likelihood (NML) code-length, which is also
 known as the stochastic complexity, can be the most ef-
 fective criterion [9]. NML is justified in the sense that it
 achieves Shtarkov’s minimax criterion [10], and it has been
 reported that NML-based model selection is effective in a
 wide range of learning issues, such as clustering [3], [11],
 denoising [12], and Bayesian network structure learning [2],
 [13]. The drawback of NML is that its naı¨ve computation
 is generally hard, requiring a number of summations over
 the whole domain of all variables. This can increase the
 computational complexity of GRD modeling explosively.
 In terms of attribute data modeling, Rissanen [9] provided
 an asymptotic expansion theorem for NML, and this enables
 us to calculate the approximate NML. However, Rissanen’s
 theorem does not apply to GRD modeling, and its asymptotic
 expression is not appropriate for GRD models. Thus, we
 propose a new theorem, based on that of Rissanen, to
 approximate NML for GRD models.
 The significance of this paper can be summarized as
 follows:
 1) A new NML-based criterion suitable for GRD model-
 ing: We propose a new criterion suitable for GRD modeling
 on the basis of NML. As GRD are a mixture of various-sized
 attribute and relational datasets, we cannot apply ordinary
 one-way asymptotic evaluation. In this paper, we deal with
 the various sizes of GRD, and derive an asymptotic form
 of NML for GRD models. This enables NML-based model
 selection in GRD modeling.
 2) Application to relational data clustering: As a critical
 application, we address the task of determining the num-
 ber of clusters in relational data clustering. We apply our
 proposed criterion to the selection of the stochastic block
 model (SBM) [14], and derive a new efficient method for
 calculating the NML code-length for SBM to determine
 the optimal number of clusters. Using artificial datasets,
 we experimentally demonstrate that our criterion is more
 effective than other possible criteria. In addition, we demon-
 strate the validity of our model selection method through its
 applications to real beer-purchase data.
 The rest of this paper is organized as follows: We first
 introduce GRD in Section II, and explain MDL-based model
 selection with the NML code-length in Section III. In Sec-
 tion IV, we present a new theorem for NML of GRD models.
 We describe an application to relational data clustering in
 Section V, and show experimental results in Section VI. We
 finally give our conclusions in Section VII.
 II. GENERAL RELATIONAL DATA MODELING
 In this section, we formulate, and describe the modeling
 and statistical model selection of GRD. We also present
 some examples of GRD model selection tasks.
 Suppose we have S types of object classes, such as
 “customers” or “items,” and let the s-th class contain Ns
 objects. LetN = (N1, . . . , NS) denote an object size vector.
 We define attribute data as a sequence of vectors attached to
 each object. We denote the attribute data of the s-th object
 class as xNss = xs1 · · ·xsNs ? XNss (s ? {1, . . . , S}).
 We also consider relational data, which consist of infor-
 mation about the relationship between objects. Relational
 data can be expressed by a network, matrix, or tensor. We
 denote relational data among the s-th and t-th objects as a
 matrix xNs?Ntst = {xst,ij} ? XNs?Ntst (s, t ? {1, . . . , S}).
 Let us now consider a mixture of B attribute or relational
 datasets D = (xN1 , . . . , xNB ), where xNb (b ? {1, . . . , B})
 denotes attribute or relational data. We refer to the data D
 as general relational data. Note that we distinguish “general
 relational data” from “relational data.”
 We now formulate the statistical modeling of GRD and
 model selection. We consider variables in the given data D to
 be instantiations of random variables ˜D = (XN1 , . . . , XNB ).
 Statistical modeling aims to grasp the characteristics of
 the joint distribution of variables ˜D through a probabilistic
 model P = {P ( ˜D)}. In this paper, we are especially
 interested in a parametric class of probability distributions
 P = {P ( ˜D; ?) | ? ? ?}, where ? is a parameter space and
 P ( ˜D; ?) denotes a probability distribution of ˜D specified
 by a parameter ? ? ?. Suppose that we are also given
 a finite countable set of candidate models M. A model
 M ? M is an indicator of the corresponding probabilistic
 model PM = {P ( ˜D; ?) | ? ? ?M}, where ?M is a
 parameter space specified by the indicator M . Statistical
 model selection then aims to choose the optimal model ˆM
 from M to represent the joint distribution of D.
 As mentioned in Section I, model selection in GRD
 modeling concerns several practical tasks. In Bayesian net-
 work structure learning, we seek to determine the optimal
 factorization structure of the joint distribution of data D.
 Suppose we have the following classes of probabilistic
 422
models:
 P (XN1 , . . . , XNB ) =
 B∏
 i=1
 P (XNi |Pai), (1)
 where Pai ? {XN1 , . . . , XNB } denotes a set of parent
 variables of XNi . Bayesian network structure learning aims
 to compose the optimal sets of parent variables {Pai}.
 Friedman et al. proposed Probabilistic Relational Models
 (PRMs), which can be thought of as an extension of
 Bayesian networks for relational database mining [15]. The
 structural search of PRMs is also a challenging model
 selection task.
 Relational data clustering is the task of partitioning ob-
 jects into clusters according to observable relational data,
 which may include network clustering and co-clustering.
 Suppose that we are given unipartite network data about a
 single type of N objects, denoted by xN?N . Let the latent
 attribute variables ZN ? {1, . . . , K}N indicate the cluster
 to which each object belongs. We can see that relational data
 clustering is a kind of GRD modeling, as we model observ-
 able relational variables XN?N and unobservable attribute
 variables ZN simultaneously, i.e., ˜D = (XN?N , ZN ). In
 relational data clustering, model selection determines the
 optimal number of clusters K. We consider an example of
 SBM in Section V [14].
 The task of rank determination in matrix factorization is
 similar to that of cluster number determination in relational
 data clustering, except that the domain spaces of latent
 variables are continuous rather than discrete.
 One crucial characteristic of GRD is the variety of data
 sizes. The size of attribute data increases linearly with
 respect to the number of objects Ns; the size of relational
 data is polynomial with respect to the number of objects, i.e.,
 O(N2s ), O(N3t ), or O(NsNt). Thus, statistics derived from
 GRD models also have various convergence or divergence
 properties: The sample distributions of MLEs in GRD mod-
 els converge to normal distributions at different rates with
 respect to the object size vector N. Therefore, we have to
 develop criteria that account for the variety of data sizes and
 asymptotic properties of GRD.
 If the probabilistic model can be factorized as (1), we
 could also consider applying existing information criteria to
 each factor P (XNi |Pai) individually and summing them. In
 this paper, we rather consider the NML code-length to de-
 velop a criterion that is independent of such a factorization.
 We experimentally demonstrate that the proposed criterion
 is more effective than the methods described above.
 III. NORMALIZED MAXIMUM LIKELIHOOD
 CODE-LENGTH
 In this paper, we consider model selection based on the
 MDL principle. This section introduces its mathematical
 formulation, and describes the NML code-length.
 A. MDL Principle and NML Code-length
 The MDL principle asserts that the best model is that
 which minimizes the total code-length required for encoding
 the given data L(D|M) as well as the model itself L(M):
 M? = arg min
 M?M
 {
 L(D|M) + L(M)}.
 In this paper, we assume that the code-length of L(M) can
 be uniformly calculated as ln |M|. With regard to the code-
 length of the given data L(D|M), the normalized maximum
 likelihood (NML) code-length is employed. The NML code-
 length is defined as follows:
 LNML(D; M) def= ? ln P (D; ˆ?(D)) + ln
 ∑
 ˜D
 P ( ˜D; ˆ?( ˜D)).
 (2)
 ˆ?(D) is the maximum likelihood estimate of ? from D.
 The NML code-length is justified in the sense that it
 achieves Shtarkov’s minimax regret [10]. Let us consider all
 possible prefix code-length assignments over the domain of
 D. When a model M is given, it is reasonable to employ the
 code-length assignment that minimizes the following “max
 regret” criterion:
 max
 ˜D
 {
 L( ˜D; M)? min
 ???M
 {? ln P ( ˜D; ?)}}. (3)
 Shtarkov showed that the NML code-length (2) minimizes
 the max regret (3). According to the MDL principle, we
 select the model that minimizes the NML code-length (2)
 as the best model [9].
 B. An Asymptotic Property of NML
 NML-based model selection suffers from two computa-
 tional problems. First, when the domain space of the input
 variables ˜D or the parameter space ?M is unbounded,
 the second term of NML (2) may diverge to infinity.
 Rissanen [12] and Hirai and Yamanishi [11] addressed
 this problem by restricting the domain range of the given
 data. Second, computation of the second term in (2) is
 generally expensive. This is because we must sum over the
 whole domain of all variables. As the number of random
 variables increases, this combinatorial computation increases
 explosively. This problem is especially remarkable in GRD
 modeling, because we consider multiple attributes and rela-
 tional variables.
 In this paper, we are concerned with the latter computa-
 tional complexity problem, provided the former divergence
 problem can be avoided.
 One approach to the reformulation of this problem is
 to use an approximation of (2). In terms of attribute data
 modeling, Rissanen [9] provided the following asymptotic
 expansion theorem for NML. Assume the attribute data
 xN are given, and that a probabilistic model PM =
 {P (XN ; ?) | ? ? ?M}, where the parameter space ?M
 is compact, satisfies the following five conditions.
 423
i) There exists the Fisher information matrix, i.e.,
 IN (?) def= ? 1N E?
 [∂2 ln P (XN ; ?)
 ∂?∂?T
 ]
 ? I(?),
 and ?c1, c2, 0 < c1 ≤ |I(?)| ≤ c2 < ∞ for all
 ? ? ?M .
 ii) The elements of I(?) are continuous in ?M .
 iii) ∫
 ?M
 √|I(?)| < ∞.
 iv) Let ˆ?(XN ) denote the MLEs of ?. Then,
 √
 N(ˆ?(XN )? ?) d? N (0, I?1(?)).
 v) There is a finite positive-definite matrix C0 that,
 for all ˜D, satisfies
 ?
 1
 N
 ∂2 ln P (XN ; ?)
 ∂?∂?T
 ???
 ?=ˆ?(XN )
 < C0 < ∞.
 Then, the following theorem holds.
 Theorem 1. (Rissanen’s asymptotic expansion theorem [9])
 Assume that a probabilistic model PM satisfies the condi-
 tions i)–v). Then, for all data XN such that ˆ?(XN ) ? ?M ,
 LNML(xN |M) = ? ln P (xN ; ˆ?(xN )) + dM2 ln
 N
 2pi
 + ln
 ∫
 ?M
 √|I(?)|d? + o(1), (4)
 where limN?∞ o(1) = 0 uniformly over all xn.
 According to (4), we obtain an analytical approximation
 of NML that is guaranteed to converge to the exact NML
 with respect to the object size N .
 IV. ASYMPTOTIC PROPERTY OF NML FOR GRD
 MODELS
 In this section, we give a theorem on asymptotic expan-
 sion of the NML code-length. This is derived by extending
 Rissanen’s theorem [9] into GRD modeling. The obtained
 formula leads to a novel criterion for GRD model selection.
 Recall that, because of the variety of data sizes in GRD,
 we cannot apply one-way asymptotic evaluations to GRD
 modeling. In order to deal with the variety of data sizes, we
 introduce the following functional matrix.
 Suppose we have dM functions f1(N), . . . , fdM (N),
 where fi : NS ? [0,∞), and a diagonal matrix function
 F (N) = diag{f1(N), . . . , fdM (N)}. For two vectors a and
 b of the same length, a > b or a ≥ b denotes compo-
 nentwise inequality. Each function fi is a nondecreasing
 unbounded function of each element of N. In this paper,
 N ? ∞ denotes that each component of N becomes
 sufficiently large. With respect to F (N), we consider a
 probabilistic model PM = {P ( ˜D; ?) | ? ? ?M} that
 satisfies the following conditions:
 i’) There exists the following matrix:
 JN(?) def= ?F?2(N)E?
 [∂2 ln P ( ˜D; ?)
 ∂?∂?T
 ]
 ? J(?)
 as N ? ∞, and ?c1, c2, 0 < c1 ≤ |J(?)| ≤ c2 <
 ∞ for all ? ? ?M .
 ii’) The elements of J(?) are continuous in ?M .
 iii’) ∫
 ?M
 √|J(?)| < ∞.
 iv’) Let ˆ?( ˜D) denote the MLEs of ?. Then,
 F (N)(ˆ?( ˜D)? ?) d? N (0, J?1(?))
 as N ? ∞, which allows MLEs to converge to
 normal variables at different rates.
 v’) There is a finite positive-definite matrix C0 that,
 for all ˜D, satisfies
 ?F?2(N)∂
 2 ln P ( ˜D; ?)
 ∂?∂?T
 ???
 ?=ˆ?( ˜D)
 < C0 < ∞.
 Under the above conditions, we give a new asymptotic
 expansion theorem of NML for GRD modeling.
 Theorem 2. (Main theorem) Let a probabilistic model PM
 satisfy conditions i’)–v’). Then, for all data D such that
 ˆ?(D) ? ?M ,
 LNML(D|M) = ? ln P (D; ˆ?(D)) + ln |F (N)| (5)
 ?
 dM
 2
 ln 2pi + ln
 ∫
 ?M
 √|J(?)|d? + o(1),
 where limN?∞ o(1) = 0 uniformly over all data D.
 In the special case where the given data D are attribute
 data xN , N = N and fi(N) = √N for all i, and Theorem
 2 is equivalent to Theorem 1. From Theorem 2, we obtain
 an approximation of NML for GRD modeling. We propose
 this approximation as a suitable criterion for model selection
 in GRD modeling.
 Proof sketch: The proof of Theorem 2 closely follows that
 given in [9] for Theorem 1. Hence, we give only a sketch
 of the proof. Let us discretize the parameter space ?M into
 rectangles whose side-length along each axis is r/fi(N).
 Rd(¯?) denotes the rectangle whose center is located at ¯?.
 We then prepare two functions,
 Pd(¯?) def=
 ∫
 ??Rd(¯?)
 |F 2(N)J(¯?)| 12
 (2pi)dM/2
 ? exp
 (
 ?
 1
 2
 (? ? ¯?)T F 2(N)J(¯?)(? ? ¯?)
 )
 d?,
 Qd(¯?) def=
 ∫
 ˜D:ˆ?( ˜D)?Rd(¯?)
 P ( ˜D; ?)d ˜D.
 The proof has three steps. For sufficiently large N, a) Pd and
 Qd become arbitrarily close. b) ln
 ∑
 ¯? Qd(¯?) converges to
 the normalized term of NML (2). c) ln∑
 ¯? Pd(¯?) approaches
 the approximation of the normalized term of NML (5).
 424
V. NML-BASED MODEL SELECTION FOR THE
 STOCHASTIC BLOCK MODEL
 In this section, we apply our proposed criterion (5) to
 model selection in relational data clustering.
 Relational data clustering is a practically important issue.
 It can be applied to a wide range of data mining tech-
 nologies, including social network analysis, recommendation
 systems, and market analysis. We introduce the stochastic
 block model (SBM) [14], which is a latent-variable model of
 the community structure within a graph, providing a model-
 based clustering of the graph’s nodes.
 At this point, we are concerned with a unipartite graph
 containing N nodes (objects) of one class and a series of
 undirected relations between two objects. The affinity matrix
 of the graph, denoted as xN?N , is square and symmetric.
 We consider the simplest case of X = {0, 1}: xij = 1
 if objects i and j are connected, otherwise xij = 0. The
 diagonal elements of xN?N are fixed to 0.
 Assume that the graph consists of K clusters, i.e., each
 of the N nodes belongs to one of K clusters. Let us
 introduce a latent variable zN = z1 · · · zN as a tuple of
 the indicator vectors zi ? {1, . . . , K}, which denote that
 the i-th object belongs to the zi-th cluster. Assume that the
 random variables Z1, . . . , ZN are generated independently
 according to the following probability distribution:
 Zi ? Categorical(p) (i = 1, . . . , N),
 where Categorical(·) denotes the categorical distribution, p
 denotes a parameter vector of it and p = (p1, . . . , pK?1) ?
 [0, 1]K?1 such that∑K?1k=1 pk ≤ 1, and pK = 1?
 ∑K?1
 k=1 pk.
 Conditioned by Zi and Zj , the edges between objects i
 and j, Xij , are assumed to be generated from a Bernoulli
 distribution:
 Xij |Zi, Zj ? Bernoulli(?ZiZj ).
 Let ? = {?kl} denote a symmetric parameter matrix of
 Bernoulli distributions and ? = (p, ?). Fig. 2 illustrates
 generative process of SBM.
 Assuming the number K of clusters, SBM is a parametric
 probabilistic model of ˜D = (ZN , XN?N ) that can be
 written as follows:
 PK =
 {
 P ( ˜D; ?) = P (ZN ;p)P (XN?N |ZN ; ?) | ? ? ?K
 }
 .
 Estimating the latent variable ZN from the observable
 relational variables xN?N is equivalent to relational data
 clustering. For example, to learn the parameters ?, Daudin
 et al. [16] developed the variational EM algorithm.
 An important issue in the inference of SBM is the
 selection of K, the number of clusters, from candidates K.
 Let us derive the NML code-length for SBM. When all data
 D = (zN , xN?N ) are given, the MLEs of p and ? are
 g1005 g1010 g1013 g1006 g1008 g1011 g1007 g1009 g1012
 g1005 g888 g1005 g1005 g1004 g1005 g1004 g1005 g1005 g1004
 g1010 g1005 g888 g1005 g1004 g1004 g1004 g1005 g1005 g1005
 g1013 g1005 g1005 g888 g1004 g1004 g1004 g1005 g1005 g1005
 g1006 g1004 g1004 g1004 g888 g1005 g1005 g1004 g1004 g1004
 g1008 g1005 g1004 g1004 g1005 g888 g1005 g1004 g1005 g1004
 g1011 g1004 g1004 g1004 g1005 g1005 g888 g1004 g1004 g1004
 g1007 g1005 g1005 g1005 g1004 g1004 g1004 g888 g1004 g1004
 g1009 g1005 g1005 g1005 g1004 g1005 g1004 g1004 g888 g1004
 g1012 g1004 g1005 g1005 g1004 g1004 g1004 g1004 g1004 g888
 
  
   	

 1=Z
 2=Z
 3=Z
   
   
   
 =?
 =
 ?NNX
 Figure 2. An illustration of generative process of SBM
 pˆk = ak/N , and ˆ?kl = a?kl/akl, where ak def=
 ∑
 i 1zi=k,
 a?kl
 def
 =
 ∑
 i,j =i xij1zi=k,zj=l, and
 akl
 def
 =
 {
 ak(ak ? 1)/2 (k = l),
 akal (k 
= l).
 Following the derivation of NML for naı¨ve Bayes models
 in [3], the normalization term of NML for SBM is written
 as
 CSBM(N, K) (6)
 =
 ∑
 ∑
 ak=N
 { N !
 a1! · · · aK !
 K∏
 k=1
 (ak
 N
 )ak
 ?
 ∏
 k≤l
 Ccat(akl, 2)
 }
 ,
 where Ccat(n, k) is the normalization term of NML for
 the categorical distribution with k states, which coincides
 with the Bernoulli distribution in the case of k = 2.
 Because Ccat(n, 2) can be calculated in O(n) time, the
 computation of (6) requires O(K2 ·NK+1) time: Fig. 3 plots
 the real computation time of CSBM(N, K) on a double-
 logarithmic scale. We can see that the computation time
 increases explosively with respect to N and K. Indeed, the
 computation of CSBM(N = 500, K = 3) takes almost one
 week (≈ 6 ? 105 s) in our experimental setting. This is
 clearly expensive.
 We now derive an approximation of CSBM(N, K) accord-
 ing to Theorem 2. We order the parameters as
 ? = (p1, . . . , pK?1, ?11, . . . , ?KK , ?12, ?13, . . . , ?K?1K).
 The dimensionality of the parameter space ?K of SBM is
 dK
 def
 = K ? 1 + K(K + 1)/2. Let us set dK functions of
 {fi(N)} as
 fi(N) =
 {√
 N (i < K),
 N (i ≥ K).
 The first K ? 1 functions correspond to the parameter p,
 and the others to ?. Using a functional matrix F (N) =
 425
102 103
 100
 102
 104
 106
 N
 CPU time (sec.
 )
 Computation time of NML
  
 
K=1
 K=2
 K=3
 K=4
 Figure 3. Real computation time of NML for SBM (6)
 diag{f1(N), . . . , fdK (N)}, we can calculate J(?) of SBM
 as:
 J(?) = lim
 N?∞
 ?F?2(N)E?
 [∂2 ln P ( ˜D; ?)
 ∂?∂?T
 ]
 =
 (
 M(p) 0
 0 {pklB(?kl)}
 )
 ,
 where M(p) denotes the Fisher information matrix of the
 categorical distribution, B(?kl) denotes that of the Bernoulli
 distribution, and
 pkl
 def
 =
 {
 p2k/2 (k = l),
 pkpl (k 
= l).
 We also obtain the limit of the sample distribution of ˆ?( ˜D)
 as
 F (N)(ˆ?( ˜D)? ?) ?d N (0, J?1(?)).
 The integral of
 √|J(?)| can be computed as
 ln
 ∫
 |J(?)| 12 d? = ?K
 2
 ln 2 + K ln ?
 (
 K + 2
 2
 )
 ? ln ?
 (
 K(K + 2)
 2
 )
 +
 K(K + 1)
 2
 ln pi.
 Hence, from Theorem 2, the following theorem is derived:
 Theorem 3. For SBM, the logarithm of the normalization
 term in the NML code-length is asymptotically expanded as
 follows:
 ln CSBM(N, K) (7)
 =
 K ? 1 + K(K + 1)
 2
 ln N ? dK
 2
 ln 2pi
 ?
 K
 2
 ln 2 + K ln ?
 (
 K + 2
 2
 )
 ? ln ?
 (
 K(K + 2)
 2
 )
 +
 K(K + 1)
 2
 ln pi + o(1).
 0 50 100
 ?4
 ?2
 0
 2
  N
 Log?regret difference (bits
 )
  
 
K=2
 K=3
 Figure 4. Asymptotic behavior of the approximate NML (7) (blue and
 red plot lines) to the exact NML (6) (black dots baseline)
 Because we can calculate (7) analytically, its computation
 time (less than 10?3 s in our experiment) is negligible.
 Fig. 4 illustrates the asymptotic behavior of the approx-
 imate NML (7) against the exact NML (6). It is clear that
 the approximate NML (7) converges to the exact NML (6)
 as N increases.
 VI. EMPIRICAL RESULTS
 A. Evaluation Using Artificial Datasets
 We conducted an experiment using artificial datasets to
 evaluate the performance of the approximate NML (7) in
 comparison with other criteria.
 1) Experimental settings: We generated 30 networks ac-
 cording to the generative model of SBM for each object
 size N and the true number of clusters Ktrue. Parameters
 ? = (p, ?) were sampled according to the following
 distributions:
 p ? Dirichlet(?),
 ?kl ? Beta(?, ?), ?lk ? ?kl (k ≤ l ? {1, . . . , Ktrue}).
 The hyper-parameters were set to ? = (100, . . . , 100),
 which has length Ktrue, and ? = 1.
 The model selection criteria were evaluated using the
 number of networks, out of 30, that accurately selected Ktrue
 as the optimal model. We refer to this metric as the recovery
 rate (RR).
 For the clustering, we employed the collapsed Gibbs
 sampling algorithm with a nonparametric Bayes model of
 SBM, known as the infinite relational model (IRM) [17].
 The algorithm generates candidate cluster assignments zN
 for various numbers K of clusters. We selected the cluster
 assignment and the number of clusters that minimized a
 certain criterion. The IRM employed two kinds of priors: the
 Chinese restaurant process (CRP) and the beta distribution.
 The hyper-parameter of the CRP was fixed to ? = ln N ,
 and that of the beta distribution was set to ? = 1/2. The
 clustering and model selection procedure is summarized in
 Fig. 5.
 426
Input: Observed Network xN?N , Object size N , hyper-
 parameters for clustering ?, ?
 Output: Selected number of clusters Kbest, selected cluster
 assignment zNbest
 Lbest ?∞
 for i = 1 ? 15 do
 Initialize zN randomly
 for j = 1 ? 100 do
 zN ? CollapsedGibbsSampler(xN?N , zN , ?, ?)
 K ? # of clusters of zN , ˆ? ? ˆ?(xN , zN ) % MLE
 L ? ? ln P (xN?N , zN ; ˆ?) + ln CSBM(N, K)
 if L < Lbest then
 Lbest ? L, zNbest ? zN
 Kbest ? # of clusters of zNbest
 end if
 end for
 end for
 Figure 5. Pseudocode for the experimental model selection procedure
 2) Criteria for comparison: In our experiment, we com-
 pared the model selection performance of our proposed
 NML criterion with seven other criteria, namely AIC [5],
 two types of BIC [6], the minimum message length
 (MML) [18], integrated classification likelihood (ICL) [19],
 [16], and Bayesian marginal likelihoods (BMLs) of SBM
 and IRM.
 AIC and BIC1 were defined according to their naı¨ve
 definitions in attribute data modeling:
 AIC(D; K) = ? ln P (D; ˆ?(D)) + dK ,
 BIC1(D; K) = ? ln P (D; ˆ?(D)) + dK
 2
 ln N.
 We also applied BIC to the models P (Zi;p) and
 P (Xij ; ?kl) individually, and obtained BIC2 by summing
 them:
 BIC2(D; K) = ? ln P (D; ˆ?(D))+ K ? 1
 2
 ln N +
 ∑
 k≤l
 ln akl.
 The general definition of MML was proposed by [18],
 from which we derived MML for SBM as follows:
 MML(D; K) = ? ln P (D; ˆ?(D)) + ln CMML(N, K), (8)
 where
 ln CMML(N, K)
 =
 K ? 1 + K(K + 1)
 2
 ln N + dK
 2
 (1? ln 12)
 ?
 K
 2
 ln 2 + K ln ?
 (
 K + 2
 2
 )
 ? ln ?
 (
 K(K + 2)
 2
 )
 +
 K(K + 1)
 2
 ln pi.
 Because MML requires prior distributions for its param-
 eters, we employed the Jeffreys prior [20] in (8), i.e.,
 Dirichlet(?) for the categorical distribution, where ? = (K+
 2)/2(1, . . . , 1), and Beta(?, ?) for the Bernoulli distribution,
 where ? = 1/2.
 Daudin et al. [16] proposed that a model could be selected
 for SBM on the basis of the ICL criterion [19], which is also
 known as the integrated completed likelihood. ICL is based
 on an approximation of BML of SBM.
 If we employ conjugate priors, we can analytically calcu-
 late BML itself. To calculate BML of SBM, we employed
 the Jeffreys prior described above. For the IRM, the hyper-
 parameter of the CRP was set to ? = ln N , and that of the
 beta distribution was set to ? = 1/2.
 3) Results: Fig. 6 shows the RR obtained by each of
 the tested criteria for object sizes ranging from 40 to 2000
 with Ktrue = 10 and 50 on semilogarithmic scales. We only
 show the RRs of AIC, BIC1, BIC2, ICL, BML of IRM, and
 NML, as the plots for MML and BML of SBM are very
 close to that of NML. In Table I, we evaluate the overall
 performance of each criterion quantitatively using the area
 under the curve (AUC). This is the ratio of the area under the
 curve to the corresponding rectangle. The closer the AUC is
 to 1, the faster the RR curve increases. The best two AUC
 values in each column are shown in bold.
 From Fig. 6, we can see that, with the exception of AIC
 and BIC2, the criteria selected the true number of clusters
 Ktrue with a high RR for large object sizes N . In terms
 of AUC, NML outperforms the other criteria, particularly
 when Ktrue is small. When Ktrue is large, NML is not
 markedly superior to that of the other criteria. Because of its
 consistently high performance, however, we may conclude
 that the approximate NML gave the best overall results in
 our experiment.
 Table I
 AREA UNDER THE CURVE (AUC)
 Ktrue 5 10 20 50 100 200
 AIC 0.774 0.703 0.754 0.577 0.344 0.120
 BIC1 0.917 0.823 0.694 0.495 0.328 0.120
 BIC2 0.026 0.087 0.013 0.005 0.000 0.000
 MML 0.917 0.848 0.737 0.569 0.344 0.120
 ICL 0.895 0.784 0.643 0.423 0.269 0.086
 BMLSBM 0.919 0.860 0.768 0.577 0.344 0.120
 BMLIRM 0.880 0.841 0.788 0.561 0.344 0.120
 NML 0.921 0.868 0.784 0.573 0.344 0.120
 B. Evaluation Using Market Datasets
 We applied the relational data clustering and model se-
 lection methods described above to brand analysis. This
 is an important issue that allows enterprises to objectively
 evaluate the brand image of their own and rival products. By
 analyzing customer purchase data, we can expect to obtain
 some insight into the relationship between different brands
 in a market.
 427
102 103
 0
 0.2
 0.4
 0.6
 0.8
 1
  N
 Recovery Rat
 e
  Ktrue =10
  
 
AIC
 BIC1
 BIC2
 ICL
 BML_IRM
 NML
 102 103
 0
 0.2
 0.4
 0.6
 0.8
 1
  N
 Recovery Rat
 e
  Ktrue =50
  
 
AIC
 BIC1
 BIC2
 ICL
 BML_IRM
 NML
 Figure 6. Results of artificial model selection experiment
 We use a dataset of beer purchases in Japan. This is QPR
 data provided by MACROMILL, Inc., and HAKUHODO,
 Inc.
 The dataset has the following specifications: Purchase data
 relate to the period November 1st 2010 to January 31st
 2011. The data relate to 3,185 customers and 65 brands of
 beer. The dataset consists of more than 20,000 beer purchase
 transactions. Brands of beer include three types: regular beer,
 “low-malt” beer, and a third type of beer. Low-malt beer is
 a beer-type alcoholic drink with a relatively low malt ratio,
 and the third beer is a beer-tasting alcoholic drink with no
 malt.
 To commence our analysis, we processed the raw purchase
 records into a 0/1-valued matrix. We set the matrix entries
 xij = 1 if the j-th customer purchased more than two cans
 of the i-th beer during the data acquisition period, and set
 xij = 0 otherwise. Next, we excluded customers whose
 column vector included fewer than five positive values. Beer
 brands whose row vector included no positive values were
 also excluded. Finally, we obtained a 0/1-valued matrix of
 51 beer brands and 101 customers.
 We conducted co-clustering analysis on the purchase re-
 lational matrix of customers and beer brands. Co-clustering
 is a framework that simultaneously partitions the rows and
 columns of a matrix (see, e.g., [21]). This enables us to
 g18629g18679g18677g18678 g18673g18671g18663g18676g18677
 g18628
 g18663
 g18663
 g18676
 g18594
 g18660
 g18676
 g18659
 g18672
 g18662
 g18677
 g4
 g17
 g18
 g24
 g28
 g38
 Figure 7. Result of co-clustering the beer purchase matrix
 Table II
 NUMBER OF BEER BRANDS IN EACH CLUSTER
 Regular Low-malt 3rd Total
 A 0 0 7 7
 B 0 4 6 10
 C 3 0 0 3
 D 0 0 4 4
 E 7 1 0 8
 F 10 6 3 19
 Total 20 11 20 51
 extract customer clusters and beer-brand clusters, as well
 as their global relationship. In this example, we applied
 SBM-based co-clustering, and employed the variational EM
 algorithm developed in [16] for SBM. The optimal numbers
 of customer and beer clusters were selected according to the
 NML code-length for SBM, which was derived in the same
 way as (7).
 Fig. 7 shows the results of co-clustering and model
 selection of the purchase relational matrix. We obtained six
 beer-brand clusters (A–F) and two customer clusters. Table
 II summarizes the types of beer brands in each cluster.
 From Table II, we can see that clusters C and E are regular
 beer-brand clusters, whereas clusters A and D consist of the
 third type of beer-brands. Customers included in the left
 cluster in Fig. 7 mainly buy regular beer, and customers
 belonging to the right cluster prefer the third type. We can
 characterize the brand clusters as follows: Cluster C is the
 top-selling cluster of regular beer, and cluster D is the top-
 selling cluster of the third type of beer. Clusters E and A
 are the middle-standing clusters of regular and third-type
 beers, respectively. Note that Table II shows one low-malt
 beer brand in cluster E. We can infer that this beer’s brand
 image is more similar to that of regular beer than low-malt
 beer, as the people who prefer regular beer brands also drink
 this low-malt beer.
 This analysis shows that the co-clustering of a beer
 purchase matrix can lead to the understanding of the rela-
 428
tionship between beer brands and customers. Our proposed
 criterion played an important role in determining the number
 of clusters in this analysis.
 We finally compare our proposed criterion with other
 ones. For example, AIC discovered four customer clusters,
 and the customer behavior suggested by each cluster is rather
 different from that given by NML. ICL selected only five
 brand clusters, whereas results of NML also distinguished
 brand clusters B and E. Each of these results shows certain
 aspects of the data, but we emphasize that the model selec-
 tion results given by NML are justified from the perspective
 of data compression.
 VII. CONCLUSION
 In this paper, we have addressed the issue of statistical
 model selection in GRD modeling. From the viewpoint
 of the MDL principle, we have clarified the asymptotic
 property of NML in GRD modeling, and have proposed a
 new model selection criterion. We have applied our proposed
 criterion to the issue of model selection in SBM learning,
 and have derived a new and efficient method of calculating
 the NML code-length for SBM. The effectiveness of our
 NML-based criterion has been demonstrated experimentally
 using artificial data and real beer-purchase data. Thus, for
 the first time, we have realized a model selection technique
 that considers the statistical characteristics of GRD models.
 We will extend our discussion to other problems and criteria.
 ACKNOWLEDGMENT
 This work was partially supported by MEXT KAKENHI
 23240019, by the Aihara Project, the FIRST program from
 JSPS, initiated by CSTP, by HAKUHODO, Inc., and by
 MACROMILL, Inc.
 REFERENCES
 [1] B. Long, Z. Zhang, and P. S. Yu, Relational Data Cluster-
 ing: Models, Algorithms, and Applications. Chapman &
 Hall/CRC, 2010.
 [2] T. Roos, T. Silander, P. Kontkanen, and P. Myllymaki,
 “Bayesian network structure learning using factorized NML
 universal models,” in Information Theory and Applications
 Workshop, Feb. 2008, pp. 272 –276.
 [3] P. Kontkanen, P. Myllyma¨ki, W. Buntine, J. Rissanen, and
 H. Tirri, “An MDL framework for data clustering,” in Ad-
 vances in Minimum Description Length: Theory and Appli-
 cations. MIT Press, 2005.
 [4] A. T. Cemgil, “Bayesian inference for nonnegative matrix
 factorisation models,” Intell. Neuroscience, vol. 2009, pp.
 4:1–4:17, Jan. 2009.
 [5] H. Akaike, “A new look at the statistical model identification,”
 IEEE Transactions on Automatic Control, vol. 19, no. 6, pp.
 716 – 723, Dec. 1974.
 [6] G. E. Schwarz, “Estimating the dimension of a model,”
 Annals of Statistics, vol. 6, pp. 461–464, 1978.
 [7] E. M. Airoldi, D. M. Blei, S. E. Fienberg, and E. P. Xing,
 “Mixed membership stochastic blockmodels,” J. Mach. Learn.
 Res., vol. 9, pp. 1981–2014, Jun. 2008.
 [8] J. Rissanen, “Modeling by shortest data description,” Auto-
 matica, vol. 14, no. 5, pp. 465 – 471, 1978.
 [9] ——, “Fisher information and stochastic complexity,” IEEE
 Transactions on Information Theory, vol. 42, no. 1, pp. 40
 –47, Jan. 1996.
 [10] Y. M. Shtarkov, “Universal sequential coding of single mes-
 sages,” Problems of Information Transmission, vol. 23, no. 3,
 pp. 3–17, 1987.
 [11] S. Hirai and K. Yamanishi, “Efficient computation of normal-
 ized maximum likelihood coding for Gaussian mixtures with
 its applications to optimal clustering,” in Proceedings of the
 2011 IEEE International Symposium on Information Theory
 (ISIT), Aug. 2011, pp. 1031 –1035.
 [12] J. Rissanen, “MDL denoising,” IEEE Transactions on Infor-
 mation Theory, vol. 46, no. 7, pp. 2537 –2543, Nov. 2000.
 [13] Y. Hayashi and K. Yamanishi, “Sequential network change
 detection with its applications to ad impact relation anal-
 ysis,” in IEEE International Conference on Data Mining
 (ICDM2012), 2012, pp. 280–289.
 [14] T. A. Snijders and K. Nowicki, “Estimation and prediction for
 stochastic blockmodels for graphs with latent block structure,”
 Journal of Classification, vol. 14, pp. 75–100, 1997.
 [15] N. Friedman, L. Getoor, D. Koller, and A. Pfeffer, “Learning
 probabilistic relational models,” in Proceedings of the 16th
 international joint conference on Artificial intelligence, vol. 2,
 1999, pp. 1300–1307.
 [16] J.-J. Daudin, F. Picard, and S. Robin, “A mixture model for
 random graphs,” Statistics and Computing, vol. 18, no. 2, pp.
 173–183, 2008.
 [17] C. Kemp, J. B. Tenenbaum, T. L. Griffiths, T. Yamada, and
 N. Ueda, “Learning systems of concepts with an infinite rela-
 tional model,” in Proceedings of the 21st national conference
 on Artificial intelligence, 2006, pp. 381–388.
 [18] C. S. Wallace and P. R. Freeman, “Estimation and inference
 by compact coding,” Journal of the Royal Statistical Society.
 Series B (Methodological), vol. 49, no. 3, pp. pp. 240–265,
 1987.
 [19] C. Biernacki, G. Celeux, and G. Govaert, “Assessing a
 mixture model for clustering with the integrated completed
 likelihood,” Pattern Analysis and Machine Intelligence, IEEE
 Transactions on, vol. 22, no. 7, pp. 719–725, 2000.
 [20] H. Jeffreys, “An invariant form for the prior probability in
 estimation problems,” in Proceedings of the Royal Society of
 London. Series A. Mathematical and Physical Sciences, vol.
 186, no. 1007, 1946, pp. 453–461.
 [21] I. S. Dhillon, S. Mallela, and D. S. Modha, “Information-
 theoretic co-clustering,” in Proceedings of the ninth ACM
 SIGKDD international conference on Knowledge discovery
 and data mining, 2003, pp. 89–98.
 429

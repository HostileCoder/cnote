Collaborative Book Recommendation Based on ReadersÕ Borrwoing Records 
 
Liu Xin1, E Haihong1, Song Junde1, Song Meina1, Tong Junjie1 
1PCN&CAD Center Lab, Beijing University of Posts and Telecommunications, China 
liuxinjackie@gmail.com,{ ehaihong,jdsong,mnsong,tongjunjie}@bupt.edu.cn 
 
 
AbstractÑBook recommendation is an important part and task 
for personalized services and educations provided by the 
academic libraries. Many libraries have the readersÕ 
borrowing records without the readersÕ rating information on 
books. And the collaborative filtering (CF) algorithms are not 
proper under this circumstance. To apply the CF algorithms in 
book recommendation, in this paper, we construct the ratings 
from the readersÕ borrowing records to enable the CF 
algorithms. And to evaluate the traditional CF algorithms, we 
show that linearly combining (blending) a set of CF algorithms 
increases the accuracy and outperforms any single CF 
algorithms. At last, we conduct the experiments based on the 
real world dataset and the results invalidate the efficiency of 
the blending methods.   
Keywords-library recommendation; collaborative filtering; 
supervised learning; ensemble learning 
I.  INTRODUCTION  
Because of the recent rise in library 3.0, academic 
libraries have become medium facilitating interactions 
between the community and librarians. Numerous academic 
libraries have begun to provide personalized services, such 
as book recommendations, to attract readers to use library 
resources [1, 2, 3, 4]. 
Recommender systems help users to discover items in 
large web shops, to navigate through portals or to find 
friends with similar interests. And in recommendation 
systems, CF algorithms are widely used and have enjoyed 
much interest and progress [5]. It predicts the potential 
interests of a given user (called an active user) by taking into 
account the opinions of users with similar taste (i.e., social 
wisdom). Compared to other recommendation technologies 
(e.g., content-based filtering [6]), collaborative filtering 
technologies have the capability of working in domains 
where itemsÕ attribute contents are difficult to obtain or 
cannot easily be parsed by automatic processed. In addition, 
CF algorithms can provide serendipitous recommendations 
and can be easily adopted in different recommender systems 
without requiring any domain knowledge [7]. In other words, 
CF helps users discover new items. And there are various 
research works on the evaluation of the CF algorithms, such 
as real time online recommendation [8, 9] and sparse 
problem [10, 11] etc. 
In order to establish recommendations, CF algorithms 
need to relate two fundamentally different entities: items and 
users. There are two primary approaches to facilitate such a 
comparison, which constitute the two main techniques of CF: 
the neighborhood approach and latent factor models. 
Neighborhood methods focus on relationships between items 
or, alternatively, between users. An item-item approach 
models the preference of a user to an item based on rating of 
similar items by the same user. Latent factor models, such as 
matrix factorization, comprise an alternative approach by 
transforming both items and users to the same latent factor 
space. The latent space tries to explain rating by charactering 
both products and users on factors automatically inferred 
from user feedback [12]. 
From the above description about the traditional CF 
algorithms, CF algorithms are based on the history of the 
feedbacks from the users, especially the rating of the items. 
But in the history of the borrowing records recorded in the 
academic libraries, there is no rating information but just the 
records of borrowing and returning with time stamp. And the 
traditional CF algorithms cannot be adopted well for that 
situation. 
In this paper, we focus on how to implement accurate CF 
algorithms based on readersÕ borrowing records without the 
feedback ratings. We transform the usersÕ ratings based on 
the borrowing records with time stamp for implementing CF 
algorithms. And at the same time, to make the traditional CF 
algorithms more accurate, we linearly combine (blend) the 
traditional ones by different methods. The results from the 
experiment on the real world data set invalidate the 
efficiency and accuracy of our methods. And as the ratings 
on the books can be transformed into time, by predicting the 
ratings, we can easily figure out how long the reader will 
borrow the book. 
This paper is organized as follows: in section II, we give 
some details of some traditional CF algorithms and describe 
them in mathematics; in section III, we introduce the 
algorithms for blending. And in section IV, we show how to 
transform the readersÕ borrowing records to the rating and 
give the results by implementing the algorithms on the real 
world data. In section V, we give the conclusion and some 
plans about the future works. 
II. COLLABORATIVE FLITERING 
Collaborative filtering uses the history of user-items 
events in order to predict future ones. These events can be 
any source of user-generated information, such as purchases, 
rating, clicks, bookmarks, favorite-adds etc. Within this work 
we focus on rating based collaborative filtering. 
The problem of rating prediction can be described with 
the help of the user-item rating matrix [ ]uiR r=  where 
2013 International Conference on Advanced Cloud and Big Data
 978-1-4799-3261-0/14 $31.00 © 2014 IEEE
 DOI 10.1109/CBD.2013.14
 159
entry uir  is the rating of user u for item i . This matrix has 
the size U M? with U being the number of users and 
M the number of items. In general R is sparsely filled, 
because a user typically dose not rate every item. The goal of 
the prediction model is to accurately predict missing values 
of this matrix, i.e. to produce predictions öuir for how an item 
i would be rated by user u . In collaborative filtering, the 
system infers a model from all available data in R . One 
possibility to do so is to use a low-rank matrix factorization. 
And for testing the accuracy and efficiency, the dataset 
usually contains two parts: the train set and the test set. We 
train the parameters for CF models based on the train set and 
test the accuracy and efficiency of the CF models based on 
the test set. 
There are two main techniques for CF: the neighborhood 
approach and latent factor models. And in this section, we 
give details on the neighborhood approaches including k-
 nearest neighbor (KNN) methods (KNNitem, KNNuser) and 
latent factor model (LFM) [13]. 
A. KNNitem 
A prediction ö
 uir for the rating of a user u on item i  is an 
item based k-nearest neighborhood model is obtained by 
calculating a weighted sum over the rating of the user u for 
the k items most similar to item i . The weights and the 
similarities are proportional to the correlations ijc between 
item i  and other item j . Therefore, a precalculated item-
 item correlation matrix C is useful, due to the constant 
access time to any item-item correlation ijc in this case. For 
one prediction, the KNNitem algorithm selects the k best 
correlated items to item i . And usually employ the 
following equation to predict the rating: 
( )
 ( )
 ( )
 ö
 k k
 k
 k
 k
 ii ui k
 i S i
 ui
 ii
 i S i
 c r i
 r i
 c
 ?
 ?
 ?
 = +
 
                                         (1) 
In the above equation, i  and ki  represent the average 
ratings of item i  and ki . And ( )S i represents the neighbor 
set of the k best correlated items to item i . 
The training time is dominated by the calculation of the 
item-item correlation matrix C , which needs 
2( * )O U M operations. And to select the k best correlated 
items, this can take up to ( )O M  operations. The sorting of 
this list takes ( log( ))O M M  time. 
B. KNNuser 
This model is exactly the same as the KNNitem, but 
items and users are flipped. Hence the prediction and training 
bounds are also reversed. In this model, a precalculated user-
 user correlation matrix 'C and usually employ the following 
equation to predict the rating: 
'
 ( )
 '
 ( )
 ( )
 ö
 a a
 a
 a
 a
 uu u i a
 u S u
 ui
 uu
 u S u
 c r u
 r u
 c
 ?
 ?
 ?
 = +
 
                                      (2) 
In the above equation, u  and 
a
 u  represent the average 
ratings of user u  and 
a
 u . And ( )S u represents the 
neighbor set of the k best correlated users to user u . 
C. LFM (latent factor model) 
This is probably the most popular collaborative filtering 
technique. A prediction is given by the dot product of a user 
feature vector up  and the item feature vector iq , then the 
rating is predicted as follows: 
ö
 T
 ui u ir p q=                                                                      (3) 
The LFM learns two factor matrices, user features 
1 2[ , ... ]UP p p p= and item features 1 2[ , ... ]MQ q q q= via 
stochastic gradient descent. The model parameterizes two 
matrices with f  rows. And to train the parameters, we have 
to minimize the following cost function: 
2 22
 ,
 min {( ) }
 ui
 T
 ui u i u iP Q
 r R
 r p q p q? ?
 ?
 ? + +          (4) 
In the above equation, the parameter ?  is the 
regularization ratio.  And to implement the stochastic 
gradient descent, we have the learning rate ? . The 
parameter f is usually predefined before parameter training. 
In practice the whole training needs a few tens of epochs 
over whole dataset until convergence, leading to (| |)O ? . 
And for storing the two parameter matrices, the memory 
consumption is ( )O M U+ . 
 
Both training and prediction time have optimal 
asymptotic runtime behavior, which makes LFM an 
excellent candidate for large scale recommendation 
applications. And it usually can predict all the missing values 
in R while the KNNuser and KNNitem usually predict a part 
of the missing values. 
Sometimes, LFM can be acted as SVD and there many 
extensions to SVD in the literature, for example the SVD++ 
model described in [14]. 
In this section, we give some details of the traditional CF 
algorithms on how to predict the ratings. And there are many 
other CF algorithms and we do not focus on them in this 
paper. 
III. BLENDING 
The combination of different kinds of collaborative 
filtering algorithms leads to significant performance 
improvements over individual algorithms. Blending 
predictions is supervised machine learning problem. Each 
160
input vector ix is the F -dimensional vector of the 
predictions in the ensemble. For N data samples, one 
collection the vectors 1 2, ... Nx x x in an N F? matrix X of 
predictions. The target values for these n data points are 
collected in an N -dimensional vector y . In some film 
recommendation systems, the entries of y are integer rating 
between 1 and 5. The blending algorithm is formally a 
function : F? ? ? . The input x is a vector of 
individual predictions, the output is a scalar.  
And sometimes, we cannot predict all the missing values, 
so have another parameter for CF algorithmsÑSR 
(successful rate). SR is the ratio of the predicted values in all 
the missing values. 
A.
  
Linear Regreesion - LR 
In statistics, linear regression is an approach to modeling 
the relationship between a scalar dependent variable y  and 
one or more explanatory variables denoted as X .  In linear 
regression, data are modeled using linear predictor functions, 
and unknown model parameters are estimated from the data. 
Such models are called linear models. Most commonly, 
linear regression refers to a model in which the conditional 
mean of y  given the value of x  is an affine function of X . 
Assuming a quadratic error function, optimal linear 
combination weighs w (vector of length N ) can be 
obtained by solving the least squares problem. For any input 
vector x , the prediction is ( ) Tx x w? = . 
We use the traditional cost function while calculating the 
parameter of the model. And then compare the prediction 
with the real data for the test set. 
B. Neural Network - NN 
Neural networks are a different paradigm for computing: 
von Neumann machines are based on the processing/memory 
abstraction of human information processing; neural 
networks are based on the parallel architecture of animal 
brains. Neural networks are a form of multiprocessor 
computer system with simple processing elements, a high 
degree of interconnection, simple scalar messages and 
adaptive interaction between elements. 
Small neural networks can be trained efficiently on huge 
data sets. The training of neural networks is performed 
stochastic gradient descent. The output neuron has a sigmoid 
activation function with an output swing of -1 to +1. To 
generate rating predictions in the range between two integers, 
we can use a simple output transformation. For example, the 
output is multiplied by 1.6a = and the constant 1b = is 
added. The learning rate is ? . 
IV. EXPERIMENETS 
In this section, we implement the traditional CF 
algorithms and the blending methods on the real world 
dataset. Before implementing CF algorithms to predict the 
integer between a certain ranges, we have to transform the 
borrowing records. 
A. Dataset and transformation 
This real world dataset is from some academic library. 
And it contains 82, 987 records form 2011-06-07 to 2012-
 06-27 which contains five different operations including 
borrowing, returning, renewing, booking and removing the 
booking with the time stamp. 
While a student borrow a book, he should renew it after 
30 days. And he can renew twice, after one renew, he can 
borrow the book for another 15 days. It means that a book 
can be borrowed at most for 60 days without penalty on the 
extra days. 
To generate the ratings for the user on the item, we first 
calculate the interval tÆ  between the book borrowing 
record and the corresponding returning record. And we 
generate the ratings based on the interval as in table I. And 
the distribution of scores over the whole dataset is shown 
figure 2. And we can easily figure out that most of the books 
just have ratings with 1. 
TABLE I.  RATING TRSANFORMATION 
tÆ (Days) Rating 
[0,15) 1 
[15,30) 2 
[30,45) 3 
[45,60) 4 
[60,+° ) 5 
 
 
Figure 1.  Distribution of the scores 
B. Set the data set and parameters 
We first extract the borrowing records from the whole 
dataset. And we have 26, 897 pairs of borrowing and return 
records between 334 users and 25, 018 books. We separate 
the data set into two parts: the train set and the test set. The 
records between 2011-06-07 and 2012-04-30 are set to be the 
train set and it has 26, 640 records. And we can see that R  
is very sparse with density is 0.3188%. The records between 
2012-04-30 and 2012-06-30 are set to be the test set and it 
has 257 records.  
To implement CF algorithms and the blending methods, 
we have the following parameters: 
161
TABLE II.  PARAMETERS 
Parameter(s) 
Related 
Methods/A
 lgorithms 
k  KNNitem, KNNuser 
f  LMF 
?  LMF 
?  LMF, NN 
,a b   NN 
And we have the two parameters to evaluate CF algorithm 
and the blending methods: SR and RMSE. We first denote 
the number of predicted value is 'N and as there N data 
samples to predict. The SR is defined as follows: 
' /SR N N=                                                                 (5) 
And RMSE is defined as follows: 
'
 2
 '
 1
 1 (P( ) )
 N
 i i
 i
 RMSE x y
 N
 =
 = ?                             (6) 
In the above equation, P( )ix  represents the predicted 
value by the individual CF algorithm or the blending of 
different CF algorithms. 
C. Results 
We first implement the three traditional algorithms and 
the performance is shown in table III. 
TABLE III.  PERFORMANCE OF THE INDIVIDUAL CF ALGORITHM 
name RMSE description 
KNNuser-1 1.33426 KNNuser, Pearson 
correlation, k =1 
KNNitem-1 1.43217 KNNitem, 
Pearson correlation, 
k =1 
LFM-1 1.110883 LMF, ? =0.01, 
? =0.05, 
f =5 
LFM-2 1.110819 LMF, ? =0.01, 
? =0.05, 
f =10 
LFM-3 1.110815 LMF, ? =0.01, 
? =0.05, 
f =15 
LFM-4 1.110747 LMF, ? =0.01, 
? =0.05, 
f =18 
LFM-5 1.110839 LMF, ? =0.01, 
? =0.05, 
f =20 
From the above table, we can see that LFM-4 performs 
the best of all and its RMSE is 1.110747. There is a certain 
value of f which make LFM change towards differently 
directions. 
1) LR:We use LR to blend the different CF algorithms. 
And the performance is shown in table IV. 
TABLE IV.  RESULTS OF LR BLENDING 
name weights RMSE 
KNNuser-
 1+KNNitem-1 
0,0 1.0 
KNNuser-1+LFM-4 0,0 1.0 
We can see that both of the results are with RMSE=1.0 
and are much better than the individual algorithm. 
2) NN:We use NN to blend the different CF algorithms. 
Here, we set NN with 3 hidden layers and run NN 1000 
times with different initial parameters to find the best one by 
using PyBrain [15].  
While we use NN for blending the traditional CF 
algorithms, we set 0.01, 1a? = = and the constant 
0b = in our experiment and the algorithm performs well 
with the above parameters. 
And the training error of 1000 times is shown in figure 2.  
 
Figure 2.  Train Error with different initializations 
From the figure 2, we can see that most of the time the 
train error is below 1 and NN is a good at training models 
and extracting the parameters. 
And the performance on the test set is shown in table V. 
TABLE V.  RESULTS OF NN BLENDING  
name Average Training 
Error(1000 times) 
RMSE(the best) 
KNNuser-
 1+KNNitem-1 
0.0567 0.6321 
KNNuser-1+LFM-4 0.0559 0.5126 
D. Comparision 
We list the results of both the individual CF algorithms 
and the combined CF algorithms in table VI. 
TABLE VI.  PERFORMANCE COMPARISON 
162
name RMSE Evaluation 
KNNitem-1 1.43217 0% 
KNNuser-1 1.33426 6.84% 
LFM-4 1.110747 22.44% 
KNNuser-
 1+KNNitem-1 (LR) 
1.0 30.18% 
KNNuser-
 1+KNNitem-1 (NN) 
0.6321 55.86% 
KNNuser-1+LFM-4 
(NN) 
0.5126 64.21% 
By blending the traditional CF algorithms, the 
performance is more than 60% at the most better than the 
individual CF algorithm. And the lowest RMSE is 0.5126, 
which means that while a reader borrows a book we can 
predict how long he will borrow with error not exceeding 
0.5126*15 8Å days. 
V. CONCLUSION AND FUTURE WORKS 
In this paper, we focus on how to implement efficient and 
accurate CF algorithms for the academic library. By 
transforming the rating information from the borrowing 
records, we can use the traditional CF algorithms to predict 
how long the reader will borrow the book.  
And by blending the individual different CF algorithms, 
we can have much more accurate algorithms for predictions. 
And NN blending method performs better than LR blending 
method. By using NN with KNNuser and LFM, the error of 
the prediction will not exceed 8 days. 
In the future, we will focus on how to evaluate SR under 
the individual CF algorithms and the blending methods as 
while we use NN with KNNuser and LFM, SR is just 2.95%. 
And we will try also more blending methods which are 
described in [16]. 
ACKNOWLEDGMENT 
This work is supported by the National Key project of 
Scientific and Technical Supporting Programs of China 
(Grant No. 
2012BAH01F02,2013BAH10F01,2013BAH07F02); the 
National Natural Science Foundation of China(Grant 
No.61072060); the National High Technology Research and 
Development Program of China Grant No. 
2011AA100706);the Research Fund for the Doctoral 
Program of Higher Education (Grant No. 20110005120007); 
the Fundamental Research Funds for the Central Universities; 
Engineering Research Center of Information Networks, 
Ministry of Education. 
REFERENCES 
 
[1] J. Serrano-Guerrero, E. Herrera-Viedma, J. A. Olivas et al. ÒA google 
wave-based fuzzy recommender system to disseminate information in 
university digital libraries 2.0Ó, Information Sciences, 2011, vol. 181, 
no. 9, pp. 1503-1516. 
[2] S. Yu Chen, F. Yi Feng, ÒApplication research of association analysis 
with ClementineÓ, the International Conference on Software 
Engineering and Data Mining, 2010, pp. 445-449. 
[3] I. S. Sitanggang, N. Husin, A. Agustina, N. Mahmoodian, ÒSequential 
pattern mining on library transaction dataÓ, the Symposium in 
Information Technology, 2010, pp. 1-4. 
[4] S. Maneewongvatana, ÒA recommendation model for personalized 
book listsÓ,  the Symposium on Communications and Imformation 
Technologies, 2010, pp. 389-394. 
[5] B. Sarwar, G. Karypis, J. Konstan and J. Riedl, ÒAnalysis of 
recommendation algorithms for e-commerceÓ, in the proceedings of 
the 2nd ACM conference on Electronic commerce, 2000, pp. 158-167. 
[6] G. Adomavicius and A. Tuzhilin, ÒToward the Next Generation of 
Recommender Systems: A Survey of the State-of-the-Art and 
Possible ExtensionsÓ, IEEE Trans. on Knowl. and Data Eng., 2005, 
vol. 17, no. 6, pp. 734-749. 
[7] N. N. Liu and Q. Yang, ÒEigenrank: A ranking-based approach to 
collaborative fitleringÓ, In Proceedings of the 31st Annual 
International ACM SIGIR Conference on Research and Development 
in Information Retrieval, 2008, pp. 83-90. 
[8] B. Chandramouli, J. J. Levandoski, A. Eldawy and M. F. Mokbel, 
ÒStreamRec: A Real-Time Recommender SystemÓ, SIGMOD, 2011, 
pp. 1243-1245. 
[9] N. N. Liu, M. Zhao, E. Xiang and Q. Yang, ÒOnline Evolutionary 
Collaborative FilteringÓ, RecSys, 2010, pp. 95-102. 
[10] X. Yang, Z. X. Zhang and K. Wang, ÒScalable Collaborative Filtering 
Using Incremental Update and Loal Link PredictionÓ, CIKM, 2012, 
pp. 2371-2374. 
[11] V. Formoso, D. FernaNdez, F. Cacheda and V. Carneiro, ÒUsing 
profile expansion techniques to alleviate the new user problemÓ, 
International Journal of Informantion Processing and Management, 
2013, vol. 49, no. 3, pp. 659-672. 
[12] Yehuda Koren and R. M. Bell, ÒChapter: Advances in Collaborative 
FliteringÓ, Recommender Systems Handbook, Springer US, Online 
ISBN 978-0-387-85819-7, 2011, pp. 145-186. 
[13] Y. Koren, R. M. Bell and C. Volinsky, ÒMatrix factorization 
techniques for recommender systemsÓ, IEEE Computer, 2009, vol. 42, 
no. 8, pp. 30-37.  
[14] Y. Koren, ÒFactorization meets the neighborhood: a multifaceted 
collaboratie filtering modelÓ, KDD, 2008, pp. 426-434. 
[15] S. Tom, B. Justin, W. Daan, S. Yi et al., ÒPyBrainÓ, Journal of 
Machine Learning Reseach, vol. 11, 2011, pp. 743-746. 
[16] M. Jarhrer, A. Toscher, R. Legenstein, ÒCombining Predictions for 
Accurate Recommender SystemsÓ, SIGKDD, 2010, pp. 693-702. 
 
163

A CLOUD PLATFORM FOR FLOW-BASED ANALYSIS OF 
LARGE-SCALE NETWORK TRAFFIC 
Wenzhao Liao1, Zhiren Fu1 
1China Telecom Co., Ltd. Shanghai Branch, Shanghai 200085, China 
liaowenzhao@shtel.com.cn, fuzr@shtel.com.cn 
Keywords: Cloud Platform, Netflow, Network Traffic, 
MapReduce 
Abstract 
Traditional flow-based network monitoring and analyzing 
tools are challenged with large-scale network traffic for the 
big size of data.  This paper proposes a Cloud Platform 
for large-scale network analysis. It exploits HDFS and DataX 
to deal with the storage of heterogeneous big data. And the 
MapReduce model is applied to concurrently compute the 
analyzing and correlating on large amount of network flow 
data. Flow-based techniques are employed in collecting 
network data to make possible an application level analysis of 
network usage. Experimental results show that the 
performance of the platform is greatly improved compared 
with that of the traditional RDBMS application. 
1 Introduction 
In order to provide a good quality of service, network 
administrators need to maintain a healthy state of the network, 
by measuring the IP traffic between network nodes, analyzing 
customer behavior and collecting statistics data about 
important business network bandwidth, as well as proper 
network planning and evaluating. They need timely and 
accurate flow monitoring and statistical tools on network 
traffic and the business hosted upon it. 
Generally there are three kinds of ways to monitor the 
network traffic:  
1) Common network monitoring tools, such as MRTG, Cacti. 
These tools use the SNMP protocol to monitor port state and 
calculate port traffic. However, these methods can only 
monitor indicators such as the bytes counter of packets 
entering and leaving a single port. The traffic information is 
too rough and it can’t be understood what the traffic carries 
and where the traffic flows to.  
2) Packet capturing software, such as PCAP or DPI (Deep 
Packet Inspection). By unpacking and investigate the details 
of each packet, these analytical methods can learn more about 
the network traffic data, and can even replay network traffic. 
However, due to the huge amount of data, the unpacking 
analysis method is costly. And it often creates new points of 
failure to the network. Therefore, it is difficult to deploy.  
3) Flow-based analysis software, such as CISCO NetFlow. 
By extracting information from the flow, it can help analyze 
the network application – which is made up of flows -- in the 
network traffic. Those flows, if enriched by information such 
as associated radius log and DNS log data, can also be 
exploited to analyze the customer behavior. Flow-based 
measurement approach strikes a balance on the granularity 
and the efficiency of traffic monitoring. Moreover, it has the 
advantages of investment saving and fast deployment. 
Although flow-based network monitoring has many 
advantages over other monitoring methods, analysis and 
processing of the data is still a problem, mainly due to the 
extremely large amount of traffic data when facing with 
large-scale network traffic monitoring. Especially when a 
long term traffic data should be analyzed, the analysis task is 
hardly to be done with the traditional data storage and 
RDBMS technology. 
This paper proposes a Cloud network analyzing platform to 
deal with ‘flow’ of data analysis and processing. It uses 
general-purpose and low-cost hardware and software to build 
a cluster solution. This system copes well with the storage of 
big network flow and the analysis and statistics on big data. It 
can be used to analyze large-scale network traffic. 
2 Why Is the Network Analyzer Flow-Based 
2.1 Network Flow 
A network flow can be defined in many ways. Cisco’s 
standard NetFlow version 5 defines a flow as a unidirectional 
sequence of packets that share all of the following 7 values: 
Ingress interface (SNMP ifIndex) 
Source IP address 
Destination IP address 
IP protocol 
Source port for UDP or TCP, 0 for other protocols 
Destination port for UDP or TCP, type and code for ICMP, or 
0 for other protocols 
IP Type of Service 
The above definition of flows is also used for IPv6, and a 
similar one is used for MPLS and Ethernet flows.  
NetFlow is a network protocol developed by Cisco Systems 
for IP traffic information collection. NetFlow has become an 
industry standard for traffic monitoring and is supported on 
various platforms. Advanced NetFlow or IPFIX 
implementations like Cisco Flexible NetFlow also allow 
user-defined flow keys.  
NetFlow was initially implemented by Cisco, and described 
in an ‘informational’ document that was not on the standards 
track: RFC 3954 – Cisco Systems NetFlow Services Export 
Version 9. The NetFlow protocol itself has been superseded 
by Internet Protocol Flow Information eXport (IPFIX). Based 
on the NetFlow Version 9 implementation, IPFIX is on the 
IETF standards track with RFC 5101, RFC 5102, etc., which 
were published in 2008. 
Many vendors other than Cisco provide a NetFlow equivalent 
technology on their routers and switches, among them some 
 259 ICSSC 2013 
typical examples are listed in table 1. 
Table1. Some Typical NetFlow Equivalent Technology. 
Protocol vendors 
Jflow or cflowd Juniper Networks 
NetStream Huawei Technologies 
Cflowd Alcatel-Lucent 
AppFlow Citrix 
2.2 Data Enrichment 
The NetFlow data should be enriched with various data 
sources to deal with various network problems. 
Multiple data sources provide valuable enrichment for the 
whole picture of what’s occurring on a network. A centralized 
platform shall be provided to quickly and easily search, 
analyze, correlate and report across multiple data types. Here 
are the examples: 
Enriching NetFlow data with RADIUS Logs—RADIUS Logs 
is helpful to ensure the authenticity of network stakeholders. 
Since IP address assignments can change after each 
connection, malicious activity that occurs one day may be 
difficult to be traced back into the affected systems once the 
assignments changed. Attaching RADIUS logs along with 
NetFlow and other sources would enable this kind of tracing. 
Enriching NetFlow Data with DNS Logs—DNS logs can be 
used to detect abnormalities and unusual instances of DNS. 
For example, traffic communicating directly with an IP 
address with no corresponding DNS lookup would be 
suspicious and need further investigation. 
At least two fields would be enriched to the flow data, i.e., the 
user identity and the domain name of the site. 
2.3 Advantages of Flow-Based Analysis 
The distributive feature of modern enterprises raises a series 
of operative and infrastructural problems to network 
administrators. Network administrators are often required to 
diagnose network problems quickly. But they are always lack 
of the global visibility to find the root cause. To deal with 
these problems, IT administrators need to clearly observe how 
traffic traverses. They have to monitor and record activities in 
order to understand how the traffic interacts among network, 
applications and users. Only with effective network analyzer, 
it would be possible to have a complete picture of network 
traffic. 
Using the flow-based traffic analysis tools, the health state of 
the network can be easily and efficiently monitored:  
1) Support analyzing network flows by customized strategies. 
The analysis can help identify top talkers and dialogue, and 
then recognize which users and applications have utilized the 
maximum bandwidth, and so on. 
2) Understand the traffic trends and usage patterns. By 
viewing the trend of network traffic, the top-level application 
and peak usage time is able to be determined. 
3) Mark specific applications as special flows. A combination 
of ports and protocols is used to mark applications as 
exceptions. That means these ‘marked’ applications shall not 
be restricted so that the traffic they’ve caused in the flow 
statement can be specifically highlighted out. 
4) Detect the bandwidth usage of specific user groups, e.g., 
the utilization rate and the bandwidth or applications assigned 
to each organization (regions, countries, or provinces) defined 
by IP addresses. 
5) Perform user behavior analysis, such as network traffic 
characteristics of specific groups of users, or the access 
pattern to specific network application from certain user 
group. 
6) Improve the accuracy of resource calculation by the use of 
real-time bandwidth and network usage statistics. 
3 Cloud Platform for the Network Analyzer 
Generally for large-scale network traffic, the time window for 
the analysis cannot be too big, and so is for the correlation 
analysis of other data sources (such as RADIUS, DNS). The 
root of these problems lies in the huge amount of data. If 
taking into account the correlation of Radius, DNS (and 
others) with network flow data, the amount of data will be 
heavily multiplied. 
Traditional flow-based monitoring and analysis system is 
running in the centralized high-performance computers, rather 
than a distributed system. A prominent problem to be 
encountered in this mode is: how to store the data, and how to 
analyze and do statistics upon them. 
The SAN is the traditional data storage system. In order to 
improve performance, data is stored in the FC-SAN. With the 
continuous expansion of the data amount, such a solution will 
face the scalability problem. And with limited fiber-switch 
port number, the total capacity of this solution will eventually 
reach the upper limit. For data analysis, even with the 
performance tuning, traditional database will spent too much 
time dealing with TBs’ data. 
In this paper, a Cloud computing platform is introduced to try 
to solve the big data problem in flow-based monitoring and 
analysis system.  
 
 
Figure1. Architecture of the Cloud platform for large-scale 
network analysis. 
Figure 1 shows the architecture of the flow measurement and 
analysis system. Flow data from routers are delivered to 
Collector nodes, as well as the Radius logs and DNS logs are. 
The Collector nodes will correlate the flow data with the log 
data, and then use the data integration tools (DataX) to load 
the data into a distributed file system. Processing nodes are 
managed by the master node to process the data.  
Each Collector node is equipped with a flow collector?a log 
260 
file reader and a DataX library . Each processing node is 
equipped with a distributed cluster file system, and a 
MapReduce library and a Hive library. A flow collector 
receives flow packets, correlates with the log data, then stores 
them into local files system. The DataX then loads files from 
local disk to the cluster file system. NetFlow packets from 
routers or monitoring servers are usually sent to cluster nodes 
in unicast. Flow data are enriched with the log data and saved 
periodically into files associated with each flow-exporting 
router, and then uploaded to the cluster file system. The flow 
collector uses the NetFlow as the collecting and processing 
tool. DataX will load the data in the local file system into the 
HDFS. Mapper and Reducer (or Hive) will analyze flow data 
with Hadoop MapReduce library. 
3.1 Distributed Computing with ‘Map-Reduce’ 
Model  
Hadoop is employed for the Cloud network analysis system. 
It provides open MapReduce software framework and 
distributed file system. HDFS is suitable for handling very 
large files with the streaming data access pattern that is a 
write-once and read-any pattern. In HDFS, a name node 
operates management of the file system metadata and 
provides management and control services, while a data node 
supplies block storage and retrieval services. A name node at 
the master site will perform recovery and automatic backup of 
name nodes. The block size (64 MB by default) and the 
number of replicated blocks in HDFS could be customized, 
according to the fault-tolerance policy. 
To meet the customized purpose of flow analysis, appropriate 
Mapper and Reducer programs should be implemented. The 
map-reduce programming model was introduced by Google 
in 2004, in a paper describing the architecture along with 
some limited aspects of their non-public implementations. 
One of the primary advantages of programming in the 
map-reduce model is that it abstracts away, and even 
eliminates – to some degree -- the complexity of writing a 
concurrent program. The programmer implements two 
functions: map and reduce. The framework takes care of 
invoking these functions on the input data and scheduling 
parallel execution of them across any number of computation 
nodes.  
3.2 Data Integration with Datax 
Input and output data for the network analyzer are data of 
heterogeneous. They may be a text file, a binary file or some 
data from RDBMS. The Cloud platform uses a high 
performance big data integration tool ‘DataX’ to access and 
integrate different data. 
 
 
Figure2. Integration of heterogeneous big data with DataX. 
DataX has some specific features: 
1) High-speed switching between heterogeneous database / 
file system data. 
2) Using Framework + plugin architecture to build a 
Framework that deals with buffering, flow controlling, 
concurrency, and loading the context of high-speed data, 
exchanging most of the technical problems, providing a 
simple interaction interface with widget. 
3) Run mode: Stand-alone, data transfer process is completed 
within a single process, the entire memory operation does not 
read and write disk, nor the IPC. 
4) Open framework, developers can develop a new plug-in to 
quickly support new database / file system.  
In our experimental platform, the corresponding data file 
reader plug-in has been developed to load data into HDFS. 
3.3 Flow Analyzing 
A network flow analysis requirement can typically be 
expressed as a query consisting of one of more of the 
following: 
A filter expression defining what flow records should be 
included in the processing. 
A set Q = {Q1,...,Qm} of aggregation queries, each defining a 
set of flow record fields to aggregate on. 
A sort condition to indicate how the end result should be 
sorted, such as sorting by the total number of bytes. 
A limit statement to restrict the number of returned rows, 
typically indicating the lines that he or she is mostly 
interested. 
To complete the query, the MapReduce programming model 
is exploited. In this model, the computation takes a set of 
input key/value pairs, and produces a set of output key/value 
pairs. Map and Reduce are two basic functions in the 
MapReduce computation. A user writes the Map method that 
takes an input pair and produces intermediate key/value pairs. 
The Hadoop MapReduce library will group the intermediate 
values according to the keys. Also, a user writes the Reduce 
method to merge the intermediate values for smaller values. 
To implement various data analysis programs with 
MapReduce, appropriate input key/value pairs for each 
analysis program have to be defined. For example, to analyze 
the behavior of a message sender, the user id and the octets 
counter can be defined as the key-value pairs.  
With MapReduce, typical flow analysis functions provided by 
well-known Internet traffic statistics programs can be realized 
without too much effort. Figure 3 shows the procedure of the 
261 
MapReduce-based program that performs mail-transform 
analysis of flow data. 
 
Flow data(HDFS) 
timestamp,srcip,dstip,srcport,dstport,ifindex,TOS,pr
 otocal,userid,domain_name
 Read each line
 Parse the 
userid,octets
 Filter(dstport=80)
 Add result set
 userA,64
 userB,64
 userA,256
 userC,128
 Intermediate pairs
 Read the 
tempdata
 userA,[64,256]
 userB,[64]
 userC,[128]
 reduce
 userA,320
 userB,64
 userC,128
 Reducer
  
Figure3. A MapReduce program for mail analysis. 
1) Input flow files: After flow data from flow probes is stored 
on the local disk, the raw NetFlow v5 files are moved to the 
HDFS. As the current Hadoop mapper supports only text files 
as the input format, NetFlow files are converted to text-format 
ones. As the size of text-format flow files is much larger than 
that of binary-format ones, binary flow files have to be 
supported as the inputs for Mapper. Otherwise, the 
gzip-compressed text flow files could be used for the input 
format. 
2) Mapper: The flow mapper reads each flow record that is 
split by new lines. A flow record has attributes of timestamp, 
IP addresses, port, protocol, flag, octet count, packet count, 
and interface numbers. After reading a flow record, we filter 
out necessary flow attributes for a flow analysis job. As 
shown in Fig. 3, when the flow analysis job sums up octet 
counts per user_id, we set key/value pairs as (user_id, octets). 
The flow map task will write its temporary results onto the 
local disk. 
3) Reducer: The flow reducer will be called with the inputs as 
intermediate values generated by flow mappers. As in the 
mail transfer example, a value list of octets belonging to the 
user_id will be summed up. After merging octet values 
associated with the user, the flow reducer writes the octet 
value for each user_id. 
4 Experimental Results 
In order to evaluate the performance of flow analysis with 
MapReduce, a small Hadoop testbed including a master node 
and four data nodes is built up. Each node has quad-core 2.83 
GHz CPU, 4 GB memory and 1.5 TB hard disk. HDFS is 
used as the distributed file system. All Hadoop nodes are 
connected with Gigabit Ethernet cards. 
Flow-tools are used to collect NetFlow v5 packets sent by a 
router. Then, exported flows are saved into a file every five 
minutes. Flow data on a network consisting of 100 interfaces 
was captured for months, and a few TB of traffic data was 
collected to test the performance of the program. 
The performance of traditional database applications was 
compared with that of MapReduce program. The experiments 
showed that the MapReduce program performance has been 
greatly improved when the data amount changed from a few 
GB to a few TB. 
Additionally, we test the hive framework. Hive provides a 
query language, HiveQL, that is similar to SQL and more 
general than the Pig query language. All data sets processed 
by Hive are exposed as tables. These tables can be processed 
by selection, insertion, joining or other operations which are 
often associated with relational algebra. For example, a Hive 
query can be written like this way:  
SELECT flows.src_addr, SUM (flows.packets), SUM 
(flows.bytes) FROM flows GROUP BY flows.src_addr;   
Similarly to SQL, Hive supports grouping of records and 
calculating sums and record counts for the aggregated data, a 
feature that has been utilized in the example above. The 
listing above also illustrates the similarity between SQL and 
HiveQL, and shows that HiveQL can be considerably less 
verbose than Pig Latin. Although it is not shown in the 
example, Hive also allows filtering, sorting and limiting of 
the result set. 
5 Conclusions 
This paper proposes a Cloud Platform for large-scale network 
analysis. It exploits HDFS and DataX to deal with the storage 
of heterogeneous big data. And the MapReduce model is 
applied to concurrently compute the analyzing and correlating 
on large amount of network flow data. Experimental results 
shows that the performance of the platform is greatly 
improved compared with the traditional RDBMS application. 
Acknowledgments 
This work is supported by Innovation Action Plan supported 
by Science and Technology Commission of Shanghai 
Municipality (No.11511500200) 
References 
[1] An Internet traffic analysis method with MapReduce, In 
proceeding of: Network Operations and Management 
Symposium Workshops, 2010, P357-361. 
[2] http://code.taobao.org/p/datax/wiki/index/ 2013 
[3] http://en.wikipedia.org/wiki/NetFlow/ 2013 
[4] http://hadoop.apache.org/ 
[5] Distributed NetFlow Processing Using the Map-Reduce 
Model,http://ntnu.divaportal.org/smash/get/diva2:35247
 2/FULLTEXT01 
262 

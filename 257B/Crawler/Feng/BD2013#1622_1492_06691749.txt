Scalable Approximation of Kernel Fuzzy c-Means
 Zijian Zhang
 Department of Electrical and Computer Engineering
 Michigan Technological University
 Houghton, MI 49931 USA
 Email: zijianz@mtu.edu
 Timothy C. Havens
 Department of Electrical and Computer Engineering
 Department of Computer Science
 Michigan Technological University
 Houghton, MI, 49931 USA
 Email: thavens@mtu.edu
 Abstract—Virtually every sector of business and industry that
 use computing, including financial analysis, search engines, and
 electronic commerce, incorporate Big Data analysis into their
 business model. Sophisticated clustering algorithms are highly
 desired to deduce the nature of data by assigning labels to
 unlabeled data. We address two main challenges in Big Data.
 First, by definition, the volume of Big Data is too large to be
 loaded into a computer’s memory (this volume changes based on
 the computer used or available). Second, in real-time applications,
 the velocity of new incoming data prevents historical data from
 being stored and future data from being accessed. Therefore, we
 propose our Streaming Kernel Fuzzy c-Means (stKFCM) algo-
 rithm, which reduces both computational complexity and space
 complexity significantly. The proposed stKFCM only requires
 O(n2) memory where n is the (predetermined) size of a data
 subset (or data chunk) at each time step, which makes this
 algorithm truly scalable (as n can be chosen based on the
 available memory). Furthermore, only 2n2 elements of the full
 N ? N (where N >> n) kernel matrix need to be calculated
 at each time-step, thus reducing both the computation time in
 producing the kernel elements and the complexity of the FCM
 algorithm. Empirical results show that stKFCM, even with very
 small n, can provide clustering performance as accurately as
 kernel fuzzy c-means run on the entire data set while achieving
 a significant speedup.
 Index Terms—kernel clustering; fuzzy c-means; scalable algo-
 rithms; streaming data; projection
 I. INTRODUCTION
 The ubiquity of personal computing technology has pro-
 duced an abundance of staggeringly large data sets which
 may exceed the memory capacity of a computer (whether
 that computer is a cell phone or a high-performance cluster).
 Large data sets come from streaming data as well those that
 are presented to a system sequentially such that future data
 cannot be accessed. These challenges stimulate a great need
 for sophisticated algorithms by which one can elucidate the
 similarity and dissimilarity among and between groups in these
 gigantic data sets.
 Clustering is an exploratory tool in which data are separated
 into groups, such that the objects in each group are more
 similar to each other than to those in different groups. The ap-
 plications of clustering algorithms are innumerable. Clustering
 is a common technique widely used in pattern recognition, data
 mining, and data compression for deducing the nature of a data
 set by assigning labels to unlabeled data. Clustering itself is a
 general task to be solved and plenty of algorithms have been
 TABLE I
 IMPORTANT ACRONYMS AND NOTATION
 Acronym Definition
 FCM fuzzy c-means
 KFCM kernel FCM
 stKFCM streaming KFCM
 KPC kernel patch clustering [1]
 RKHS reproducing kernel Hilbert space
 Notation Definition
 c or k number of clusters
 N number of objects
 n number of objects in data chunk
 x feature vector ? Rd
 X set of x
 X set of X , {X0, . . . , Xt, . . .}
 U partition matrix
 ui ith column of U
 v cluster center
 w weight vector
 ?i kernel representation of xi, i.e., ?(xi)
 ? set of ?
 ?(xi,xj) kernel function, ?(xi,xj) = ?i · ?j
 K kernel matrix, K = [?(xi,xj)], ?i, j
 HK reproducing kernel Hilbert space imposed by K
 d?(xi,xj) kernel distance, ||?i ? ?j ||2[i] set of integers, {1, 2, . . . , i}
 proposed for solving this problem such as k-means, single-
 linkage clustering, and Gaussian-mixture-model clustering.
 The k-means algorithm is one of the most popular clustering
 algorithms due to its simplicity. For a set of N feature
 vectors (or objects), the program will choose k cluster centers
 randomly at the beginning. Each data point will be assigned
 to its nearest cluster center, then the cluster centers will be
 recomputed. These steps are repeated until the algorithm con-
 verges (and there are many ways by which convergence can be
 defined). The output will be a partition matrix U ? {0, 1}N?k,
 a matrix of Nk values. Each element uik is the membership
 of vector xi in cluster k; and the partition matrix element
 uik = 1 if xi belongs to cluster k and is 0 otherwise.
 Fuzzy c-means (FCM) is analogous to the k-means algo-
 rithm with fuzzy partitions, which gives more flexibility in
 that each object can have membership in more than one cluster.
 The constraint on fuzzy partitions is that all the memberships
 of an object must sum to 1, thus ensuring that every object has
 unit total membership in a partition: (∑k uik = 1). Hence, the
 partition matrix U ? [0, 1]N?c, where the partition elements
 are now on the interval [0, 1].
 161
 2013 IEEE International Conference on Big Data
 978-1-4799-1293-3/13/$31.00 ©2013  IEEE 
The FCM (as well as the k-means) model is based on the
 assumption that the feature vectors are grouped in similarly-
 sized hyperspheres. Kernel methods can overcome this lim-
 itation by projecting the vectors into a higher dimensional
 feature space where the patterns can be discovered as linear
 relations [2]. Consider some non-linear mapping function
 ? : x ? ?(x) ? RD? where D? is the dimensionality of
 the higher-dimensional feature space created by the function ?.
 For (most) kernel algorithms, including kernel FCM, explicitly
 transforming x is not necessary. Instead, a kernel matrix
 K is used, which consists of the pairwise dot products of
 the feature vectors in a transformed high dimensional space
 HK ; this space is called the Reproducing Kernel Hilbert
 Space (RKHS). Given a set of N objects, literal kernel
 FCM (KFCM) requires to store an N ? N kernel matrix
 K = [?(xi,xj)] = [?(xi) · ?(xj)], i, j ? [N ], which poses
 challenges for clustering Big Data. Although an abundance of
 research has been conducted on fuzzy clustering algorithms
 for Big Data [1, 3–9], only a select few of these algorithms
 are appropriate for kernel methods.
 In this paper, we devise an approximation of the KFCM
 algorithm for streaming data, i.e., big data which could be
 viewed as data streams. We assess the performance of our
 algorithm by comparing the partition matrix to that of the
 literal KFCM. Empirical results demonstrate that our algorithm
 could provide similar results to the literal KFCM while only
 requiring access to small data chunks in the current and
 previous time step. The memory requirement is reduced from
 O(N2) to O(n2), where N and n are the size of the full data
 set and each subset data chunk, respectively.
 II. RELATED WORK
 To date, a substantial number of algorithms have been
 developed for clustering big data. Roughly, these algorithms
 can be categorized into three classes: sampling, distributed
 clustering, and data transformation algorithms.
 A. Sampling and non-iterative extension
 Sampling the data set is the most basic and obvious way to
 address big data. In sampling methods, algorithms are run on a
 reduced representative sample, and then the sample partition is
 non-iteratively extended to approximate the clustering solution
 for the remaining data in the full data set. If the data are
 sufficiently sampled, there is only a small difference between
 the result of the approximate partition and the result of
 clustering the entire data set. These algorithms are also called
 extensible algorithms. The notion of extensibility for FCM was
 introduced in [10]. Algorithms that produce an approximate
 result of the full data set by first solving the problem using
 a sample set and then non-iteratively extending the result on
 the full data set are referred to as extensible algorithms.
 Sampling approaches can be further divided into two cat-
 egories: random sampling and progressive sampling. Pro-
 gressive sampling schemes for FCM were well studied in
 [11]. The authors showed that progressive sampling is widely
 used in many clustering algorithms. Sampling schedule and
 termination criteria are the most central components of any of
 these approaches. The most well-known progressive sampling
 method is generalized extensible fast FCM [12] which is the
 extension of [10]. In [12], the algorithm starts with statistics-
 based progressive sampling and terminates with a representa-
 tive sample that is appropriate to capture the overall nature of
 the data set. Reference [12] extends this algorithm so that it
 could be applied to more general cases. Instead of clustering
 numerical object data, [13] and [14] extend the algorithm to
 attack the problem of clustering numerical relational data. In
 kernel clustering, cluster centers are linear combinations of
 all the data points to be clustered; hence, the sampling ap-
 proaches mentioned previously are inappropriate. The authors
 of [15] and [8] tackled the problem of kernel clustering by
 proposing a novel sampling of the kernel matrix which results
 in significant memory savings and computational complexity
 reduction while maintaining a bounded-error solution to the
 kernel k-means and KFCM problem; although, the solutions
 in [8, 15] are not truly scalable to big data as they require the
 loading of an N?n rectangular of the kernel matrix; hence, as
 N grows, eventually there is a point at which the only loadable
 n becomes less than 1, thus invalidating the scalability of the
 algorithm.
 B. Distributed clustering
 Distributed clustering algorithms could be classified into
 two types: incremental loading of data subsets that can be
 fit into current memory capacity and divide-and-conquer ap-
 proaches.
 1) Incremental clustering: Algorithms in this category se-
 quentially load small chunks or samples of the data, clustering
 each chunk in a single pass, and then combining the results
 from each chunk. Representative algorithms are [16] and [17].
 In these approaches, the clusters are updated periodically using
 information from both the incoming data and obsolete data.
 Single pass FCM (spFCM) was proposed in [3]. This
 algorithm performs weighted FCM (wFCM) on sequential
 chunks of data, passing clustering centers from each chunk
 onto the next. At each time step, the algorithm clusters a union
 set consisting of data in the current step and the cluster centers
 passed from previous step. The authors of [4] extend [3] by
 incorporating more results from multiple preceding time steps
 and in [5], they propose an algorithm that passes c weight
 values which are sums of membership values of the points
 in the current subset onto next iteration. Another incremental
 algorithm called bit-reduced FCM (brFCM) was proposed in
 [6]. This algorithm first bins the data and then clusters the
 bin centers. While the performance of brFCM highly depends
 on the binning strategy; brFCM has been showed to provide
 very efficient and accurate result on image data. Havens et al.
 extended spFCM and other incremental FCM algorithms to
 kernel clustering in [7], although the results of these kernel
 extensions were disappointing.
 Bradley introduced a data compression technique in [18].
 This algorithm uses a buffer to contain the current subset
 of data. The data are then compressed twice. In the first
 162
compression, objects that are unlikely ever to move to other
 clusters are discarded. Those objects are found by calculating
 the Mahalanobis distance between the object and the cluster
 center. Those that fall within a chosen radius are then chosen.
 In the second compression period, more cluster centers are
 introduced into the data set in order to find more stable points.
 After these two compressions, the space will be filled with
 new data. The algorithm will keep running until all the data
 have been processed. Farnstrom introduced a special case of
 this algorithm [19] in which all the points in the buffer are
 discarded each time.
 Gupta uses evolutionary techniques to search for the global
 optimal solution to the sum of the squares (SSQ) problem
 which is required to find cluster centers [20]. Each chunk is
 viewed as a generation. The fittest cluster centers survive to
 the next generation; bad centers are killed off with new ones
 selected. In kernel methods, no actual cluster centers exist in
 RKHS. Passing cluster centers to next time step is unpractical.
 The author of [1] presented Kernel Patching Clustering(KPC)
 algorithm. This algorithm selects approximate pseudo-centers
 at each time step, merging them repeatedly until the entire data
 set has been processed. Since the space complexity depends on
 only the size of chunks, algorithms of this type are (usually)
 truly scalable.
 2) Divide and conquer: Algorithms in this category cluster
 each chunk in sequence as well. But rather than passing
 the clustering solution from one chunk onto the next, these
 algorithms aggregate the solutions from each chunk in one
 final run. Due to this final run, most of the algorithms in
 this type are not truly scalable. Online FCM (oFCM) was
 proposed in [9]. oFCM aggregates the solutions from each
 data chunk by performing wFCM on all the resultant cluster
 centers. Again, it was shown in [7], that the kernel extension
 of oFCM performed poorly. Reference [21] views the problem
 of merging different results from disjoint data sets as the
 problem of reaching a global consensus, while [22] assigns a
 weight to the cluster centers in each chunk, and then performs
 LSEARCH on all the weighted centers retained. LSEARCH
 is a local search algorithm that starts with an initial solution
 and then refines it by making local improvements. Other
 algorithms like those proposed in [23] and [24] use special
 initialization techniques to improve the accuracy of the k-
 means algorithm.
 C. Data transformation methods
 Algorithms of this sort transform data into other structures
 with the intention of making the clustering process more
 efficient. Perhaps one of the earliest well-known clustering
 algorithms for data streams is BIRCH [25], which transforms
 the entire data set into a tree-like structure called a clustering
 feature (CF) tree. The leaves of the CF tree are then clustered.
 PAM [26] transforms the data set into a graph structure, and
 then searches for a minimum on the graph. CLARANS [27] is
 a variation of CLARA [26] and draws a sample of the data set
 and applies PAM on the sample. The key difference between
 CLARA and CLARANS is that CLARA draws the sample at
 the beginning of the search while CLARANS draws it at each
 step of the search. The benefit of the CLARANS approach
 over CLARA is that the search is not confined to a localized
 area.
 While many of the algorithms mentioned in Section II
 produce high-quality partitions for big data sets, unless noted,
 they are not appropriate for kernel clustering. The only (that
 we know of) truly scalable approach to kernel fuzzy clustering
 is the spKFCM algorithm proposed in [7, 28] and, as shown in
 [7], the spKFCM approach produces less-than-desirable results
 for some data sets.
 III. KERNEL FCM
 Kernel FCM (KFCM) can be generally defined as the
 constrained minimization of
 Jm(U ;?) =
 c∑
 j=1
 n∑
 i=1
 umij ||?i ? vj ||2, (1a)
 =
 c∑
 j=1
 (
 n∑
 i=1
 n∑
 k=1
 (
 umiju
 m
 kjd?(xi,xk)
 )/2 n∑
 l=1
 umlj
 )
 ,
 (1b)
 where U is a fuzzy partition, m > 1 is the fuzzification pa-
 rameter, and d?(xi,xk) = ?(xi,xi)+?(xk,xk)? 2?(xi,xk)
 is the kernel-based distance between the ith and kth feature
 vectors. The function ?(xi,xk) = ?(xi) · ?(xk) is the kernel
 function, which can take many forms, including the popular
 radial basis function (RBF), polynomial, and linear forms.
 KFCM solves the optimization problem min{Jm(U ;?)} by
 computing iterated updates of
 uij =
 [
 c∑
 k=1
 (
 d?(xi,vj)
 d?(xi,vk)
 ) 1
 m?1
 ]
 ?1
 , ?i, j, (2)
 where the kernel distance between input datum xi and cluster
 center vj (in the RKHS) is
 d?(xi,vj) = ||?(xi)? vj ||2. (3)
 The cluster centers v are linear combinations of the feature
 vectors,
 vj =
 ∑n
 l=1 u
 m
 lj?(xl)∑n
 l=1 u
 m
 lj
 . (4)
 Equation (3) cannot by computed directly, but by using
 the identity Kij = ?(xi,xj) = ?(xi) · ?(xj), denoting
 u˜j = umj /
 ∑
 i |umij | where umj = (um1j , um2j , . . . , umnj)T , and
 substituting (4) into (3) we get
 d?(xi,vj) =
 ∑n
 l=1
 ∑n
 s=1 u
 m
 lj u
 m
 sj?(xl) · ?(xs)∑n
 l=1 u
 2m
 lj
 + ?(xi) · ?(xi)? 2
 ∑n
 l=1 u
 m
 lj?(xl) · ?(xi)∑n
 l=1 u
 m
 lj
 =u˜Tj Ku˜j + eTi Kei ? 2u˜Tj Kei
 =u˜Tj Ku˜j + Kii ? 2(u˜Tj K)i, (5)
 where ei is the n-length unit vector with the ith element equal
 to 1. This formulation of KFCM is equivalent to that proposed
 163
in [29] and, furthermore, is identical to relational FCM [30]
 if the kernel ?(xi,xk) = ?xi,xj? is used [31].
 Equation (5) shows the obvious problem which arises when
 using kernel clustering with big data: the distance equation’s
 complexity is quadratic with the number of objects, i.e. O(N2)
 (assuming the kernel matrix is precomputed). Furthermore, the
 memory requirement to store K is also quadratic with the
 number of objects.
 A. Weighted KFCM
 Assume that each data point in X has a different weight,
 wi, which represents its influence on the clustering solution.
 These weights can be applied to the KFCM objective at (1b)
 by
 Jm(U ;?) =
 c∑
 j=1
 n∑
 i=1
 wiu
 m
 ij ||?i ? vj ||2, (6)
 where it is now obvious how w ? Rn, wi ≥ 0, affects the
 solution of KFCM. Hence, the only difference between KFCM
 and wKFCM is that the distance at (5) in the iterated updates
 of (2) are computed with the weights included, i.e.,
 dw? (xi,vj) = 1?w ? umj ? (w ? u
 m
 j )TK(w ? umj ) + Kii
 ?
 2
 ?w ? umj ?
 ((w ? umj )TK)i , (7)
 wherew is the vector of weights and ? indicates the Hadamard
 product. The idea behind wKFCM will be instrumental in our
 design of the proposed stKFCM algorithm.
 IV. STREAMING KFCM ALGORITHM
 Consider a streaming data set X = {X1, X2, . . . , Xt, . . .},
 and its projection onto a set of RKHSs HKt , such that
 ? = {?1, ?2, . . . , ?t, . . .} are the kernel representations of X ,
 where ?t = {?t1, ?t2, . . . , ?tn}, and ?ti = ? (xti) ? HKt . The
 goal of the proposed stKFCM algorithm is to approximate the
 clustering solution of KFCM on X , while only having access
 to a limited number of chunks of X up to some time t. The
 naive solution is to store all history of X and run KFCM on
 all the samples. However, the memory requirement for storing
 the kernel matrix K is O(N2), where N is the number of
 samples in the history; hence, KFCM is not appropriate for
 big data, which all streaming data sets become at some point.
 The proposed stKFCM algorithm only requires access to Xt
 and Xt?1 at time step t and only requires O(n2) storage
 requirement, where n is the size of the data chunk Xt. Figure 1
 illustrates the sub-matrices required by the stKFCM algorithm.
 In similar spirit to stKFCM, Havens proposed the stream-
 ing kernel k-means (stKKM) algorithm in [32]. It provides
 accurate results for using kernel k-means with streaming
 or incremental data. The main idea of this algorithm is to
 take the cluster centers V t?1 = {vt?11 , . . . ,vt?1c }, where
 vt?1i ? HKt?1 , and project them into HKt (the RKHS
 produced by Kt) as meta-vectors. We will explain meta-
 vectors in the following section, which proposes stKFCM.
 Using these meta-vectors, information is passed from previous
 n
 n
 Kt-1 K(t-1, t)
 Kt
 N
 N
 stKFCM only requires storage of 2 (n ? n)
 matrices at each time step
 Fig. 1. The stKFCM algorithm only requires storage of two (n?n) portions
 of the full (N ?N) kernel matrix at each time-step. The sub-matrix Kt,t?1
 is used to project the cluster centers from time (t?1) into the RKHS imposed
 by the kernel matrix Kt.
 time steps into the current time step. Then, at each time step,
 the data chunk Xt is clustered together with the (appropriately
 weighted) meta-vectors. We now propose the use of meta-
 vectors to approximate KFCM with the stKFCM algorithm.
 A. Meta-vectors
 Assume we are clustering not only the current data chunk
 Xt, but also a set of meta-vectors A = {a1,a2, ...,an}, ai ?
 HKt . The meta-vectors ai are linear combinations of all ?ti ?
 ?t; i.e., aj =
 ∑n
 i=1 ?ij?i, ?ij ? R.
 Proposition 1. Let the partitions U t and U? denote fuzzy
 partition values of Xt and A, respectively. Let wXt and w?
 be the weights of Xt and A. Since the cluster centers are linear
 combinations of the feature vectors, the cluster center vtc in
 the kernel feature space can be written as
 vtk =
 n∑
 i=1
 q˜tik, (8)
 where
 qtk =
 ?
 ???
 wXt1 (ut1k)m +
 ∑c
 j=1 ?
 t
 1jw
 ?
 j (u?jk)m
 wXt2 (ut2k)m +
 ∑c
 j=1 ?
 t
 2jw
 ?
 j (u?jk)m
 . . .
 wXtn (utnk)m +
 ∑c
 j=1 ?
 t
 njw
 ?
 j (u?jk)m
 ?
 ??? ; (9a)
 q˜tk =
 qtk
 |wXt ? (utk)m|+ |w? ? (u?k )m| . (9b)
 Proof: The meta-partition q˜t at (9) is formed by repre-
 senting the cluster center vtk as the weighted linear sum of the
 data chunk ?t (i.e., Xt) and the meta-vectors A, and using the
 fact that A is a linear sum of ?t itself. See [32] for a more
 detailed proof of a similar proposition.
 Now we show how to project the cluster centers produced at
 time t?1 (i.e., in HKt?1 ) to the current time t. Note that this
 164
proposition is similar to that proposed in [32] for the use with
 stkKM; however, it is important to note that the formulation
 of q˜k for KFCM clustering is different from that of kernel
 k-means.
 Proposition 2. A projection of vt?1k ? HKt?1 into HKt can
 be computed by the optimization over meta-vectors ak,
 argmin
 ak
 ||vt?1k ? ak||2 =
 n∑
 i=1
 ?tik?ti. (10)
 Proof: The optimization has the closed form solution of
 ?tk =
 (
 Kt
 )
 ?1K(t,t?1)q˜t?1k , (11)
 K(t,t?1) = ?(xti,xt?1j ), i, j = 1, ..., n. (12)
 Remark 1. The closed form solution for the weights ?tk at
 (11) is the solution to the optimization under the squared
 Euclidean norm. The inverse operation in (11) is often best
 replaced, in practice, with a pseudo-inverse, which we denote
 by (·)†. One could also use a simple gradient descent to
 minimize the quadratic. Furthermore, this is only one way to
 project the cluster center vt?1k into the current RKHSHKt . We
 imagine that an L1-norm optimization could also find utility
 when a sparser solution for ?tk is desired.
 The distances between the cluster center vTk and each of ai,
 ?ti, and an arbitrary feature vector ?(x) are computed as
 ||ai ? vtk||2 = (?ti)TKt?ti + (q˜tk)TKtq˜tk ? 2(?ti)TKtq˜tk;
 (13a)
 ||?ti ? vtk||2 = Ktii + (q˜tk)TKtq˜tk ? 2(Ktq˜tk)i; (13b)
 ||?(x)? vtk||2 = ?(x,x) + (q˜tk)TKtq˜tk ? 2?(x, Xt) ˜qtk.
 (13c)
 These distances at (13) allow us to propose the stKFCM
 algorithm at Algorithm 1. The algorithm has the following
 basic steps:
 1) The KFCM solution is computed for the first data chunk;
 2) The weights of each cluster center are computed as
 the sum of the partition elements associated with each
 center;
 3) The cluster centers are projected into the next time step;
 4) The meta-partition q˜t is computed;
 5) The partition of the meta-vectors A is updated;
 6) The partition of the feature vectors Xt is updated;
 7) Optionally, the partition of the full data set X can be
 computed in one single-pass at the end.
 The stKFCM algorithm is essentially a single-pass algorithm
 that computes the KFCM cluster solution of each Xt together
 with the weighted meta-vectors A, which are the projected
 cluster centers from step t ? 1. Hence, the (compressed)
 information from all previous time-chunks is passed down
 through the meta-vectors A.
 Remark 2. The important projection step at Line 3 of the
 stKFCM algorithm is equivalent to taking the vectors vt?1
 (appended by n zeroes) represented in the RKHS of the kernel
 matrix
 Kt?1,t =
 [
 Kt?1 K(t?1,t)
 K(t,t?1) Kt
 ]
 ,
 and using the Nystrom approximation to represent them in the
 low-rank approximation of the kernel matrix computed as (and
 reordered such that the time t columns are first)
 ˜Kt,t?1 =
 [
 Kt|K(t,t?1)
 ]T (
 Kt
 )
 ?1
 [
 Kt|K(t,t?1)
 ]
 .
 Furthermore, it is known that the error ?Kt,t?1? ˜Kt,t?1?2 ≤
 ?n+1 + O(N/√n), where ?n+1 is the (n + 1)th eigenvalue
 of Kt,t?1 [33]. Jin et al. [34] also showed that this error is
 further bounded if there is a large eigengap, which is often
 the case for data sets that can be partitioned into high-quality
 clusters. What this shows for the stKFCM algorithm is that
 the projection error at each step is bounded; hence, the total
 error is bounded by the size and number of data chunks used
 to complete the stKFCM process.
 V. EXPERIMENTS
 We evaluated the performance of our algorithm on data
 sets for which ground truth exist. In these experiments, we
 compared the hardened partition from the proposed stKFCM
 to the the recently proposed Kernel Patch Clustering (KPC)
 [1], as well as the KFCM partition run on the whole data set.
 We present results for different chunk sizes, from 0.0001N
 to 0.5N . The value of the fuzzifier m was fixed at 1.7. The
 experiments on the 2D15 and 2D50 were run on a Intel Core 2
 Duo core processor with 4 GB of memory. Results of MNIST
 and Forest data set were generated by a quad-core CPU with
 32 GB of memory. The results are expressed as the mean
 and standard deviation over 100 independent experiments, with
 random initializations and random data sample ordering.
 A. Data sets
 1) 2D15: This synthetic data set is composed of 5,000 2-
 dimensional vectors. As shown in Fig. 2(a), it is obvious that
 15 clusters are preferred in this data set. We used an RBF
 kernel with width of 1 on this data set.
 2) 2D50: This data set consists of 7,500 2-dimensional
 vectors with 50 clusters preferred. An RBF kernel with a width
 of 1 was used.
 3) MNIST: These data were collected from 70,000 28?28
 images of handwriting digits from 0 to 9 by the National Insti-
 tute of Standards and Technology (NIST). We normalized the
 value of each pixel to the unit interval and organized the pixels
 column-wise into a single 784-dimensional vector. Therefore,
 this data set is composed by 70,000 784-dimensional vectors
 with 10 clusters preferred. An inhomogeneous polynomial
 kernel with degree of 5 was used in our experiment.
 165
Algorithm 1: Streaming Kernel Fuzzy c-Means (stKFCM)
 Input: number of clusters – c; fuzzifier – m;
 X = {X0, X1, X2, . . .}; kernel – ?
 Compute K0 = ?(X0, X0)
 1 U0 =KFCM(c, m, K0)
 q˜0c = u0c/|u0c |, c = 1, . . . , k
 for t = 1, 2, . . . do
 Kt = ?(Xt, Xt), K(t,t?1) = ?(Xt, Xt?1)
 for k = 1 to c do
 2 wk = |qt?1k |
 3 ?tk = (Kt)†K(t,t?1)q˜(t?1)k
 u?ik = 1, if i = k, else u?ik = 0, U t = [0]n?c
 while any u?ij or utij changes do
 4 Compute q˜tk with (9)
 for i, j = 1, . . . , c do
 5 u?ij =
 ?
 ? c∑
 k=1
 (d?(ai,vtj)
 d?(ai,vtk)
 ) 1
 m?1
 ?
 ??1
 where d?(ai,vtj) is computed with (13a)
 for i = 1, . . . , n, j = 1, . . . , c do
 6 utij =
 ?
 ? c∑
 k=1
 (d?(xti,vtj)
 d?(xti,vtk)
 ) 1
 m?1
 ?
 ??1
 where d?(xti,vtj) is computed with (13b)
 Compute q˜tk with (9)
 7 Optional extension: The partition of the full data set X is
 computed by the following steps.
 for i = 1, . . . , N , j = 1, . . . , c do
 uij =
 ?
 ? c∑
 k=1
 (d?(xi,vtj)
 d?(xi,vtk)
 ) 1
 m?1
 ?
 ??1
 where d?(xi,vtj) is computed by (13c).
 4) Forest: This data set is composed of 581,012 carto-
 graphic variables that were collected by the United States Ge-
 ological Survey and United State Forest Service (USFS) data.
 There are 10 quantitative variables and 44 binary variables.
 These features were collected from a total of 581,012 30?30
 meter cells of the forest, which were then determined to be
 one of 7 forest cover types by the USFS. We normalized the
 features to the unit interval by subtracting the minimum and
 then dividing by the subsequent maximum. We used the RBF
 kernel with a width of 1.
 B. Evaluation criteria
 1) Adjusted Rand Index: Rand index is one of the most
 popular comparison indices of measuring agreement between
 two crisp partitions of a data set. It is the ratio of pairs
 of agreement to the number of pairs. The Adjusted Rand
 Index (ARI) we are using here is a bias-adjusted formulation
 0 0.2 0.4 0.6 0.8 1
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 Feature 1
 Fe
 at
 ur
 e 
2
 (a) 2D15
 0 0.2 0.4 0.6 0.8 1
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 Feature 1
 Fe
 at
 ur
 e 
2
 (b) 2D50
 Fig. 2. Synthetic data sets
 developed by Hubert and Arabie [35]. The result is a number
 between 0 and 1 where 1 indicates perfect match. In order
 to compute ARI, we first harden the fuzzy partition and then
 compare it with the ground-truth partition.
 2) Purity: Purity, also called clustering accuracy, is an
 external validation measure to evaluate the quality of the
 clustering solution. The purity of each cluster is given by the
 ratio between the amount of right assignments in this cluster
 and the size of the cluster. The purity of the clustering solution
 is then expressed as a weighted sum of the individual purities.
 Thus, the purity is a real number between 0 and 1. The larger
 the purity, the better the performance.
 3) Run time: Our algorithm could be used either as an
 approximation for unloadable data or acceleration for load-
 able data. Thus, time consumption is a crucial criteria. We
 compared times to compute the partition matrix with different
 chunk size as well as to KFCM run on the entire data set. All
 the times are recorded in seconds.
 Table II contains the results of our experiments. On the
 2D15 and 2D50 data sets, both KPC and stKFCM are success-
 ful at finding the preferred partitions. However, the stKFCM
 shows better results than KPC on the 2D50 data set, equaling
 the performance of KFCM down to the 2% chunk size.
 Furthermore, stKFCM is much faster than KPC at small data
 chunk sizes for 2D15 and 2D50. This is because the KFCM
 iterations at each chunk converge faster with the stKFCM
 166
algorithm. Note that KPC is faster than stKFCM for larger
 chunks; this is because of the inverse calculation at Line 3 of
 stKFCM. Both KPC and stKFCM produce very good speedup
 over KFCM at small data chunk sizes, while still producing
 partitions nearly equivalent to the literal KFCM.
 All three fuzzy clustering algorithms produce partitions
 that do not match well to the ground-truth for the MNIST
 and Forest data sets (which is also the case of k-means and
 other similar crisp partitioning algorithms). This does not
 alarm us as there is a big difference between classification
 and clustering results; i.e., classification uses user-supplied
 labels (a.k.a. ground truth), while clustering aims to find
 natural groupings. Hence, these results simply tell us that the
 natural groupings in these two data sets (as produced by c-
 means partitioning) do not match well to the ground truth
 labels. The aim of our proposed algorithm is to approximate
 the partitions of the KFCM for large data sets. And both
 KPC and stKFCM succeed at that for the MNIST data. The
 stKFCM algorithm exceeds the performance of KFCM for all
 chunk sizes, except for 0.02%, while KPC meets or exceeds
 the KFCM performance for chunk sizes > 0.5%; clearly,
 the stKFCM outperforms the KPC algorithm for the MNIST
 data. We have seen this behavior, i.e., the sampled algorithm
 exceeding the performance of the literal algorithm, in other
 studies [8, 15] and attribute it to the influence of outliers or
 noise on the literal algorithm. This hypothesis has not been
 proved.
 On the Forest data, both KPC and stKFCM struggle to
 match the performance of KFCM (run on the entire data set)
 in terms of the ARI criteria; however, in terms of Purity both
 KPC and stKFCM perform fairly, with KPC having a slight
 edge here. We believe that this is caused by the fact that the
 classes in the Forest data have very unbalanced numbers of
 samples; two of the seven classes, Spruce-Fir and Lodgepole
 Pine, comprise greater than 85% of the data set. Hence, the
 KPC and stKFCM algorithms, which sequentially operate on
 small samples of the data set, can be presented with samples
 that are comprised mostly of these two classes.
 Overall, these results are very pleasing as they show that
 stKFCM, even with very small data chunk sizes, achieves
 clustering performance near to that of KFCM run on the entire
 data set. Furthermore, the projection method used in stKFCM
 shows better performance than the medoid method used by
 KPC, even showing better run-time for some data sets.
 VI. CONCLUSION
 Big data analysis has become very pertinent. It creates many
 wonderful opportunities accompanied by numerous big chal-
 lenges. Both memory space and computational resources are
 important considerations in real life applications. In this paper,
 we proposed the stKFCM algorithm that significantly reduces
 both the memory requirement and computational complexity
 for performing KFCM clustering. Empirical results show that
 stKFCM achieves accurate results while only requiring access
 to very small portion of the kernel matrix.
 In the future, we will examine how we can better represent
 and pass on the information from previous data chunks,
 including using hybrids of random sampling and projection
 methods. We will also look at other methods of projection,
 with a focus on overall clustering performance.
 REFERENCES
 [1] S. Fausser and F. Schwenker, “Clustering large datasets with kernel
 methods,” in Proc. Int. Conf. Pattern Recognition, Nov. 2012, pp. 501–
 504.
 [2] J. S. Tayler and N. Cristianini, Kernel Methods for Pattern Analysis.
 Cambridge University Press, Cambridge, UK, 2004.
 [3] P. Hore, L. O. Hall, and D. B. Goldgof, “Single pass fuzzy c means,”
 in Fuzzy Systems Conferences, 2007.
 [4] P. Hore, D. B. Goldgof, and L. O. Hall, “Creating streaming iterative
 soft clustering algorithms,” in NAFIPS ’07 Annual Meeting of the North
 American, 2007.
 [5] P. Hore, “A fuzzy c means variant for clustering evolving data streams,”
 in IEEE International Conference on Systems, Man and Cybernetics,
 2007.
 [6] S. Eschrich, J. Ke, L. O. Hall, and D. B. Goldgof, “Fast accurate fuzzy
 clustering through data reduction,” IEEE Trans. Fuzzy Systems, vol. 11,
 pp. 262–269, April 2003.
 [7] T. Havens, J. C. Bezdek, C. Leckie, L. O. Hall, and M. Palaniswami,
 “Fuzzy c-means algorithms for very large data,” IEEE Trans. Fuzzy
 Systems, vol. 20, no. 6, pp. 1130–1146, 2012.
 [8] T. Havens, R. Chitta, A. Jain, and R. Jin, “Speedup of fuzzy and
 possibilistic c-means for large-scale clustering,” in Proc. IEEE Int. Conf.
 Fuzzy Systems, Taipei, Taiwan, 2011.
 [9] P.Hore, L. O. Hall, D. B. Goldgof, and W. Cheng, “Online fuzzy c
 means,” in NAFIPS, 2008.
 [10] N. Pal and J. Bezdek, “Complexity reduction for ”large image” process-
 ing,” IEEE Trans. Syst., Man, Cybern, vol. 32, no. 5, pp. 598–611, Oct
 2002.
 [11] F. Provost, D. Jensen, and T. Oates, “Efficient progressive sampling,”
 Fifth KDDM, ACM Press, no. 23-32, 1999.
 [12] R. J. Hathaway and J. C. Bezdek, “Extending fuzzy and probabilistic
 clustering to very large data sets,” Computation Statistics Data Analysis,
 2006.
 [13] J. C. Bezdek, R. J. Hathaway, J. Huband, C. Leckie, and R. Kotagiri,
 “Approximate clustering in very large relational data,” International
 Jounal of Intelligent Systems, vol. 21, pp. 817–841, 2006.
 [14] L. Wang, J. C. Bezdek, C. Leckie, and R. Kotagiri, “Selective sampling
 for approximate clustering of very large data sets,” International Jounal
 of Intelligent Systems, vol. 23, pp. 313–331, 2008.
 [15] R. Chitta, R. Jin, T. Havens, and A. Jain, “Approximate kernel k-means:
 Solution to large scale kernel clustering,” in Proc. ACM SIGKDD Conf.
 Knowledge Discovery and Data Mining, 2011, pp. 895–903.
 [16] F. Can, “Incremental clustering for dynamic information processing,”
 ACM Trans. Inf. Syst, vol. 11, no. 2, pp. 143–164, 1993.
 [17] F. Can, E. Fox, C. Snavely, and R. France, “Incremental clustering
 for very large document databases initial marian experience,” Inf. Sci,
 vol. 84, no. 1-2, pp. 101–144, 1995.
 [18] P. S. Bradley, U. Fayyad, and C. Reina, “Scaling clustering algorithms to
 large databases,” in the Fourth International Conference on Knowledge
 Discovery and Data Mining, 2000, pp. 51–57.
 [19] F. Farnstrom, J. Lewis, and C. Elkan, “Scalability for clustering algo-
 rithms revisited,” ACM SIGKDD Explorations, vol. 2, pp. 51–57, 2000.
 [20] C. Gupta and R. Grossman, “Genic: A single pass generalized incre-
 mental algorithm for clustering,” in SIAM Int. Conf on Data Mining,
 2004.
 [21] P. Hore, L. O. Hall, and D. B. Goldgof, “A cluster ensemble framework
 for large data sets,” in Systems, Man and Cybernetics, 2006.
 [22] S. Guha, A. Meyerson, N. Mishra, R. Motwani, and L. O’Callaghan,
 “Clustering data streams: Theory and practice,” IEEE Trans. Knowl.
 Data Eng, vol. 15, no. 3, pp. 515–528, Jun 2003.
 [23] N. Ailon, R. Jaiswal, and C. Monteleoni, “Streaming k-means approxi-
 mation,” in NIPS, 2009.
 [24] B. Bahmani, B. Mosesley, A. Vattani, R. Kumar, and S. Vassilvitskii,
 “Scalable k-means++,” VLDB Endowment, vol. 5, no. 7, pp. 622–633,
 2012.
 167
TABLE II
 CLUSTERING RESULTS*
 KFCM KPC [1] stKFCM
 Data set Purity ARI Time
 (secs)
 n Purity ARI Time (secs) Purity ARI Time (secs)
 50% 0.94 (0.03) 0.91 (0.05) 4.0 0.94 (0.04) 0.91 (0.06) 30
 2D15 25% 0.94 (0.04) 0.91 (0.05) 2.0 0.93 (0.04) 0.90 (0.05) 9.3
 N = 5, 000 0.95 0.91 5.5 10% 0.94 (0.04) 0.91 (0.05) 1.05 0.93 (0.04) 0.89 (0.05) 1.6
 c = 15 (0.04) (0.05) 5% 0.93 (0.04) 0.90 (0.06) 0.77 0.92 (0.04) 0.88 (0.05) 0.73
 d = 2 2% 0.91 (0.04) 0.89 (0.05) 1.1 0.92 (0.04) 0.89 (0.05) 0.42
 1% 0.88 (0.05) 0.85 (0.07) 1.2 0.88 (0.18) 0.85 (0.19) 0.09
 50% 0.85 (0.03) 0.84 (0.04) 31 0.88 (0.03) 0.83 (0.04) 126
 2D50 25% 0.88 (0.03) 0.82 (0.04) 15 0.88 (0.03) 0.82 (0.04) 36
 N = 7, 500 0.92 0.84 56 10% 0.87 (0.03) 0.82 (0.03) 8.8 0.87 (0.03) 0.82 (0.04) 6.8
 c = 50 (0.02) (0.04) 5% 0.85 (0.03) 0.79 (0.03) 6.2 0.88 (0.03) 0.84 (0.04) 2.9
 d = 2 2% 0.82 (0.03) 0.75 (0.04) 5.8 0.88 (0.02) 0.84 (0.03) 1.7
 1% 0.79 (0.03) 0.72 (0.04) 6.8 0.79 (0.27) 0.77 (0.24) 6.2
 10% 0.20 (0.01) 0.038 (0.0007) 689 0.19 (0.03) 0.037 (0.0013) 4488
 5% 0.20 (0.01) 0.039 (0.0040) 112 0.27 (0.02) 0.035 (0.015) 1460
 MNIST 2% 0.20 (0.01) 0.038 (0.0057) 46 0.26 (0.02) 0.037 (0.015) 415
 N = 70, 000 0.20 0.027 ** 1% 0.20 (0.02) 0.036 (0.0103) 24 0.23 (0.01) 0.044 (0.0121) 193
 c = 10 (0.01) (0.00) 0.5% 0.20 (0.02) 0.033 (0.011) 14 0.20 (0.01) 0.0401 (0.0107) 69
 d = 784 0.2% 0.16 (0.01) 0.023 (0.013) 14 0.23 (0.01) 0.047 (0.0094) 14
 0.1% 0.18 (0.02) 0.016 (0.0103) 19 0.24 (0.01) 0.045 (0.0007) 15
 0.05% 0.18 (0.02) 0.011 (0.0067) 33 0.23 (0.02) 0.043 (0.0098) 94
 0.02% 0.16 (0.01) 0.0051 (0.0037) 61 0.22 (0.02) 0.033 (0.0105) 60
 Forest 0.2% 0.52 (0.01) 0.0014 (0.022) 121 0.51 (0.02) 0.019 (0.019) 1122
 N = 581, 012 0.52 0.03 ** 0.1% 0.52 (0.01) 0.0010 (0.025) 81 0.51 (0.01) 0.017 (0.019) 333
 c = 7 (0.03) (0.03) 0.05% 0.51 (0.01) 0.0078 (0.024) 60 0.50 (0.01) 0.015 (0.027) 118
 d = 54 0.02% 0.52 (0.03) 0.0089 (0.023) 57 0.49 (0.00) 0.0011 (0.0006) 43
 0.01% 0.51 (0.02) 0.0018 (0.0087) 65 0.49 (0.00) 0.00 (0.00) 29
 *Mean and standard deviation over 100 independent trials. **Timing information inappropriate for MNIST and Forest data as these experiments were
 performed on a high-performance computing cluster.
 [25] T. Zhang, R. Ramakirshnan, and M. Livny, “Birch: An efficient data
 clustering method for very large databases,” in ACM SIGMOD Int Conf.
 Manag. Data, 1996, pp. 103–144.
 [26] L. Kaufman and P. J. Rousseeuw, Finding Groups in Data: an Intro-
 duction to Cluster Analysis. John Wiley and Sons, 1990.
 [27] R. Ng and J. Han, “Clarans: A methods for clustering objects for spatial
 data mining,” IEEE Trans. Knowl. Data Eng, vol. 14, no. 5, pp. 1003–
 1016, Oct 2002.
 [28] T. C. Havens, J. C. Bezdek, and M. Palaniswami, Computational
 Intelligence: Revised and Selected Papers from IJCCI 2010. Berlin:
 Springer, 2012, vol. 399, ch. Incremental Kernel Fuzzy c-Means, pp.
 3–18.
 [29] Z. Wu, W. Xie, and J. Yu, “Fuzzy c-means clustering algorithm based
 on kernel method,” in Proc. Int. Conf. Computational Intelligence and
 Multimedia Applications, Sept. 2003, pp. 49–54.
 [30] R. Hathaway, J. Davenport, and J. C. Bezdek, “Relational duals of the
 c-means clustering algorithms,” Pattern Recognition, vol. 22, no. 2, pp.
 205–212, 1989.
 [31] R. Hathaway, J. Huband, and J. C. Bezdek, “A kernelized non-euclidean
 relational fuzzy c-means algorithm,” in Proc. IEEE Int. Conf. Fuzzy
 Systems, 2005, pp. 414–419.
 [32] T. Havens, “Approximation of kernel k means for streaming data,” in
 21st International Conference on Pattern Recognition, 2012.
 [33] P. Drineas and M. W. Mahoney, “On the nystrom method for approxi-
 mating a gram matrix for improved kernel-based learning,” J. Machine
 Learning Research, vol. 6, pp. 2153–2175, 2005.
 [34] R. Jin, T. Yang, Y. F. Li, and Z. H. Zhou, “Improved bounds for the
 nystrom method with application to kernel classification,” IEEE Trans.
 Info. Theory, vol. 10.1109/TIT.2013.2271378, 2013.
 [35] L. Hubert and P. Arabie, “Comparing partition,” J. Class, vol. 2, pp.
 193–218, 1985.
 168

A Case Study on Entity Resolution for Distant Processing of Big 
Humanities Data 
 
1*Weijia Xu      2*Maria Esteva    3 +Jessica Trelogan     4   Todd Swinson 
*Texas Advanced Computing Center, +Institute of Classical Archaeology,   Department of Computer Sciences   
University of Texas at Austin 
1xwj@tacc.utexas.edu,  2maria@tacc.utexas.edu 3j.trelogan@austin.utexas.edu 4swinson@gmail.com 
 
Abstract—At the forefront of big data in the Humanities, 
collections management can directly impact collections access 
and reuse. However, curators using traditional data 
management methods for tasks such as identifying redundant 
from relevant and related records, a small increase in data 
volume can significantly increase their workload.   In this 
paper, we present preliminary work aimed at assisting 
curators in making important data management decisions for 
organizing and improving the overall quality of large 
unstructured Humanities data collections. Using Entity 
Resolution as a conceptual framework, we created a similarity 
model that compares directories and files based on their 
implicit metadata, and clusters pairs of closely related 
directories. Useful relationships between data are identified 
and presented through a graphical user interface that allows 
qualitative evaluation of the clusters and provides a guide to 
decide on data management actions. To evaluate the model’s 
performance, we experimented with a test collection and asked 
the curator to classify the clusters according to four model 
cluster configurations that consider the presence of related and 
duplicate information. Evaluation results suggest that the 
model is useful for making data management action decisions.  
Keywords: Collections Management; Entity Resolution; 
Distant Processing; Digital Humanities 
I.  INTRODUCTION 
One of the challenges introduced by the big data 
phenomenon in the context of Digital Humanities is that 
traditional data management methods are not suitable for 
increasing amounts of digital data. The concepts of “close 
reading and distant reading,” in much use in current Digital 
Humanities discussions, relate to this problem. They refer 
respectively to the methods and cognitive demands involved 
in the close study of a work or passage of text, (such as that 
done by traditional literary critics like Harold Bloom, who 
finds distant reading “absurd.” See [1]) versus the 
possibility of studying aggregates of digital texts using 
computational analysis methods (Moretti; Matthew Jockers 
prefers to call this “macroanalysis.” See [2]). A similar 
problem applies to Humanities data management in 
reference to collection processing activities such as 
organizing, describing, accessing and preserving digital 
collections. Loosely borrowing on these concepts, we can 
say that there are close and distant data processing methods; 
the former understood as linear review methods traditionally 
used by Humanities scholars and archivists, and the latter as 
computational methods that narrow large amounts of 
information to render a meaningful representation. We 
contend that both; distant and close processing are needed 
for making sense of big Humanities data.  The problem 
relates to the mismatch between close processing and the 
amount of data available for curation. Just a small increase 
in the amount of data makes time consuming for scholars 
and curators to read a text, or to look at image, or video 
files, one after another.  On the other hand, close processing 
may be needed for fine grain analysis of the contents. 
Ideally, close processing can be preceded by distant 
processing methods to present the curator with a synthesis 
involving valid patterns found in the data for making 
research data management decisions.  
Research data management has the goal of making data 
and information accessible throughout the duration of the 
project and to its disposition or long term archiving [3]. 
Managing data translates into making decisions about what 
actions to pursue such as what to keep, discard, merge, and 
organize. In projects with large teams or especially long 
lifecycles, increasingly typical in Humanities, researchers 
commonly share, copy and paste, reuse, select, duplicate, 
and transform data in response to research questions or as 
solutions to specific problems, but seldom document their 
actions along the way. As data are analyzed, new datasets 
are collected or produced as by-products, and these are 
added into the research loop, creating new relationships that 
may not be obvious or explicitly documented. Data 
provenance may get blurry or lost entirely in the midst of 
redundancy and disorder. Furthermore, for data to transition 
to archival collections, data provenance, inter-relationships, 
and transformations need to be documented or inferred by 
the archivists. 
The goal of this project is to assist users in overcoming 
these challenges with large unstructured Humanities 
datasets. Using Entity Resolution (ER) as a conceptual 
framework [4], we developed a three-stage method that 
combines both distant and close processing. After the first 
stage which involves metadata extraction and pre-
 processing, the second computational stage as distant 
processing narrows and groups the metadata into clusters 
that the user can evaluate within a graphical interface. At the 
post-processing stage, the user reviews the results and 
decides what data management actions to pursue as close 
processing (e.g. discard, merge, reorganize, re-evaluate or 
leave data as is).  
ER encompasses a variety of methods developed within 
different disciplines to resolve cases of data ambiguity and 
2013 IEEE International Conference on Big Data
 113978-1-4799-1293-3/13/$31.00 ©2013  IEEE 
repetition, to link relevant information points, and to 
improve the overall quality of the data [4]. ER is a well-
 known problem in Artificial Intelligence (AI). It specializes 
in finding duplicate records and locating records that might 
refer to the same entity such as a person, address, or any 
other information point. The problem that we address in this 
project is similar to the applications of ER in AI, but it 
presents considerable differences as well. In AI, ER aims to 
identify records of the same thing but with different partial 
information, the entity is clear in that case. In this project, 
by contrast, we do not have a fixed concept of entity, but we 
aim to find connections between sets of data that may or 
may not contain duplicate information. 
For this study we used an archaeological collection 
generated over decades of research and publication activities 
by the Institute of Classical Archaeology (ICA) at the 
University of Texas at Austin. This collection, amassed by 
several generations of research staff and collaborators from 
a variety of disciplines, is fairly typical of a large 
archaeological research project. While the case study is 
focused on the ICA collection’s particular formation history 
research and processing requirements, the approach is 
widely applicable to any large and unstructured data 
collection that contains redundant data alongside data that 
need to be assessed and re-organized for study, archiving, 
and dissemination.  
We created a similarity model that allows directories 
and files to be compared based on their implicit metadata 
and that clusters closely related pairs of directories. We call 
implicit metadata the terms and filenames that users create 
to label their data as they go about their work, while explicit 
metadata refers to that which is created following 
descriptive metadata standards. Knowledge from the curator 
about the collection is incorporated in the first and third 
stages of the method. Useful relationships between data are 
found based on a scoring system assigned to numerical 
series and tags found in the implicit metadata. Results, 
presented through a graphical user interface, allow 
qualitative evaluation of the clusters and consequently 
making decisions about data management actions. To 
evaluate the model’s performance we classify the clusters 
according to four model cluster configurations that consider 
the presence of related and duplicate information. Finally, 
an action score is applied to each cluster as an indicator of 
the curator’s confidence to make decisions about data 
management actions.   
II. BACKGOUND AND RELATED WORK 
A. Formation and Processing of Unorganized Data 
Aggregations  
Problems making sense and processing collaborative, 
and un-ruled aggregations of data in networked or virtual 
organizations emerged early on in the history of digital 
information management and archiving. In Thirty years of 
Electronic Records, Linda Henry describes the early efforts 
at the National Archives and Records Administration 
(NARA) to appraise unorganized materials accumulated in 
multiple personal computers. Her narrative points to the lack 
of appropriateness of traditional methods used by archivists 
to approach this task  [5].  For cases of chaotic groups of 
records within unmanaged environments whose access can 
only be understood by the creator, the UK Public Records 
Office recommends leaving them out of inventories as long 
as the information is duplicated elsewhere [6].  
Many of the reasons why these aggregations are chaotic 
derive from the way in which they originate and evolve. In 
her description of the formation process of an organizational 
archive dating from the mid 80’s to the mid 2000’s, Esteva 
mentions that, despite the fact that a shared network drive 
was implemented for file sharing and exchange, each staff 
member kept and discarded files idiosyncratically and 
nobody could find each other’s information [7]. Using 
ethnographic methods, Boticelli studied the records derived 
from collaborative research projects and found that the 
authorship and origin of the records are difficult to pinpoint 
due to changes in organizational structure and functions of 
the projects as well as changes in the value of the data as the 
research proceeds [8]. In the case of archaeology datasets, 
Trelogan explains that taking care of data management 
throughout the research project and beyond, places a burden 
on the research team [9]. In her study of digital preservation 
practices of archaeologists and art historians, Beaudoin 
describes faculty image collecting habits as resulting in 
disconnected, largely invisible, “hoards” or “silos” of data 
that not only replicate past work, but are largely invisible as 
part of the scholarly record [10]. Across these papers the 
commonality is that faculty, research staff, and archivists 
are overwhelmed by the belief that they are incapable of 
dealing with large, unstructured collections of digital data.  
Preliminary findings from the ChartEX project, that 
develops methods for analysis of digitized medieval 
manuscripts while examining researchers response to them, 
suggest that to conduct their work, researchers need to 
combine both distant and close data examination methods 
[11]. In this project we provide that kind of complementary 
approach. Our aim is to alleviate the burden on Humanities 
researchers and archivists trying to make sense of large 
varied datasets by providing a distant processing model that 
also allows for close processing of more manageable chunks 
of information. 
B. Data and Collection’s Organization  
Xu and Esteva have experimented with various 
computational analysis methods for purposes of 
understanding data and archival collections’ organization 
and aiding their processing. To find the stories, as records 
related to a same project or event, amongst the text 
documents of different staff members in an organization, 
they developed a text mining method to find similarities 
between text segments [12]. Using the file system metadata 
of that same dataset, they created a treemap visualization to 
114
uncover individual staff recordkeeping practices across time 
[13]. The same team developed an interactive visual 
analytics framework that allows conducting collections 
structural and functional analyses, and includes a data 
mining method to infer four types of data organization 
criteria [14, 15]. With a focus on managing GIS archival 
data, Heard and Marciano created an application that 
identifies geographic data and allows the user, via a map-
 based interface, to explore metadata and records from a “top 
down” or “bottom up” including a geographical perspective 
[16]. The work presented here fills a gap in big data 
processing in the Humanities, in that it focuses on data 
organization considering the presence of duplicate and 
redundant information present in the collection.  
C. Entitiy Resoultion  
Entity resolution is originated from a classic problem in 
database research for duplicate records detection [4, 17]. In 
large database system in the real word, there are duplicate 
representations of the same object, also known as “entities” 
in relational databases. Those duplicate records may not be 
exactly the same or share a common key reflecting their 
connection in the system. The value of each record could be 
only partially matched to other records or present certain 
errors that make detecting those duplicates a difficult task. 
A common example is when multiple different formats of 
the same address are registered in a database. This problem 
is also related to record linkage, which is to identify records 
referring to the same entity in different databases.  
A typical entity resolution process includes: data 
preparation, which transforms data in a uniformed model; 
field matching, which breaks records by field and defines 
similarity models between fields; and duplicate record 
detection, which utilizes data mining techniques such as 
clustering and learning classification, to identify possible 
duplicated records [4, 18]. Over the years, there have been a 
number of studies in this area [17, 19, 20]. ER application 
has also broadened into fields like social network analysis 
and web data mining [21-24].  
Until now, ER has not being used in the context of 
collections management and curation. Our work focuses on 
utilizing the general ER framework to help identify related 
records. There are several differences between our 
conception and the traditional ER approach. First, most of 
the existing works focus on the structured data source, such 
as a database, or metadata from web documents. In this 
project, we are dealing with less structured data that also 
lacks uniform and consistent metadata. Secondly, the entity 
is generally clearly defined in ER, such as a product, or a 
person etc. In this project, the entity is not explicitly defined. 
We consider each sub-directory in the collection as a 
representation of some abstracted concept or theme that 
might be important for record keeping and should be taken 
into account in any curatorial decisions. Hence, the 
detection of duplicated representations of the same entity is 
translated to the detection of sub-directories that might be 
related to the same theme or concept. In this exploration, we 
focused on data modeling and similarity scores.  
III. CASE STUDY COLLECTION  
As a test case we used a collection generated by the 
Institute of Classical Archaeology (ICA), which has been 
conducting research in Greek agricultural territories since 
1974. ICA’s work has included dozens of excavation 
projects in southern Italy and Ukraine, in addition to 
intensive field surveys, campaigns dedicated to site and 
object conservation, and a huge variety of related studies. 
The resulting digital collection consists of 5TB of data (over 
one million files) generated by hundreds of researchers from 
many different countries and disciplines, each using distinct 
recording methods and technologies. It contains digitized 
and born-digital photographs, drawings, maps, plans, and 
notes as well as more complex datasets including GIS data, 
3D models, and relational databases.  
Throughout its long history, parts of the collection have 
been copied, transferred, and amalgamated from every 
available form of detached media, personal computers, and 
servers, and it has suffered a great deal from duplication and 
corruption in the process. In addition, there is a general 
dearth of descriptive metadata and, although some smaller 
projects have achieved a degree of consistency, there have 
been no systematically adopted principles for file naming or 
organization throughout the entire collection. Implicit 
descriptive metadata has been inconsistently applied in the 
form of filenames and directory labels that encode things 
like names or initials of places and people, abbreviations 
and acronyms, dates, site codes, and subjects, which provide 
the only information pointers.  
In previous work, the lifecycle of the case study 
research collection was modeled into three stages: 1) the 
collection of new primary data (e.g. photography and 
illustrations of objects), to 2) its study and synthesis (e.g. 
quantification and analysis of a particular class of artifact), 
and finally 3) to its publication and dissemination [15]. The 
motivation for the current project was to locate all relevant 
data related to one excavation, and, after having reduced as 
much irrelevant or duplicated data as possible, prepare it for 
archiving and presentation in print together with an online 
companion that will include the entire dataset.  
The sample used here contains over 100,000 files, most 
of which are images of varied formats, either scanned from 
analog media or captured digitally, and spread across 3033 
directories. The implicit metadata takes the form of names 
given to files and directories by the researchers, 
photographers and scanning technicians in the course of 
their work, and includes non-standardized terms and 
numbers that change over time.  
IV. METHOD AND IMPLEMENTATION  
Due to the diversity of file formats in this collection, a 
major problem is that there is no way to resolve redundancy 
with direct content-based comparison methods. Exact 
115
duplicates can be detected by finding file
 checksums, but checksums also indicate c
 some duplication is done deliberately, so 
based on checksums alone may be mislead
 modification in a file (e.g. minor edits an
 result in a different checksum, even though
 the file may be considered a duplicate. 
complex in the case of image data. Using im
 software for identification of duplicate im
 bring together different related images, nor
 for purposes of grouping related data in d
 (e.g. spreadsheets and text documents).  Fu
 wise content comparisons among files are 
expensive and not practical for large-scale 
approach focuses on the available implicit
 directory level to return clusters that may
 files. The curator further evaluates if the res
 are useful.    
In a nutshell, our method includes a
 natural language processing (NLP) and
 methods. We first tokenize each term i
 structure and submit a list of terms to t
 determines which ones are meaningful for
 organizational needs. After pre-processing, 
represented by a set of tokens that appea
 within that directory. To relate and compar
 developed a scoring model that considers
 tags, series and structural information. Usin
 comparison between any two directories yie
 quantifies the similarity between two 
proceed with a pair-wise comparison a
 directories in the test collection. Next, we f
 directories with low similarity as indicated
 vector and conduct cluster analysis with
 directories. Figure 1 shows an overview of t
 Figure 1: Overview of the processing workflow 
2ø?ı?ø?"? æø"?®ı?øß"?©?Ø-?ßÆæº
 %Ø-œß?ø"¸ Æø??ßæøÆ?œ"? ?œ?¸"æº"?æŒı?øœÆæº
 %æŒı?ø?"¸ Æø??ßæøÆ?œ"
 2øæ??œœ"? ???"¸Æø??ßæø°"? æø?
 Tags Series D
 '®ßø??ßÆº?"Œ?ß?¸?ß?"? øæŒ"ß??"¸?ß?"?æØØ
 s with identical 
orrupt data, and 
discarding files 
ing. A minimal 
d resizing) will 
 for the curator, 
This gets more 
age recognition 
ages would not 
 would be useful 
ifferent formats 
rthermore, pair-
 computationally 
collections. Our 
 metadata at the 
 contain related 
ultant groupings 
 combination of 
 data analysis 
n the directory 
he curator, who 
 the collection’s 
each directory is 
r in all the files 
e directories, we 
 three features:: 
g this model, the 
lds a vector that 
directories. We 
mongst all the 
ilter out pairs of 
 by the resultant 
 the remaining 
he workflow. 
 
A. Data Pre-processing and Mode
 The raw input data is the hier
 entire collection including all the d
 generated the structural file using a
 utility named “find.” We adopted
 OpenNLP package (http://opennlp.
 common separators to generate tok
 numbers that appear in the filename
 One of the challenges presente
 identify directories that contain s
 different, may be related by proven
 For example, a set of image files o
 user or project may have been,
 versioned by other users at diffe
 approach is to utilize the numbers a
 in the naming conventions to i
 However, since numbers or sequen
 may not mean anything, we 
sequences, and terms within a direc
 interested first in identifying if a 
directory.  A series could be numbe
 by a camera, or numeric codes used
 of series is not trivial because numb
 present in different formats and valu
 Definition 1 and introduce Algorithm
  
Definition 1: A series is an orde
 numerals (or equivalent) i.e. (a1, a
 n>3. Furthermore, the difference b
 consecutive numerals are within a g
 Algorithm 1: Series Detection 
1.   sort the input sequences, 
2.   init: Index i ä0 ; series S ä empty 
3.   For a value ai in the list;  
4.     if S is empty 
5.       S’ ä (ai, ai+1, ai+2) 
6.     If S is not empty  
7.          S’ ä S U ai 
8.  Check if S’ is a series based on the c
 9.     if S’ is a series,  
10.        i ä i+1  
11.   else, 
12.       i ä i+1 and S ä S’ 
13.   Continue with line 3 
 
In this study, we used a heuris
 the series score as following: 
 
???? ? ??"""? ? ?"
 ? ? ? ?"??"
 ? ? ? ? ?
 ? ? ?
  
The d is calculated as? ? ?????""?
 definitely indicates a series, a scor
 probably a list of categorical numbe
 with high value of d could also b
 "ø?œ-Øßœ"
 irectory path
 ??ßÆæº
 ling 
archical structure of the 
irectories and files. We 
 Linux operating system 
 a tokenizer from the 
apache.org/), which uses 
ens as well as to extract 
s and directory labels.     
d by this collection is to 
ets of files that, while 
ance, function, or theme. 
riginally created by one 
 reused, modified and 
rent time periods. Our 
nd sequences embedded 
dentify these relations. 
ces in an individual file 
consider all numbers, 
tory. Specifically, we are 
series, exists in a given 
ring sequences generated 
 in filenames. Detection 
ers for example, may be 
es. We define a series in 
 1 for series detection. 
red set of at least four 
2, a3, …, an) and where 
etween intervals of two 
iven threshold .    
 
omputed series score 
tic measure to determine 
? ? ??
 ??
 ?
 .  
" . A value of 1.0 of d 
e of less than 1.0 means 
rs. And a set of numbers 
e a series with a large 
116
difference between any two consecutive m
 based on the knowledge we had about the
 this could be just random sequential numb
 photographic camera but with no bearing on
 Tags are the tokens extracted from the 
are generally very short and may be idiosyn
 noise we selected tags based on thei
 occurrences in the directories. Next, we as
 familiar with the collection, to highlight go
 from the list of frequent tags. Good tag
 information such as locations, dates, nam
 abbreviations. Some examples of good tags
 are: Metaponto, CH (for Chersonesos), an
 Bad tags are those that, though commonl
 indicative of the functions and provenance 
Some examples of those are: music, friends,
 In a collection, each directory may 
number of tags depending on how many f
 within and how varied their names are. To 
computation costs and based on studying th
 of tags distribution, we fixed the number o
 directory. The five tags were selected
 following rules:  
• analyze the tags in files in the immedia
 • in the first position is the most frequen
 • in the next four positions are the most 
• if there are not 4 good tags then fill in 
the most frequent tags 
• if there are not 5 unique tags in the cur
 fill in the remaining positions with UN
 When identifying series and tags, only th
 and directory labels immediately inside 
analyzed. Previous labels in the full path 
directory are not considered, and the childr
 further down the tree are not considered eith
 B.  Overall Similarity Scoring Model 
After each directory has been analyzed, pair
 need to be compared. The scoring model fo
 directories consists of the following three fe
 1. lists of series identified inside a directory
 2. top five tags exctracted inside a directory
 3. directory path of the directory 
A tag score for each directory pair 
calculating the number of matches be
 directories’ “top 5 tag” vectors.  A positiv
 position of the vector adds 1 to the t
 maximum score of 5.  UNKNOWN tags ar
 same, so the tag score can be high even if
 tag matches, for example, considering: [a,
 UNK] [b, c, UNK, UNK, UNK], score=4
 matches, but since we match UNK’s to ea
 matches are counted. A series score fo
 directories is calculated as follows: 
• 0=no series overlap at all 
eans. However, 
 test collection, 
ers placed by a 
 similar content.   
filenames which 
cratic. To reduce 
r frequency of 
ked the curator, 
od and bad tags 
s contain useful 
es, and project 
 in the collection 
d IT (for Italy). 
y used, are less 
of the collection. 
 lost, and files.   
have a different 
iles are included 
reduce noise and 
e unique number 
f tags to five per 
 based on the 
te directory 
t tag 
frequent tags 
these spots with 
rent directory, 
KNOWN 
e names of files 
a directory are 
of the analyzed 
en in directories 
er 
-wise directories 
r comparing two 
atures:  
   
  
is computed by 
tween the two 
e match in each 
ag score, for a 
e considered the 
 only one actual 
 c, UNK, UNK, 
. Here, only “c” 
ch other, 3 more 
r each pair of 
• 1=some series overlap (any ran
 series of the same number of di
 directories) 
• 2=complete series overlap (both
 exact same number of series, an
 the same size, min, and max)  
For the structural comparison,
 suffix scores between the paths of 
The prefix score is the number o
 directory labels in the path that ar
 directories, starting from the left.  T
 but starts from the right.  One can 
longest common prefix or suffix b
 comparing each directory name 
characters in a string. 
We calculate the overall simila
 pair of directories as follows. For 
vector: [tag score, series score, pre
 The scores are normalized and tran
 best score, and the highest score is 
score for the directory pair is calcul
 weight vector to obtain the weighte
 score vector. Lower scores mean mo
 similar. To group directories and 
mean clustering to group the direc
 groups. K values of 21, 50 and 
experiment. For each k value, we
 times and chose the one with the
 errors as the output to present to the
 C. Clustering Analysis and Evalua
  
Figure 2. Screenshot of the GUI showing ho
 can be closely evaluated by the curator.  
Using the similarity scoring
 cluster analysis with a standa
 algorithm. For evaluation, for 
calculated its compactness and su
 help users access the clustering re
 graphical user interface shown in Fi
 ge overlap between 
gits between the two 
 directories contain the 
d each of the series has 
 we calculate prefix and 
each pair of directories. 
f consecutive individual 
e the same between two 
he suffix score is similar 
view these scores as the 
etween two strings, but 
in a path instead of 
rity score between each 
each pair we construct a 
fix score, suffix score]. 
sformed so that 0 is the 
the worst. The similarity 
ated by multiplying by a 
d sum of elements in the 
re similar.  Zero is most 
present this we used k-
 tories into a k value of 
125 were used in our 
 ran the algorithm five 
 lowest sum of squared 
 evaluators.   
tion  
 
w the contents of each cluster 
 model we conducted 
rd k-mean clustering 
each cluster we also 
m of square errors. To 
sults we implemented a 
gure 2.  
117
V. RESULTS EVALUATION AND A
 The test collection consists of 3,033 
with various nested sub-directories. Comp
 requires running 4.6 million pair-wise co
 computed value distributions based on 
features in logarithmic scale. The result
 Figure 3.  
The tag feature includes the top 5 tags sel
 directory. The tag feature comparison i
 number of common items found between 
Therefore, the result ranges from 0 (no mat
 (exactly the same set of tags). For the seri
 value means that there is no overlap (or n
 between the two directories. Value 1 mean
 overlap between the two series, and value 2
 directory might have a series that is a supe
 from the other directory. Figure 4 indica
 cases, the majority of the directories have n
 top 5 tag selection (88.8%) and no relation
 (93.8%).  This means that we selected
 directories which have highest scores in bo
 features. In this set, there are 415 directorie
 has the exact set of top 5 tags and strong s
 with at least one other directory. Not
 directories account for about 13.7% of 
directories in the test collection.  
Figure 3. Tag and series features value distribution. 
A. Evaluation Criteria  
Our evaluation criteria included
 classification model to typify the cluster
 scoring system to indicate the usefulnes
 cluster. We mentioned that it is import
 redundancy be automatically discarded, 
represent significant, intentional decisions m
 creators.  In the process of reducing 
decisions about whether to delete, merge
 leave data untouched must be weighed in 
rest of the data within which it is stored. Fo
 defined four different clustering models th
 and duplicate data and used those to compa
 results. The four models are shown in Fig
 clusters share a set of duplicate data; 2) one
 1
 10
 100
 1000
 10000
 100000
 1000000
 10000000
 0 1 2 3
 nu
 m
 be
 rs
  (l
 og
  s
 ca
 le
 )
 Scores
 Tag F
 Series
 NALYSIS 
directories each 
utationally, this 
mparisons.  We 
tags and series 
s are shown in 
ected from each 
s based on the 
two directories.  
ches found) to 5 
es feature, the 0 
o series feature) 
s that there is an 
 means that one 
rset of the series 
tes that in both 
o overlap in the 
ship in the series 
 the subset of 
th tag and series 
s, each of which 
eries connection 
e that the 415 
total number of 
 
 a clustering 
s and an action 
s of a resultant 
ant that not all 
as some may 
ade by the data 
redundancy, the 
, reorganize, or 
context with the 
r this reason, we 
at share related 
re our clustering 
ure 4: 1) pair of 
 cluster contains 
a subset of duplicate data; 3) P
 related, but not duplicate data; 4) c
 by duplicate data. A fifth clustering
 not having related nor duplicate d
 useful result. For the results eva
 result was compared to the mo
 containing one or more of them. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4. Cluster classification model with fi
 We then proceeded to score th
 scoring system allowed us to 
performance of the model in relat
 actions. The scoring represents the
 the curator in making a data mana
 cluster; 1= some confidence, but ne
 and 2=complete confidence, take ac
 A curator actively working w
 reviewed the clustering results after
 clustering types and clarifying the
 scoring system. Note that the evalu
 (See Figure 2), only involves revie
 labels and file names, not reading
 contents. At this point, the curator d
 data management action to pursue,
 notes about the characteristics of 
improve or refine the model.  
 
B. Quantitative Analysis of Cluste
 We reviewed the three sets of 
different k-value (21, 50, and 12
 which one better achieved the chal
 redundant information for action 
merging all data related to one pro
 photographic campaign, publication
 each set provided different results.  
4 5
 eature
  feature
    
Type 1: two clusters share 
a subset of duplicates
 Type 2: 
a clu
   
Type 3: two clusters 
contain useful related info 
but no duplicates 
Type
 are e
   
Type 5: Bad cluster; nothing relat
 Feature
 air of clusters contains 
luster is formed entirely 
 model was identified as 
ata and thus as a non-
 luation, each clustering 
dels and classified as 
ve clustering types. 
e clustering results. This 
determine the overall 
ion to data management 
 level of confidence of 
gement action (0=ignore 
eds further investigation; 
tion).  
ith the test collection 
 being trained to identify 
 meaning of the action 
ation done with the GUI 
wing directory structure, 
 or observing the files 
id not define the type of 
 but was asked to make 
the resulting clusters to 
ring Comparisons     
clustering results with a 
5), with an eye toward 
lenges of: a) identifying 
and b) identifying and 
ject (e.g excavation site, 
, etc.). We observed that 
one cluster contains 
ster of duplicates 
 
  
 4: two clusters 
ntirely duplicates 
ed or useful 
118
Figure 5. Comparison of different k values in relation t
  
Figure 5 shows a comparison of the d
 clusters, indicating the percentage of ind
 clusters labeled by the curator within the di
 Because each clustering result was classifie
 one or more clustering types, the sum of t
 individual types may exceed 1. In fact, 
indicates that the clustering is less decisive 
it contains mixed cluster types. Hence, a 
smaller clusters can further separate out 
reduc the number of multiple types in eac
 the clustering results, the type 3, whi
 containing related directories but no duplica
 most common occurring.  We observe that t
 type 5 clusters, labeled as useless, decrease
 increase. This indicates that with highe
 clustering results become more ho
 consequently, more helpful for the cu
 decisions.  
Figure 6. Comparison of action decisions and their con
 corresponding k value clusters. 
Figure 6 shows a comparison betw
 decisions made by the curator, their assoc
 levels, and the corresponding k value cluste
 each column shows the number of clu
 decision made. The confidence level of 
shown in different colors.  0 indicates no 
and 2 indicates high confidence in the action
 When the k value is small, the cluste
 mixed types of clustering models within. H
 with scoring 1 will need close processing.
 0
 0.5
 1
 1.5
 2
 2.5
 21k 50k 125k
 R
 A
 T
 IO
 0
 10
 20
 30
 40
 50
 60
 70
 80
 P
 er
 ce
 nt
 ag
 e
  
o clustering types.  
ifferent k value 
ividual types of 
fferent k results. 
d as mapping to 
he percentage of 
a higher value 
to the curator as 
large number of 
those cases and 
h cluster.  In all 
ch are clusters 
ted data, was the 
he percentage of 
s as the k values 
r k values the 
mogenous and 
rator to make 
 
fidence in relation to 
een the action 
iated confidence 
rs. The length of 
sters with each 
each decision is 
confidence at all 
 decision. 
rs often contain 
ence, all clusters 
 Instead, for the 
k125 clustering results, we observ
 action and no action decisions narro
 C. Qualitative Evaluation of the C
 The collection’s curator compl
 of the clusters focusing on their co
 in general, across the different k v
 (types 1-4) presented data in ways 
patterns that would have been imp
 linear review. She was able to deriv
 and to create new organizationa
 clusters. We describe some of obse
 findings in relation to data ma
 processing practices.  
Large sectors of the collection
 up as types 1 and 4 clusters, were 
backups that could be immediately
 the backups were located in deeply
 opportunity to observe this trend in 
have presented itself using close 
nested directories are confusing to r
 A number of type 2 clusters w
 the curator. While these were in 
other files, they represented delibe
 creators and, in several cases, allow
 the provenance of files selected out
 directory (either to be processed, re
 research project). These duplicates 
be described with their provenan
 would otherwise be lost if they were
 The curator identified a number
 clusters that never would have occu
 In more than one cluster, files from
 campaign were identified as relate
 auto-generated by a digital camera,
 separated in two very different di
 archive and labeled differently. 
algorithm also discloses different la
 case due to sequencing series that al
 of the relationship between the fi
 same photographic campaign, 
photographer.  
The most “successful” result w
 value of 125. Although all three 
useful, and very different, clusters,
 terms of confidence scores (See Fig
 a score of 2, an immediate decision
 close processing. The clusters with
 useful, in that they indicate the ne
 and as a guide for what to modify
 the algorithm. In relation to the lat
 that, considering the amount of seq
 collection, the series score needs t
 the comparison, and that a dynamic
 would be useful to data organization
 type-5
 type-4
 type-3
 type-2
 type-1
 Score 2
 Score 1
 Score 0
 e that the ratio between 
ws down (70 vs. 49).  
lustering Results.   
eted a qualitative review 
ntent. She observed that 
alues, the useful clusters 
that allowed discovering 
ossible to detect through 
e a list of “action items” 
l groups based on the 
rvations to illustrate the 
nagement and archival 
, most of which showed 
identified as useless old 
 deleted. In every case, 
 nested directories. The 
the collection may never 
processing methods, as 
eview. 
ere of direct interest to 
fact identical copies of 
rate choices by the data 
ed for reconstruction of 
 and copied into another 
used, or fed into another 
were left as is, but could 
ce information, which 
 found out of context.  
 of related data as type 3 
rred as being connected. 
 the same photographic 
d because of file names 
 although they had been 
rectory locations in the 
This means that the 
yers of similarity, in this 
lowed for reconstruction 
les as belonging to the 
or sharing the same 
as the set that used a k 
k value sets contained 
 it performed the best in 
ure 6). For clusters with 
 could be made without 
 scores of 1 were also 
ed for close processing 
 in a second iteration of 
ter, the curator observed 
uencing numbers in this 
o have less relevance in 
 review of the good tags 
 goals.  
119
The qualitative evaluation was instructive for: a) 
establishing priorities for close processing (those action 
items with confidence score =1), b) reducing unnecessary 
redundancy, c) informing a deeper understanding of the 
contents of the archive and d) indicating what needs to be 
refined in a next iteration of the algorithm. The exercise of 
reviewing three sets of clusters took a total of ~8 hours. Not 
only did this “distant” method speed up the process of 
reviewing and tidying the archive in comparison to 
reviewing the collection item by item, it also provided 
significant insight into the formation process and embedded 
meaning in the collection. The curator noted that actions 
could be determined after close processing of the cluster 
results, including directives like “delete directory,” “merge 
with directory Y after deleting duplicates”, or “leave as is.”  
VI. CONCLUSIONS AND FUTURE WORK 
As a preliminary attempt to apply general ER framework 
and techniques in the realm of collection curation, our initial 
efforts focused on practical benefits for a specific collection. 
Many aspects of this presentation are specifically tied to the 
test collection and utilize heuristics from that collection’s 
curator. On one hand, this is necessary due to a lack of prior 
knowledge or prior models applicable to this problem. On 
the other hand, the heuristic knowledge, such as series 
distribution and customized tags lists, simplify the 
computations and the complexity of the processing 
workflow. Still, we think that using ER as a framework for 
data curation processes is a viable approach that can be 
generalized to other collections with additional learning 
methods to improve its robustness and effectiveness.   
There is an important part missing from the work 
presented here, namely the scope of the results in relation to 
the entire collection. Understanding how complete is the 
representation provided by the distant processing is not yet 
possible because there is no suitable benchmark set. 
Creating such a benchmark is not trivial, but is something 
we will strive for in future work.  
In the future, we want to investigate what kind of results 
might be produced by further modifying the algorithm as a 
consequence of observations during the first clustering 
iteration evaluation and of the emergence of new 
requirements as the data are cleaned, reorganized, and 
filtered. We also plan to visualize the cluster review process 
including other metadata such as identical checksums, and 
file format identification that can help the curator validate 
his cluster evaluation and improve action decision making.  
ACKNOWLEDGMENTS 
Funding for this work was provided by grants from the 
National Archives and Records Administration and the 
Packard Humanities Institute.  
REFERENCES 
[1]  R. Serlen, "The Distant Future? Reading Franco Moretti," Literature 
Compass, vol. 7, pp. 214-225, 2010. 
[2]  M. L. Jockers, Macroanalysis: Digital Methods and Literary History: 
University of Illinois Press, 2013. 
[3]  C. L. Borgman, "The conundrum of sharing research data," J. Am. 
Soc. Inf. Sci. Technol., vol. 63, pp. 1059-1078, 2012. 
[4] F. Naumann and M. Herschel. (2010). An introduction to duplicate 
detection. Available: 
http://dx.doi.org/10.2200/S00262ED1V01Y201003DTM003 
[5] L. J. Henry, "Appraisal of electronic records," in In Thirty Years of 
Electronic Records, B. I. Ambacher., Ed., ed Maryland, USA: The 
Scarecrow Press, 2003, p. 216. 
[6] P. R. Office. (2000). Guidance for an Inventory of Electronic 
Records: a Toolkit. Available: 
http://www.nationalarchives.gov.uk/documents/inventory_toolkit.pdf 
[7] M. Esteva and H. Bi, "Inferring intra-organizational collaboration 
from cosine similarity distributions in text documents," presented at 
the Proceedings of the 9th ACM/IEEE-CS joint conference on Digital 
libraries, Austin, TX, USA, 2009. 
[8] P. Botticelli, "Records appraisal in network organizations," 
Archivaria, vol. 1, 2000. 
[9] J. Trelogan., A. Rabinowitz, M. Esteva, and S. Pipkin, "What do we 
do with the mess? Managing and preserving process history in 
evolving digital archaeological archives," presented at the 38th 
Conference on Computer Applications and Quantitative Methods in 
Archaeology, Granada, Spain, 2010. 
[10] J. E. Beaudoin, "Specters in the Archive: Faculty Digital Image 
Collections and the Problems of Invisibility," The Journal of 
Academic Librarianship, vol. 37, pp. 488-494, 2011. 
[11] S. R. Jones and H. Petrie, "ChartEx: Discovering spatial descriptions 
and relationships in medieval charters paper," presented at the Digital 
Humanities 2013, Lincoln, Nebraska, USA, 2013. 
[12] W. Xu and M. Esteva, "Finding stories in the archive through 
paragraph alignment," Literary and Linguistic Computing, vol. 26, 
pp. 359-363, 2011. 
[13] W. Xu, M. Esteva, and S. D. Jain, "Visualizing personal digital 
collections," presented at the Proceedings of the 10th annual joint 
conference on Digital libraries, Gold Coast, Queensland, Australia, 
2010. 
[14] W. Xu, M. Esteva, S. D. Jain, and V. Jain, "Analysis of large digital 
collections with interactive visualization," presented at the 2011 IEEE 
Conference on Visual Analytics Science and Technology (VAST'11), 
Providence, RI USA, 2011. 
[15] M. Esteva, J. A. Trelogan, W. Xu, A. J. Solis, and N. E. Lauland, 
"Lost in the Data, Aerial Views of an Archaeological Collection," 
presented at the Digital Humanities 2013, Lincoln, Nebraska, USA, 
2013. 
[16] J. R. Heard and R. J. Marciano, "A system for scalable visualization 
of geographic archival records," in Large Data Analysis and 
Visualization (LDAV), 2011 IEEE Symposium on, 2011, pp. 121-122. 
[17] A. K. Elmagarmid, P. G. Ipeirotis, and V. S. Verykios, "Duplicate 
Record Detection: A Survey," Knowledge and Data Engineering, 
IEEE Transactions on, vol. 19, pp. 1-16, 2007. 
[18] T. N. Herzog, F. Scheuren, and W. E. Winkler. (2007). Data quality 
and record linkage techniques.  
[19] H. Kopcke, A. Thor, and E. Rahm, "Evaluation of entity resolution 
approaches on real-world match problems," Proc. VLDB Endow., vol. 
3, pp. 484-493, 2010. 
[20] P. Christen and SpringerLink (Online service). (2012). Data matching 
concepts and techniques for record linkage, entity resolution, and 
duplicate detection.  
[21] M. Bilgic, L. Licamele, L. Getoor, and B. Shneiderman, "D-dupe: An 
interactive tool for entity resolution in social networks," in Visual 
Analytics Science And Technology, 2006 IEEE Symposium On, 2006, 
pp. 43-50. 
[22] L. Getoor and C. P. Diehl, "Link mining: a survey," ACM SIGKDD 
Explorations Newsletter, vol. 7, pp. 3-12, 2005. 
[23] I. Bhattacharya and L. Getoor, "Collective entity resolution in 
relational data," ACM Transactions on Knowledge Discovery from 
Data (TKDD), vol. 1, p. 5, 2007. 
[24] C. P. Diehl, G. Namata, and L. Getoor, "Relationship identification 
for social network discovery," in AAAI, 2007, pp. 546-552. 
120

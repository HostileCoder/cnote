Civil Transportation Event Extraction from Chinese 
Microblog 
 
Jiaxi XIONG 
Department of Information Security 
and Engineering 
Shanghai Jiaotong University 
Shanghai, China 
Email: jiaxi.xiong@sjtu.edu.cn 
Yonggang HAO 
Department of Information Security 
and Engineering 
Shanghai Jiaotong University 
Shanghai, China 
Email: hg1673@163.com 
Zheng HUANG* 
Department of Information Security 
and Engineering 
Shanghai Jiaotong University 
Shanghai, China 
Email: huang-zheng@sjtu.edu.cn
  
Abstract—People produce hundreds of millions of microblogs 
everyday. With its 140-character message, Microblog has yielded 
an enormous corpus of information, which is noisy but 
informative in some way. However, previous work with standard 
NLP tools of event extraction performs poorly on Microblog. In 
this paper, we adopt a series of methods to extract events from 
Chinese microblogs. In particular, we grab the chatters from 
Sina Weibo to extract civil transportation information. We 
eliminate buzz in Weibo, use CRF methods to filter microblogs so 
as to focus on transportation, and we also use CRF to recognize 
named entities and to extract events.  
Keywords—Event Extraction; Chinese Microblog; NLP; NER; 
CRF;  
I.  INTRODUCTION  
Social Networking has developed dramatically in recent 
years. By exploring sites as Facebook and Twitter, Renren and 
Weibo likewise for Chinese, users publish their thoughts, share 
their thoughts, comment on others’, at an unprecedented rate.  
Sina Weibo, launched in august 2009, is an extremely 
popular online microblog service consisting of more than 500 
millions of users. Yet the amount of microblogs people 
produce everyday through Weibo has exceeded 100 million1. 
Not surprisingly, the enormity of information presents high 
opportunities to exploit this corpus of data to obtain real-time 
information, as well as to make certain prediction. In this case, 
it is quite demanding that we extract events effectively through 
these microblogs. The first step is to grab microblogs. Rather 
than Weibo API, here we have chosen python tools Scrapy to 
fulfill the task. 
Event extraction for Chinese characters is composed of 
several modules: Word segmenting, Part-of-Speech tagging 
(POS tag), Named Entity Recognition (NER) and Event 
tagging. Compared with foreign language as English, Chinese 
phrase doesn’t contain space which serves as natural 
segmenting tag. Therefore, word segmenting is a thorny point 
in Chinese event-extraction. We have tried several tools for this 
task, and eventually we have chosen Language Technology 
Platform (LTP). 
Event extraction for Chinese microblog is even harder than 
traditional text. Previous research in event extraction has focus  
TABLE I.  PRECISION AND RECALL OF NER USING MODEL TRAINED ON 
THE CORPORA OF LIBERATION DAILY. WE CAN SEE A 30% DECREASE IN F1 
SCORE WHEN WE APPLY THE MODEL ON WEIBO CORPORA 
NE Type Corpora Precision Recall F1 
Name Liberation Daily 96.65% 97.86% 96.87% 
 Name Weibo 61.05% 62.33% 61.85% 
Toponymy Liberation Daily 92.89% 93.36% 92.97% 
Toponymy Weibo 72.04% 77.35% 75.47% 
 
mainly on news article. Trained on this genre of text, 
traditional NLP tools performed well and steadily in similar 
text. Unfortunately, with the development of social media, the 
performances of these “off the shelf” tools are weak on 
microblog corpora, as we can see in Table 1. There are several 
reasons for this: 
Firstly, due to the popularization of the user community of 
microblog, unlike those news articles written by professional 
journalist which are strict, structured and conformist, the 
content of microblog is unstructured, occupied by large amount 
of casual expressions, and even contain some emoticons like 
“ORZ” and 9(???=)?????: . Secondly, since 
there is word limit of 140, users tend to minimize their 
expressions, which makes it difficult to extract whole essential 
factors of one event from a single microblog. In addition, this 
kind of microblog is filled with user’s personal preference, 
rendering it more confusing to extract event equitably. Thirdly, 
today’s hot issue may well be in tomorrow’s landfill, especially 
when it comes to such vogue case as cyber-style of writing. 
“Zhenhuan Style”, originated from a hit drama in early 2012, 
have once occupied Weibo for several months, yet seldom 
mentioned after one year. Models trained on these pop 
elements will soon lose its effectiveness and become redundant. 
Last but not least, microblog users frequently mention their 
mundane events in daily life, such as what they eat and where 
they have been, which are only informative to their close social 
network relationships. Therefore, these redundancies of 
information will have to be removed in order to better extract 
event of general interest. 
After close scrutiny of the existing NER and Event tagging 
tools, we have chosen CRF++, a simple, customizable, and 
open source implementation of Conditional Random Fields 
1, http://baike.baidu.com/view/2762127.htm 
2013 International Conference on Cloud Computing and Big Data
 978-1-4799-2829-3/13 $26.00 © 2013 IEEE
 DOI 10.1109/CLOUDCOM-ASIA.2013.48
 577978-1-4799-2830-9/14 $31.00 © 2014 IEE
(CRFs) for segmenting/labeling sequential data. Meanwhile, 
we wrote a series of scripts to eliminate the interference caused 
by casual expressions in Weibo. 
In this paper, we focus on a specific task: to extract civil 
transportation information using chatter from Sina Weibo. For 
one thing, even though there are several Chinese microblog 
networks, Weibo possess the biggest user community and 
therefore yield the biggest dataset. For another, we can get a 
relatively more precise character dataset and avoid certain buzz 
since we have confined the topic in an isolated circle55civil 
transportation in this case. In order to focus on this one topic, 
we also use method of CRF to filter the microblogs so that our 
basic corpora are solely consisting of clean microblogs 
concerning transportation. 
The rest of the paper is organized as follows. Next we 
introduce the overview of our microblog processing system and 
the proposed approaches. From Section 4 to 9, we will describe 
each procedure of our system respectively. Then in section 10 
we describe the testing standards and present our analysis on 
the result we obtained. We cite related work in section 11, and 
we conclude in section 12. 
II. SYSTEM OVERVIEW 
An overview of the several components of our system for 
extracting civil transportation information from Weibo is 
presented in Fig.1.  
In the first place we virtually log in Sina Weibo, and grab 
microblogs from certain account who regularly issue contents 
concerning civil transportation. A raw dataset is obtained in the 
end. 
 In the second place, we use a series of scripts to remove 
noise such as emoticons and unrelated links. Next, we 
introduce method of CRF to filter unrelated topics, and we 
obtain our basic corpora free of noise. 
 In the third place, microblogs are segmented and POS 
tagged. We then annotate the data in order to make training set 
and test set. By adopting the method of CRF again, we obtain a 
model through training on the dataset. 
Finally, we use this model on the testing set and get the 
precision and recall rate. We add at last a script to calculate the 
F score. 
III. PROPOSED APPROACH 
As mentioned above, our system are composed of three 
modules: python tools Scrapy to grab microblog, LTP tools to 
segment words and CRF++ to do NER and Event Tag. 
A. Scrapy55To Virtually log in and grab microblog  
Sina Weibo provides official APIs for grabbing microblogs. 
However, after experimentation and examination, we found 
that official APIs have several limitations for the number of 
microblog grabbed, the IP for log in and the authority of 
grabbing, so that they cannot satisfy our needs in the research. 
So we chose a web crawling tools Scrapy. 
 
Fig. 1.
  
System Overview 
Scrapy is a fast high-level screen scraping and web 
crawling framework, used to crawl websites and extract 
structured data from their pages. It can be used for a wide range 
of purposes, from data mining to monitoring and automated 
testing2.  
B. LTP55To segment sentence and to POS tag 
There are dozens of Chinese natural language processing 
tools. To find one with highest efficiency we have chosen three 
most famous and widely used tools: ICTCLAS3 (Institute of 
Computing Technology, Chinese Lexical Analysis), LTP4 
(Language Technology Platform), and Stanford Word 
Segmenter5. 
2, http://scrapy.org       
3, http://www.ictclas.org 
4, http://www.ltp-cloud.com/  
5, http://nlp.stanford.edu/software/segmenter.shtml                                      
Weibo 
Raw corpora 
Test 
Set 
Train 
Set 
Grabbing Weibo
 Filter 
Segmenting& POStag
 NER Event Tag 
Annotation 
Result 
Model 
Standardization
 Basic corpora 
Display 
Corpora 
Expansion 
578
TABLE II.  ACCURACY OF THE THREE TOOLS FOR PROCESSING WEIBO 
TEXT 
ICTCLAS 83.2% 75.38% 
LTP 83.44% 74.1% 
Stanford Seg 81.5% / 
 
To evaluate the effects of the three tools on Weibo text, we 
select randomly 100 Chinese microblogs, segment and tag 
them manually, then we tried the three tools on the same 
sample. The results are shown in Table 2.  Given the result of 
comparison, we finally chose LTP as our tools for sentence 
segmenting and POS tagging. LTP can achieve a relatively 
high accuracy in NER, so we also use it to do this task. The 
result of NER by LTP will be listed as a reference for later use. 
C. CRF55To filter microblogs, to do NER and to tag event 
Conditional Random Fields (CRFs) are a class of statistical 
modeling method often used in pattern recognition and 
machine learning to make structured prediction. As an ordinary 
classifier predicts a label for a single sample regardless of the 
"neighboring" samples, a CRF can take into account the 
context; For example, the linear chain CRF popular in NLP 
predicts sequences of labels for sequences of input samples. 
Principle: Conditional Random Fields can be defined as 
followed [13]:  
 9Let G = (V, E) be a graph such that Y= (Yv) v V, so that 
Y is indexed by the vertices of G. Then (X,  Y) is a conditional 
random field in case, when conditioned on X, the random 
variables Yv obey the Markov property with respect to the 
graph P (Yv|X, Yw,  w? v) =  p (Yv|X,  Yw,  w~v), where w~v 
means that w and v are neighbors in G”. 
Here X may range over natural language sentences and Y 
stand for the label sequence. 
CRF is an undirected graphical model. Its nodes are divided 
into two disjoint sets X and Y, X stands for the observed 
variables, Y stands for the output variables. Then the 
conditional distribution p(Y|X) is modeled. The aim of the 
CRF is to find out the label sequence yRY, who maximize the 
conditional probability for a sequence X. 
y=argmax p(Y|X)                         (1) 
Therefore, Named Entity task can be regarded as a 
sequence labeling task. Thus CRF can be used to do NER. 
Specifically, CRFs are often applied in shallow parsing, 
named entity recognition and gene finding, among other tasks, 
being an alternative to the related hidden Markov models. In 
computer vision, CRFs are often used for object recognition 
and image segmentation. 
Tool: CRF++ is a simple, customizable, and open source 
implementation of  CRFs  for segmenting and labeling 
sequential data. CRF++ is designed for generic purpose and 
will be applied to a variety of NLP tasks, such as Named Entity 
Recognition, Information Extraction and Text Chunking6. 
 To use CRF++, a training file, a test file and a template file 
are necessary. We need to write a specific template file in 
which we describe which features are used in training and 
testing. Fig.2 reveals the format of training file and the 
specification of the signs in a template file.  
IV. GRABBING MICROBLOGS 
To successfully grab microblog, our approach consists of 
two modules: 
The first one is responsible for virtually login. By analysis 
the URL for first login, we obtain four values: “servertime” 
time of server, “nonce” random number of login, “RSA 
Pubkey” RSA public key, and “rsakv”. After encryption, we 
encapsulate these information along with user name and 
password into a form demanded by Sina Weibo, post it to 
server, and then we will receive the response and complete the 
virtual login. Later we submit request repetitively to crawl 
microblogs. 
From the grabbed data, we obtain the user account, content 
of a microblog, time of issue, number of comment, number of 
forward, etc. In our case, we are only interested in the content 
of microblogs, while others will become a sort of noise during 
later analysis. Therefore, our second module analyzes the 
grabbed html data by using regular expression, solely 
extracting the content of each microblogs. 
After these operations, we obtain raw corpora of 
microblogs. 
V.
  
STANDARDDIZING MICROBLOGS
  
Obviously, the raw corpora of microblogs contain huge 
amounts of noise, which are useless in the processing of 
microblogs, thus to be eliminated in standardization. Our 
standardization module has functions as follows: 
 Removing meaningless signs, such as “#topic name#”, 
“[Emoticon]”, “@username”, some web links like 
“http://t.cn/z8ne0w2”, etc. 
 Discarding microblogs shorter than 7 words.  
 Transforming complex Chinese into simple type, in 
order to avoid word repetition in later analysis 
We may delete some useful information during the process 
of this module, but we do reduce the redundancies in later 
process.  We test the performance of NER in both 
unstandardized dataset and standardized dataset in order to see 
the effect of our standardization. 
VI. FILTING MICROBLOGS 
Since we grabbed microblogs from the accounts who 
frequently issue microblogs concerning transportation, the 
majority of these microblogs indicate the real-time traffic jam, 
traffic accident, construction information, etc. Not surprisingly, 
most of these accounts, especially for those ordinary users 
(compared with some official accounts who focus on reporting 
transportation information), may issue some noisy unrelated 
microblogs, such as forecasting the weather, saying good night, 
or reporting their location. Fig.2 demonstrates two microblogs 
from a same Weibo uid. The first one is about traffic condition 
while the second has nothing to do with transportation. 
6, http://CRFpp.googlecode.com/svn/trunk/doc/index.html?source=navbar  
579
 
 
Fig. 2. Example of two microblogs obtained from the same uid “207073197”, 
the first one is useful transportation information telling which road has a 
traffic jam in Shanghai, while the second one is just an advertisement of a 
musical program, that is to say, noise. 
In order to discern those microblogs who really talk about 
transportation, we first scan them to identify some features. For 
example, if a microblog contains words like “?”(road) or “?
 ?”(traffic jam), it is very likely to have something to do with 
transportation. On the other hand, if a microblog contains 
words like “??”(weather), “@”, it is most probably noise. In 
this way, we define the first feature as “Whether this microblog 
contains the word ‘?’(road)”. If so, we mark a “1” at the end, 
otherwise we mark a “0”. Similarly, we define other 6 features.  
At the end, we annotate manually 900 microblogs whether they 
are about transportation. 
With the seven features and the manually annotation result, 
we train 470 microblogs and obtain a CRFs model. We then 
use the rest 430 microblogs to test our model. We find out that 
our model achieves a high precision of 86.95%. With this 
model, we can easily annotate following grabbed microblogs, 
and filter those noisy topics unrelated to transportation. 
VII. SEGMENTING MICROBLOGS AND POS TAGGING 
LTP is a language platform based on XML presentation. 
Thus our basic corpora in form of TXT will first be 
transformed into XML. Then we invoke module IRLAS, which 
is responsible for segmenting and POS tagging. 
In our basic corpora, each microblog is stocked in one line, 
so that LTP can read and process the input file line by line. 
After processing, we got our result file also in format TXT. 
Each line lay one segmented word, along with its POS tag and 
NER result. 
VIII.  EXTRACTING NAMED ENTITIES 
Our origin dataset consists of 3 columns: segmented word, 
POS Tag, NER by LTP. To extracting named entities, we have 
to first annotate manually each line with correct named entity 
tags.  Assuming that the longest named entity is composed of 7 
Chinese characters, in the template file, we define the window 
size as 7. In other words, when we processed each segmented 
Chinese character, we examined three characters before it, 
itself and three characters after it. We also consider the 
relations between the three columns.  
In this way, our template contains 70 macro template 
structures.  By using this template to train our origin dataset, 
we obtain a CRF model that can be used to tag named entities. 
 
TABLE III.  EVENT TAGS SYMBOLS SET 
Tag Symbol Implication 
B-E Begin of Event 
I-E Internal of Event 
E-E End of Event 
O OTHER 
K-E Keyword of Event 
D-E Description of Event 
T-E Time of Event 
L-E Location of Event 
IX. EXTRACTING EVENTS 
We use our CRF model obtained during the process of 
NER to tag named entity of another basic dataset (result of 
LTP), and get a new dataset consist of 4 columns, of which the 
first (segmented words), the second (POS tag) and the fourth 
(our NER result) are useful in this module.  
Similarly, we annotate each line manually with event tags. 
Table 3 lists the possible symbols of event tags. 
As for the training template, we adopt seven different 
templates so as to select the one with best performance:  
In the first one, we just considered the relationship between 
two interfacing line. The window size is 5. In template 2, since 
some named entities and event body are longer than five words, 
we adjust the window size to 7. In template 3 we considered 
the relationship between two columns, that is to say, between 
words and POS tags, POS tags and NER references, and words 
and NER references. We also considered relationships between 
three interfacing lines. Template 4 abandons POS tags and 
focuses on relationships between words and NER references. 
While template 5 abandons NER references and focus on 
relationships between words and POS tags. In template 6 we 
neglect the relations between columns and focus solely on 
relations between lines in one column. In the last template, we 
considered all the relationships. There are 70 template 
structures in this template.  
We obtain 7 CRF models for Event tagging with different 
performance. We choose the best one amongst as our model to 
tag event.  
X. EXPERIMENT 
To prepare for the experiment, we first establish our 
corpora of experiment. Then we do the segmenting and POS 
tag. We annotate the dataset, and then we do the NER and 
Event tagging.  
A. Corpora of Experiment 
In order to get real-time transportation information, we first 
chose 12 Weibo accounts who officially issue microblogs 
concerning transportation information of Shanghai. We also 
selected 50 accounts of ordinary users who occasionally report 
traffic condition when they actually meet some specific traffic 
condition. After virtually log in Sina Weibo, we encapsulated 
the uids of these accounts into a list and transmit it to Scrapy as 
a reference. Then we ran Scrapy and crawled microblogs from 
580
these accounts. Our original dataset of 2000 microblogs were 
obtained from 1st Aug 2013 to 7th Aug 2013. 
In order to improve and perfect our corpora during the 
process of NER, we also added corpora obtained from 
Revolution Daily about transportation from 13th Aug 2013 to 
17th Aug 2013.  
B. Evaluation Criteria 
To evaluate the performance of CRF++ on named entity 
recognition and event tag, we use three parameters: P 
(precision), R (recall rate), and F (F-measure). 
Precision measures the percentage of correct named entities 
tagged by CRF++ over the total number of named entities 
tagged by CRF++. Recall measures the percentage of named 
entities tagged by CRF++ over the total number of named 
entities in the file. F-measure, a measure that combines 
precision and recall, is the harmonic mean of precision and 
recall. 
rp
 rp
 measureF
 +??
 ??+?
 = )(
 )1(
 - ??
 ??
                 (2) 
In equation 2, ? stands for the relative weight of P in R. 
Normally, ? equals 1, we get an F1 score. 
C. Result and Analysis of NER 
We use our CRF model described in section 8 to test on the 
two set of testing data: the unstandardized dataset (with noise 
like “@username”) and the standardized dataset. Their 
performances are in table 4. 
An increase of 17.3% of precision can be observed from the 
NER result, which means by eliminating informal expressions, 
our efficiency of extracting named entities increases. 
D. Result and Analysis of Event Tagging 
Our seven templates produce seven models during the 
training. Table 5 lists the training result of the seven templates, 
including their number of features, number of iteration, training 
time and the size of each model file.  
From the result of training in table 5, we can see that as we 
aggrandize the window size, considering more relations 
between columns and lines, it will cost longer time to train a 
model, and the model size will become larger. 
By using these seven template files, we obtain seven 
models. Table 5 and Fig. 3 illustrate the performance of the 
seven models. From Fig 4, we can see that for model 1, 2 and 3, 
as we expand the scope of relationships, both P and R augment, 
which means in certain scope, the more features we consider, 
the better result we will obtain.  
TABLE IV.  NER RESULTS 
 P R F1 
Unstandardized 73.59% 57.85% 64.99% 
Standardized 86.33% 66.85% 75.35% 
 
TABLE V.  TRAINING RESULTS OF SEVEN TEMPLATES FOR EVENT 
TAGGING 
Template No. Feature Iteration 
training 
time(s) 
Size 
Model(kb) 
1 3508246 109 606.70 22262 
2 9742306 111 763.59 69526 
3 10113045 113 787.62 72133 
4 13263404 124 960.45 106647 
5 14446950 76 667.04 113030 
6 14335048 112 921.01 112238 
7 14901880 117 1011.96 116453 
TABLE VI.  TESTING RESULTS OF SEVEN MODELS FOR EVENT TAGGING 
Models P R F-1 F-0.1 
1 37.24% 79.05% 50.63% 78.18% 
2 37.62% 81.45% 51.46% 80.52% 
3 38.18% 81.67% 52.03% 80.76% 
4 16.76% 74.38% 27.36% 71.93% 
5 27.00% 84.80% 40.96% 83.04% 
6 35.01% 83.56% 49.34% 82.42% 
7 36.49% 82.01% 50.52% 81.01% 
2022'
 32022'
 42022'
 52022'
 62022'
 72022'
 82022'
 3 4 5 6 7 8 9
 Rtgekukqp
 H/3
  
Fig. 3. Example of input and output data for LTP 
Model 4 neglect the result of POStag, and it receive the 
lowest precision rate. In other words, simply considering the 
relation between words and its NER tags cannot effectively 
identify its event tag. Compared with model 4, model 5 
achieves a better performance in both precision and F-measure, 
which means that POS tags can affect the result of event 
tagging more deeply than NER tags do. 
As we compare the performance between model 6 and 7, 
we found that the consideration of tags in one lines will 
increase the recall rate while reduce the precision. In other 
words, the relationships between segmented words, POS tags 
and NER tags as a whole can help improve the recognition of 
events, but may affect the judgment of event type in a bad way. 
We can see that the performance of model 7 is worse than 
that of model 3, which means, with more thorough 
consideration of all the relationships, there will be more 
limitation in recognition of named entity and event tag, thus 
reduce the recall rate. 
Overall, we chose model 3 as our CRF model for event 
tagging. 
581
TABLE VII.  EXAMPLE OF EVENT TAG ABOUT A CIVIL FLOOD IN CERTAIN 
PLACES, AND CALLS ATTENTION OF ALL THE DRIVERS. AFTER PROCESSING, WE 
IDENTIFY THE KEY WORD “ROAD”, EVENT”FLOOD”,LOCATION OF 
EVENT”CENTURY AVERNUE, ZHOUHAI, ETC”, AND TIME OF EVENT “17:05”. 
Origin 
Microblog 
%???%39<27 ????????x?w?0
 u?? ' 0 3????????????????
 ????????????????????
 ?N??????"
 K-E ??"
 B-E ??"
 L-E 0u"? ' 0 3"??"???"????"
 T-E 39<27"
 E. Demonstration of Events. 
Finally, to demonstrate the civil transportation event we 
extract from the microblog, we designed a script to find out all 
the segmented words whose event tag is K-E(keyword of 
event), B-E(begin of event), L-E(location of event) or T-E(time 
of event), according to the tag set in table 3. With these four 
elements, we can easily confirm the event information. Table 7 
illustrates an example of our Event Tag. 
XI. RELATED WORK 
There has been relatively little previous work on building 
NLP tools for Chinese Microblog. Yet for common named 
entity recognition and research on English Microblog as 
Twitter, there are several:  
There has been considerable published result on general 
Named Entity Recognition. The methods can be divided into 
three categories: method based on rules, method based on 
statistics and method based on the mixture of the two. For 
method based on statistics, there are mainly three models: 
Maximum Entropy, Hidden Markov and Conditional Random 
Fields.  Della Pietra and Mercer R L were the first to induce 
maximum entropy model into natural language processing [2]. 
Chen A, Peng F and Shan R built up conditional random fields 
model and maximum entropy model, and obtained an F value 
up to 86.2%[3]. 
As for the research on information extraction within 
microblog, by using distant supervision, Benson et al. trained 
a relation extractor which identifies artist and venue 
mentioned in twitter of users who list their location as New 
York [8]. T.Sakaki M.Okazaki and Y.Matsuo designed a 
system proved to be capable of recognizing almost all 
earthquakes reported by the Japan Meteorological Agency. 
They realize this system by training a classifier to recognize 
tweets reporting earthquakes in Japan [9]. Ritter A and Clark 
S have designed a novel part-of-speech tagger module and 
word segmenting method; they have also introduce entity 
classification so that the precision of named entity recognition 
were higher than that of traditional methods [4]. 
XII. CONCLUSION 
In this paper we have shown an event extraction system 
which can extract civil transportation event from microblog 
text like Weibo. We have used Scrapy to crawl microblogs, 
CRF methods to standardize and filter microblogs, LTP to 
segment each blog and tag it with POS, and CRF methods 
again to do named entity recognition and event tag. At last we 
obtained a model trained on our corpus. By using this model, 
we can tag event from newly grabbed microblogs and extract 
their event information. The precision of our NER reaches xxx, 
and the precision of event tagging reaches xxx. At last we can 
obtain from each microblog the keyword of event, the event 
phrase, its location, and its time. 
The civil management can benefit greatly from our system. 
Event extracted from social media will help the city managers 
to know what's going on in the city and they can be more 
prepared for those coming events. For example, city 
administration can allocate adequate public transportation 
service for a big event which involves several thousand people. 
Another example is that city administration can allocate 
enough police force to the place that a crime prone event is 
going to happen.  In this way, event extraction can make the 
smart city even smarter. 
XIII. ACKNOWLEDGMENT  
This material is based upon work rctvkcnn{"uwrrqtvgf"
 d{"KDO"Ujctgf"Wpkxgtukv{"Tgugctej"Rtqitco0 
REFERENCES
  
[1] Sitaram Asur and Bernardo A.Huberman, Predicting the Future With 
Social Media. Web Intelligence and Intelligent Agent Technology (WI-
 IAT), 2010 IEEE/WIC/ACM International Conference on  (Volume:1 ) 
[2] Della Pietra S, Della Pietra V, Mercer R L, et al. Adaptive language 
modeling using minimum discriminant estimation[C]. Acoustics Speech 
and Signal Processing, ICASSP-92. USA,1992:633-636 
[3] Chen A, Peng F, Shan R, et al. Chinese named entity recognition with 
conditional probabilistic models[C]. Proceedings of the 5th SIGHAN 
Workshop on Chinese Language Processing. Australia, 2006: p173-176. 
[4] Alan Ritter, Mausam, Oren Etzioni and Sam Clark, Open Domain Event 
Extraction from Twitter,  Proceedings of the 18th ACM SIGKDD 
international Conference on Knowledge Discovery & Data Mining 
(KDD 2012) 
[5] Alan Ritter, Sam Clark, Mausam, and Oren Etzioni, Named Entity 
Recognition in Tweets: An Experimental Study,  Proceedings of the 
2011 Conference on Empirical Methods in Natural Language 
Processing (EMNLP 2011) 
[6] J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: 
Probabilistic models for segmenting and labeling sequence data, In Proc. 
of ICML, pp.282-289, 2001 
[7] F. Sha and F. Pereira. Shallow parsing with conditional random fields, In 
Proc. of HLT/NAACL 2003 
[8] E. Benson, A. Haghighi, and R. Barzilay. Event discovery in social 
media feeds. In ACL,2011. 
[9] T. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake shakes twitter users: 
real-time event detection by socialsensors.In WWW,2010 
[10] Dr.Rakesh ch. Balabantaray, Suprava Das and Kshirabdhi Tanaya 
Mishra, Case Study of Named Entity Recognition in Odia Using CRF++ 
Tools. (IJACSA) International Journal of Advanced Computer Science 
and Applications, Vol. 4, No.6, 2013 
[11] WENG Jui-Yu?YANG Cheng-Lun?CHEN Bo-Nian.IMASS?An 
Intelligent Microblog Analysis and Summarization System[C]. 
Portland? Oregon ? Association for Computational Linguistics?
 2011?133-138. 
[12] Brendan O’Connor, Michel Krieger, David Ahn, TweetMotif: 
Exploratory Search and Topic Summarization for Twitter.In 
Proceedings of the Fourth International AAAI Conference on Weblogs 
and Social Media. 2010. 
 
582

Scalable Context-Aware Role Mining with
 MapReduce
 Zhiwei Yu
 School of Computer Science and Engineering
 University of New South Wales
 Sydney, Australia
 Raymond K. Wong
 School of Computer Science and Engineering
 University of New South Wales
 Sydney, Australia
 Chi-Hung Chi
 ISSL
 CSIRO
 Tasmania, Australia
 Abstract—Cloud computing platforms facilitate efficiently pro-
 cessing complicated computing problems of which the time
 cost used to be unacceptable. Recent research has attempted
 to use role-based approaches for context-aware service recom-
 mendation, yet role mining problem has been proven to be
 difficult to compute. Currently proposed role-mining algorithms
 are inefficient and may not scale to cope with the huge amount
 of data in the real-world. This paper proposes a novel algorithm
 with much better runtime complexity, and in MapReduce style
 to take advantage of popular distributed computing platforms.
 Experiments running on a medium-sized high performance com-
 puting cluster demonstrate that our proposed algorithm works
 well with both running time complexity and scalability.
 I. INTRODUCTION
 Recently, cloud computing platforms are widely applied
 for dealing with various of mining missions on big data. They
 involve the internet for providing on-demand access to shared
 resources, such as compute units, data, storage and applica-
 tions. MapReduce programming model [5] is one of the most
 popular distributed computing technologies for distributing big
 data processing across a large cluster of compute nodes in
 the Cloud. It is quite effective in simplifying the development
 of data-intensive applications, through the use of a simple
 API, and with a robust runtime system. These infrastructures
 truly facilitate efficiently processing complicated computing
 problems of which the time cost used to be unacceptable.
 Role-based approaches are widely applied to data security
 domain [16] [9] [8]. Further, recent research has attempted
 to use role-based approaches to recommend mobile services
 to other members among the same context group [18] [21].
 However, role mining problem has been proven to be difficult
 to compute, it is in fact NP-hard [15].
 Two context-aware role mining algorithms based on matrix
 calculation have been recently proposed [21] - one is an ap-
 proximation algorithm and another one is a complete algorithm
 for verification. Both of them have their weaknesses and may
 not applicable under some circumstances; and therefore, this
 paper is a continuation work of [21]. Let u to be the number
 of users, b to be the number of behaviors, c to be the number
 of contexts, and r to be the total number of roles; although
 algorithm C2 (AC2) in [21] is a complete algorithm that can
 cover all roles, it has a complexity of O(u2 ? 2c) because
 of the combination procedure for listing the power set S =
 {?context, behavior? couples of the user}. As a result, if the
 number of contexts becomes large, AC2 will not be practical
 any more. Besides, the algorithm V2 (AV 2) has a complexity
 of O(u2?c)), which is much faster than AC2. However, AV 2
 is not complete, which means that its role coverage could be
 very low (< 10%) in certain situation, e.g., when the number
 of contexts and users are very large.
 In this paper, similar to [18] and [21], we extend the
 behavior pattern recognition method [3] to identify context-
 aware roles from multi-user behavior patterns. Different from
 [18], [21] and all other related work to this paper, the proposed
 algorithm AD1 is complete as well as with applicable and
 practical running time complexity. To apply compute cluster
 for parallel computing, we have applied MapReduce [6] frame-
 work in this algorithm, which can be deployed to an Apache
 Hadoop environment. Leverage on high performance compute
 cluster, algorithm AD1 shows strong scalability. Experiments
 are also performed to demonstrate the capacity and scalability
 of this algorithm.
 The organization of this paper is as follows. Section II
 summarizes the related work. Section III defines the context-
 aware role mining problem. Section IV presents our proposed
 new algorithm for context-aware role mining, and it is divided
 into five subsections. The first subsection talks about the main
 challenges for the algorithm design; the second subsection
 talks about main procedure and the third subsection explains
 the partition method; in the fourth subsection, methods for
 reducing the search space are discussed. Lastly, we present this
 map-reduce-style code in detail. After that, Section V presents
 experiment results to show the performance and scalability of
 the new algorithm. Finally, Section VI concludes the paper.
 II. RELATED WORK
 The concept of roles has been used in many areas such
 as linguistics, knowledge representation, relational database,
 and access control [2], [10], [14]; especially, roles have been
 widely used in data security domain, under RBAC, e.g., [16].
 Numerous role mining approaches have been proposed, e.g.,
 [8], [9], [17], [21].
 Recently, context, which used to be mostly disregarded in
 RBAC, is considered as an important factor. To the best of
 our knowledge, [1], [12], [18] are the only papers addressing
 context-aware RBAC. However, both [1] and [12] did not
 describe how to discover or identify context-aware roles. Tra-
 ditional RBAC-based role mining algorithms (such as [9] and
 [17]) cannot be directly applied, since traditional RBAC mod-
 els only consider 2-D parameters, namely ?user, permission?,
 while context-aware role mining approach needs to consider
 2013 IEEE International Conference on Big Data
 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 467
Fig. 1. User-context-behavior matrix (BiUCB)
 Fig. 2. Role-context-behavior matrix (CBA)
 at least 3-D dataset ?user, context, behavior? where context
 includes time and scene, etc.
 Apache Hadoop is an open source system that allows for
 the processing of big data across cluster of workstation class
 computers [20]. Two of the key ideas of Apache Hadoop are
 originated from Google file Systems (GFS) [11] that allows
 for the distribution of data across thousands of computers in
 a cluster, and MapReduce [6] that handles the breakdown of
 a problem into subtasks so that they can be processed by the
 cluster. MapReduce [5] is a programming model from Google.
 The model comprises of two main functions, Map and Reduce
 functions. The Map function takes in a key/value pair and
 outputs an intermediate list of key/value pairs. The Reduce
 functions will than take all values associated to the same key
 and produce the final output list of key/values. Users are tasked
 with providing the Map and Reduce implementation.
 Apache Mahout is part of the “Hadoop” family which
 provides scalable machine learning libraries to handle rec-
 ommendation mining problems under “Hadoop” platform.
 Apart from typical clustering and singular value decomposition
 (SVD) functionality, it also provides Naive Bayes, decision
 tree, etc., type of classifiers. Besides, frequent itemset mining
 is also supported. On the other hand, Chen et al. [4] and
 Wang et al. [19] are recent examples of MapReduce-based
 solutions to frequent itemset mining problem. To the best
 of our knowledge, this is the closest related mining problem
 considered in “Hadoop”, and context-aware role mining is a
 new area due to be explored.
 This paper is a significant extension of the work done in
 [18], [21], in which [21] proposed a simpler and more efficient
 algorithm to mine roles than [18]. However, the matrix-based
 algorithms presented in [21] does not guarantee completeness,
 i.e., the algorithm may fail to uncover some role patterns. This
 paper provides a scalable algorithm that is based on the ideas of
 divide and conquer and also Map-Reduce paradigm distributed
 computing as solutions to role mining problem.
 III. PROBLEM DEFINITION
 A service recommendation engine must know the services
 consumption behaviors in the user community before any
 recommendation can be made. We firstly study the knowledge
 acquisition by defining the role mining problem.
 Fig. 3. User-role matrix (UA)
 We follow the definition of the context-aware role mining
 problem from [18] and [21]. However, for the simplicity of
 presenting and computing, a traditional u ? c UCB matrix
 with b behaviors in cells is transformed to a u? (c? b) matrix
 by spreading each column in UCB to b columns and setting
 the respective values in the new column to be 1 according
 to the old cell values in UCB. We call this matrix BiUCB,
 of which each column represents a different context-behavior
 pair(cb-pair). As a result, a role can be presented as a set of
 users plus a set of column numbers of BiUCB.
 Definition 1 - Context-aware role mining problem (CA-
 RMP): Given a user behavior pattern configuration bpc =
 ?U , C,B, BiUCB?, where U is a set of users, C is a set of
 contexts, B is a set of behaviors, BiUCB ? U ? ?C,B?
 is the user-context-behavior relation. CA-RMP is to find a
 state ?R,UA,CBA? that is consistent with bpc, such that for
 any ?R?, UA?, CBA?? that is consistent with bpc, |R| ≤ |R?|,
 where R, R? is a set of roles, UA, UA? ? U ? R is the
 user-role assignment matrix, CBA, and CBA? ? R ? ?C,B?
 is the role-context-behavior assignment matrix. The state is
 consistent with bpc, if every user in U has the same set of
 context-behavior assignment as in bpc. Each role must have
 at least two u ? U (minu ≥ 2) and two ?c, b? ? ?C,B?
 (minc ≥ 2) to ensure generality.
 One can summarize the relationship between BiUCB, UA
 and CBA by the following formula:
 BiUCB = UA? CBA
 where ? represents a boolean matrix multiplication between
 two boolean matrices in which X ? Y =
 ?
 k
 (Xik ? Ykj) [13].
 Suppose there is a user set U = {u1, u2, u3, u4, u5}, a
 context set C = {c1, c2, c3, c4, c5}, and a behavior set B =
 {1, 2, 3}. A sample UCB can be constructed based on users’
 behavior patterns as shown in Figure 1 - where each column
 denotes a ?c, b? ? ?C,B?, each row denotes a u ? U , and a
 value of 1 in cell (i, j) represents that the user ui exhibit the?c, b?j ; otherwise, a value of 0.
 After mining roles from the behavior pattern configuration,
 we obtain a state ?R,UA,CBA?, where R = {r1, r2, r3, r4},
 the resulting CBA and UA matrices are recorded in Figures 2
 and 3, respectively. As shown in Figure 2, four mined roles r1,
 r2, r3 and r4 are listed in a CBA matrix, e.g., r2 has behavior
 1 under context c1 and c3, and behavior 2 under context c2.
 Figure 3 shows a UA matrix, where a 1 in cell (i, j) represents
 that user ui can play role rj . e.g., user u1 can play role r2.
 IV. MAP-REDUCE MERGING-STYLE ALGORITHM (AD1)
 Generally, a modern high performance computing cluster
 could have over ten thousands of computing units. By applying
 a powerful cluster and well designed distributing algorithms
 468
with strong scalability, we expect to process huge input data
 sets with hundreds times of scale bigger than that for some
 standalone algorithms but with the same time cost.
 A. Challenges in Distributing Role Mining Algorithm Design
 The Divide and Conquer theory is often applied for obtain-
 ing strong scalability. By this approach, an essential issue is
 how to divide the data into parts and merge the intermediate
 results later. A naive divide policy is to separate the matrix
 by selecting parts of the columns and rows first, and then
 simply apply some standalone mining methods to find all the
 valid roles in the sub matrices, and finally merge all the roles
 generated from the sub matrices together. However, the roles
 related to the data across multiple sub matrices will be omitted.
 As a result, this kind of partition method can not lead to a
 complete algorithm.
 Generally, the original input data set of a role mining job is
 not very huge. A typical one should be no more than 1 million
 users, 100 contexts and 100 behaviours. However, the number
 of roles r is sensitive to all of b, c, u, minu and minc and r
 could be a combination of b ? c in the worst case. The huge
 computing load comes from the tremendous number of 1 on
 1 comparing between roles.
 minu and minc are the key parameters to control the output
 scale and the algorithm complexity. In most of the cases for
 getting a practical output, the role number should always not
 be very huge and could be well controlled by adjusting minu
 and minc. However, since the data is partitioned, it is hard to
 apply them to filter the data distributed in different machines.
 B. The Main Procedure of Algorithm (AD1)
 To avoid the problem of the naive partition method above,
 a bottom up merge-style algorithm AD1 is proposed to com-
 pletely cover all the roles. The main idea of AD1 is to firstly
 compute all the intermediate roles with fewer cb-pairs and
 then generate the more complex roles with more cb-pairs by
 merging them. For example, suppose we have two roles:
 role1 : {u1, u2, u5, u6} ? {cbpair1}
 role2 : {u3, u4, u5, u6} ? {cbpair2}
 after one round of merging, a new role is generated:
 role3 : {u5, u6} ? {cbpair1, cbpair2}
 The user set of role3 is generated by getting the intersec-
 tion set of the user sets of role1 and role2, while the cb-pair
 set of role3 is generated by getting the union set of the cb-pair
 sets of role1 and role2.
 The essential process of AD1 is that, in each round of
 iterations, all the roles generated from last round are merged
 with each other. During this process, the logic is simple so that
 the tasks are easy to split and distribute to multiple computing
 units. The running time complexity of AD1 is:
 O(log(b? c)? r2 ? u)
 The factor log(b ? c) is the round number of the merge
 iteration, for after each round of this iteration, the maximum
 Fig. 4. Partition Policy in Algorithm AD1
 number of cb-pairs of the covered roles will become twice
 until it reaches b? c. Generally, for the strong requirement of
 minu and minc, there should always be no new qualified role
 generated after looping for only very few rounds far less than
 log(b?c). The factor r2 is for the number of all the role pairs.
 The factor u is for merging the user set operations between
 the two roles.
 When BiUCB is a random matrix, roles with more users
 and cb-pairs have less probability to be formed. Further, this
 probability reduces fast (exponential) with the increasing of
 user and cb-pair numbers in roles. Hence, r is close to the
 number of roles with very few users and cb-pairs, which means
 r is close to a polynomial function with u,c and b.
 C. The Partition Policy of Algorithm (AD1)
 In each round of the merging process, we distribute the
 huge number of merging tasks to multiple computing units.
 The total number of merging operations for each round is
 O(r2). We do not simply generate the data of all the role
 pairs and split it by the build-in partition tools of Hadoop,
 because it will definitely generate huge intermediate data and
 bring unacceptable I/O cost. Instead, each mapper job loads all
 the roles to the memory and choose respective different role
 pairs according to their job IDs.
 A partition example is shown in figure 4. All the role pairs
 are separated uniformly to each mapper, the numbers of role
 pairs assigned to each mapper are nearly even.
 By the merging operations in the mapper, most of the role
 pairs will not generate any new role. As a result, the scale of
 the mapper output is very small, which brings extra benefits
 for reducing the cost of network transferring within the cluster.
 D. Search Space Reduction
 The primary search space for each round of the merge
 iteration is O(r2), which is still very large. Actually, only
 very small part of this search space is useful for generating
 new roles. It can be dramatically reduced by the operations in
 this subsection. For facilitating the expression, basic necessary
 formulations are introduced in table I.
 Definition 2 - u-pairs(u1)={?u, |Scb(u) ? Scb(u1)| ≥ minc}
 u-pairs is the essential concept for applyingminc to reduce
 the search space. We also introduce the following lemmas that
 are useful for reducing the search space:
 469
Lemma 1:
 ?r1, r2, n, r2 ? Succn(r1) ? n > R(r1) + 1?
 ?m, r3,m < n ? r3 ? Succm(r1), r2 ? Succn(r3)
 Lemma 2:
 ?r1, r2, |Su(r1) ? Su(r2)| < minu?
 ?n, r3 ? Succn(r1), r4 ? Succn(r2),|Su(r3) ? Su(r4)| < minu
 Lemma 3:
 ?u1, u2, r1, u1 ? r1 ? u2 ? r1 ? |Scb(r1)| ≥ minc?
 u1 ? u-pairs(u2) ? u2 ? u-pairs(u1)
 Lemma 4:
 ?u1, r1, u1 ? r1 ? |Scb(r1)| ≥ minc?
 Su(r1)  u-pairs(u1)
 Lemma 5:
 ?u1, r1, |u-pairs(u1) ? Su(r1)| < minc?
 ?n, |u-pairs(u1) ? Su(Succn(r1))| < minc
 These lemmas are apparent. From them, we have the
 following operations for reducing the search space:
 • From Lemma 1, in each round we only process the
 new roles generated in the last round.
 • From Lemma 2, by applying a reverse index for
 finding roles related to a single user, we only process
 the role pairs have more than minu common users.
 • From Lemma 3,4 and 5, if |u-pairs(u1) ? Su(r1)| <
 minc, we delete u1 from r1 and then if |Su(r1)| <
 minu, we delete role1. This step is very effective to
 reduce the number of useless intermediate roles.
 To summarize, by the above approaches for search space
 reduction, the running time complexity of AD1 becomes:
 O(log(b? c)? r ? (ave-user? ave-role))
 The ave-user is the average user number of each role and
 ave-role is the average roles number related to each user.
 ave-user ? ave-role is for finding all the related roles having
 more than minu common users with current role. If the BiUCB
 matrix is sparse, it will gain much performance benefit.
 The total memory cost for loading the reverse index and u-
 pairs could be well controlled, for they are both static data so
 that multiple jobs can just load these data with general share
 memory methods without copying them once for each job. As
 a result, the total memory cost on a single cluster node will
 not increase with the number of CPU cores increasing.
 TABLE I
 FORMULATION FOR SEARCH SPACE ANALYSIS
 Expression Explanation
 Ncb(r1) the cb-pairs amount of role r1
 Nu(r1) the user amount of role r1
 Scb(u1) the cb-pairs set of user u1
 Scb(r1) the cb-pairs set of role r1
 Su(cb1) the user set of cb-pair cb1
 Su(r1) the user set of role r1
 R(r1) the round number that role r1 is generated
 Succn(r1) the role set generated by merging role r1 and other roles in round
 n; we also have ?r ? Succn(r1), ?r2, r = r1  r2
 Fig. 5. Main Procedure of Algorithm AD1
 E. The Pseudocode of Algorithm (AD1)
 In the following pseudocodes in this paper, the parameter
 roles is a list to store the known roles. Each item in this
 list is an object composed of 4 attributes: users, cb, nstatus,
 ostatus. The attribute users is the set for storing all the users
 acting this role and the attribute cb is the set for storing all
 the context-behavior pairs (cb-pairs) acted by this role. The
 attribute nstatus is for storing the status being processed in
 the current round of iteration while the attribute ostatus is for
 storing the status marked in the last round of iteration. The
 status of a role could be “old”, “new”, or “sub”, of which the
 “old” and “new” status marks are used for avoiding the role
 comparing operations that have been executed before, and the
 “sub” status is used for deleting the detected sub roles.
 The pseudocode of the main procedure of AD1 is summa-
 rized in Figure 5 and a detailed explanation is given below:
 • line 3, we get the column number of BiUCB .
 • line 4-5, the role set as initialized. For each role in this
 set, its user set is the same as one column in BiUCB,
 470
and it has only one respective column number in its
 cb-pairs. The status of each role is set to “new” so
 that any role pair will be compared in the next round.
 • In line 7-19 and line 22-34, some commands for
 submitting jobs to Apache Hadoop are applied.
 • line 7-19, u-pairs restricted by minu and minc for all
 users are generated. In line 8 and line 9, all the rows
 and columns of BiUCB are loaded by each mapper
 job as the indexes with the name of ucbIndex and
 cbuIndex. In line 10, we set the number of cores
 used. In line 11, the input directory is assigned, which
 contains several files, each one of them contains the
 allocated part number for a single mapper to read. In
 line 12, the output data directory is assigned. In line
 13-15, the source code files of mapper and reducer are
 assigned. In line 16-19, the partition and sort rules are
 assigned. In line 18, the reducer job number is set to
 1, so that there is only one output file and could be
 load in line 24 of the next round of iteration.
 • line 20 of the main procedure, there is a level-1 loop
 of maximum log(b? c) times.
 • line 22-34, one round of role merge job is submitted.
 In line 23, all the known roles will be loaded by each
 mapper job as a dictionary with the name of oldrole.
 In line 24, u-pairs will be loaded by each mapper job
 as a dictionary with the name of upair.
 • line 35, the roles generated in this round are counted.
 If no role is generated, the iteration in line 20 stops.
 • line 37, roles with less than minc cb-pairs are deleted.
 The u-pairs mapper pseudocode of AD1 is summarized in
 Figure 6 and a detailed explanation is given below:
 • line 3-4, two indexes for finding all cb-pairs of each
 user and all users of each cb-pair are loaded.
 • line 5, each mapper read a different ID and the total
 part number from respective file in the input directory.
 • line 6, there is the level-1 loop of u times.
 • line 7, input users are separated by allcount and se-
 lected by no. By the modulus and remainder, different
 mappers are allocated different users.
 • line 8, we print the u-pair of the role itself.
 • line 9, we get all the cb-pairs related to u1. line 10-13,
 all the users which have some common cb-pairs with
 u1 are selected and the number of common cb-pairs
 are counted.
 • line 14, there is the level-2 loop of maximum u times.
 • line 15, the users with smaller id than u1 and less than
 minc cb-pairs are skipped.
 • line 16-17, the u-pairs are printed to standard out.
 The output of u-pairs mapper are partitioned by the first
 user id , and sorted by the whole line. The u-pairs reducer
 pseudocode of AD1 is summarized in Figure 7 and a detailed
 explanation is given below:
 Fig. 6. u-pairs Mapper Code of Algorithm AD1
 Fig. 7. u-pairs Reducer Code of Algorithm AD1
 • line 4-6, u-pair text lines are read and parsed from the
 standard I/O input one by one.
 • line 7-8, add uid2 to the u-pair list of uid1
 • line 10 and 14, only the users with more than minu
 u-pairs will be printed to standard I/O output.
 • line 11 and 15, one single user and its u-pair list are
 printed to standard I/O output.
 • line 12-13, start to build u-pair list for the next user.
 The Merger mapper pseudocode of AD1 is summarized in
 Figure 8 and a detailed explanation is given below:
 • line 3 and 4, the known roles and sub roles are loaded
 from the output of the Merger reducer of last round.
 • line 5, from the data in roles, a index for finding all
 the roles of a user is build and it is loaded in the share
 memory.
 • line 6, all u-pairs are loaded.
 471
Fig. 8. Merger mapper code of algorithm AD1
 • line 8, each mapper reads a different ID and the total
 part number from respective file in the input directory.
 • line 9 and 19, there are the level-2 and level-3 loops
 of maximum r times. But we ignore the role pairs that
 have been merged in the former rounds by line 22-23.
 • line 10, input roles are separated by allcount and
 selected by no. By the modulus and remainder, each
 mapper is allocated a different part of role pairs.
 • line 11-13, the sub roles are outputted to the stdout
 and skipped in the following processing.
 • line 14, output the current role with status ”old”.
 • line 15-18, all the related roles which have some same
 user with the roles[i] are selected. And the number of
 common users are counted.
 • line 20-21, only the roles with bigger position in the
 role list are merged with role[i].
 Fig. 9. Merger reducer code of algorithm AD1
 • line 24-25, the user intersection and cb-pairs union of
 these two roles is computed.
 • line 26 and 30, the roles with only 1 cb-pair will not
 be processed after the first round.
 • line 27-29 and 31-33, a sub role is detected and added
 to the sub role list.
 • line 34-36, if we can not find enough u-pairs for a
 single user in the user intersection of the two roles,
 this user is deleted from the intersection.
 • line 37 and 38, a new role is created. Its user set is
 the pruned user intersection of these two roles and its
 cb-pair set is the union of that of these two roles.
 In the Merger mapper output, one role is presented as one
 text line with three parts separated by tabs. The first part is its
 cb-pair list, the second part is its user list and the last part is
 its status. The input of the Merger reducer is sorted by the first
 2 parts of a role text line, so that the same roles will be place
 in the neighbor lines, which allows further status merging.
 The function of Merger reducer is mainly about removing
 duplicate roles and merging the status marks for the same
 roles. Its pseudocode is summarized in Figure 9 and a detailed
 explanation is given below:
 • line 4-6, role text lines are read and parsed from the
 standard I/O input one by one.
 • line 8-11, the statuses of the same roles are merged.
 For each role, the “sub” mark replaces the “old” and
 “new” mark, while the “old” mark only replaces the
 “new” mark.
 • line 14 and 17, the roles are printed to standard output.
 V. EXPERIMENTS
 The performance and scalability of AD1 are tested on the
 cluster at the Faculty of Engineering in University of New
 South Wales:
 472
• Leonardi Cluster; Torque 5.4.0; Python 2.4.3.
 Leonardi is a medium-sized high performance Computing
 cluster. It currently consists of 2,944 AMD Opteron 6174
 2.20GHz processor cores, with a total of 5.8TB of physical
 memory (essentially 2GB of memory per core) and 100TB
 of usable disk storage. Leonardi runs the Rocks clustering
 platform on top of CentOS Linux. The maximum of 192
 processor cores are used exclusively for our experiments.
 Restricted by the platform of Torque in the Leonardi
 cluster, we do not use Hadoop to manage the mapper and
 reducer jobs. Instead, we develop a simple module to finish
 splitting, sorting and merging work to simulate the map-
 reduce computing model on this platform. Since most of
 the time cost of AD1 are generated by its mapper jobs, the
 implementation of our module will not affect the measurement
 of the experiments. We also believe that these experiments can
 be easily replicated using Hadoop.
 A. Performance trend of algorithm AD1
 In Figure 10(a), X-axis is set to user number from 2000
 to 10000 and Y-axis is set to time cost. The input BiUCB
 matrices are all random sparse matrices with 50 contexts, 3
 behaviors and the same density; minu = 8 and minc = 3 are
 applied to control the total time cost. In Figure 10(b), X-axis
 is set to context number from 50 to 100 and Y-axis is set to
 time cost. The input BiUCB matrices are all random sparse
 matrices with 1000 users, 3 behaviors and the same density;
 minu = 3 and minc = 2 are applied to control the total time
 cost. In both of them, CPU core number is 48. The time cost of
 AD1 increases according with increasing of context and user
 numbers. We also simulate a polynomial trend for them. These
 lines could approximately act as a O( n3 ) incremental trend.
 B. Time Cost Reduction with minu and minc
 In all the following experiments, the input BiUCB matrix
 is a random sparse one with 50000 users, 30 contexts, and 3
 behaviors, the average number of cb-pairs for each user is 9;
 In Figure 11, the number of CPU cores is 48. In Figure
 11(a), X-axis is set to the minu from 4 to 32 and Y-axis is set
 to the time cost; minc = 5 is also applied to control the total
 time cost. In Figure 11(b), X-axis is set to the minc from 3 to
 6 and Y-axis is set to the time cost; minu = 17 is also applied
 to control the total time cost. The time cost of AD1 decrease
 dramatically according with the increasing of minu and minc.
 C. Time Cost Reduction with Reverse Index
 In Figure 12, the number of CPU cores is 48, X-axis is
 three algorithms and Y-axis is set to the time cost. minu = 16
 and minc = 5 are also applied to control the time cost. A?D1 is
 the algorithm without the revert index. AD1 obtains consider-
 able performance improvement with search space reductions.
 D. Scalability test of algorithm AD1
 In Figure 13, time costs (seconds) are recorded according
 with the increasing of CPU cores from 3 to 192 and minu =
 17 and minc = 4 are applied to control the total time cost.
 The second row is the total time cost; the third row is the time
 Fig. 12. Time Cost Reduction with Revert Index
 Fig. 13. Execution time vs. CPU cores
 cost of all the mappers while the fourth row is the time cost
 of all the reducers; the last row is the time cost of standalone
 steps in the process, such as generating the share revert index.
 In Figure 14, X-axis is set to be the number of CPU cores
 and Y-axis is set to the time cost reduced multiples. The total
 time cost when CPU core number is 3 is set as the baseline,
 while the value for any other CPU core numbers are 26162
 divided by their total time cost.
 The time cost for task deploying, data transferring and
 the job management increases with the increasing of CPU
 core number and also there are some standalone steps in the
 algorithm. As a result, the total time cost is not exactly reduced
 linear related to the increasing of CPU cores, yet still reduced
 dramatically. The values in the last row in Figure 13 are almost
 consistent. For this test data set, after the the CPU core number
 become laster than 96, the total time cost will not reduce
 very much more, for the time cost portion of standalone steps
 becomes considerable.
 VI. CONCLUSIONS
 In this paper, we have proposed a novel algorithm for
 context-aware role mining. Role mining problem has been
 Fig. 14. Scalability of algorithm AD1
 473
(a) Execution time vs. number of contexts (b) Execution time vs. number of users
 Fig. 10. Performance trends of algorithms AD1
 (a) Execution time vs. minu (b) Execution time vs. minc
 Fig. 11. Time Cost Reduction with minu and minc
 proven to be difficult to compute, it is in fact NP-hard [15],
 and also shown to be hard to approximate [7]. The algorithm
 proposed in this paper offers complete role coverage, and
 yet provides efficient and scalable performance, in which
 the partition and search space reduction problems are also
 overcomed. AD1 is a distributed computing algorithm that
 has been deployed on a high performance cluster to test
 and confirm its performance on tens thousands of users and
 complex context-behaviors structures.
 REFERENCES
 [1] S. Aich, S. Mondal, S. Sural, and A. K. Majumdar. Role based
 access control with spatiotemporal context for mobile applications.
 Transactions on Computational Science, 4:177–199, 2009.
 [2] B. J. Biddle. Role Theory: Expectations, Identities, and Behaviors. New
 York: Academic Press, 1979.
 [3] H. Cao, T. Bao, Q. Yang, E. Chen, and J. Tian. An effective approach
 for mining mobile user habits. In CIKM, pages 1677–1680, 2010.
 [4] G.-P. Chen, Y.-B. Yang, and Y. Zhang. Mapreduce-based balanced
 mining for closed frequent itemset. In ICWS, pages 652–653, 2012.
 [5] J. Dean and S. Ghemawat. Mapreduce: Simplified data processing on
 large clusters. CACM, 51(1):107–113, 2008.
 [6] J. Dean and S. Ghemawat. Mapreduce: simplified data processing on
 large clusters. Commun. ACM, 51(1):107–113, Jan. 2008.
 [7] A. Ene, W. G. Horne, N. Milosavljevic, P. Rao, R. Schreiber, and R. E.
 Tarjan. Fast exact and heuristic methods for role minimization problems.
 In SACMAT, pages 1–10, 2008.
 [8] M. Frank, J. M. Buhmann, and D. A. Basin. On the definition of role
 mining. In SACMAT, pages 35–44, 2010.
 [9] M. Frank, A. P. Streich, D. A. Basin, and J. M. Buhmann. A
 probabilistic approach to hybrid role mining. In ACM Conference on
 Computer and Communications Security, pages 101–111, 2009.
 [10] Y. Fukazawa, T. Naganuma, K. Fujii, and S. Kurakake. Construction
 and use of role-ontology for task-based service navigation system. In
 International Semantic Web Conference, pages 806–819, 2006.
 [11] S. Ghemawat, H. Gobioff, and S.-T. Leung. The google file system. In
 Proceedings of the nineteenth ACM symposium on Operating systems
 principles, SOSP ’03, pages 29–43, New York, NY, USA, 2003. ACM.
 [12] D. Kulkarni and A. Tripathi. Context-aware role-based access control
 in pervasive computing systems. In I. Ray and N. Li, editors, SACMAT,
 pages 113–122. ACM, 2008.
 [13] H. Lu, J. Vaidya, and V. Atluri. Optimal boolean matrix decomposition:
 Application to role engineering. In ICDE, pages 297–306, 2008.
 [14] C. Masolo, L. Vieu, E. Bottazzi, C. Catenacci, R. Ferrario, A. Gangemi,
 and N. Guarino. Social roles and their descriptions. In KR, pages 267–
 277, 2004.
 [15] J. B. Orlin. Contentment in graph theory: Covering graphs with cliques.
 Indagationes Mathematicae (Proceedings), 80(5):406–424, 1977.
 [16] R. S. Sandhu, E. J. Coyne, H. L. Feinstein, and C. E. Youman. Role-
 based access control models. IEEE Computer, 29(2):38–47, 1996.
 [17] J. Vaidya, V. Atluri, Q. Guo, and Q. Guo. The role mining problem:
 finding a minimal descriptive set of roles. In SACMAT, pages 175–184,
 2007.
 [18] J. Wang, C. Zeng, C. He, L. Hong, L. Zhou, R. K. Wong, and J. Tian.
 Context-aware role mining for mobile service recommendation. In SAC,
 pages 173–178, 2012.
 [19] S.-Q. Wang, Y.-B. Yang, Y. Gao, G.-P. Chen, and Y. Zhang. Mapreduce-
 based closed frequent itemset mining with efficient redundancy filtering.
 In ICDM Workshops, pages 449–453, 2012.
 [20] T. White. Hadoop: The Definitive Guide. O’Reilly Media, Inc., 1st
 edition, 2009.
 [21] R. K. Wong, V. W. Chu, T. Hao, and J. Wang. Context-aware
 service recommendation for moving connected devices. In International
 Conference on Connected Vehicles and Expo (ICCVE), 2012.
 474

Practical Distributed Classification using the
 Alternating Direction Method of Multipliers Algorithm
 Peter Lubell-Doughtie and Jon Sondag
 Intent Media
 New York, USA
 {peter.lubell-doughtie, jon.sondag}@intentmedia.com
 AbstractÑWe describe a specific implementation of the Al-
 ternating Direction Method of Multipliers (ADMM) algorithm
 for distributed optimization. This implementation runs logistic
 regression with L2 regularization over large datasets and
 does not require a user-tuned learning rate metaparameter
 or any tools beyond MapReduce. Throughout we emphasize
 the practical lessons learned while implementing an iterative
 MapReduce algorithm and the advantages of remaining within
 the Hadoop ecosystem.
 Keywords-distributed algorithms; distributed computing; op-
 timization; predictive models
 I. INTRODUCTION
 When used at scale, statistical modeling algorithms need
 to function correctly on large amounts of data in a practical
 amount of time. In many cases production-scale datasets do
 not fit into the memory available on a single machine and
 the dataset needs to be distributed over a cluster.
 The MapReduce programming model [1] allows clients
 to run computations on large distributed datasets with com-
 modity machines. Apache Hadoop is a well known, widely
 used, and broadly supported MapReduce platform which
 works with the Hadoop Distributed File System (HDFS) and
 supports arbitrarily large datasets [2].
 The ADMM algorithm [3] is a distributed convex opti-
 mization algorithm that can be used to fit machine learning
 models with convex lossÑincluding linear regression, logis-
 tic regression, and support vector machinesÑand L1 or L2
 parameter regularization. We chose to implement ADMM
 using Hadoop in order to benefit from the wealth of existing
 Hadoop utilities and the communityÕs extensive knowledge.
 Regardless of the problems concerning HadoopÕs support
 for iteration [4], we show that Hadoop can be used to
 implement an efficient iterative algorithm which is Ògood
 enoughÓ in the sense of [5]. That is to say, the benefits
 of using a reliable platform that easily integrates with our
 existing toolset outweigh the small increase in runtime per
 job. To support iteration within Hadoop, we write data
 from external storage to HDFS, which reduces network
 latency when transferring large amounts of data. We also use
 HadoopÕs Input Splits to persistently partition data between
 mappers over multiple iterations.
 Many challenging problem domains, for example click at-
 tribution in online advertising, involve noisy observations of
 sparse data. Modeling in these domains requires significant
 amounts of data and an interpretable model. Here we focus
 on logistic regression with L2 norm.
 Traditional batch optimization algorithms for logistic re-
 gression fit the model parameters iteratively and require that
 the training dataset fit into memory for good performance.
 ADMM also proceeds iteratively but solves an intermediate
 optimization problem in parallel on subsets of the training
 data, then combines the intermediate solutions to find a
 consensus result at each iteration. The time-consuming and
 memory-intensive intermediate optimization problems are
 solved in parallel.
 When executing long-running modeling jobs hand-tuning
 parameters can be especially time-consuming. To avoid
 this we use ADMM with methods that automatically tune
 parameters while still guaranteeing convergence. This makes
 for an easy transition from small-scale prototypes fit using
 batch optimization methods on a sample of the dataset, to
 large-scale models fit over the entire dataset.
 Our main contribution is an adaptation of ADMM that
 makes large-scale logistic regression user-friendly on a pop-
 ular platform. Using our implementation, data scientists can
 accurately fit models without tuning meta-parameters.
 The rest of the paper is organized as follows. In section II
 we formulate logistic regression using the ADMM algorithm
 and describe the equations necessary to calculate interme-
 diate values. Based upon the formulae and procedures we
 develop, section III provides a brief review of MapReduce
 and the necessary details concerning how to implement the
 ADMM algorithm within Hadoop MapReduce. We present
 the results of using ADMM to model a production scale
 dataset of website visitors in section IV. In section V we
 review recent work related to iterative optimization with
 Hadoop, and in section VI we conclude.
 II. PROBLEM FORMULATION
 In this section we describe exactly how we formulate reg-
 ularized Logistic Regression with ADMM. Given a matrix
 of features A ? Rm?n and a vector of their labels b ? Rm
 with bi ? {?1, 1}, our goal is to compute the probability
 Pr(bi = 1|Ai) for each row 1 ² i ² m. The first column of
 the feature matrix represents the intercept and all values in
 this column are set to 1.
 In general, the (not necessarily distributed) convex model
 fitting problem with regularization can be written as:
 minimize 1
 m
 m·
 i=1
 li(ATi x? bi) + r(x), (1)
 2013 IEEE International Conference on Big Data
 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 773
where li is the loss for training example i, r is a reg-
 ularization term, and x ? Rn is the vector that solves
 the optimization problem and defines the logistic regression
 model.
 In the case of L2 regularized logistic regression the
 problem becomes:
 minimize 1
 m
 m·
 i=1
 log(1 + exp(?biATi x)) + ??x?22,
 where ? is the regularization factor.
 If we reformulate the above in ADMM global consensus
 form and reflect the partitioning of training examples across
 N machines in the cluster, the problem becomes:
 minimize 1
 m
 m1·
 i1=1
 log(1 + exp(?bi1ATi1x1)) + . . .
 +
 1
 m
 mN·
 iN=1
 log(1 + exp(?biNATiN xN )) + ??z?22
 subject to xj ? z = 0, j = 1, . . . , N
 where xj ? Rn is the logistic regression fit on node j.
 z ? Rn is a variable representing the global consensus,
 the vector of parameters that defines the model result. The
 constraint, xj ? z = 0, enforces equality of the parameter
 estimates across machines.
 The ADMM algorithm for the above problem is:
 xk+1j := argmin
 xj
 1
 mj
 mj·
 ij=1
 log(1 + exp(?bijATijxj )) (2)
 +
 ?k
 2
 ?xj ? zk + ukj ?22
 zk+1 :=
 ?
 ???
 ???
 xøk+1q + uøkq if q = 1
 N?k
 2? + N?k (xø
 k+1
 q + uøkq ) otherwise
 (3)
 uk+1j := u
 k
 j + x
 k+1
 j ? z
 k+1
 , (4)
 where k is the iteration number, ?k is the penalty parameter
 (see Section III) for iteration k, and xøq represents the average
 across all mappers of the qth element of the vector xø ?
 Rn. Formally, xøq =
 1
 N
 ·N
 j=1 xqj , and similarly for uø. The
 first elements of xø and uø, xø1 and uø1, correspond to the
 intercept and are not regularized, therefore we must treat
 them differently from the other values in the vector.
 Aij and bij are the portion of the data assigned to and
 used in mapper j. The values of the xj are calculated in
 parallel on the mappers while the values of z and u are
 calculated on the reducer. The convex minimization problem
 for each xj update is solved using the L-BFGS (low-memory
 BFGS) method [6]. We use a Java implementation of L-
 BFGS which is self contained and has performed well on
 Hadoop nodes.1
 III. IMPLEMENTATION
 In this section we describe implementing ADMM for
 Hadoop MapReduce and provide practical details relevant
 to building models within a production environment. The
 MapReduce model divides into three phases: the map, the
 shuffle, and the reduce. In the map phase the input data
 is split among a set of mappers and each mapper applies a
 function to the set of data assigned to it. In the shuffle phase
 the output of the mappers is assigned to a reducer. In the
 reduce phase the mapper output is aggregated to create the
 final values, which are then output by the reducer.
 The map and reduce steps can be expressed as:
 map (k1, v2) ?list(k2, v2)
 reduce (k2,list(v2)) ?list(v2),
 where each mapper has a unique key, k1. The shuffle is
 concerned with mapping the intermediate key-value pairs,
 (k2, v2), output by the mappers to the reducers [1].
 To implement MapReduce in Hadoop we define classes
 for the mapper and the reducer, along with a driver that han-
 dles input arguments and specifies the mapper and reducer
 classes (the shuffle is handled internally by Hadoop). In
 the context of ADMM, each mapper performs the resource
 intensive task of computing the current xj . After all mappers
 have completed their computations, we use a single reducer
 to compute the z and uj updates.
 A. Persistent Data with Input Splits
 When each mapper calculates the xk+1j values on iteration
 k + 1, it must use the ukj values that were calculated for
 the same mapper in the previous iteration. Hadoop does not
 normally accommodate this sort of persistence. [3] suggests
 using Apache HBase, a distributed data store, however
 this would add a new component and its accompanying
 complexity to the modeling framework.
 As an alternative, we use HadoopÕs notion of input splits
 to associate the correct xj and uj values with each other, and
 with the location on HDFS of the data that they correspond
 to. Input splits specify a split length and a split ID. These
 respectively determine the size of the split data and which
 node to use when performing computations on that data. To
 ensure that the correct uj values are loaded when calculating
 an xj value, the mapper reads the split ID and chooses the
 ui values based on this split ID. The z values are the same
 in each xj formula, i.e. across mappers. To reduce transfer
 cost we replicate the current z values across all mappers at
 the start of each iteration.
 1https://code.google.com/p/vladium/
 774
1: procedure ADMM(A, b, N , maxIterations)
 2: k = 0
 3: while notConverged and k < maxIterations do
 4: for j = 1? N do
 5: update xkj using Eq. 2
 6: end for
 7: update zk using Eq. 3
 8: for j = 1? N do
 9: update ukj using Eq. 4
 10: end for
 11: update ?k using Eq. 5
 12: k ? k + 1
 13: end while
 14: write zk to S3.
 15: end procedure
 Figure 1. The ADMM procedure implemented for Hadoop MapReduce.
 notConverged is a helper function that evaluates the norms to check for
 convergence.
 B. Automatically Updating ?
 We use the reducer to update the penalty parameter ?.
 The number of iterations to convergence depends upon ?,
 and hand-tuning it can take a significant amount of time
 because we must often run the algorithm for some time to
 determine the efficacy of the chosen value.
 By varying ? per iteration we can avoid spending time
 hand-tuning ?. We use the scheme suggested in [3] to update
 ? on each iteration:
 ?k+1 :=
 ?
 ??
 ??
 ? incr?k if ?rk?2 > µ?sk?2
 ?k/? decr if ?sk?2 > µ?rk?2
 ?k otherwise,
 (5)
 where rk is the primal residual, sk is the dual residual, and
 µ > 1, ? incr > 1, ? decr > 1 are user-specified parameters. We
 have used the recommended values of µ = 10 and ? incr =
 ? decr = 2.
 C. Considerations for MapReduce
 Figure 1 shows the coordination algorithm for executing
 the ADMM iterations. The feature matrix A, the vector of
 target values b, the number of mapper nodes N , and the
 maximum number of iterations maxIterations, are input
 to the procedure. The procedure begins by initializing the
 current iteration number to zero and passing control to the
 driver. On line 3 the driver checks that the algorithm has
 not converged and that the number of iterations is less than
 the maximum number of iterations. We assume there is a
 helper function notConverged, which returns true unless
 ?rk?2 ² pri and ?sk?2 ² dual.
 If the driver has not converged and the maximum number
 of iterations has not passed, we distribute the z and ui to
 the mappers and begin the next iteration. Lines 4 through
 Figure 2. The Job Runner sets paths for the input data, the output data,
 the Jar file with ADMM, and the value of any adjustable parameters, e.g.
 the number of iterations. ADMM computes and stores intermediate data in
 HDFS, then after the final iteration, the problem solution (zk in ADMM
 notation) is output to S3.
 12 show the xkj , zk, ukj , and ?k updates that occur in the
 mappers and reducer on each iteration. Alternatively, if the
 while loop condition is false, the algorithm is complete and
 we output the current vector of predicted weights, zk, to
 persistent storage and exit, as show on line 14.
 The output data is written to, and the input data is read
 from, AmazonÕs simple storage service (S3). To reduce the
 network latency involved in transferring data from S3 to
 the MapReduce nodes, we store the dataÑfeature matrix
 A and target values bÑdirectly on the nodes via HDFS.
 An advantage of running Hadoop on AmazonÕs Elastic Map
 Reduce (EMR) service and using data stored in S3 is that
 there are no transfer fees when moving data between these
 two systems. The infrastructure workflow is shown in Figure
 2.
 Wherever feasible, we have made our implementation of
 the ADMM algorithm generic. Arguments to the driver allow
 users to exclude columns from the input data, optionally add
 an intercept, set the initial ?, set the maximum number of
 iterations, and set the input and output locations. We have
 released an implementation of ADMM for Hadoop as an
 open source Java library.2
 IV. RESULTS
 As part of our modeling pipeline, we run the ADMM
 algorithm daily on rolling historical data consisting of ap-
 proximately 25 million records with 440 features per row.
 Our training examples are real vectors representing website
 visit attributes and our labels are real scalars representing
 each visits known value. The goal is to build a model that
 accurately segments unseen visitors by predicted value based
 upon their attributes.
 As depicted in Figure 2, we load the data and a Jar
 file containing the ADMM algorithm from S3 into EMR.
 To evaluate the performance of our implementation of the
 ADMM algorithm, we examine the output of one dayÕs run.
 We measure performance by comparing the change in loss
 function per iteration. We have also examined equivalent
 2https://github.com/intentmedia/admm
 775
Figure 3. The difference between the modelÕs predictions of the primal
 objective values and the ÒtrueÓ value as approximated by the model after
 many iterations. The loss decreases dramatically at first and then continues
 to decrease during later iterations.
 results from other days and verified that the results do not
 vary substantially from day to day.
 We plot the progress of our algorithm by showing that
 the value we are minimizing in Equation 1 decreases per
 iteration. Figure 3 shows this through the estimated loss in
 primal objective value, which we calculate by subtracting
 from the current primal objective value an approximated
 minimum primal objective value taken after many iterations
 (75 in this case). The estimated loss is plotted for each of
 the first 30 iterations on a logarithmic scale. We see that the
 loss decreases, and therefore that the accuracy consistently
 improves as more iterations pass. On a cluster of 20 high
 memory quadruple extra large EMR instances, 25 iterations
 run in 1 hour and 25 minutes.
 V. RELATED WORK
 The algorithm and our implementation are based upon
 [3], which introduced the ADMM algorithm. [4] presents a
 more efficient approach to iterative computation in Hadoop
 requiring modifications to core Hadoop code, which reduce
 the time needed to transfer data and initialize MapReduce
 jobs. This is not the approach we took as we remained
 within the Hadoop ecosystem and avoided the ramp-up
 costs associated with a less well known solution. Based on
 our experience, MapReduce is Ògood enoughÓ for ADMM
 and logistic regression. We evaluate the cost as described
 in [5], and conclude that the reduction in implementation
 costs gained by using Hadoop outweigh the performance
 improvements that could be gained by using a less well-
 known platform.
 [7] presents a MapReduce implementation of tree ensem-
 ble learning and uses a wrapper for MapReduce to coor-
 dinate iterations of the underlying algorithm. [8] presents a
 distributed method for classification that is similar to random
 forests. Apache Mahout contains an implementation of logis-
 tic regression that uses stochastic gradient descent (SGD).3
 As noted in [9], this SGD implementation is sequential,
 which substantially limits the performance of the algorithm
 and makes it difficult to scale. To address this, [9] presents a
 scalable convex loss system that uses a Hadoop compatible
 variant of MapReduce.
 VI. CONCLUSION
 Using an ADMM implementation for MapReduce allows
 a simpler workflow when moving modeling ideas from
 prototype to production-scale. We showed a specific example
 of using the ADMM algorithm to build a logistic regression
 model. Importantly, ADMM is a generic optimization algo-
 rithm that can be applied to other problems with minimal
 modification. Our contribution is a distributed implementa-
 tion on Hadoop that allows users to experiment with many
 different models at scale. In future work we plan to support
 additional convex loss optimization problems.
 REFERENCES
 [1] J. Dean and S. Ghemawat, ÒMapreduce: simplified data pro-
 cessing on large clusters,Ó in Proceedings of the 6th conference
 on Symposium on Opearting Systems Design & Implementation
 - Volume 6, ser. OSDIÕ04. Berkeley, CA, USA: USENIX
 Association, 2004, pp. 10Ð10.
 [2] T. White, Hadoop: The Definitive Guide, 1st ed. OÕReilly
 Media, Inc., 2009.
 [3] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, ÒDis-
 tributed optimization and statistical learning via the alternating
 direction method of multipliers,Ó Found. Trends Mach. Learn.,
 vol. 3, no. 1, pp. 1Ð122, Jan. 2011.
 [4] Y. Bu, B. Howe, M. Balazinska, and M. D. Ernst, ÒHaloop:
 efficient iterative data processing on large clusters,Ó Proc.
 VLDB Endow., vol. 3, no. 1-2, pp. 285Ð296, Sep. 2010.
 [5] J. Lin, ÒMapreduce is good enough? if all you have is a
 hammer, throw away everything thatÕs not a nail!Ó CoRR, vol.
 abs/1209.2191, 2012.
 [6] J. Bonnans, Numerical optimization: theoretical and practical
 aspects, ser. Universitext (1979). Springer-Verlag New York
 Incorporated, 2003.
 [7] B. Panda, J. S. Herbach, S. Basu, and R. J. Bayardo, ÒPlanet:
 Massively parallel learning of tree ensembles with mapreduce,Ó
 in Proceedings of the 35th International Conference on Very
 Large Data Bases (VLDB-2009), 2009.
 [8] J. Lin and A. Kolcz, ÒLarge-scale machine learning at twitter,Ó
 SIGMOD, 2012.
 [9] A. Agarwal, O. Chapelle, M. Dudõ«k, and J. Langford, ÒA
 reliable effective terascale linear learning system,Ó CoRR, vol.
 abs/1110.4198, 2011.
 3https://cwiki.apache.org/confluence/display/MAHOUT/Logistic+Regression
 776

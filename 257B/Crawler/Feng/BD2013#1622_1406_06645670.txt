A Framework for Integrating a Decision Tree 
Learning Algorithm and Cluster Analysis 
 
Masaki KUREMATSU* and Hamido FUJITA* 
* Faculty of Software and Information Science, Iwate Prefectual University, Takizawa, Japan 
{kure, issam}@iwate-pu.ac.jp 
 
 
Abstract— We proposed a modified decision tree learning 
algorithm to improve this algorithm in this paper. Our 
proposed approach classifies given data set by a traditional 
decision tree learning algorithm and cluster analysis and 
selects whichever is better according to information gain. In 
order to evaluate our approach, we did an experiment using 
program-generated data sets. We compared ID3 which is 
one of well-known decision tree learning algorithm to our 
approach about the recall ratio in this experiment. 
Experimental result shows the recall ratio of our approach 
is similar than the recall ratio of a traditional decision tree 
learning algorithm. Though we can not show the advantage 
of our approach according to the experiment, we show it is 
worth using cluster analysis to make a decision tree. In 
future, we have to evaluate our approach according to cross-
 validation method using big and complex data sets in order 
to say the advantage of our approach. We think our 
approach is not good for all data set, so we try to find the 
situation which our approach is better than other 
approaches according to the experimental results. In 
addition to, we have to show how to explain a decision tree 
by our approach to keep the readability of a decision tree. 
I. INTRODUCTION 
A decision Tree learning algorithm is one of well-
 known supervised machine learning algorithms. The 
purpose of this algorithm is to make a decision tree from 
transaction data set. A decision tree shows a set of 
classification rules which show how to decide a class label 
of each data item based on attribute values. We can use 
these rules to classify new data items. Though there are 
some statistical methods or machine learning algorithms 
to extract rules from data set, for example, SVM (support 
vector machine [1]), ANN (Artificial Neural networks [2]), 
analysts and researchers often use this algorithm. One of 
reasons is that it is easy to understand a decision tree made 
by this algorithm. The other reason is that there are some 
tools we can use this algorithm, for example, Weka [3], R 
[4] and SPSS [5]. So some researches use this algorithm in 
many research filed, such as marketing, psychology and 
medical [6].  
There are some issues about a decision tree learning 
algorithm, though analysts and researchers use this 
algorithm to analysis huge data. One well-known issue is 
that we need good training data set to use this algorithm. 
The accuracy rate of a decision tree depends on training 
data set. If there is big difference between training data set 
and target data set we want to classify, a decision tree 
made from training data doesn’t work and the accuracy 
rate of it is not appropriate. However, it is not easy to 
prepare good training data set before learning. So we need 
to improve a decision tree learning algorithm to manage 
the impact of training data set. 
We had been trying to improve a decision tree learning 
algorithm. First approach was that we made a decision 
tree for a class label one by one [7]. In this approach, we 
rearranged a class label of each data items before making 
a decision tree. We selected a class label and replaced 
other labels to “not this class”. Next, we made decision 
trees for each class. The number of decision tree is same 
as the number of class labels. When we predicted a class 
label of new data, we selected a class label which the 
prediction rate was highest in all decision trees. Our 
experimental results about this approach indicated the 
possibility that our approach improved a decision tree 
learning algorithm. However, we could not say that we 
solved issues about a decision tree learning algorithm. 
Second approach was that we divide training data set and 
make a decision tree using divided data set [8]. In this 
approach, we think class labels of training data set were 
too rough. So we split given data set into some data subset 
using a class label. All data items in each dataset has same 
class label. Next, we divided each data subsets by a cluster 
analysis. Therefore, we divided each class into more 
detailed ones. We made a decision tree using training data 
set with more detail class labels. This experimental result 
indicated the possibility of our approach, but we could not 
say that we solved issues about a decision tree learning 
algorithm, too. Third approach was that we classified leaf 
nodes in a decision tree by cluster analysis to optimize the 
size of leaf nodes [9]. In this approach, we classified data 
set based on two relations. One is the relation between a 
class label and an attribute and the other is the relation 
between attributes. First relation is used in a traditional 
decision tree learning algorithm and second relation is 
used in cluster analysis. Experimental result showed the 
possibility that our approach was better than a traditional 
decision tree learning algorithm. So it is worth enhancing 
this approach to solve issues about a decision tree learning 
algorithm.  
In this paper, we propose a framework for decision tree 
learning embedding cluster analysis. We think this 
approach based on our third approach. The difference 
between our third approach and the proposed approach is 
how to use cluster analysis. Though, in third approach, we 
use cluster analysis when we can not split data set by a 
decision tree learning algorithm, we use cluster analysis 
whenever we try to split given data set. In other words, we 
embed cluster analysis in a decision tree learning 
algorithm. 
Next section describes our approach in detail. Section 3 
describes the experiment for evaluation our research. We 
also describe discussion about our approach according to 
225
 SoMeT 2013 • 12th IEEE International Conference on Intelligent Software Methodologies, Tools and Techniques • September 22-24, 2013, Budapest, Hungary
 978-1-4799-0421-1/13/$31.00 ©2013 IEEE
the experimental result and future works in this section. 
Finally, we conclude this paper in Section 4. 
II. A FRAMEWORK FOR DECISION TREE LEARNING 
WITH CLUSTER ANALYSIS  
A decision tree learning algorithm tries to divide data 
set based on an attributes. It doesn’t think two or more 
than attributes on same time. We think it is a week point 
of a decision tree learning algorithm. Our new approach 
tries to divide data set based on attributes using cluster 
analysis. Our idea is simple. Whenever we divide data set, 
we do it by a decision tree learning algorithm and cluster 
analysis and select better one. In order to compare the 
results of these algorithms, we use information gain [10] 
as criteria.  Information gain shows how much the entropy 
decreases if an attribute value is defined.  In other words, 
it shows which attribute is better to classify data set. We 
get information gain using formula (1), (2) and (3) . 
 
∑ ∑
 ∑
 = =
 =
 ==?==
 ?=
 ?=
 m
 j
 n
 k
 jjj
 n
 i
 ii
 Y
 vYXPvYXPvYPYXH
 PPXH
 YXHXHgain
 1 1
 2
 1
 2
 )|(log)|()()|(
 )(log)(
 )|()( (1)
  
(2)
  
(3)
  
Where X is given data set and Y is an attribute. H(X) is the 
entropy of X, H(X|Y) is the entropy of X when Y decided, 
n is the number of class in X, m is the number of Y’s value. 
 We show our proposed algorithm in Figure 1. We 
explain it as follow. 
Input of this function is data-set and path. Output of it is 
a set of paths that includes a given path. 
(1) If all data items in given data set have same class label, 
this algorithm stops and returns path. 
(2) This algorithm tries to split given data set by decision 
tree learning algorithm. This algorithm calculates the 
information gain of every attribute using the given data 
set. It splits the given data set into subsets using the 
attribute for which information gain is maximum value. 
We regard these subsets as candidate subsets, i.e. 
candidate child nodes. We show the maximum 
information gain as “gaind”. 
(3) This algorithm tries to split the given data set into 
clusters by cluster analysis as candidate data subsets, 
i.e. candidate child nodes. It calculates the information 
gain of clusters. We show this value as “gainc”. 
(4) If “gaind” is more than “gainc”, this algorithm accepts 
datasets by a decision tree learning algorithm as data 
subsets. It makes edges using the selected attributes 
value and an operator. Otherwise, it accepts clusters as 
data subsets. It makes edges centroid values of each 
cluster. It could add additional paths by adding a 
defined path to an edge related to data sets. 
(5) This algorithm split every data subsets recursively.  
III. EXPERIMENTS  
A. Overview of our experiments 
In order to evaluate our approach, we compare the 
performance of our approach to the performance of a 
traditional decision tree algorithm. At first, we make a 
decision tree using each approach and classify same data 
set by them. Next, we divide the number of data predicted 
correctly by the number of all data items to get the recall 
ratio of the decision tree. If the recall ratio of our approach 
is higher than the recall ratio of a traditional decision tree 
learning algorithm, we can say that our approach has 
advantage. We also compare these decision trees. We 
explain the outline of our experiment in next paragraph.  
We make some data group using a program. Each data 
group has 60 data items which has 2 attributes defined by 
formula (4) based on the Box–Muller transform [12].  
 
???
 ?
 ???
 ?
 +?
 +?=???
 ?
 ???
 ?
 mYX
 mYX
 Z
 Z
 )2sin()log(
 )2cos()log(
 2
 1
 π?
 π?   (4) 
 
Where X and Y are probability values from the uniform 
distribution on the interval (0, 1] . The Box–Muller 
transform is a pseudo-random number sampling method 
for generating pairs of independent, standard, normally 
distributed random numbers, given a source of uniformly 
distributed random numbers. ? and m are parameters of 
this formula. ?  influence the range of value and m 
influence the center of distribution. We make 6 data 
groups in Figure 2. 
When we evaluate our approach, we make pair of data 
groups as data set. 
To do this experiment, we implemented our approach by 
C programming language. There are some decision tree 
learning algorithms and cluster analysis algorithms. We 
need to adjust our idea for selected algorithms. In this 
paper, we use ID3 [10] as a decision tree and K-Means 
[11] as cluster analysis. ID3 makes a binary decision tree, 
so we defined K of K-means as 2. When we divide data 
set by K-Means, we have to decide two seed data items. 
 
function DTCL(data-set, path) 
  if   the number of class label in data-set = 1   then 
      return path 
  else  
     SetD ? Split given dataset by Decision Tree Learning Algorithm
 gainD ? Information Gain( SetD) 
SetC ? Split given dataset by Cluster Analysis 
     gainC ?Information Gain( SetC) 
     if  gainD > gainC  then 
        n ? | SetD | 
        data-subset[1..n]? SetD 
edge[1..n]  ? a condition formula consists of selected  
the attribute and an operator 
    else  
        n ? | SetC | 
        data-subset[1..n]? SetC 
for i=1 to n do edge[i]  ? centroid  of data-subset[i] 
    endif 
    for i=1 to n do DTCL(data-subset[i],  path+edge[i]) 
endif 
Figure 1.  The Algorithm of Making a Decision Tree 
Embedding Cluster Analysis 
M. Kuremts and H. Fujita • A Framework for Integrating a Decision Tree Learning Algorithm and Cluster Analysis
 226
We do it as following. At first, we find the attribute whose 
variance is the largest. Next, we select 2 data items. One 
has the minimum value of the selected attribute and the 
other has the maximum value of this attribute. We use 
these data items as seed for K-Means. In addition to, we 
calculate the distance between data items by Euclidian 
distance. 
 
#1 (?,m)=(1,0)               #2 (?,m)=(1,1), (1,-1)    #3 (?,m)=(1,2),(1-2) 
 
#4(?,m)=(0.5,0)        #5  (?,m)=(0.5,1),(0.5,-1)   #6 (?,m)=(0.5,2),(0.5,-2)
 Figure 2.      6 data groups based on the Box-Muller transform 
B. The Result of experiments 
We show the part of the result of this experiment as 
Table 1. Each decision tree made by our approach in this 
table has nodes made by Cluster Analysis. “ID3” and 
“Our Approach” show the number of data item evaluated 
correctly and the recall ratio by each algorithm, 
respectively. ”CA Nodes” shows nodes made by Cluster 
Analysis in a decision tree. “UPDOWN” shows the 
number of correct data by our approach minus the number 
of correct data by ID3.  
TABLE I.   
EXPERIMENTAL RESULT 
Case#  Pair ID3 Our  Approach 
CA  
Nodes
 UP 
DOWN
 #1 #1,#2 80 67% 80 67% 0 0
 #2 #1,#3 88 73% 88 73% 0 0
 #3 #1,#4 87 72% 87 72% 0 0
 #4 #1,#5 88 73% 88 73% 0 0
 #5 #1,#6 90 75% 90 75% 0 0
 #6 #2,#3 84 70% 84 70% +1 0
 #7 #2,#4 91 76% 91 76% 0 0
 #8 #2,#5 81 68% 82 68% +1 +1
 #9 #2,#6 84 70% 84 70% 0 0
 #10 #3,#4 90 75% 90 75% 0 0
 #11 #3,#5 91 76% 91 76% 0 0
 #12 #3,#6 82 68% 82 68% 0 0
 #13 #4,#5 86 72% 86 72% 0 0
 #14 #4,#6 90 75% 90 75% 0 0
 #15 #5,#6 87 72% 87 72% 0 0
  
C. Discussion 
According to the result of this experiment, the recall 
ratio of our approach is similar than the recall ratio of ID3.  
There is one case where our approach is better than ID3. 
However the difference between our approach and ID3 is 
very small. There are two decision trees have nodes made 
by cluster analysis. Though our approach increased the 
recall ratio in case #8, it didn’t change the recall ration in 
case #6.  Figure 3 shows the distribution of data items in 
case #8. In this figure, “o” shows data items classified in 
correct class by both decision trees and “n” shows data 
items classified in wrong class by both decision trees 
We analyzed these decision trees. There is no difference 
between ID3 and our approach about data items’ class 
label in case #6. Otherwise, three data items which 
classified in correct class with ID3 were classified in 
wrong class with our approach in case #8. We show these 
data items as “i” in figure 3. Four data items which 
classified in wrong class with ID3 were classified in 
correct class with our approach in case #8. We show these 
data items as “k” in figure 3. Figure 3 shows they are 
close. The experimental result says that the difference 
between a decision tree made by our approach and a 
decision tree made by ID3 is small. In order to check this, 
we classified all data groups with ID3 and our approach, 
respectively. ID3 algorithms classified 174 data items 
correctly and the recall ratio is 0.48. Our approach 
classified 175 data items correctly and the recall ratio is 
0.49. The decision tree made by our approach has 2 nodes 
made by cluster analysis. We think our approach is better 
than ID3 according to data sets. 
Figure 3.  Distribution of data items in case #8 
To get the relation between our approach and data set, 
we compared the experimental result to the result of 
analysis using k-nearest neighbor algorithm (k-NN) [14]. 
We defined k as 3. Table 3 shows the result of comparison 
among two decision tree learning algorithms and kNN. 
“kNN(#)” shows the number of data items classified 
correctly with kNN and “kNN(recall)” shows the recall 
ratio of kNN. Bold letters shows kNN is better than 
decision tree learning algorithms. “Average” shows the 
average of the entropy of 3 nearest data items of each data 
item and “SD” shows the standard distribution of this 
entropy. If average is small, most of nearest data items are 
in same class. The correlation coefficient between the 
recall ratio and average is -0.82. The correlation 
coefficient between recall ratio and SD is -0.76. These 
values say that there is strong negative correlation 
between the recall ratio and these values. This result is 
within the scope of the assumption. If data items which 
have same data label gather, the entropy becomes small 
227
 SoMeT 2013 • 12th IEEE International Conference on Intelligent Software Methodologies, Tools and Techniques • September 22-24, 2013, Budapest, Hungary
and it is easy to split dataset. So, we can not find the 
explicit point for using our approach. 
TABLE II.   
COMPARISON AMONG DECISION TREES AND KNN 
Case# ID3 Our kNN (#) kNN(recall) Average SD
 #1 80 80 77 64% 0.54 2.37
 #2 88 88 107 89% 0.18 1.41
 #3 87 87 86 72% 0.47 2.22
 #4 88 88 85 71% 0.45 2.18
 #5 90 90 112 93% 0.06 0.82
 #6 84 84 76 63% 0.48 2.25
 #7 91 91 100 83% 0.25 1.65
 #8 81 82 66 55% 0.52 2.34
 #9 84 84 80 67% 0.38 2.02
 #10 90 90 116 97% 0.03 0.58
 #11 91 91 110 92% 0.08 0.96
 #12 82 82 69 58% 0.56 2.42
 #13 86 86 99 83% 0.23 1.57
 #14 90 90 120 100% 0 0
 #15 87 87 110 92% 0.15 1.29
  
Our experiment result shows it is worth using a cluster 
analysis in a decision tree learning algorithm. However, 
we can not say our approach is useful. In order to show 
the advantage of our approach, we do some following 
works in future. One is that we have to evaluate our 
approach. We classified small and simple data sets and 
evaluated the recall ratio. We have to evaluate our 
approach by cross-validation method using big and 
complex data sets, i.e. open data sets in machine learning 
repository powered by University of California Irvine. We 
think our approach is not good for every data set, so we 
try to find the situation which our approach is better than 
other approaches according to the experimental results. In 
addition to, we have to show how to explain a decision 
tree by our approach. A decision tree made by our 
approach has nodes made by cluster analysis. We can not 
explain paths include these nodes in a same way. We 
should keep the readability of a decision tree, so we 
should ready how to explain paths in a decision tree. We 
continue to modify our approach based on the result of 
analysis of experimental results. We have to do spirally. 
IV. CONCLUSIONS  
In this paper, we proposed a modified decision tree 
learning algorithm to improve it. Our proposed approach 
classifies given data set by a traditional decision tree 
learning algorithm and cluster analysis and selects 
whichever has the greater information gain. In order to 
evaluate our approach, we did an experiment using 
program-generated data sets. We compared ID3 which is 
one of traditional decision tree learning algorithms to our 
approach about the recall ratio in this experiment. 
Experimental result shows the recall ratio of our approach 
is similar than the recall ratio of ID3. Though we can not 
show the advantage of our approach according to this 
experiment, we evaluate it is worth using cluster analysis 
to make a decision tree. In future, we have to evaluate our 
approach according to cross-validation method using big 
and complex data sets. We think our approach is not good 
for all data set, so we try to get features of data set which 
show our approach is better than other approaches 
according to the experimental results. In addition to, we 
have to show how to explain a decision tree by our 
approach to keep the readability of a decision tree.  
ACKNOWLEDGMENT 
This work was supported by Japan Society for the 
Promotion of Science, Grant-in-Aid for Scientific 
Research (C):24500121. Part of this research was done as 
part of the Master thesis of Ms Saori AMANUMA who 
completed her master degree at the school of Software and 
Information Science of Iwate Prefectural University (IPU).  
We thank Dr. Jun Hakura of IPU for his constructive 
comments that could contribute in having better revision 
on this work. 
REFERENCES 
[1] Cortes Corinna and Vapnik Vladimir N, "Support-Vector 
Networks", Machine Learning, 20(1995) 
[2] Rosenblatt, Frank, “Principles of Neurodynamics”, Spartan Books, 
1962 
[3] Weka 3 - Data Mining with Open Source Machine Learning 
Software in Java, http://www.cs.waikato.ac.nz/ml/weka/ (Access : 
2013/06/12) 
[4] The R Project for Statistical Computing,  http://www.r-project.org/  
(Access : 2013/06/12) 
[5] IBM SPSS software,  
http://www-01.ibm.com/software/analytics/spss/ 
(Access: 2013/06/12) 
[6] Mirjana Peji? Bach, Dijana ?osi?, “Data mining usage in health 
care management: literature survey and decision tree application”, 
Medicinski Glasnik, Vol.5, No. 1,PP.57-64 (2008) 
[7] Masaki Kurematu?Jun Hakura, Hamido Fujita, “An Extraction 
of Emotion in Human Speech Using Speech Synthesize and Each 
Classifier for Each Emotion” , WSEAS TRANSACTIONS on 
INFORMATION SCIENCE & APPLICATIONS ?Vol.3, No. 5?
 pp. 246-251(2008)  
[8] Masaki KUREMATSU, Saori AMANUMA, Jun HAKURA and 
Hamido FUJITA , “An Extraction of Emotion in Human Speech 
Using Cluster Analysis and a Regression Tree”, Proceedings of 
10th WSEAS International Conference on Applied Computer 
Science, pp.346-350(2010)  
[9] Saori Amanuma, Masaki Kurematsu and Hamido Fujita?“An 
Idea of Improvement Decision Tree Learning Using Cluster 
Analysis”? The 11th International Conference on Software 
Methodologies, Tools and Techniques, pp.351-360(2012) 
[10] Quinlan, J. R.  “Induction of Decision Trees”, Machine  Learning, 
Vol.1, No.1,pp.81-106(1986) 
[11] J. B. MacQueen, "Some Methods for classification and Analysis 
of Multivariate Observations", Proceedings of 5th Berkeley 
Symposium on Mathematical Statistics and Probability, pp. 281-
 297(1967) 
[12] G. E. P. Box and Mervin E. Muller, “A Note on the Generation of 
Random Normal Deviates”, The Annals of Mathematical Statistics, 
Vol. 29, No. 2 pp. 610–611(1958) 
[13] L. Breiman, J. H. Friedman, R. A. Olshen and C. J. 
Stone, ”Classification and Regression Trees”, Wadsworth 
International Group, (1984) 
[14] T.M. Cover and P.E. Hart, “Nearest neighbor pattern 
classification.”, IEEE Trans. Inform. Theory, IT-13(1):21–27, 
(1967). 
[15] UCI Machine Learning Repository,   
http://archive.ics.uci.edu/ml/datasets/,  (Access : 2013/05/29) 
 
 
M. Kuremts and H. Fujita • A Framework for Integrating a Decision Tree Learning Algorithm and Cluster Analysis
 228

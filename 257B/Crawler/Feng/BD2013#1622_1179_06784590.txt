Kernel SODA: A Feature Reduction Technique Using Kernel Based Analysis
 Yinan Yua,b, Tomas McKelveya
 Signals and Systems
 Chalmers University of Technology a
 Gothenburg, Sweden
 {yinan; tomas.mckelvey}@chalmers.se
 S.Y. Kungb
 Electrical Engineering
 Princeton University b
 Princeton, USA
 {yinany; kung}@princeton.edu
 Abstract—A feature extraction technique called Successively
 Orthogonal Discriminant Analysis (SODA) has been recently
 proposed to overcome the limitation of Linear Discriminant
 Analysis (LDA), whose objective is to find a projection vector
 such that the projected values of data from both classes
 have maximum class separability. However, in LDA, only one
 such vector can be found due to the rank deficiency for
 binary classification problems. On the other hand, as a feature
 extraction technique, the proposed algorithm SODA attempts
 to obtain a transformation matrix instead of a vector. In
 this paper, the kernel version of SODA is presented in both
 intrinsic space and empirical space. To obtain the solution
 without sacrificing numerical efficiency, we propose a relaxed
 formulation and data selection for large scale computations.
 Simulations are conducted on 5 data sets from UCI database
 to verify and evaluated the new approach.
 Keywords-Feature extraction, SODA, Kernel, Discriminant
 Analysis, big data
 I. INTRODUCTION
 As a solution to the curse of dimensionality, feature
 reduction [1] for classification has been a popular topic
 for decades. There are many reasons why we should care
 about the dimensionality. 1) Overfitting is unevadable for
 high dimensional feature space, which might ruin the gen-
 eralization ability of the classifier. 2) When the number of
 variables is too large, high storage capacity is required, and
 computational complexity is yet another issue. 3) In many
 cases, high dimensionality causes computational instability
 and singularity [2] 4) Class separability is very likely to be
 enhanced by eliminating redundant information.
 There are two types feature reduction techniques: feature
 selection and feature extraction. Feature selection [3] is
 to select a subset of the variables with respect to some
 criteria. On the other hand, feature extraction attempts to
 find a function f(x) : Rm ? Rk, which transforms
 data from the original space Rm to a low dimensional
 feature space Rk, where m > k. In this paper, we focus
 on the development of a feature extraction technique for
 classification. The proposed approach is the kernel extension
 of a previously presented technique called Successively
 Orthogonal Discriminant Analysis (SODA) [4]. The original
 technique is closely related to Linear Discriminant Analysis
 (LDA) [5] and Principal Component Analysis (PCA) [6].
 The paper is organized as follows. Section II reviews the
 relation between LDA, PCA and SODA. Section III presents
 the formulations of Kernel SODA in both intrinsic space and
 empirical space. For simple numerical implementations, a
 relaxed KSODA algorithm is developed in Section IV. The
 experimental results are shown in the last section to verify
 the proposed technique.
 II. RELATED WORK
 A. Linear Discriminant Analysis (LDA)
 Linear Discriminant Analysis (LDA) has a very long
 history. The underlying idea is based on Fisher criteria for
 maximizing the class separability.
 Given class label c ? {+,?} and training data Xc =
 {xc1, · · · ,xcNc}, the class separability is measured using
 the ‘between-class scatter matrix’ SB and the ‘within-class
 scatter matrix’ SW , which are respectively defined as:
 SW =
 1
 N+
 N+∑
 i=1
 (x+i ? µ+)(x+i ? µ+)T
 +
 1
 N
 ?
 N
 ?∑
 j=1
 (x?j ? µ?)(x?j ? µ?)T
 SB = (µ+ ? µ?)(µ+ ? µ?)T (1)
 where µ+ and µ? are the mean vector estimated from the
 corresponding class + and ?. In LDA, we would like to
 find a vector w that maximizes the following Fisher score:
 J = w
 TSBw
 wTSWw
 , (2)
 and the solution vector is the generalized eigenvector cor-
 responding to the largest generalized eigenvalue of the
 problem SBw = SWw?. However, problems occur when
 the within matrix SW is singular. There are many ways of
 tackling this problem [7], [8] and one way is to compute
 the eigenvector of the Fisher matrix F = SW+SB. We will
 adopt this method in this paper.
 2013 12th International Conference on Machine Learning and Applications
 978-0-7695-5144-9/13 $26.00 © 2013 IEEE
 DOI 10.1109/ICMLA.2013.20
 72978-0-7695-5144-9/13 $31.00 © 2013 IEE
B. Principal Component Analysis (PCA)
 One of the most famous dimensionality reduction tech-
 niques is the Principal Component Analysis (PCA), which
 finds the subspace with the largest variation. PCA can be
 formulated in an iterative fashion. Namely, we are trying to
 find a matrix W =
 [
 w1, · · · ,wk
 ]
 whose columns satisfy:
 maximize
 wi
 wTi Swi
 subject to wi ? w1,...,i?1
 wTi wi = 1
 wi ? Span(S)
 (3)
 where S = XXT is the covariance matrix of the training
 data and Span(S) denotes the range space of matrix S. When
 the dimensionality of the data vector is high, PCA can be
 implemented iteratively without computing the covariance
 matrix S [9]. The solution of PCA provides a transformation
 matrix that projects the data to a low dimensional subspace,
 where the data representation is enhanced. The data vector
 in the principal component space is thus represented as:
 x?WTx. (4)
 C. Successively Orthogonal Discriminant Analysis
 Successively Orthogonal Discriminant Analysis (SODA)
 integrates the ideas of LDA and PCA. Defined as a succes-
 sive approach, SODA adopts the Fisher discriminant score as
 its objective function for each iteration. The output of SODA
 is a transformation matrix that projects the data onto a new
 feature space with enhanced class separability. Similarities
 and differences of LDA, PCA and SODA can be found in
 Table I. The formulation of SODA is illustrated as follows.
 SODA formulation. The matrix W =
 [
 w1 · · ·wk
 ] defines
 a map Rm ? Rk, whose columns satisfy:
 maximize
 wi
 wTi SBwi
 wTi SWwi
 subject to wi ? w1,...,i?1
 wTi wi = 1
 wi ? Span(SW )
 (5)
 where Span(SW ) denotes the range space of matrix SW .
 Similarly, the resulting data vector is mapped to a reduced
 feature space:
 x?WTx. (6)
 SODA and PCA share the same concept of optimal
 subspace projection. PCA’s optimality enjoys the advantage
 of having a global criterion, however it does not take into
 account of teacher’s information. On the other hand, SODA
 takes full account of teacher’s information, but its optimality
 is formulated and executed step by step. Both PCA and LDA
 have their kernelized variance called KPCA and KDA. This
 motivates us to extend SODA to its kernel model.
 The existing feature reduction techniques based on dis-
 criminant analysis [11] mostly depend on the number of
 classes C. For binary classification problem, ı.e. C = 2,
 due to rank deficiency, a transformation matrix can not be
 found. On the other hand, SODA overcomes this limitation
 by modifying the searching space. More details can be found
 in later sections.
 III. THEORY OF KERNEL SODA
 In kernel based analysis, we define a function ?(x) : x?
 ?, which maps the data vector from the original space to
 intrinsic space ? to obtain a better separation.
 A. KSODA in Intrinsic Space
 KSODA can be formulated in the intrinsic space with an
 explicit expression of ?(x). The transformation matrix is
 denoted by U =
 [
 u1, · · · ,uk
 ]
 in the intrinsic space.
 Let c ? {+,?} denote the class label and Nc the total
 training size of class c, we can define the between-class
 scatter matrix S?B and within-class scatter matrix S
 ?
 W in the
 intrinsic space as follows:
 S?B = (m?+ ?m??)(m?+ ?m??)T (7)
 S?W =
 ∑
 c?{+,?}
 1
 Nc
 Nc∑
 j=1
 (?cj ?m?c )(?cj ?m?c )T, (8)
 where
 m?c =
 1
 Nc
 Nc∑
 j=1
 ?(xcj). (9)
 KSODA (intrinsic space). The matrix U = [u1 · · ·uk]
 defines a map RJ ? Rk, whose columns satisfy:
 maximize
 ui
 uTi S
 ?
 Bui
 uTi S
 ?
 Wui
 subject to ui ? u1,...,i?1
 uTi ui = 1
 ui ? Span(S?W )
 (10)
 where Span(S?W ) denotes the range space of matrix S?W .
 In this case, the data vector is transformed in the following
 way:
 x? ?(x)? UT ?(x). (11)
 Note that after the transformation x ? ?(x), the SODA
 algorithm can be applied directly.
 73
LDA PCA SODA
 Type Classifier Feature Reduction Feature Reduction
 Output wm?1 Wm?k Um?k
 Purpose Enhance Class Separability Enhance Data Description Enhance Class Separability
 Objective Function Direct Direct Successive
 Implementation Direct Direct / Iterative Iterative
 Table I
 THE RELATION BETWEEN LDA, PCA AND SODA.
 B. KSODA in Empirical Space
 When the Gaussian RBF kernel is adopted, the dimension
 of the intrinsic space is infinity and hence computations can
 not be carried out directly. In this case, it is necessary to
 resort to a kernel learning model proposed below.
 From the theory of Reproducing Kernel Hilbert Space
 (RKHS) [12], [13], [14], each vector ui ? H can be written
 as a linear combination of all the training data ?1, · · · , ?N
 drawn from H, ı.e., we have
 ui =
 N∑
 j
 ?jai,j = ?ai (12)
 where ? =
 [
 ?1, · · · , ?N
 ]
 is the training data matrix, the
 scalar ai,j is the jth element of vector ai. Similar to the
 SODA formulation, vector ui is the ith column of the
 transformation matrix U that can be written as
 U = ?A (13)
 where A =
 [
 a1, · · · ,aN
 ]
 As a result of plugging Equation (12) into Equation (10),
 define matrices M and N [10]:
 M = (M+ ?M?)(M+ ?M?)T . (14)
 and
 N =
 ∑
 c?{+,?}
 Kc(I? 1NcE)K
 T
 c , (15)
 where the row vectors of Mc are written as (Mc)j =
 1
 Nc
 ∑Nc
 t=1 k(xj ,xct). Matrix Kc denotes the kernel matrix
 Kc = ?T?c and E is a matrix with all ones as its entries.
 This leads to an equivalent Kernel SODA learning model in
 the empirical space.
 Denote k(x) = ?T ?(x), we formulate KSODA in the
 empirical space as:
 KSODA (empirical space). Find optimal vectors a1 · · ·ak,
 such that:
 maximize
 ai
 aTi Mai
 aiTNai
 subject to aTi Kaj = 0, ?i = j
 aTi Kai = 1
 (16)
 The optimal transformation matrix U can be written as:
 U = ?
 [
 a1 · · ·ak.
 ]
 Therefore, in the empirical space, the transformation
 follows:
 x? k(x)? AT k(x). (17)
 Equivalence: The empirical formulation is a direct result
 of kernelization of the intrinsic formulation [15], whose
 equivalence is straightforward ı.e. UT ?(x) = AT k(x).
 IV. IMPLEMENTATION AND APPROXIMATION
 For user’s choice, we describe two variance of kernelized
 SODA learning models.
 A. KSODA implementation
 There are two major steps involved in the algorithm:
 • Initialization:
 Q(0) = N+, D(0) = K+, ∆ =M+ ?M? (18)
 • Step 1: Computing ai and normalization
 ai =
 Q(i)∆√
 ∆TQ(i)TKQ(i)∆ (19)
 • Step 2: Let D(i+1) = D(i) ? aiaTi , update Q(i+1)
 according to
 Q(i+1) = D(i+1)ND(i+1) (20)
 • Go to Step 1 until i = k.
 B. Approximation
 For numerical efficiency, we introduce two approximation
 strategies:
 - Relaxation on orthogonality condition.
 - Data selection for big data scenario and the purpose of
 numerical invertibility.
 They are elaborated below.
 74
1) Relaxation on orthogonality:: Due to the complexity
 of the weighted orthogonality constraint ATKA = I, we
 define a relaxed formulation called KSODA (II).
 KSODA (II). Find optimal vectors a1 · · ·ak, such that:
 maximize
 ai
 aTi Mai
 aiTNai
 subject to aTi aj = 0, ?i = j
 aTi ai = 1
 ai ? Span(N)
 (21)
 where Span(N) denotes the range space of matrix N and
 the transformed data vector is represented as:
 x? =
 [
 a1 · · ·ak
 ]T k(x). (22)
 The optimization problem stated in (21) can be solved by
 Algorithm KSODA(II). Similar proof can be found in [4].
 Algorithm KSODA(II)
 - Construct the matrix X =
 [
 x1, · · · ,xN
 ]
 . Define output
 dimension k.
 - ComputeM and N from Eq. (14) and (15) respectively,
 - Let N(0) = N
 F(1) = (N(0))+M
 - For i = 1 : k
 - Solve for F(i)ai = ?iai
 where ?i is the largest and only eigenvalue of F(i).
 - Let D(i) = Im?m ? aiaTi be the deflation matrix
 N(i) = D(i)N(i?1)D(i)
 F(i+1) = (N(i))+M
 - Form matrix: A =
 [
 a1 · · ·ak
 ]
 - Transformation of the features: x? = ATK(X,x)
 C. Data selection and numerical invertibility
 From the RKHS theories, the solution vectors ui can
 be written as a linear combination of all the training data,
 namely ui = ?ai. That means the size of the kernel matrix
 we have to compute is in the order of the training size N .
 Furthermore, the solution of Formulation KSODA is based
 on the pseudo-inverse of a N ?N matrix N, which results
 in a N3 computational complexity.
 To tackle this problem, we choose a subset G ? ? to
 approximate the span of the whole training space. We call
 such matrixG the basis matrix. Note that without ambiguity,
 we use capital letter for the set of some training patterns (e.g.
 ? = {?1, · · ·?N}) and boldface letter for the corresponding
 matrix (e.g. ? = [?1, · · · , ?N]).
 Figure 1. An intuitive illustration of the basis selection. If two vectors
 ?i and ?j are very ’similar’ to each other, we assume that the spaces they
 span are collinear and there is no point to include both of them into the
 basis matrix G. The similarity measure is naturally defined by normalizing
 the kernel function k(xi,xj)√
 k(xi,xi)
 √
 k(xj ,xj)
 . By excluding similar vectors,
 we could reduce the number of vectors in the basis matrix.
 The idea is described as follows: In the intrinsic space,
 given normalized vectors ?t, t = 1, · · · , N , we would like
 to find a basis matrix G =
 [
 ?1, · · ·?n
 ]
 , such that for some
 small number ?, GTG = ?, where ? is a full rank matrix
 and ?i,j?l,l < ?, ?l, i, j with i = j.
 An illustrative example can be found in Figure IV-C.
 As we know that similarities between vectors always cause
 singularity and hence numerical instability. Therefore, by
 finding such basis matrix G, the robust invertibility of the
 matrix N is enhanced.
 Practically, the idea can be implemented as shown in
 Algorithm Basis G.
 V. EXPERIMENTAL RESULTS
 Parameter setting: In this section, we conduct simula-
 tions on 5 UCI [16] data sets to compare the classification
 results using original space, PCA, Kernel PCA, SODA,
 KSODA and KSODA (II). The classifier we used is Support
 Vector Machine (SVM) [17] with rbf kernel (? = 1).
 We also compared the classification results with LDA and
 Kernel LDA. The parameter ? for the kernel based methods
 are set to be 0.5 for consistent and fair comparison. The
 reduced dimensionality for the feature reduction techniques
 under comparison is k = 4. This is chosen based on cross
 validation on the data set arcene and then applied to the rest
 of the data sets.
 75
Data set Dimension Original PCA KPCA LDA KLDA SODA KSODA KSODAII
 arcene 104(4)? 200 50% 21.67 % 18.75 % 37.48% 13.81% 31.99% 15.62 % 13.63 %
 sonar 60(4)? 208 27.52% 33.89 % 35.83 % 27.83% 19.88% 25.42% 20.38 % 17.43 %
 wdbc 30(4)? 259 9.17% 7.35 % 5.38 % 3.84% 7.18% 3.44% 2.44 % 2.36%
 vehicle
 van 18(4)? 199 3.27% 12.23 % 17.86 % 2.01% 8.42% 2.39% 1.83 % 1.51%
 saab 18(4)? 219 17.38% 26.20 % 28.19% 13.55% 18.19% 13.97% 12.16 % 10.55%
 bus 18(4)? 218 2.23% 11.78 % 10.40% 3.21% 2.39% 3.10% 1.92 % 1.21%
 opel 18(4)? 212 17.28% 27.53 % 27.48 % 13.04% 14.43% 12.99% 11.67 % 10.34%
 segment
 brickface 19(4)? 330 0.87% 6.91% 5.96% 0.67% 1.05% 0.62% 0.59 % 0.53%
 sky 19(4)? 330 0.25% 0.47% 0.26% 0.21% 1.78% 0% 0 % 0.01%
 foliage 19(4)? 330 2.94% 7.69% 9.68% 4.03% 4.91% 3.28% 2.08 % 2.05 %
 cement 19(4)? 330 1.74% 8.09% 7.46% 3.65% 4.89% 1.83% 1.45 % 1.45%
 window 19(4)? 330 3.83% 10.20% 9.83% 4.34% 6.77% 3.66% 3.12 % 2.46%
 path 19(4)? 330 0.42% 3.31% 5.73% 0.60% 1.01% 0.56% 0.33 % 0.32%
 grass 19(4)? 330 0.58% 0.37% 0.42% 0.42% 0.21% 0.39% 0.21 % 0.19%
 Table II
 CLASSIFICATION ERROR COMPARISON BETWEEN DIFFERENT FEATURE REDUCTION TECHNIQUES. NOTE THAT THE KSODA IS ALREADY
 SUBSTANTIAL BETTER THAN SODA. HOWEVER, KSODA (II) OFFERS NOTICEABLE FURTHER IMPROVEMENT OVER KSODA.
 There are another two parameters to choose for Kernel
 SODA algorithms, which are the size of the basis matrix
 G and the tolerant ratio ?. These selections depend on the
 capacity of the computational device, the kernel parameters
 and the data properties. Here, we choose NG = 100 and
 ? ? [0.5, 1) is selected for each data set by cross-validation.
 Data description: The data sets we used are arcene,
 sonar, wdbc, vehicle and segment. The basic properties of
 the data sets are summarized in Table II.
 Vehicle and segment have more than two classes. Since
 we focus on the study of binary classification in this paper,
 the results shown are based on the averaged error rate of
 one-versus-one scheme for all classes from the data sets.
 Testing method: We divide each data set randomly into
 training set (80%) and testing set (20%). This procedure is
 repeated for 10 times. Furthermore, at the first time of the
 tests, 20% of the training set is left out for cross validation.
 The classification results in terms of error probabilities
 shown in Table II are based on the average error of the
 two classes:
 Perr =
 1
 2
 ∑
 c
 # of misclassified testing data in class c
 # of testing data in class c ? {+,?}
 (25)
 Testing results: On 13 out of 14 data sets, KSODA(II)
 has achieved the best classification results in terms of the
 averaged classification error defined in Equation (25). First,
 the original space of data set arcene has 10000 variables. It
 is obvious that it suffers from overfitting using SVM with
 rbf kernel, which results in a 50% error probability. In such
 scenarios, PCA/KPCA with extremely low dimensionality
 will do an even better job than the original space. However,
 when the number of original variables is reasonably small
 compared to the number of samples, PCA/KPCA do not
 achieve a better performance in general. LDA outperforms
 SVM on original space for roughly 50% cases. Since the
 parameter selection of kernel SVM is a key for high perfor-
 mance, LDA enjoys the advantage of simplicity.
 On average, SODA/Kernel SODA algorithms give the
 76
Algorithm Basis G
 - Let X =
 [
 x1, · · · ,xN
 ]
 , construct an empty m?NG
 matrix XG. Choose a small number ? as the threshold.
 Choose a kernel function K and the size of the basis
 NG.
 Set counter k = 1.
 - for i = 1 : NG
 Let XG(:, i) := X(:, k) ()
 - for j = k + 1 : N
 For given kernel function K, compute:
 K = K(XG,xk) (23)
 Normalization:
 K?ij =
 Kij
 ??(xi)?2??(xj)?2 , ?i, j (24)
 where ??(xt)?2 =
 √
 K(xt,xt) is the norm of
 vector ?t (t < i) on intrinsic space.
 - if: K
 ?
 ij
 min(Ktt) < ?, ?i = j, t < i,
 let k = j and return to ().
 else: let k = k + 1.
 - end
 - end
 - Replace the matrix X in Algorithm KSODA(II) by XG.
 best classification accuracy. The reason is that they do not
 only take into consideration of the class separability on
 the best one dimensional subspace, but also extend the
 development on k dimensions, which allow high flexibility
 that LDA does not provide. Kernel SODA, on the other hand,
 uses the kernel trick to further explore the intrinsic non-
 linear structure in the data and therefore results in an even
 lower classification error rate. Examples of visualization for
 these feature reduction techniques on data sets sonar, wdbc,
 vehicle and segment can be found in Figure 1 and 2.
 VI. ACKNOWLEDGMENT
 This material is based on research sponsored by DARPA
 under agreement number FA8750-12-2-0126. The U.S. Gov-
 ernment is authorized to reproduce and distribute reprints
 for Governmental purposes notwithstanding any copyright
 notation thereon.
 This work is also sponsored by the Swedish Research
 Council (VR) which is gratefully acknowledged.
 REFERENCES
 [1] Fukumizu, K., Bach, F. R. and Jordan, M. I., Dimensionality
 Reduction for Supervised Learning with Reproducing Kernel
 Hilbert Spaces. The Journal of Machine Learning Research,
 Vol. 5, pp. 73-99, Jan. 2004.
 [2] Hastie, T., Tibshirani, R., and Friedman J., The Elements of
 Statistical Learning: Data Mining, Inference, and Prediction.
 Second Edition, Springer, Feb. 2009
 [3] Guyon I. and Elisseeff A., An introduction to variable and
 feature selection. The Journal of Machine Learning Research,
 vol. 3, pp. 1157-1182, March 2003.
 [4] Yu Y., Mckelvey T. and Kung S.Y., A Classification Scheme
 for ’High-Dimensional-Small-Sample-Size’ Data Using SODA
 and Ridge-SVM with Microwave Measurement Applications.
 Proceeding of IEEE International Conference on Acoustics,
 Speech and Signal Processing (ICASSP), Vancouver, Canada,
 May 2013.
 [5] Duda, R.O., Hart, P.E. and Stork, D.G., Pattern Classification,
 2nd Edition, John Wiley & Sons, New York, 2011.
 [6] Jolliffe I.T., Principal Component Analysis. Series: Springer
 Series in Statistics, 2nd ed., Springer, 2002.
 [7] Hoerl A. E. and Kennard R. W. , Ridge Regression: Biased
 Estimation for Nonorthogonal Problems. Technometrics, vol.
 12, No. 1, pp. 55-67 Feb., 1970.
 [8] Yang J. and Yang J., Why can LDA be performed in PCA
 transformed space? Pattern Recognition, vol. 36, no. 2 , pp.
 563-566, Feb. 2003.
 [9] Diamantaras K. I., Kung S.Y., Principal component neural
 networks: theory and applications. John Wiley & Sons, 1996.
 [10] Mika S., Ratsch G., Weston J., Scholkopf B., and Mullers
 K. R., Fisher discriminant analysis with kernels. Proceedings
 of the IEEE Signal Processing Society Workshop in Neural
 Networks for Signal Processing IX, pp. 41 - 48, Aug 1999
 [11] Nie F., Xiang S., Liu Y., Hou C., and Zhang C., Orthogonal
 vs. uncorrelated least squares discriminant analysis for feature
 extraction. Pattern Recognition Letters, vol. 33, pp. 485491,
 2012.
 [12] Slavakis K., Theodoridis S., Yamada I., Online classification
 using kernels and projection-based adaptive algorithms. IEEE
 Transactions on Signal Processing, vol. 56(7), pp. 2781-2797,
 2008.
 [13] Bouboulis P. and Theodoridis S., Extension of Wirtinger
 Calculus to Reproducing Kernel Hilbert Spaces and the complex
 kernel LMS. IEEE Transactions on Signal Processing, vol.
 53(3), pp. 964-978, 2011.
 [14] Slavakis K., Bouboulis P., Theodoridis S., Online Learning in
 Reproducing Kernel Spaces. E-reference for Signal Processing,
 Elsevier, 2013.
 [15] Kung S.Y., Kernel Methods and Machine Learning. Cam-
 bridge University Press, 2013.
 [16] http://archive.ics.uci.edu/ml/.
 [17] Mavroforakis M., Theodoridis S., A Geometric Approach to
 Support Vector Machine (SVM) Classification. IEEE Transac-
 tion on Neural Networks, vol. 17(3), pp.671-683, 2006.
 77
?0.2 ?0.1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
 ?0.3
 ?0.2
 ?0.1
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 Kernel PCA
 ?0.8 ?0.6 ?0.4 ?0.2 0 0.2 0.4 0.6
 ?0.5
 ?0.4
 ?0.3
 ?0.2
 ?0.1
 0
 0.1
 0.2
 0.3
 0.4
 Linear PCA
  
 
?2 ?1 0 1 2 3 4 5 6
 x 10?3
 0
 0.002
 0.004
 0.006
 0.008
 0.01
 0.012
 0.014
 Kernel SODA
  
 
?0.005 0 0.005 0.01 0.015 0.02 0.025 0.03
 ?0.005
 0
 0.005
 0.01
 0.015
 0.02
 0.025
 0.03
 0.035
 Linear SODA
  
 
Class +: Training
 Class ?: Training
 Class +: Testing
 Class ?: Testing
 ?0.3 ?0.2 ?0.1 0 0.1 0.2 0.3 0.4 0.5
 ?0.4
 ?0.3
 ?0.2
 ?0.1
 0
 0.1
 0.2
 0.3
 Kernel PCA
 ?0.15 ?0.1 ?0.05 0 0.05 0.1 0.15 0.2 0.25 0.3
 ?0.2
 ?0.15
 ?0.1
 ?0.05
 0
 0.05
 0.1
 0.15
 Linear PCA
  
 
7.7254 7.7255 7.7256 7.7257 7.7258 7.7259 7.726 7.7261 7.7262
 x 10?4
 8.121
 8.1215
 8.122
 8.1225
 8.123
 8.1235
 8.124
 x 10?4 Kernel SODA
 ?5.2 ?5 ?4.8 ?4.6 ?4.4 ?4.2 ?4
 x 10?4
 ?6
 ?5.8
 ?5.6
 ?5.4
 ?5.2
 ?5
 ?4.8
 ?4.6
 ?4.4
 x 10?4 Linear SODA
  
 
Class +: Training
 Class ?: Training
 Class +: Testing
 Class ?: Tesing
 Figure 2. Visualization of the first two components of data set sonar and
 wdbc using different feature reduction techniques.
 ?0.5 ?0.4 ?0.3 ?0.2 ?0.1 0 0.1 0.2 0.3 0.4 0.5
 ?0.45
 ?0.4
 ?0.35
 ?0.3
 ?0.25
 ?0.2
 ?0.15
 ?0.1
 ?0.05
 Kernel PCA
 ?0.4 ?0.3 ?0.2 ?0.1 0 0.1 0.2 0.3
 ?0.15
 ?0.1
 ?0.05
 0
 0.05
 0.1
 Linear PCA
  
 
?6.85 ?6.8 ?6.75 ?6.7 ?6.65 ?6.6 ?6.55 ?6.5 ?6.45 ?6.4 ?6.35
 x 10?5
 ?8
 ?7.9
 ?7.8
 ?7.7
 ?7.6
 ?7.5
 ?7.4
 ?7.3
 ?7.2
 ?7.1
 ?7
 x 10?5 Kernel SODA
 ?0.06 ?0.055 ?0.05 ?0.045 ?0.04 ?0.035 ?0.03
 ?0.035
 ?0.03
 ?0.025
 ?0.02
 ?0.015
 ?0.01
 Linear SODA
  
 
Class +: Training
 Class ?: Training
 Class +: Testing
 Class ?: Testing
 ?1 ?0.9 ?0.8 ?0.7 ?0.6 ?0.5 ?0.4 ?0.3 ?0.2 ?0.1 0
 ?0.4
 ?0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 Kernel PCA  
?0.8 ?0.6 ?0.4 ?0.2 0 0.2 0.4 0.6
 ?0.8
 ?0.6
 ?0.4
 ?0.2
 0
 0.2
 0.4
 0.6
 Linear PCA
 ?1.524 ?1.5235 ?1.523 ?1.5225 ?1.522 ?1.5215 ?1.521 ?1.5205 ?1.52 ?1.5195
 x 10?3
 2.643
 2.644
 2.645
 2.646
 2.647
 2.648
 2.649
 x 10?3 Kernel SODA
  
 
0.005 0.01 0.015 0.02 0.025
 0.004
 0.006
 0.008
 0.01
 0.012
 0.014
 0.016
 0.018
 0.02
 0.022
 Linear SODA
  
 
Class +, Training
 Class ?, Training
 Class +, Testing
 Class ?, Testing
 Figure 3. Visualization of the first two components of one example from
 data set vehicle and segment using different feature reduction techniques.
 78

World Congress on Internet Security (WorldCIS-2013) 
LElA: The Live Evidence Information Aggregator 
Towards Efficient Cyber-Law Enforcement 
Irvin Romem, Spyridon Dosis, Oliver Popov 
Department of Computer and Systems Sciences 
Stockholm University 
Sweden 
{irvin, dossis, popov}@dsv.su.se 
Keywords-Digital Forensics, Cybercrime, Digital Evidence, Big 
Data, Hadoop, Hypervisors, Collaborative Live Investigation 
I. INTRODUCTION 
The Internet today, in its most basic fonn, is a massively 
distributed, complex information sharing organism. Billions of 
interconnected elements of varying capabilities, attributes and 
functions work together through this information sharing, 
providing society with an endless array of beneficial everyday 
services. Such services span various activities that are part of 
the social, economic and the political structures of society. 
Business, leisure, education, transportation, healthcare, and 
various government services are some of these activities that 
have adopted the Internet, in some way or another, in order to 
improve the ways in which these services have traditionally 
been delivered. One only needs to think of recent innovations 
such as mobile banking and payment systems, online shopping, 
e-Voting, e-Health services, transport information systems, 
online social networks, or crowd sourcing crisis information in 
order to see how critical these services, and their underlying 
technological infrastructures, actually are in maintaining a 
modicum of order in an individual's life today. From this, it is 
not difficult to realize that we greatly rely on these Critical 
Internet Infrastructures in order to support and facilitate our 
normal day to day activities. 
In the same way that everyday activities are quickly being 
adopted into the "Digital Realm" through the use of technology 
and the Internet, so are malevolent activities. Evildoers are hard 
at work also creating novel and innovative ways to perpetrate 
978-1-908320-22/3/$25.00©2013 IEEE 
their malicious activities on the Internet. The crime scene is 
quickly moving from a physical reality to a more virtual one. 
The forms of evidence found in these "Digital Crime Scenes" 
have also moved from the traditional fingerprints, footprints, 
hair samples, blood samples or other DNA related evidence, 
into more digital artifacts.. Such digital forms of evidence 
commonly include hard-disk drives, live (RAM) memory, 
network traffic captures, mobile devices, RAID sets [1], and 
virtually any other form of technology that records past events 
of its actions; that can be captured and can be analyzed during 
or after the criminal event and whose integrity can be verified. 
This opens the floor to almost any fonn of computer system 
or device (physical or virtual) that can be thought of, and thus 
brings in the concept of heterogeneity among devices. 
Different devices may have different operating systems, 
software applications, storage formats, encoding fonnats and 
communication protocols [2]. This heterogeneity makes the job 
of a Digital Investigator a lot more difficult because of the wide 
variety in which evidence could manifest itself in. 
Additionally, Electronic Crime cases today commonly 
involve more than just a one device. Several physical devices 
including laptops, mobile phones, GPS devices and even 
embedded devices such as onboard vehicle computer systems 
may be seized at a go and brought in for analysis. The vast 
realm of the Internet also comes into play including web 
application accounts, online email accounts, cloud storage 
facilities, network traffic captures and logs [3]. All these 
evidence forms could easily be part of a single case in today's 
world and even more so in the imminent realm of the Internet 
of Things. The sheer volume of data that one would have to sift 
through in order to investigate a single case could be in the 
order of Terabytes and can be a more than daunting task to 
perform. [4] 
The final problem that investigators grapple with is the 
relative immaturity of present forensic tools. Current industry 
standard forensic tools such as EnCase, FTK, XRY, Volatility 
and Wireshark 1, so far, do not cater for the highly divergent 
nature of digital evidence sources. Most, if not all tools, focus 
on a single niche area such as Filesystem Data, Live Memory, 
Network Traffic, Mobile Devices or Log data. There are no 
tools that provide a comprehensive method to interface with all 
1 EnCase® Forensic and FTK® (Forensic ToolKit) are traditional file system 
forensics tools. MicroSystemation XRY is for mobile forensics, Volatility for 
live memory forensics and Wireshark for network forensics. 
156 
World Congress on Internet Security (WorldCIS-2013) 
the variety present to provide a uniform investigation platform. 
In addition to this, current tools have rather limited capabilities 
for dealing with extremely large datasets, and quickly become 
problematic when presented with such. 
In this paper, we present the multi-tiered architecture of a 
comprehensive, scalable, distributed incident response and 
digital investigation platform, that is, LELA-the Live Evidence 
Information Aggregator. LElA aims at combating cybercrime 
through innovatively solving the aforementioned problems of 
the inherent complexity of the "Internet-of-Things scale" of 
impending cases, as well as the insurmountably large amounts 
of heterogeneous data against the current deficiency in physical 
resources, as well as skill, within Law Enforcement Agencies. 
The rest of this paper is organized as follows: In Section II, we 
review related work outlining shortcomings of previous similar 
solutions. Section III describes the requirements for a 
comprehensive digital investigation platform. The functionality 
of the LELA system is described in Section IV. The proof of 
concept implementation and results are outlined in Section V. 
In Section VI, we discuss the shortcomings and future work of 
this study. Finally, we conclude this paper with a summary in 
Section VII. 
IT. BACKGROUND AND RELATED WORK 
There have been several efforts directed towards making 
the Digital Investigation process more efficient. Most have 
been motivated by the requirements of legal systems, evolution 
in the digital crime scene, the sheer workload that burdens the 
analysts and technological revolutions. Some of these efforts 
include: Delegation and Collaboration among teams; Reduction 
of evidence sizes through filtering out known files; and simple 
automation of important but mundane, repetitive tasks (such as 
indexing data for subsequent searches, file carving, parsing 
running process and TCP flows). Most of these capabilities 
have been implemented in current industry standard forensic 
tools, however, investigators and analysts still continue to be 
overburdened. This is because of the massive amounts of 
heterogeneous and disjointed datasets that they are tasked to 
make sense of and present unambiguous conclusions. 
Several unidirectional solutions have been proposed in a 
bid to solve this multi-faceted problem, however, none has 
been unequivocally successful. In recent times there have been 
initiatives to centralize processing and data collection onto 
powerful centralized mainframes and data storage centres, 
respectively. There has also been a push towards having the 
different parties, involved in solving a case to work together, 
even from geographically separate locations [5], particularly 
among technical staff in niche areas (filesystem forensics, 
network forensics, live memory forensics or mobile forensics) 
and the legal experts. Collaboration has been the mainstay of 
the attempt to get cases solved faster. 
Reducing the amount of data that is needed to be collected 
is also a means of reducing the amount of time needed to 
analyze the data. This has previously been done through 
"Known File Filtering" as well as through heuristic analysis of 
the entire captured data. Network Security Monitoring has also 
been an avenue for gathering data through the assistance of 
Intrusion Detection Systems (IDS's) assisted through Artificial 
978-1-908320-22/3/$25.00©2013 IEEE 
Intelligence. However, this has been the specific mandate of 
the IDS, centralized or distributed, as the case may be, with 
terminating (end) devices or intermediary devices generally 
playing very minor roles in this task. 
As far as is known to the author, there has not been much 
done, through any single initiative, in terms of expanding the 
scope of data captured to be the mandate of all possible devices 
of reasonable capability. Enabling individual devices to 
natively act as part of the Incidence Response System, towards 
the aim of collecting potential evidentiary data, has not been 
widely studied. Additionally, collaboration on the human 
processing level has been emphasized, but it has not been 
introduced among unrelated networked devices. These devices 
could possibly be harnessed to work together towards aiding in 
intelligent real-time capturing, filtering and processing in order 
to attain and retain that which could be considered as possible 
evidentiary data, antecedent to the event of a crime being 
detected. It is for these reasons that we delve into this area to 
explore it further. 
Notable related studies include [6], that describes a live 
network forensics system that provisions varying Intrusion 
Detection Systems on host machines based on their respective 
resource costs. It works in a virtualized environment where 
snapshots are taken periodically and used to revert the system 
back to the point before an attack began. Each system rollback 
results in varying IDS's being deployed to collect new and 
possibly better information. This presupposes that the attacker 
re-enacts their malicious behavior repetitively each time their 
efforts are thwarted by the system. Storage of the potential 
evidentiary information in a forensically sound manner is not 
particularly dealt with in this study, however, the generated 
attack graphs may be useful in understanding the actions that 
were undertaken by the attacker. 
In [7]-[9] distributed system architectures for proactive 
collection and summarization of evidence, with centralized 
data storage and processing, are described. They are, however, 
particularly directed at closed domain enterprise systems, 
where there is some form of control and order instigated by 
system administrators. The system being proposed in this study 
is aimed at being universal - applying to the entire Internet. 
The work done by Redding in [10] is the most closely 
related study done in the area of pro-active and collaborative 
computer forensic analysis among heterogeneous systems. 
Redding proposes a peer-to-peer framework for network 
monitoring and forensics through which network security 
events can be collected and shared among the peers. "Analysis, 
forensic preservation and reporting of related information can 
be performed using spare CPU cycles," [10] together with 
other spare, under-utilized, or unused resources. The system 
described, however, is again designed for an "administratively 
closed environment." This means that all the devices that are 
within the domain of this system are centrally controlled. An 
administratively open system is what is not dealt with in 
Redding's work, and thus it is what is sought after in the 
current study as will be described later in this paper. 
There have been several efforts in terms of standardization 
of evidence formats (in the form of pre-processing) in order to 
facilitate uniform, seamless data exchange and communication 
157 
World Congress on Internet Security (WorldCIS-2013) 
of forensic artifacts between heterogeneous entities. Members 
of the Common Digital Evidence Storage Format Working 
Group have generally been at the forefront of this [2]. Other 
notable efforts include [11] which makes use of the Resource 
Description Framework (RDF) from Semantic Web 
technologies as a common data representation layer for digital 
evidence related metadata, using ontologies for describing the 
vocabulary related to this data, and [12] where a detailed 
ontology of Windows Registry artifacts of interests is 
introduced. The Open Forensic Integration Architecture (FIA) 
in [3] and FACE [4] describe methods for the integration of 
digital evidence from multiple evidence sources in a bid to 
facilitate more efficient analysis. The Advanced Forensic 
Format [13], AFF4 [1] and XlRAF [14] describe annotated 
evidence storage formats that allow for addition of arbitrary 
metadata as well as interoperability among different tools. 
All these evidence integration ideas assume that acquisition 
of the relevant data from the respective sources has already 
been performed. This is a stumbling block in the process of 
making digital investigations more efficient. This is because 
these methods have not been integrated into the evidence 
capture process. They are seen as a subsequent step rather than 
part of the process. As part of this study we integrate these 
hitherto unlinked processes. 
The proposed idea that this study covers is composed of 
several areas of specialization, namely: The Internet of Things 
(loT), Intrusion Detection Systems, Peer to Peer Networks, 
Virtualization infrastructures, Large Scale Cloud storage and 
Semantic Web technologies. Most of these technologies have 
been previously harnessed in different capacities, singularly or 
in small clusters, towards the benefit of digital forensics for 
today's complex internetworked and intertwined cyber realm. 
However, to the author's knowledge, there has so far not been 
any work done that aims to merge all these technologies 
together in order to provide a singular scalable solution that 
solves the recurring problems of large amounts of data, several 
sources of data, heterogeneity among systems, insufficient 
processing power, security and privacy - that are constantly 
troubling digital forensic analysts and law enforcement 
agencies worldwide. 
Ill. CHARACTERISTICS OF THE DESIRED SOLUTION 
In light of the current state of electronic crime, the present 
state of forensic tools, and from experience, we describe below 
a wish-list of characteristics that one would like to have in a 
Cyber-Law Enforcement solution: 
• Distribution: The ability to deal with massive amounts of 
distribution in terms of participants, data storage, 
processing and dissemination. The system needs to be 
able to handle the heterogeneity that may come with 
distributed systems as well. 
• Scalability: Large scale interconnectivity, as well as the 
possibility of new entities joining, as well as others 
leaving the system dynamically and gracefully without 
drastic negative effects on the system. The ability to 
easily improve or extend the capabilities of the system 
through new modules is also desired. 
978-1-908320-22/3/$25.00©2013 IEEE 
• Availability: Providing suitable levels of functionality as 
and when required. 
• Universality: Among the heterogeneity and lack of 
standardization among vendors of different systems, there 
needs to be some standardization and common 
understanding between the systems on the level of 
communication and storage of potential evidentiary 
information. 
• Responsiveness: The system should be able to aptly detect 
when a security policy has been irrecoverably violated, 
thus collecting information in order to pursue the 
perpetrators of the criminal actions. This also improves on 
efficiency and privacy in that the system does not have to 
perpetually be collecting all possible information from all 
possible systems. 
• Resource Sharing: Today, large complex problems that 
are being solved through collaboration and sharing of 
resources as seen in Crowd sourcing, P2P networks, and 
cloud infrastructures. They provide on demand rapid 
availability of large amounts of resources from collective 
resource pools providing speed, efficiency and the 
benefits from "the wisdom of the crowd". 
• Integrity (Trust, Reliability Accuracy): As a system 
facilitating law enforcement in digital crimes, the levels 
of trust, reliability, accuracy and integrity of the 
information needs to be high enough to be accepted as a 
veritable source of evidentiary information for a court of 
law. The Daubert standards and the chain of custody need 
to be adhered to. 
• Privacy Confidentiality: Personally identifiable and 
secret information must be maintained as anonymous and 
confidential as is reasonably acceptable, unless 
incriminated. Unauthorized access to such information is 
not to be allowed. 
• Security: In addition to ensuring the security of the 
potential evidentiary information that it aims to collect 
and process, it must also take its own security into 
consideration - especially in terms of authentication, 
authorization, accountability and non-repudiation of 
activities undertaken. 
IV. THE LIVE EVIDENCE INFORMATION AGGREGATOR 
The hypervisor-based, peer-to-peer distributed system that 
is meant to be the LElA is a 4-tiered architecture with the 
following main components that work together in varying 
capacities: 
a) The Host-based Hypervisor (HbH) 
b) The Peer-to-Peer Distribution Architecture (P2P-da) 
c) The Cloud-based Backend (CBB) 
d) The Law Enforcement Controller (LEe) 
The diagrams that follow portray the architecture of the LElA. 
Figure 1 shows 3 layers of the architecture in a figurative 
manner, while Figure 2 displays the layered nature of the 
entire architecture. 
158 
World Congress on Internet Security (WorldCIS-2013) 
llost 8Ji!oed IfyJK'fVlSOf 
(libH) ?m1 UYft 
?:ttf-::::::-?_ Pt't!f.lo-Peer Oistribuhon 
AtdWtec:hn{P1P4I} 
... ,.. 
CIIOud·Nsed 8.id:end 
(UB) S\j)S.y5tC'lI\ I.liytf 
Figure 1. High level view of the LElA architecture 
} Host·based 
Peer4to--Peer } DistributIon 
Architecture 
(PIP.dol 
} 
CkJud·8ased 
llackend 
i--- -------- --:e? ---- --- ------
 I } ?;o,c.m.nt 1 ___________________________ ? Controller 
Figure 2. Layered/Tiered representation of the LElA architecture 
The functionality of each of the layers of the LElA system is 
briefly described in the following sections. 
A. The Host-based Hypervisor (HBH) System 
The Host-based Hypervisor (HbH) system is composed of a 
virtualization layer managed by a hypervisor - a privileged 
secure platform managing the guest operating system (OS). 
The hypervisor contains an inbuilt host-based intrusion 
detection system. Security utilities within the guest OS such as 
anti-malware tools and intrusion detection systems maintain 
their own data and logs that are accessible to the HbH .. 
Individual HbH systems communicate with each other 
through the Peer-to-Peer Distribution Architecture (P2P-da) in 
sharing information about malicious activity that they have 
discovered. This collaboration of HbH systems helps improve 
accuracy ofTDSs and eventually data collection. 
The HbH maintains a hash list of the local files on its guest 
operating system (Local - Known Data Hash-List, L-KDHL) 
which is periodically cross-checked and updated against a 
Master - Known Data Hash-List (M-KDHL) stored at the 
CBB. This is managed by the Cloud-based Backend 
Differencing Engine (CBB-DE) component of the CBB. User 
data and its corresponding hashes are also subjected to a 
similar procedure. 
Having privileged access, the HbH is able to directly 
interact with the file system, network interfaces, memory 
978-1-908320-22/3/$25.00©2013 IEEE 
caches and other low-level resources, which are all primary 
sources of prime evidentiary data in digital investigations. The 
embedded IDS's (em-IDS) also collect information mostly in 
the form of logs which are parsed to result in alerts. When 
evidentiary data from the local HbH, or neighbouring HbH is 
in transit towards the CBB, it is always held in temporary 
storage and in an encrypted form. 
B. The Peer-to-Peer Distribution Architecture (P2P-da) 
The P2P-da is a convergence of P2P protocols that provides 
a reliable, scalable overlay network among heterogeneous 
devices. The particular P2P protocols that contribute to the 
formation of the overall P2P-da functionality are: Epidemic 
(Gossiping) protocols [15], Gradient (Hierarchical) overlay 
protocols [16] and the Bit-torrent protocol[17]. 
1) Maintenance of the P2P Overlay 
Here a gradient overlay network based on a utility metric of 
the relative performance capability of a HbH system is built 
and maintained through the "random neighbor" information 
sharing paradigm of gossiping protocols. A hierarchy of 
systems is thus formed with the less endowed elements on the 
outer edges and the more capable elements closer towards the 
centre of the LElA system (that is, the CBB). 
2) Dissemination and Aggregation of Malicious Behaviour 
Tnformation Alerts 
Collaboration across the entire LElA system is facilitated 
through message passing over the P2P-da. This enables more 
informed detection and evidence collection mechanisms. There 
are 2 main message types: Management messages, and security 
incident control messages. The former enables dissemination of 
information to keep detection mechanisms up-to-date, while 
the later facilitates reaction to the detection of a malicious 
event. 
3) Incident response data collection 
This is triggered by the detection of malicious events via 
the collective knowledge gained through collaborating HbH 
systems, the em-IDS and guest OS security mechanisms. 
Correspondence with the Cloud-Based Backend-Differencing 
Engine (CBB-DE) filters out known system files through 
facilitating the hash comparisons. Primary analysis for IP 
addresses and hostnames on the data collected may result in 
triggering of other HbH systems to capture data. 
The actual data collection procedure involves 3 stages: 
a) Data Partitioning 
Different data formats (memory dumps, logs, files, packet 
captures, disk images) are compressed and stored 
temporarily on the HbH system in a modified AFF4 data 
structure that contains simple RDF metadata describing 
the evidence. This data structure is termed as the Incident 
Data Archive (IDA). Each IDA data structure is 
partitioned in equal size pieces that will be referred to as 
shards. The shard is a signed and encrypted partition of 
the IDA analogous to the idea of a "piece" in the 
BitTorrent Protocol. A metadata file termed as the 
"reflection" (which corresponds to the BitTorrent 
Metadata file) is also created and sent directly to the 
159 
World Congress on Internet Security (WorldCIS-2013) 
CBB. In this way the CBB acts as the "tracker" and 
"leaches" TDAs from participating HbH systems in the 
P2P-da, thus benefiting from the high throughput of the 
BitTorrent protocol 
b) Shard Distribution 
Shards are distributed to more capable neighbours 
(supporters), facilitated by the gradient overlay. Each time 
a shard is passed on it increases its "heat level". After a 
certain "heat" threshold (the "melting point") a HbH 
system is obliged to directly upload to the CBB, else an 
election procedure is initiated to determine which 
previously supporting HbH should be delegated the 
uploading task. For reliability reasons, HbH systems are 
only allowed to partake in uploading a certain number of 
IDA shards governed by the "dependency value". 
c) Rapid fragment reconstruction 
For a particular shard, downloads are initiated from all 
their respective supporter locations. This is done for 
redundancy purposes. Similar to the BitTorrent Protocol 
download, priority is given to the shards that are the least 
commonly available, that is, those that have the fewest 
recorded supporters. 
In order to reconstitute the IDA, individual hashes of 
shards are verified as they are received, against that in the 
reflection. Several supporters upload at the same time, 
thus if a shard is in error, that from another supporter is 
taken. Once successfully transferred, shards are deleted 
from supporting HbH systems. 
C. The Cloud-based Backend (CBB)System 
The CBB system is a highly available, scalable, responsive, 
centralized back end storage service capable of storing large 
amounts of data in a homogeneous form. It is subdivided into 3 
major components: The Storage System (SS), the Differencing 
Engine (DE) and the HbH Master Peers. 
The Storage System (SS) is built upon the Hadoop HDFS 
architecture [18] that provides not only the raw storage 
capabilities but also scalability, availability, reliability and 
responsiveness. The Differencing Engine (DE) filters out 
known files before having them stored on the CBB. This is 
provisioned through the Mapreduce [19] capabilities supported 
by Hadoop. The DE also provides a query-response mechanism 
to the HBH systems with information on known benign data as 
part of the Master Known Data Hash-List (M-KDHL). The M?
 KDHL contains data about known files, memory processes, 
protocol flows, and log entries and thus enables their removal 
from IDAs being prepared. This reduces the size of IDAs 
before being stored on the Storage System (SS) of the CBB. 
The HbH Master Peers are a particular set of well-endowed 
peers that are directly connected to the core CBB system (that 
is, the SS and DE) providing an interface to the rest of the 
LElA system through the P2P-da. They do not have other core 
functionalities unrelated to their LELA responsibilities and are 
essentially the backbone of the P2P-da and ultimately the 
provider of connectivity of the LELA system outwards to the 
other HBH systems. The HBH Master Peers also serve as the 
978-1-908320-22/3/$25.00©2013 IEEE 
central point through which system software updates and 
malicious event detection heuristics are originated from and 
disseminated outwards to the HBH systems in the wild. 
D. The Enforcement Controller System 
The Law Enforcement Controller is the main interface that 
law enforcement personnel interact with in order to perform 
their directed analysis for a particular digital investigation case. 
Through it, a Law Enforcement Agent can initiate specific 
queries to the data sets stored on the CBB, thus retrieving 
detailed, structured information as well as new knowledge 
inferred through correlation of data originating from different 
sources that may help in solving a case. The aim of this is to 
automate otherwise manual tasks of correlating data from 
different heterogeneous sources in order to pose valid 
assertions based on the data that could assist a forensic analyst 
in performing their duties of making sense of digital artifacts. 
This functionality is described in more detail by Dosis in [20]. 
Additionally, from the new found knowledge, patterns of 
malicious activities are to be learnt and stored. These Malicious 
Activity Patterns are to be used as feedback to the HbH 
systems in order to improve the detection capabilities of the 
inbuilt IDS's and thereby also improve the accuracy of 
collection of data of potential forensic evidentiary use. 
V. PROOF OF CONCEPT EVALUATION AND RESULTS 
A partial proof of concept of the LELA system was created. 
It focused on the remote extraction and compression of disk 
data evidence from small scale devices over the Internet and 
the subsequent storage on a Hadoop cluster. Due to time 
constraints, the P2P-da and the HbH were not actually 
developed, however the networking capability between devices 
and the cloud storage, as well as the concept of privileged 
access to the device being captured, were maintained. 
Four different small scale devices were used in testing and 
measuring the performance of the application. The table below 
outlines the specifications of the devices being captured. 
TABLE!. SMALL SCALE DEVICE SPECIFICATIONS 
Chumby Classic Busybox v 1.6.l 64MB 64MB 
HTC Incredible S Android OS v2.3.3 768MB 1. 1GB (Gingerbread) 
HTC MyTouch CyanogenMod 10.2 Alpha 768MB 4GB 4G Slide 
Samsung Galaxy Android OS, v4.0.3 Tab 2 1GB 8GB 
(WiFi Only) (Ice Cream Sandwich) 
In evaluating the performance of the proof of concept LELA 
system, the time taken to perform the evidence capture on 
certain fixed of partition sizes was measured. The resulting 
data was recorded and is portrayed in the graph below. 
160 
World Congress on Internet Security (WorldCIS-2013) 
o 500 1000 1500 7000 1500 '000 3500 4000 4500 
Partition Size Used 
Figure 3. Perfonnance of the LElA Proof of Concept 
The Chum by device was captured, but not graphed as its 
partitions were too small (32 MB each) to provide useful data. 
From the graphs one can see a step-like curve in all of the 
cases. All curves also seem to begin with a more linear shape 
before a more exponential relationship takes over. This is 
particularly clear with the "HTC MyTouch 4G Slide". Tn 
general, it seems like there is more of an exponential 
relationship between the Partition Size and the File Transfer 
Time. One could posit that as the partition sizes increase, even 
to sizes substantially larger than those in the graph, the 
relationship will become ever more exponential. This is a 
particularly clear motivation for a more efficient data transfer 
mechanism such as a high-throughput P2P network between 
the device being captured and the eventual evidence storage 
location. 
VI. CONCLUSION 
Tn this study we outlined the plethora of problems that 
plague digital investigations making them cumbersome slow 
processes. The architecture of a comprehensive, more efficient 
cyber-Iaw enforcement platform was proposed. Finally, a small 
proof of concept application was developed and its 
performance analyzed. 
VII. FUTURE WORK 
Though this architecture is promising, several parameters 
within the communication protocols need further optimization. 
A PKI infrastructure can be infused in the system in order to 
improve the security of the communication and storage 
facilities. The concept of privacy needs to be addressed within 
the scope of this solution. Finally, an experiment with a wider 
scope would be greatly desired in order to better drive this 
architecture towards becoming a reality. 
978-1-908320-22/3/$25.00©2013 IEEE 
REFERENCES 
[I] M. Cohen, S. Garfinkel, and B. Schatz, "Extending the advanced 
forensic format to accommodate multiple data sources, logical 
evidence, arbitrary information and forensic workflow, " Digit. 
Investig., vol. 6, pp. S57-S68, Sep. 2009. 
[2] B. C. Frank Adelstein, J. K. Eoghan Casey, Simson L. Garfinkel, Chet 
Hosmer, and and P. T. Jim Lyle, Marcus Rogers, "Standardizing 
Digital Evidence Storage, " Commun. ACM, vol. 49, no. 2, pp. 67-68, 
2006. 
[3] S. Raghavan, A. Clark, and G. Mohay, "FIA: an open forensic 
integration architecture for composing digital evidence, " Forensics 
Telecommun. In! Multimed. Lect. Notes Inst. Comput. Sci. Soc. 
Informatics Telecommun. Eng., vol. 8, pp. 83-94, 2009. 
[4] A Case, A Cristina, Marziale, G. G. Richard, and V. Roussev, 
"FACE: Automated digital evidence discovery and correlation, " Digit. 
Investig., vol. 5, pp. S65-S75, Sep. 2008. 
[5] M. Davis, G. Manes, and S. Shenoi, "A network-based architecture for 
storing digital evidence, " Adv. Digit. Forensics IFIP Int. Con! Digit. 
Forensics, vol. 194, pp. 33-42, 2005. 
[6] S. Zonouz, K. Joshi, and W. Sanders, "Floguard: cost-aware 
systemwide intrusion defense via online forensics and on-demand IDS 
deployment, " in Computer Safety, Reliability, and . . .  , 20 1 1, pp. 338-
 354. 
[7] C. Shields, O. Frieder, and M. Maloof, "A system for the proactive, 
continuous, and efficient collection of digital forensic evidence, " Digit. 
Investig., vol. 8, pp. S3-S 13, Aug. 20 1 1. 
[8] Yu, Y. V. Ramana Reddy, S. Selliah, S. Reddy, V. Bharadwaj, and S. 
Kankanahalli, "TRINETR: An architecture for collaborative intrusion 
detection and knowledge-based alert evaluation, " Adv. Eng. 
Informatics, vol. 19, no. 2, pp. 93- 10 1, Apr. 2005. 
[9] M. Cohen, D. Bilby, and G. Caronni, "Distributed forensics and 
incident response in the enterprise, " Digit. Investig., vol. 8, pp. S I 0 I?
 S II 0, Aug. 20 II. 
[ 10] S. Redding, "Using Peer-to-Peer Technology for Network Forensics, " 
Adv. Digit. Forensics IFIP Int. Fed. In! Process., vol. 194, pp. 14 1-
 152, 2005. 
[ 1 1] B. Schatz and A. Clark, "An open architecture for digital evidence 
integration, " in AusCERT Asia Pacific Information Technology Security 
Conference, 2006, no. May, pp. 15-29. 
[ 12] D. Kahvedzi6 and T. Kechadi, "DIALOG: A framework for modeling, 
analysis and reuse of digital forensic knowledge, " Digit. Investig., vol. 
6, pp. S23-S33, Sep. 2009. 
[ 13] S. Garfinkel, "AFF : A New Fonnat for Storing Hard Drive Images, " 
Assoc. Comput. Mach. Commun. ACM, vol. 49, no. 2, pp. 85-87, 2006. 
[ 14] W. Alink, R. a. F. Bhoedjang, P. a. Boncz, and A. P. de Vries, "XIRAF 
- XML-based indexing and querying for digital forensics, " Digit. 
Investig., vol. 3, pp. 50-58, Sep. 2006. 
[ 15] M. Jelasity, S. Voulgaris, R. Guerraoui, A-M. Kermarrec, and M. van 
Steen, "Gossip-based peer sampling, " ACM Trans. Comput. Syst., vol. 
25, no. 3, pp. I-36, 2007. 
[ 16] Sacha, Dowling, R. Cunningham, and R. Meier, "Discovery of 
stable peers in a self-organising peer-to-peer gradient topology, " in 
International Conference on Distributed Applications and 
Interoperable Systems (DAIS), 2006, pp. 70-83. 
[ 17] B. Cohen, "Incentives build robustness in BitTorrent, " Work. Econ. 
Peer-to-Peer Syst., 2003. 
[ 18] K. Shvachko, H. Kuang, S. Radia, and R. Chansler, "The Hadoop 
Distributed File System, " 2010 IEEE 26th Symp. Mass Storage Syst. 
Techno!., pp. 1- 10, May 20 10. 
[ 19] J. Dean and S. Ghemawat, "MapReduce: simplified data processing on 
large clusters, " Commun. ACM, pp. 1- 13, 2008. 
[20] S. Dosis, Homem, and Popov, "Semantic Representation and 
Integration of Digital Evidence, " Procedia Comput. SCi., vol. 22, pp. 
1266--1275, 20 13. 
161 

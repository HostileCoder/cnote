Modeling Energy Savings in Volunteers Clouds
 Congfeng Jiang?, Jian Wan?
 Christophe Ce«rin , Paolo Gianessi , Yanik Ngoko 
 ? Hangzhou Dianzi University
 1, 2nd Street, Xiasha Higher Education Zone, Hangzhou 310018, China
 Email: cjiang@hdu.edu.cn
   Universite« de Paris 13, PRES Sorbonne Paris Cite«
 LIPN, UMR CNRS 7030,
 99, avenue Jean-Baptiste Cle«ment, 93430 Villetaneuse, France
 Email: christophe.cerin@lipn.univ-paris13.fr
 AbstractÑIn this paper we propose different models for the
 energy consumption in a special existing cloud system named
 SlapOS. In this cloud, the data center comprises dedicated and
 volunteer machines; these latter ones are not always available.
 Our objective is to state how to plan the run of applications
 for minimizing the global energy consumption; we propose
 two modelings. In the first model, we assume that we have a
 finite number of homogeneous volunteers nodes on which our
 applications can be run. The objective is to determine which
 application to run on each node in order to minimize the
 overhead in energy consumption caused by these runs. We show
 that the key computational challenge in this problem consists in
 finding a feasible solution when it exists. We propose for it a
 polynomial time algorithm. In the second model, we assume that
 the volunteers nodes are heterogeneous. In this case, we show
 again how to find a feasible solution in polynomial time. But
 in comparison to the homogeneous case, the key computational
 problem to solve here is NP-hard. We then propose an ILP
 (Integer Linear Programming) formulation for addressing and
 evaluate it throughout various simulations of the SlapOS system
 in a realistic volunteer computing context.
 Index TermsÑEnergy efficiency, performance evaluation and
 benchmarking , cloud computing, Energy consumption, Volun-
 teer and desktop grid computing, Linear programming and
 optimization.
 I. MOTIVATIONS
 SlapOS [1] is an open source Cloud Operating system
 which was inspired by recent research in Grid Computing [2]
 and in particular by BonjourGrid [3], [4], a meta Desktop
 Grid middleware for the coordination of multiple instances
 of Desktop Grid middleware. Figure 1 shows the current
 architecture. SlapOS is based on two types of nodes: SlapOS
 Nodes and SlapOS Master. SlapOS is an hybrid cloud in the
 sense that each of its nodes can be dedicated (Data center in
 Figure 1) or dynamically provisioned from volunteers (Home
 cloud in Figure 1).
 The MasterÕs role is to install applications and run processes
 on SlapOS nodes. It acts as a central directory of all SlapOS
 Nodes, knowing where each SlapOS Node is located and
 which software can be installed on each node. The role of
 SlapOS Master is to allocate processes to SlapOS Nodes. In
 comparison to the traditional cloud view, SlapOS innovates
 in considering the possibility to build data centers-like with
 dedicated and volunteer nodes. The usage of volunteer nodes
 can improve the cloud elasticity at lower prices. Indeed,
 for managing temporal overhead in users requests or extra-
 storage demands, it might be cheaper for the cloud owners,
 to negotiate the subscription of volunteer nodes, instead of
 buying new machines. Moreover, we can benefit from the fact
 that volunteer machines might consume less energy.
 Regarding fault tolerance issues, volunteer nodes imply that
 we maintain multiple active copies for each application in
 order to have a minimum of service availability. Doing so
 however, we increase the global energy consumption. The
 purpose of this paper is to optimize these excesses in energy
 consumption by adequate placements of cloud applications.
 In this paper, we propose two models for optimizing the run
 of applications in our volunteer context. In the first model,
 we assume that we have a finite number of homogeneous
 volunteers nodes on which our applications can be run. Ho-
 mogeneous volunteers means that the computing nodes used
 in the cloud platform have the same specification. This implies
 for instance that each node can run the same maximal number
 of applications and that each machine is assumed to have
 similar (or identical) power consumption characteristics. Het-
 erogeneous volunteers means that the computing nodes may
 have different specifications e.g. different power consumption
 characteristics. Considering the homogeneous case first then
 the heterogeneous case is conventional in distributed comput-
 ing. Results obtained in the analysis of the homogeneous case
 are extended in the heterogeneous case. Our modeling in the
 homogeneous case also serve as introducing the discussion and
 the key concepts. This step by step methodology can also help
 understand the rationale of the modeling and the refinement
 process.
 Whatever case we consider, the objective is to determine
 which application to run in each node, in order to minimize
 the total overhead in energy consumption caused by these runs.
 In the homogeneous case, we show that the key computational
 challenge consists in finding a feasible solution when it exists.
 In the heterogeneous case, we show again how to find a
 feasible solution in polynomial time. But in comparison to the
 homogeneous case, the key computational problem to solve
 2013 International Conference on Cloud Computing and Big Data
 978-1-4799-2829-3/13 $26.00 © 2013 IEEE
 DOI 10.1109/CLOUDCOM-ASIA.2013.78
 52978-1-4799-2830-9/14 $31.00 © 2014 IEE
here is NP-hard. We then propose an ILP formulation for
 addressing it and evaluate it throughout various simulation of
 the SlapOS system in a realistic volunteer computing context.
 We would like to assert that the paper is about mod-
 eling first and not about experiments on a real system.
 However, we estimate that our modeling is a good com-
 promise between the parameters we consider impacting the
 phenomenon and the realism of current cloud technolo-
 gies. For instance, our parameter settings consider the Sla-
 pOS cloud system running in production and available at
 https://slapos.cloud.univ-paris13.fr.
 The organization of the paper is as follows. In section 2,
 we introduce some applications of our work and we give
 further motivations. We formalize the energy consumption
 problem in our cloud system in section 3 and section 4, step
 by step and we conduct extensive experiments under different
 configurations. Section 5 is about related works. Section 6
 concludes the paper.
 Fig. 1. The SlapOS Architecture.
 II. APPLICATIONS OF OUR WORK
 SlapOS has been ported on a Nexus S mobile phone and
 under Android 4.0.4. Mobile phones are potential volunteers
 for hosting applications! The following experiment has been
 made by the SlapOS team. They deployed the Wordpress
 (which contains an Apache server) application on the Nexus
 and the operating system was a ÕchrootedÕ Debian. The
 standby processor option (Wave Lock) was disabled and Wifi
 was enabled (Wifi Lock). A script was coded for writing the
 date in a file every 5 minutes. They measured the battery life
 when Wordpress was running and when it was not running.
 The script was running in these two cases.
 They noticed a battery life of 22 hours without Wordpress
 and 12 hours with Wordpress running. They also noticed that
 starting a new Wordpress instance took few seconds with
 Android. These observations reinforce our initial intuitions
 about the current technology: a) SlapOS applications may
 reduce drastically the battery life. This high penalty might
 not motivate volunteers to keep their nodes in SlapOS clouds
 b) the penalty in restarting an application when needed is
 not prohibitive. In another words, we could imagine at large
 scale to use volunteers nodes (Tablets, Smartphones, PCs) for
 hosting Web applications under the condition that we know
 how to optimize the energy consumption.
 This work is a first stage for improving the SlapOS
 scheduler and application manager in this direction. In its
 actual version, SlapOS implements the following behavior.
 For joining a SlapOS cloud, any new volunteer must first
 register its machine to a SlapOS master node by declaring
 various parameters such as the memory space, disk space,
 CPU type, location of the machine. . . Then, the volunteer can
 choose from the catalog of SlapOS applications some that he
 accepts to install on his machine. Finally, the volunteer can let
 his machine opened for other users (the machine is registered
 as public) or not. For any user, SlapOS maintains a list of
 executable applications. These applications are those that the
 user accepted to install or those that are located on machines
 nodes registered as public.
 Our study considers the restricted setting where SlapOS
 manages a finite set of client/server applications on volunteer
 nodes. We call such a system a volunteer cloud. At each
 time instant, we must run multiple instances of the server
 part of each application on volunteers nodes. This is both
 for managing the flow of user requests and preventing faults
 in execution. However these runs are energy consuming. For
 reducing the consumption, we propose at any time instant
 to only start on volunteer nodes, a subset of servers (we
 are referring to the server part of applications) among those
 that can be started. Naturally, these servers must be on
 available nodes and will be used for all client executions.
 Since volunteers nodes can get disconnected at anytime, we
 need in this strategy to constantly adapt the subset of servers
 that are maintained active. In a technical viewpoint, this might
 require to implement mechanisms for data migration. We do
 not consider this issue in this paper.
 In summary, given a set of applications, a set of volunteer
 nodes with their availability distribution, the objective of this
 work is to propose a strategy for making the servers running
 and available on each volunteer node in order to minimize
 the global energy consumption. In the mid term, we expect to
 include such solutions into the SlapOS system.
 III. PROBLEM DESCRIPTION Ð FIRST REFINEMENT
 A. Initial Assumptions
 We have N client/servers applications. At each time instant,
 we must maintain available k instances (or copies) of the
 server part of each application. In practice, a challenging
 question consists in determining the good value of k. This
 requires a large system analysis that we do not consider in
 this paper. We assume in our modeling that k is a known
 input.
 The applications can be run on m volunteers machines. We
 assume that the time is discretized in periods of size Æt for
 which we have the availability of machines. In each period,
 this is given by the vector M of size m (m is the number of
 machines) such that Mi = 1 when the machine is available
 in the interval of time; Mi = 0 otherwise. We are interested
 in finding, for any time interval [t, t + Æt], the applications
 53
to keep active on each machine. Let us remark that, in doing
 this, we do not consider the global optimization problem in
 the multiple time intervals and that, in practical cases, m is
 far larger than k.
 We admit a piece-wise constant model for the energy
 consumption. At any time instant, if there is no application
 running on machine i, its power consumption is P basei . If
 however there is an application j running on machine i,
 we have an extra power consumption of P ei,j(t). We can
 relate this model to the one used in [5]. In this solution,
 the power consumption for a given computer h is given by
 Cminh + f ? (Cmaxh ?Cminh ). Cminh corresponds to the power
 used when the host is idle and Cmaxh is the power consumption
 when the host is fully used. Our model generalizes this
 formulation by not considering an explicit formulation of the
 extra consumption led by applications run.
 We denote by Pi,j(t) the power consumption of machine i
 at time t running application j. From our previous assumption,
 we have Pi,j(t) = P basei +P ei,j(t). On machine i, the induced
 energy consumption in interval [t, t +Æt] is given by:
 Ei,j(t, t +Æt) = º t+Ætt Pi,j(t).dt
 =
 º t+Æt
 t (P basei + P ei,j(t)).dt
 = Æt.P basei +
 º t+Æt
 t P
 e
 i,j(t).dt
 B. First Modeling
 We assume that machines are identical. This implies that the
 value of Æt.P basei is the same, whatever machine. On identical
 machines, it is also normal to expect a low variation in the
 energy consumed by a same application on different machines.
 This means given an application j and two machines i, i? that
 we have
 º t+Æt
 t
 P ei,j(t)dt 
 º t+Æt
 t
 P ei?,j(t)dt. Therefore, we
 propose to use a constant E+j =
 º t+Æt
 t P
 e
 i,j(t).dt ?i, j.
 Admitting that we have the constants E+j and the availabil-
 ity matrix in a time interval [t, t + Æt], we are looking in
 this first modeling for an assignment of applications copies
 that will minimize the excess in energy consumption caused
 by applications run. For estimating this excess, we consider
 that the real-time power consumption is linear with respect to
 the number of application run on each machine. Therefore,
 let us admit that on each machine i, we assigned copies
 of the applications Zi ? {1, . . . , N}; the excess of energy
 consumption (that we want to minimize) is:
 m·
 i=1
 ·
 j?Zi
 E+j .Mi
 Let us remark that to assign an application copy on a
 machine means that we maintain this application active on it.
 We include Mi in this formulation because there is no energy
 consumption led by machines that are not available. There are
 some additional constraints that must be fulfilled in order to
 ensure that any assignment Z1, . . . , Zm is valid. The Figure 2
 presents them.
 C1: Applications copies must be put on machines that
 are available;
 C2: k copies of each applications must be assigned on
 exactly k distinct machines;
 C3: On any machine i, there is a constant number q
 stating the maximal number of application copies that
 can be assigned to it.
 Fig. 2. Constraints in the first modeling
 The idea with constraint C2 is to ensure that we have enough
 replications executed for any application while ensuring that
 copies of the same applications are not put on the same
 machine. Let us remark that there is no interest in putting all
 applications copies in the same machine since an execution
 fault that occurs in this machine can cause the unavailability
 of the application. A parameter q is introduced in constraint
 C3 for keeping a reasonable CPU load on machines. Indeed,
 the more we assign application copies on a machine, the lower
 is their service response time. For this first modeling we have
 the following results:
 Fact 3.1: Any feasible solution will lead to the same total
 energy consumption. This cost is equal to
 N·
 j=1
 E+j .k.
 This simple observation suggests that we can find an optimal
 solution by looking just for a feasible solution.
 Property 3.2: The conditions
 m·
 i=1
 Mi ³ k and
 (
 m·
 i=1
 Mi).q ³ N.k are necessary and sufficient for the
 existence of a feasible solution.
 Proof: If k <
 m·
 i=1
 Mi then, it will be impossible to put
 applications copies on distinct machines. The constraint C1
 will be violated. Given the availability matrix, the total number
 of copies that can be run in the time interval [t, t + Æt] is
 (
 m·
 i=1
 Mi).q. There is a feasible solution only of this number
 is at least equal to the number of application copies (N.k)
 Proposition 3.3: If a feasible solution exists, then it can be
 found in O(N.k + m).
 Proof: Indeed, a feasible solution can be obtained from
 Algorithm 1 (see page 4).
 The worst case complexity of this algorithm is in O(N.m).
 However, if there exist a feasible solution, then we will always
 have M [L[i]] = 1 and |ZL[i]| < q in executing the while loop
 (because of the ordering). Since the ordered list can be created
 in O(m), we have the proof.
 IV. INTRODUCING A FORM OF HETEROGENEITY Ð SECOND
 REFINEMENT
 A. Second modeling
 Let us now consider a setting where machine are het-
 erogeneous. Our initial assumption states that the energy
 consumption led by the run of application j in machine
 54
Data: A vector T [1 . . . N ] of application copies; the
 availability matrix M [1 . . . m]; m empty sets
 Z1, . . . , Zm
 Result: An assignment Zi for each machine i
 1 Create and ordered list L of machine indices (1, . . . , m)
 from the available to the non available
 2 for j ? {1, . . . , N} do
 3 T [j] = k
 4 end
 5 for j ? {1, . . . , N} do
 6 i = 0
 7 while (T [j] ³ 0 and i < m) do
 8 if (M [L[i]] = 1) and (|ZL[i]| < q) then
 9 ZL[i] = ZL[i] ? {j}
 10 T [j] = T [j]? 1
 11 end
 12 i = i + 1
 13 end
 14 end
 Algorithm 1: Feasible solution in homogeneous case
 i is Æt.P basei +
 º t+Æt
 t P
 e
 i,j(t).dt. Since the machines are
 heterogeneous, both terms of this sum might differ for an
 application ran in distinct machines. Let us denote the first
 term by Ebasei and the second by E+i,j .
 In the first modeling, we stated that the minimization goal
 was to reduce the excess in energy consumption led application
 runs. This formulation is natural since either the machine
 will be available and not used or, it will be available and
 exploited in our assignment. It is not easy however in the
 heterogeneous case to find a good formulation for capturing
 the excess in energy consumption. Indeed, the machines here
 have different energy consumptions when they are idle. It
 is important to include this difference in our minimization
 objective. Therefore, in the heterogeneous case, what we are
 looking for is an assignment Z1, . . . Zm that minimizes
 m·
 i=1,|Zi|>0
 Ebasei +
 m·
 i=1
 ·
 j?Zi
 E+i,j
 Any assignment here must again respect the constraints de-
 fined on Figure 2. We however consider here that the maximal
 threshold above which there is a performance degradation
 due to the execution of multiple application can differ per
 machines. We then reformulate the constraint C3 as follows
 in Figure 3.
 C3: On any machine i, there is a constant number qi
 stating the maximal number of application copies that
 can be assigned to it.
 Fig. 3. Constraints C3 in the second modeling
 We now discuss about the feasibility and the complexity in
 the heterogeneous case.
 B. Feasibility and complexity in the heterogeneous case
 As for the homogeneous one, we cannot always ensure that
 there is a feasible solution in the heterogeneous case. We
 can also establish that such a solution however will exist if
 the conditions
 m·
 i=1
 Mi ³ k and (
 m·
 i=1
 Mi.qi) ³ N.k are met.
 Finally, we have the following result:
 Proposition 4.1: A feasible solution in the second modeling
 can be found in O(N.k + m2).
 Such a solution can be obtained by adapting our previous
 algorithm for building a feasible solution in the homogeneous
 case. We propose Algorithm 2.
 The initial ordering that we did ensure that we will always
 have (M [L[i]] = 1) and (|ZL[i]| < qL[i]). Since L can be
 created in O(m2), we have the result.
 Proposition 4.2: The feasible solution returned by Algo-
 rithm 2 is not always optimal.
 Proof: There is indeed a simple counter example. Let
 us consider the case where we have N application and one
 copy for each k = 1. Let us also assume that we have
 l machines available such that qi > N . Finally, let us
 assume that ?i, j, Ebaseim + E+im,j > Ebasei + E+i,j where
 im = arg{max{qu|u is available }
 u?1,...,m
 }. It is clear in this setting
 that the feasible solution returned by Algorithm 2 will not be
 optimal. One can criticize this proof since we chose k = 1.
 However the argument used in it can easily be generalized
 even for k > 1.
 Data: A vector T [1 . . . N ] of application copies; the
 availability matrix M [1 . . . m]; m empty sets
 Z1, . . . , Zm
 Result: An assignment Zi for each machine i
 1 Create and ordered list L of machine indices (1, . . . , m)
 in using as first criteria the availability and second one
 the value of qi (from greater to smaller)
 2 for j ? {1, . . . , N} do
 3 T [j] = k
 4 end
 5 for j ? {1, . . . , N} do
 6 i = 0
 7 while (T [j] ³ 0 and i < m) do
 8 if (M [L[i]] = 1) and (|ZL[i]| < qL[i]) then
 9 ZL[i] = ZL[i] ? {j}
 10 T [j] = T [j]? 1
 11 end
 12 i = i + 1
 13 end
 14 end
 Algorithm 2: Feasible solution in heterogeneous case
 Since any feasible solution will not necessarily be optimal,
 we need to find another approach for obtaining the optimal
 solution in the heterogeneous case. The following result shows
 that even in feasible cases, it will not be simple to find an
 optimal solution.
 55
Theorem 4.3: The computation of the optimal solution in
 the heterogeneous case is NP-hard
 Proof: The proof is obtained by a reduction to the
 minimum knapsack problem. Let us indeed assume that ?i, j,
 E+i,j is the same. Then the problem can be reduced to the
 one of choosing a subset of machines with minimal base
 energy consumption and whose global capacities is enough
 for executing copies. More formally let us consider a variable
 xi defined as follows:
 xi =
 {
 1 if at least one application is assigned on i;
 0 otherwise.
 Then the optimization problem to solve is the following:
 Minimize
 m·
 i=1
 (Ebasei ).xi
 such that
 1) xi ? {0, 1}, i = 1, . . . , m
 2)
 m·
 i=1
 xi.qi ³ N.k
 In this formulation we did not include the energy consump-
 tion induced by the execution of applications since its global
 cost will be the same whatever feasible solution we have.
 This formulation corresponds exactly to the minimum knap-
 sack problem. Since this latter problem is NP-hard, we have
 the proof.
 This NP-hardness reduction is based on the idea that it is
 hard to determine the right subset of machines that should
 be used in order to reduce energy consumption. In the case
 however where this subset is known, the problem is not
 necessarily hard. More formally we have the following result:
 Theorem 4.4: If k = 1 and we have the subset of machines
 used in the optimal solution, then, the optimal feasible solution
 can be computed in pseudo-polynomial time.
 Proof: The proof is obtained from a reduction to the
 assignment problem. Let us assume that in the optimal so-
 lution, applications must be deployed exactly on the machines
 1, . . . , p. We propose to associate the heterogeneous case with
 the following assignment problem:
 For each machine i, we create qi Óunit machinesÓ i1, . . . , iqi
 such that on each of them, we can run at most one application
 at a given time instant. Given any application j, its energy
 consumption on a unit machine iv is E+iv,j = E
 +
 i,j .
 Let us consider that Q =·pi=1 qi. If Q > N , we also create
 Q?N virtual applications such that on any unit machine iv ,
 the energy consumption of the virtual copy is 1.
 The goals of this assignment problem are the following:
 1) there is one application assigned to any unit machine;
 2) any application is assigned to only one unit machine;
 3) the sum of the energy consumed by applications on
 assigned machine is minimized.
 It straightforward to notice that the creation of this as-
 signment problem from the initial heterogeneous problem can
 be done in pseudo polynomial time. Virtual applications in
 this reduction serve for having a bijection as in the classical
 assignment problem.
 From any solution of this assignment problem, we can
 derive a solution for the original heterogeneous problem. This
 can be done by putting the application j on the machine i
 if in the related assignment problem, j is assigned to a unit
 machine iv . Since this assignment problem can be solved in
 O(Q3) by the mean of the Hungarian algorithm, we have the
 proof.
 Thus, in the case where k = 1, we can find an optimal
 solution by building 2m subset of machines and solving on
 each an assignment problem.
 The algorithm proposed for k = 1 can be generalized for
 an arbitrary k. The idea is to consider successive allocations
 of one copy of each application until all copies are deployed.
 This means that we solve k assignment problems. The solution
 that results in the lowest energy consumption is retained. In
 doing so, we obtain a local solution to the global optimization
 problem obtained however in exponential time. If this can be
 interesting for small values of k, we do believe that if we
 have to spend an exponential time, near exact or exact solution
 are preferable. Consequently, in what follows, we propose a
 mixed integer linear programming model that can be used in
 the general case.
 C. An integer linear programming model for the second
 modeling
 The proposed model are based on the following variables
 and constants:
 ¥ For any machine i and application j, we have a decisional
 variable Ai,j . If Ai,j = 1, then the application j must be
 deployed on machine i. Otherwise, Ai,j = 0
 ¥ For any machine i, we have a 0-1 variable xi that will
 comprise 1 if at least one application is assigned to it.
 We derive the following model:
 z =
 m·
 i=1
 N·
 j=1
 E+i,j .Ai,j +
 m·
 i=1
 Ebasei .xi (1)
 min z s.t: (2)
 Ai,j ? {0, 1} ?i, j (3)
 xi ? {0, 1} ?i (4)·m
 i=1 Ai,j .Mi = k ?j (5)·N
 j=1 Ai,j .Mi ² qi.xi ?i (6)
 Let us remark here that the equation 3 ensures both that the
 constraints C1 and C2 are met. The equation 6 ensures that
 we can assign at least one qi applications on the machine i.
 Moreover, an assignment of at least one application implies
 to set xi to 1. The proposed formalization is a standard 0-
 1 programming problem. The next section is devoted to our
 experiments.
 56
D. Experiments
 We performed some experiments in order to observe:
 ¥ How the ILP runtime behave with a huge number of
 machines (because it is one of the key factors that usually
 have impact on the user experience);
 ¥ How is the energy distributed in using our allocation
 modeling.
 For observing the energy distribution, we will use two
 notions: the distribution of the energy and the service distribu-
 tion. The distribution of the energy captures the total energy
 consumption when doing experiments with a given number of
 nodes. The service distribution metric returns for each machine
 the total number of applications copies that were run on them
 during the experiments.
 E. Experimental Settings
 We created various configurations with different number
 of machines ( from 100 to 4000). Given a machine number
 we created an availability matrix for a period of 24 hours
 discretized in 1 hour. We assumed that the availability of our
 machines obey to a Weibull distribution with the parameters
 ? = 0.393, and k = 2.964 as in [6]. We assumed while using
 this distribution, that a machine is available if the probability
 returned by the implementation is respectively 0.2, 0.4, 0.6, or
 0.8 according to the experiment. In this way we obtain the
 availability of machines during one hour.
 We assumed that we have 20 applications with 3 copies.
 The queue lengths and the energy values (E+, Ebase) were
 generated with a Gaussian law. The parameters used for
 generation are given in Figure 4. Finally let us recall that by
 the mean of our generation approach for the matrix availability,
 it might happen that some machines donÕt be used. Thus,
 we reported in the same figure the mean effective number
 of machines used for each configuration.
 F. Experimental Results
 We used Matlab for implementing and generating optimal
 solutions of our Integer Linear Programming model.
 In Figure 7 we present the evolution of the cumulative
 energy consumed due to application runs, when considering
 various number of machines. The accumulated energy dimin-
 ishes as we increase the number of machines. But, the reason
 for this is simple: the more we have machines, the more we
 have available ones, suitable for running our application. This
 also means that the consumed energy decreases here because
 we used a random setting generation. Finally let us precise
 that in this plot and in the others, it is the cumulative results
 observed for a period of 24 hours that are reported.
 For the service distribution metrics, we plotted the figure
 where the x axis is machine node, y axis is hours (1 to 24), z
 axis is the accumulated application instances executed on the
 node for each hour.
 Figures 5 and 6 are samples of our results. This kind of
 figure, more exactly the shapes, may help the applications
 provider to find some trends for application distribution and
 migrations. From the service distribution results, we can see
 that the accumulated service instance execution decreases
 when the number of machines increases (from 100 to 500).
 It is intuitive that for large scale cloud system, more nodes
 can reduce the workload on individual node and balance the
 workload. However, Figures 5 and 6 also show that more
 energy is consumed if few machines (acceptable probability:
 0.4) are available for the same amount of application replica-
 tions in the heterogeneous volunteer cloud. One explanation
 is that with less machines we have less choice to allocate
 services on machines. Therefore, a larger system scale is
 not only important for load balancing but also important for
 application offloading; this is because we can find optimal
 application placement such that the energy consumption is also
 minimal (Fig. 7). Tabular 4 summarizes the above results and
 discussion.
 If we extend the system scale to very large (i.e., 1 million
 nodes), itÕs nearly impossible for the solver to find an optimal
 application placement solution in 1 minute due to the huge
 searching space. Actually in Matlab (version 2010b), it costs
 almost 28.3613 seconds to generate such solution with 20
 applications and each application with 3 copies in a system
 with 3000 node(available node number is 2607). We can see
 in Fig.8 that the solution time increases almost linearly with
 the number of available nodes. In our simulation, when the
 number of available nodes is 3474 (accept prob. is 02. for
 4000 total nodes in one case), then the size of the problem is
 too large for the current solver in Matlab without hacking into
 the optimization solver. Moreover, if we increase the scale of
 the problem in other dimension, i.e., the number of application
 types from 20 to 100, or increase the number of copies from
 3 to upper value, it cost much more time to generate such
 solution. Therefore, we should ÕtradeoffÕ between the real-
 time response of the application allocation and the optimal
 allocation with less energy consumption.
 Fig. 5. Services distribution(100 nodes,acceptable probability=0.2)
 V. RELATED WORK
 The context of our work is related to volunteer computing
 and Desktop Grid Computing [2]. Desktop Grids (DG) have
 57
Node acc.prob av.nodes ass.apps µe ?e avg qi. stddev qi energy solu.time
 100 0.2 93 60 0.4991 0.2868 5.4255 2.6940 1.1606 0.6335
 100 0.4 40 60 0.5183 0.2912 5.2826 2.7862 2.9799 0.3763
 500 0.2 431 60 0.5081 0.2873 5.4362 3.0287 0.3274 4.0052
 500 0.4 168 60 0.5081 0.2891 5.4043 2.8258 0.6866 1.6589
 1000 0.2 873 60 0.5014 0.2894 5.5487 2.8081 0.1192 6.6276
 1000 0.4 362 60 0.5007 0.2893 5.5470 2.9084 0.3069 3.3893
 2000 0.2 1749 60 0.4983 0.2884 5.5597 2.8579 0.0657 17.3444
 2000 0.4 682 60 0.4968 0.2898 5.6012 2.8022 0.1651 5.5039
 3000 0.2 2607 60 0.4980 0.2894 5.4956 2.8779 0.0387 28.3613
 3000 0.4 1059 60 0.5005 0.2898 5.3654 2.8180 0.1003 8.2305
 4000 0.4 1426 60 0.5000 0.2884 5.3212 2.8794 0.0715 13.0007
 Fig. 4. Overall Statistics for the experiments (10 examples of one scheduling interval). Here acc.prob is the accept probability, av.nodes is the number of
 available nodes, ass.apps is the number of assigned application instances,solu.time is the time consumed by the Matlab solver for one solution, µe and ?e
 are the mean and standard deviation value used for generating machines energy consumptions.
 Fig. 6. Services distribution (3000 nodes,acceptable probability=0.4)
 Fig. 7. Energy consumption of different allocations with same application
 instances on different available nodes (energy consumption is normalized)
 been successfully used for solving scientific applications at
 low cost. DGs middleware such as Condor [7], BOINC [8],
 XtremWeb [9], OurGrid [10] provide researchers a wide
 range of high throughput computing systems by utilizing idle
 resources. However, we did not find in this context a work
 that aims at optimizing the energy consumption in a setting
 similar to the one considered in this paper.
 In [11] authors introduce a synthesis of the usage of meth-
 Fig. 8. Solution time(in seconds) for one scheduling interval (mean value
 of 20 measuring on the experimenting laptop)
 ods and technologies used for energy-efficient operation of
 computer hardware and network infrastructure. They consider
 the ICT field in general and they focus on energy-aware
 scheduling in multiprocessor and grid systems, on the power
 minimization in clusters of servers, on the power minimization
 in wireless and wired networks. Clouds are not specifically
 considered in their work.
 In [12] authors propose an efficient resource power manage-
 ment policy for virtualized Cloud data centers. The objective is
 to continuously consolidate virtual machines. They show that
 the dynamic reallocation of virtual machines brings substantial
 energy savings. They propose four criteria for migrating the
 virtual machines. Authors do not discuss about the quality
 of the solution and they recognize that despite the fact that
 they use heuristics, the algorithms provide good experimental
 results. We believe that our solution can be extended for
 consolidating virtual machines in a volunteer context.
 In [13] Beloglazov and Buyya propose online algorithms
 for virtual machines placement with guaranty of performance.
 They conduct competitive analysis and prove competitive
 ratios of optimal online deterministic algorithms for a single
 virtual machine migration and dynamic virtual consolidation
 problems. An interesting future work is to extend this result
 to volunteers clouds contexts.
 Beaumont, Eyraud-Dubois and Larcheveöque consider in
 [14] the problems of reliable service allocation in clouds.
 Among the different papers introduced in this section, it is
 a paper that cover similar problems than ours. They consider
 first that mapping virtual machines having heterogeneous com-
 58
puting demands onto physical machines having heterogeneous
 capacities can be modeled as a multi-dimensional bin-packing
 problem. They assume that each virtual machine comes with
 its failure rate (i.e. the probability that the machine will fail
 during the next time period). But they do not consider that
 services should be duplicated on different machines to derive
 a robust solution.
 The authors focus on formal specifications of the problems
 as we usually do before deriving complexity results, then they
 show and prove complexity results. For instance, to prove
 Theorem 5.1 they use a reduction to the 3-partition problem
 [15]. For the experiments they consider only n = 15 machines,
 far below our study, because they say that computing the
 reliability of an allocation takes time 2n.
 In [5] authors study the problem of energy-aware resource
 allocation for hosting long-term services or on-demand com-
 pute jobs in clusters. They do not capture the possibility
 of copies but they have a constraint on the RAM available
 on each node. The objectives and constraints lead to greedy
 algorithms. This work is similar to our work but the main
 differences are that for each job, allocated to a machine, we
 must have to decide the fraction of CPU to use and we need
 also for an estimate of the RAM consumption of the job. The
 fraction of CPU is for studying a form of heterogeneity but
 it does not include all the cases that can be derived from our
 heterogeneous modeling.
 We do not model the commonly-used Dynamic Voltage
 and Frequency Scaling (DVFS) power management technique
 [16], [17] as it is now available on most processors including
 processors for smartphones and tablets. DVFS is able to reduce
 the power consumption of a CMOS integrated circuit, by
 scaling the frequency at which it operates, and when the load
 varies dynamically.
 VI. CONCLUSION
 In this paper we explain how to optimize the energy
 consumption in a context where volunteers nodes are used for
 building a cloud. Our proposal consists in two models: one
 in homogeneous context and the other one in heterogeneous
 ones. We then propose various experiments using Matlab for
 solving the combinatorial problem in order to observe the
 system behavior suggested by our models. The simulation
 results show that our modeling is effective and suitable for
 real volunteer cloud environment.
 For future, since we need to offload the copies of applica-
 tions to various machines with different availability patterns,
 we should also consider more realistic issues, including the
 performance and user experiences on the devices network
 latency, and data transportation and communication time.
 We can also use a weighted objective function for energy
 consumption and other performance overheads like application
 migrations. The network-dependent factors have significant
 impact on user-perceived performance of the hosting appli-
 cations. The network communications also consumes lots
 of energy. The cellular connectivity is one of the biggest
 contributors of energy consumption in a smartphone [18]. The
 task affinity and QoS guarantees are also important in real
 volunteer cloud environment.
 VII. ACKNOWLEDGMENT
 The funding supports of this work by Natural Science Fund
 of China (No. 61003077), Natural Science Fund of Zhejiang
 Province (Y1101092), and Research Fund of Department
 of Education of Zhejiang Province (No. GK100800010) are
 greatly appreciated. In France, this work is funded by the FUI-
 12 Resilience project from the ministry of industry.
 REFERENCES
 [1] Jean-Paul Smets-Solanes, Christophe Ce«rin, and Romain Courteaud.
 SlapOS: A Multi-Purpose Distributed Cloud Operating System Based
 on an ERP Billing Model. In Hans-Arno Jacobsen, Yang Wang, and
 Patrick Hung, editors, IEEE SCC, pages 765Ð766. IEEE, 2011.
 [2] Christophe Ce«rin and Gilles Fedak. Desktop Grid Computing. Chapman
 & Hall/CRC Numerical Analysis and Scientific Computing Series, 2012.
 [3] Heithem Abbes, Christophe Ce«rin, and Mohamed Jemni. BonjourGrid
 as a Decentralised Job Scheduler. In APSCC, pages 89Ð94. IEEE, 2008.
 [4] Heithem Abbes, Christophe Ce«rin, and Mohamed Jemni. BonjourGrid:
 Orchestration of multi-instances of grid middlewares on institutional
 Desktop Grids. In IPDPS, pages 1Ð8. IEEE, 2009.
 [5] Damien Borgetto, Henri Casanova, Georges Da Costa, and Jean-Marc
 Pierson. Energy-aware service allocation. Future Generation Comp.
 Syst., 28(5):769Ð779, 2012.
 [6] Bahman Javadi, Derrick Kondo, Jean-Marc Vincent, and David P.
 Anderson. Discovering Statistical Models of Availability in Large
 Distributed Systems: An Empirical Study of SETI@home. IEEE Trans.
 Parallel Distrib. Syst., 22(11):1896Ð1903, 2011.
 [7] Ali R. Butt, Rongmei Zhang, and Y. Charlie Hu. A self-organizing flock
 of condors. J. Parallel Distrib. Comput., 66(1):145Ð161, 2006.
 [8] David P. Anderson. BOINC: A System for Public-Resource Computing
 and Storage. Grid Computing, IEEE/ACM International Workshop on,
 0:4Ð10, 2004.
 [9] G. Fedak, C. Germain, V. Neri, and F. Cappello. XtremWeb: a generic
 global computing system. Cluster Computing and the Grid, 2001.
 Proceedings. First IEEE/ACM International Symposium on, pages 582Ð
 587, 2001.
 [10] Nazareno Andrade, Walfredo Cirne, Francisco Vilar Brasileiro, and
 Paulo Roisenberg. OurGrid: An Approach to Easily Assemble Grids
 with Equitable Resource Sharing. Job Scheduling Strategies for Parallel
 Processing, 9th International Workshop, JSSPP 2003, Seattle, WA, USA,
 June 24, 2003, Revised Papers, 2862:61Ð86, 2003.
 [11] Andreas Berl, Erol Gelenbe, Marco Di Girolamo, Giovanni Giuliani,
 Hermann de Meer, Dang Minh Quan, and Kostas Pentikousis. Energy-
 Efficient Cloud Computing. Comput. J., 53(7):1045Ð1051, 2010.
 [12] Anton Beloglazov and Rajkumar Buyya. Energy Efficient Allocation of
 Virtual Machines in Cloud Data Centers. In CCGRID, pages 577Ð578.
 IEEE, 2010.
 [13] Anton Beloglazov and Rajkumar Buyya. Optimal online deterministic
 algorithms and adaptive heuristics for energy and performance effi-
 cient dynamic consolidation of virtual machines in Cloud data centers.
 Concurrency and Computation: Practice and Experience, 24(13):1397Ð
 1420, 2012.
 [14] Olivier Beaumont, Lionel Eyraud-Dubois, and Hubert Larcheveöque.
 Reliable service allocation in clouds. In IPDPS, pages 55Ð66. IEEE
 Computer Society, 2013.
 [15] M. R. Garey and David S. Johnson. Computers and Intractability: A
 Guide to the Theory of NP-Completeness. W. H. Freeman, 1979.
 [16] Mark Weiser, Brent B. Welch, Alan J. Demers, and Scott Shenker.
 Scheduling for Reduced CPU Energy. In OSDI, pages 13Ð23. USENIX
 Association, 1994.
 [17] Etienne Le Sueur and Gernot Heiser. Dynamic Voltage and Frequency
 Scaling: The Laws of Diminishing Returns. In Proceedings of the 2010
 Workshop on Power Aware Computing and Systems (HotPowerÕ10),
 Vancouver, Canada, Oct 2010.
 [18] Aaron Carroll and Gernot Heiser. An analysis of power consumption in
 a smartphone. In Proceedings of the 2010 USENIX Annual Technical
 Conference, pages 1Ð12, Boston, MA, USA, Jun 2010.
 59

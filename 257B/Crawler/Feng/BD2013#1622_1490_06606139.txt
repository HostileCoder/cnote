Wavelet Network based Online Sequential Extreme 
Learning Machine for Dynamic System Modeling 
 
Dhiadeen M Salih1, S. B. Mohd Noor2, M. H. Marhaban3, R. M. K. Raja Ahmad4 
Department of Electrical and Electronic Engineering,  
Universiti Putra Malaysia, 43400 Serdang, Selangor, Malaysia 
1dhiadeen@gmail.com, 2samsul@eng.upm.edu.my, 3hamiruce@eng.upm.edu.my, 4kamil@eng.upm.edu.my                 
 
 
Abstract— Wavelet network (WN) has been introduced in 
many applications of dynamic systems modeling with different 
learning algorithms. In this paper an online sequential extreme 
learning machine (OSELM) algorithm adopted as training 
procedure for wavelet network based on serial-parallel nonlinear 
autoregressive exogenous (NARX) model. The proposed model 
used as system identification for nonlinear dynamic systems. The 
main advantage of OSELM over conventional algorithms is the 
ability of updating network weights sequentially through data 
sample-by-sample in a single learning step. This attains good 
performance at extremely fast learning. The initial kernel 
parameters of WN played a big role to ensure fast and better 
learning performance. Simulation of the proposed scheme 
applied to nonlinear dynamic systems validates that WN-OSELM 
is superior in terms of identification accuracy and fast learning 
ability compared to NN-OSELM. 
Keywords— Neural Network (NN), Extreme Learning Machine 
(ELM), Nonlinear ARX Model, Waveleons, Dilation, Translation. 
 
I.  INTRODUCTION  
Wavelet network is an ordinary feed-forward neural 
network with wavelet functions in the hidden layer nodes. It 
holds same properties of neural networks and wavelets 
decompositions. The hidden layer nodes, whose activation 
functions drawn from an orthogonal wavelet family called 
Waveleons. Wavelet network successfully applied in system 
identification applications, for instance black-box identification 
of nonlinear dynamic systems  [1], and a new class wavelet 
netwok based upon forward orthogonal least squares (OLS) 
algorithm [2]. However, the scope focused on the training 
algorithms that used for online training in real time applications 
instead off-line algorithms like back-propagation, which 
requires a heavy computation load and learning scheme is 
based on batch mode which is not applicable for online 
learning [3]. An on-line identification scheme proposed by [4] 
based on recursive weight learning algorithm with the growing 
network technique to adjust the weights so that the identified 
model can adapt to variations of the characteristics in the 
system. For fast training of the wavelet neural network, a PID 
back-propagation algorithm proposed in [5] try to solve two 
major limitations of the original BP algorithm, which are low 
speed and local minimum problem. Moreover, a learning 
algorithm called extreme learning machine (ELM) proposed 
for single-hidden layer feed forward neural networks (SLFNs) 
[6] with batch-mode learning algorithm based on fixed network 
architecture where the network kernels assigns arbitrarily. 
However, ELM learning method has verified noble capabilities 
to resolving regression and classification problems [6]. Lately, 
ELM techniques have received great attention in computational 
intelligence and machine learning communities, as it provides 
better generalization performance at extremely fast learning 
speed beside the ability of appraising the network parameters in 
a single learning step [7]. Wavelet neural networks with the 
extreme learning machine was introduced in [8], as a 
composite bounded non-constant piecewise activation wavelet 
function, where a differential evolution and extreme learning 
machine algorithm proposed to find the optimal network 
parameter and reduce the number of hidden nodes used in the 
network. On the other hand, an online sequential extreme 
learning machine (OSELM) developed based on ELM with 
feed forward neural networks in a unified way to deal with 
industrial applications that the training data come one by one or 
chunk-by-chunk not in batch mode [9]. In this paper, wavelet 
network with OSELM is proposed to estimate nonlinear 
dynamic systems based on parallel-serial NARX model. 
Section II introduces the WN-NARX model structure, Section 
III presents the OSELM algorithm for WN-NARX model, 
Section IV discusses the simulation results of the proposed 
scheme on nonlinear dynamic systems and Section V 
concludes the paper.   
 
II. NARX MODEL USING WAVELET NETWORK   
The nonlinear autoregressive and exogenous inputs 
(NARX) models are useful idea for nonlinear system 
identification problems. Their structures construct from modest 
building blocks of linear dynamic system and nonlinear static 
transformation, where, these combinations can take a number 
of ways to map model output by available well-known scheme, 
like Wiener and Hammerstein models and others [10]. In 
general, the nonlinear autoregressive and exogenous inputs 
(NARX) model can expressed with the following nonlinear 
equation, 
 
 
where  f  is unknown nonlinear mapping functions, ny refer to 
the number of lagged feedback predictor sequences y(k) 
(output), nu to the feedforward predictor   u(k)  (input) and e (k) 
is the fitting residual  assumed to be bounded and uncorrelated 
with the inputs. In exercise, many types of functions, such as 
( ) (1)         )()(),...1( , )(),...1()(   keunkukuynkykyfky +????=
 978-1-4673-5769-2/13/$31.00 ©2013 IEEE
polynomials splines, kernel functions, and other basis functions 
can be selected to represent the nonlinear mapping functions in 
model (1). Moreover, wavelet networks adopted to represent a 
mapping function because of its decompositions properties and 
localization ability in both time and frequency domain [2]. 
Wavelet network in NARX model can be expressed by 
expanding each functional component of the model into a 
weighted sum of the wavelet expansions, 
 
 
 
where, w refer to the wavelet network output layer weights, and 
?(t,d,X ) to the wavelet activation functions for m hidden layer 
nodes while, t  and  d  are translation and dilation vectors 
induce a multiresolution analysis for the wavelet function. In 
(2) and (3), X represents n number of regressors and n = ny + nu. 
In this work a serial-parallel NARX model structure is used as 
shown in Figure1, where the real plant outputs are used for 
prediction of the future model’s outputs so that the stability and 
approximation of the network is guaranteed  [11].  
 
Fig. 1. Seiral- Parallel WN-NARX 
 
III. WN-NARX BASED ONLINE SEQUENTIAL-ELM 
The performance of ELM provides the necessary 
background for the development of online sequential extreme 
learning machine OS-ELM for WN, where the learning 
procedure consists of two phases, namely an initialization 
phase and a sequential learning phase: 
1) Initialization phase  
Initializing wavelet mother function parameters namely 
dilation and translation is a critical issue, since this may make 
some wavelets too local (small dilations), which could make 
the components of the gradient of the cost function remarkably 
small in areas of interest and consequently effect on speed of 
convergence and the accuracy of learning algorithm  [12]. For 
that reason, it is important to choose good initial parameters 
since there will be no changes once they are initialized because 
OS-ELM based on fixed structure network [9]. In this paper, 
the initializing WN parameters done with density function and 
recursive algorithm, where the procedure based on the input 
domains of input space where the wavelets are not zero [13]. 
Consider a network with form of (2), and a domain defined by 
[a, b] containing values of the jth samples of the input vectors 
X. Now, consider the waveleon i, then the dilation parameter di 
defined as the inverse of the scale of the concerned wavelet is 
di = Si
 -1, and considering the scaling for the first waveleon as 
the center of the parallelepiped then scaling S1= 0.5 (a- b). To 
initialize t1, a point p selected between a and b so it divides 
interval [a, b] into two parts. To find the point p, the “density” 
functions ? estimated from available input output observations 
[x, f(x)] are used such that the point p to be the center of gravity 
of domain [a, b] where, 
) (4                              )()(           
/)(
 /)(
                      xd
 a
 b
 xxpa
 b
 dxdxxdf
 dxxdf
 ?(x) ∫ ?=
 ∫
 =
 Recursively, in each sub-interval [p, b] same initializing 
procedure applied for t2, S2, t3, S3, and so on for all waveleons. 
However, the number of data required to fill up appropriate 
matrix should be at least equal to the number of hidden nodes 
[9].  
2) Sequential learning phase  
     The WN-NARX structure in (2) with K arbitrary distinct 
samples of (Xj, yj) ? RK, j = 1, …,K, can generate an single 
estimated output for jth samples from weighted sum of 
waveleons output as stated below,  
) (5                            ]).,,( .......  ).,,([ 1111 mjmmmjj wXdtwXdty ?++?=
  
Since sequential learning algorithm based on fixed network 
architecture, then (5) can be written compactly as, 
     (6)                                                                                                                    ?HY =
 where, Y is an (K?1) output vector, H is (K?m)   hidden layer 
nodes (waveleons) output matrix, and ? is the (m?1) weight 
vector. 
             :    ,      :   ,       
),,(),,(
 :::
 ),,(....),,(
          
11
 11
 1111
 ??
 ?
 ?
 ?
 ??
 ?
 ?
 ?
 =??
 ?
 ?
 ?
 ??
 ?
 ?
 ?
 =??
 ?
 ?
 ?
 ??
 ?
 ?
 ?
 =
 kmmmkk
 mm
 y
 y
 Y
 w
 w
 dtXdtX
 dtXdtX
 H ?
 ??
 ??
  
Base on ELM algorithm theories [6], when the number of 
training samples equals to the number of hidden nodes, i.e. 
K=m (H is square matrix), and all hidden node parameters are 
independent from training datasets, an analytically and simple 
way the output weights ? can be calculated by inverting H in a 
single step realizing zero training error. However, when the 
number of training samples is greater than the number of 
hidden nodes (K>m), one can calculate the estimated weights 
? by using a pseudo inverse of H to give a small nonzero 
training error |e|>0 which is the difference between estimated 
and the real output. To minimize this error, the norm of error 
that include the estimated weight, ?, as a cost function F, 
 
By considering that the estimation problem in (7) becomes a 
linear in the parametric problem, the least squares methods 
may be used to solve the cost function and estimate the weights 
[14] , thus the solution for (7) is  
     (8)                                                                             Yˆ                                              †H?=
      (7)                                              ˆ    min)ˆ(                         YHF ??=?
 (3)    )]( ,... )1(  ),(  , )( ,...., )1( [       
)(2      ,,     ,    ),,( 
1
   ),( 
unkukukuynkykyX
 RXdteXiditi?
 m
 i
 iwwXf
 k
 k
 kiikkk
 ????=
 ?+∑
 =
 =
where,   H 
† is pseudo inverse of H.  The equation given by (8) 
can now be rewritten as below: 
 
Now, for the given WN-NARX structure in (2) with K arbitrary 
distinct samples, assume a chunk of initial training data set 
{(???  , ???) for ??=1, 2… ?? } is applied with m  hidden layer 
nodes, where  m <  ??, then, 
 
thus,  
 
where, 
 
 
 
 
Now, if another chunk of data arrived {(???  , ???) ? ??= ??+1, ??+2… ??+ ??}, then the minimizing problem (7) become, 
     (12)                                                 min)(                             
1
 0
 1
 1
 0
 1 ˆˆ ??
 ???
 ?
 ???
 ???
 ?
 =
 Y
 Y
 H
 H
 F ? ?
 and same as (8) and (9)   
     (13)                                                                                 
1
 0
 1
 0
 11ˆ ??
 ???
 ???
 ???
 ?
 =
 ?
 Y
 Y
 H
 H
 )(T
 T
 1?
 where, 
     (14)                                                             
  
 
                                         
1
 0 
1
 0
 1 ???
 ?
 ???
 ?
 ??
 ???
 ???
 ???
 ?
 =
 H
 H
 H
 H
 T
 T
   
 
 
To present ?0 as a function of ?1, (14) can state as: 
  
and by (11) we can get 
             (15)                                                                                                                         1
  
1 0 1 HHTT
 T+=
 Now, rewrite (13),
                                          )  ()(                                         11
 1
 11 ˆ YHYHT
 T
 0
 T
 0? +=
 ?  
and multiply the first term in the bracket by (T0T0
 -1) 
  )    ( 11
 1
 00
 1
 11 ˆ YHYHTT)(T
 T
 0
 T
 0? +=
 ??  
and using (10), then (15), 
                  )    ) ( ()(  )   ()(               110 111
 1
 11100
 1
 11 ˆ ˆˆ YHHHTTYHTT
 TTT ??? +?=+= ??
 finally, 
 
En route for generalizing of the previous arguments, assume an 
s sequences of the data set {(Xjs ,yjs) ? js = Ks +1, Ks +2…. Ks 
+Ks+1} are available, where Ks and K 1  +s  are arbitrary distinct 
samples, then a recursive algorithm for updating the least-
 squares solution written as follows: 
 
where, 
then 
and  
 
To find (Ts+1)
 -1 Woodbury formula in [15] can be used. Note 
that instead of chunk-by-chunk, the training data are received 
one by one, which means K=1 and hence the weights can be 
updated sample by sample. Finally, the proposed OS-ELM 
algorithm can be summarized in the following four steps: 
1) initialize WN parameters as described in section III.1.   
2) for the first data sequence (s = 0), determine ?s and Ts 
using (17) and (18) for the K samples.  
3) for the next data sequence (s= s+1), determine Ts+1  and 
?s+1 using (19) and (20).  
4) set ?s = ?s+1 and Ts = Ts+1   go to step3 until all training 
data finish. 
 
IV. SIMULATION EXAMPLES  
The simulation based on comparison between WN-OSELM 
and NN-OSELM applied to three nonlinear dynamic systems, 
all the trained and the tested data samples for the input output 
database are carried out in MATLAB 7.11 environment 
running in an ordinary PC with 2.27 GHZ CPU, and the hidden 
nodes activation functions are selected to be derivative of 
Gaussian function for WN and sigmoid function for NN. The 
serial-parallel NARX inputs are selected to be two feedback 
predictors (ny =2) from the real outputs of the plants, and one 
forward predictor (nu =1) from the inputs. The simulation 
carried out in two stages for both NN and WN identification 
models: stage one is the best fit performance analysis and stage 
two is network size analysis which are described respectively 
below. For the sake of simplicity we denote NN to NN-
 OSELM and WN to WN-OSELM. 
Stage 1: Best Fit Perfermances 
 In this stage, the simulation is done with seven hidden 
layer nodes for both NN and WN applied to the following 
dynamic systems to determine the best fit performance: 
A. Magnetic Levitation: The equation of motion for this 
system is  
 
 
where y(t) is the movement displacement of the magnet ball, 
i(t) is the current that controls the electromagnet flux, M is the 
mass of the magnet ball, g is the gravitational constant, the 
factor ? is a tacky friction parameter, and ? is a field power 
constant. The simulation result in Figure 2 shows notable 
                                                                    )(                                 (9)ˆ 1 YHHH TT? ?=
  (11)                                                                      )  (                                         000 HHT
 T
 =
 1  1
 1
 1
 1   
1
 1
   1
 1111
 1111
 1 :, : , 
),,(),,(
 :::
 ),,(....),,(
  ˆ
 ???
 ??
 ?
 ?
 ?
 ??
 ?
 ?
 ?
 =
 ??
 ?
 ?
 ?
 ??
 ?
 ?
 ?
 =
 ??
 ?
 ?
 ?
 ??
 ?
 ?
 ?
 ??
 ??
 =
 KkmmmK
 mmKK
 mm
 y
 y
 Y
 w
 w
 dtXdtX
 dtXdtX
 H ?
 [ ]                                                                            1100
 1
 0
 101 HHHHH
 H
 HHT TTTT +=??
 ???
 ?
 =
      (17)                                                               )(                                        1ˆ sYsHsTs
 T? ?=
      (18)                                                                  )(                                       sHsHsT
 T
 =
              (19)                                                                                                       1 1 1 +++ += sHsHsTsT
 T
                    (20)                          )      (  ) (                       ˆˆˆ 1 1 1 
1
 1 1 ssHsYsHsTss ???
 T
 +++
 ?
 ++ ?+=
                   (16)                                              )   ()(                                0 11110 1 ˆˆˆ ??? HYHT
 T1
 ?+= ?
 (21)                        
)y( 
   
)(
 )(
    
)y( 
            
2
 2
 2
 dt
 td
 M
 ?
 ty
 ti
 M
 ?
 g
 dt
 td
 ???
 ???
 ?
 ????
 ???
 ?
 +?=
              :    ,:  ,
 ),,(),,(
 :::
 ),,(....),,(
  
1 00 
1
 0
 1   
1
 0
   0
 110
 1111
 0 ˆ
 ???
 ??
 ?
 ?
 ?
 ??
 ?
 ?
 ?
 =
 ??
 ?
 ?
 ?
 ??
 ?
 ?
 ?
 =
 ??
 ?
 ?
 ?
 ??
 ?
 ?
 ?
 ??
 ??
 =
 Kkmmm0K
 mmKK
 mm
 y
 y
 Y
 w
 w
 dtXdtX
 dtXdtX
 H ?
    (10)                                                                 )(                                 ˆ 00
 1
 00 YHT
 T? ?=
differences in best fit performances, where WN reached 99.80 
% of best fit and NN reach 87.20 %. Note that this system has 
fast dynamic response yet WN was capable to extract plant 
dynamics. 
 
Fig. 2. WN and NN based OS-ELM for Magnetic Levitation system 
B. Continuous Stirred Tank Reactor (CSTR):The dynamics 
of the CSTR system can describe by, 
 
 
 
 
where l(t) is the fluid level, r(t) is the concentration result at the 
output, ?1(t) is the flow rate of first liquid feed source with 
concentration C1= 24.9, and ?2(t) is the flow rate of the second 
feed with concentrations C2 = 0.1. Similarly, the simulation 
result in Figure 3 displayed that, WN much better than NN, 
where the best fits was 91.66 %, while NN shows 80.13 %. 
Note that NN suffers from sharp discontinuity ends, while WN 
with decompositions properties a superb estimate capability 
with sharp discontinuities system responses. 
C. Robot Arm: The system consists of a single-link robot 
arm that controls the movement of the arm. The equation of 
motion dynamics is, 
 
 
where m(t) is the angle of the arm, and T is the DC motor 
torque. However, the simulation result in Figure 4 shows better 
best fit and good estimation with 99.66 % for WN and 95.45% 
for NN. Note that WN still higher.  
 
Fig. 3. WN and NN based OS-ELM for CSTR system 
 
Fig. 4. WN and NN based OS-ELM for Robot Arm system 
For more investigation on stability performance,  the training 
errors are shown in Figure 5 for the robot arm system; it is 
obvious that the error decline gradually for the NN with none 
soothe convergence rippling with each training sample unlike 
WN, where it was stable all the line. As a result from previous 
tests, it is obvious that WN can offer excellent approximation 
ability turn out to be factual even for functions with sharp 
discontinuities. 
 
Fig. 5. The Training errors for Robot Arm system  
Stage 2 : Network Size 
The simulations are applied to the three dynamic systems 
with different numbers of neurons and waveleons in the hidden 
layers, while the numbers of regressors are fixed in all tests and 
the trained data are fed sample by sample (K=1). On behalf of 
that, a comparison illustrated according to training time and 
normalized mean square error (NMSE) and tested with 7, 15, 
and 25 numbers of hidden nodes. For the first evaluation with 7 
hidden nodes, Table1 shows that WN is less NMSE error and 
more training time than NN for all systems, where WN is 
taking more time in about 0.1- 0.2 seconds.  
TABLE I.  TRAINING TIME AND NORMALIZED MSE FOR 7 HIDDEN 
NODES OF WN AND NN BASED OS-ELM 
Magnetic Levitation CSTR Robot Arm
 NN WN NN WN NN WN
 Training Time 
(second) 
0.1248 0.3120 0.3744 0.4368 0.2340 0.4212 
NMSE 0.1426 0.0023 0.1528 0.0661 0.0244 0.0018 
0 200 400 600 800 1000 1200 1400 1600 1800 2000
 -1
 0
 1
 2
 3
 4
 5
 6
 7
 Samples
 O
 ut
 pu
 ts
  
 
Plant
 NN
 WN
 Best Fit
 NN = 87.20 %
 WN= 99.80 %
 0 500 1000 1500 2000 2500 3000 3500 4000
 19.5
 20
 20.5
 21
 21.5
 22
 22.5
 23
 Samples
 O
 u
 tp
 ut
 s
  
 
Plant
 NN
 WN
 Best Fit
 NN = 80.13 %
 WN= 91.66 %
 0 200 400 600 800 1000 1200 1400 1600 1800 2000
 -0.6
 -0.4
 -0.2
 0
 0.2
 0.4
 0.6
 Samples
 O
 ut
 pu
 ts
  
 
Plant
 NN
 WN
 Best Fit
 NN = 95.47 %
 WN= 99.66 %
 0 500 1000 1500 2000 2500 3000 3500 4000
 -0.04
 -0.02
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 Samples
 Tr
 ai
 n
 in
 g 
Er
 ro
 r
  
NN
 WN
 610 615 620 625 630 635
 -0. 04
 -0. 02
 0
 0. 02
 0. 04
 0. 06
  
 
(22)                                       )(.20 )()(
 )(  
            21 tlt?t?dt
 tld
 ??+=
 ( ) ( ) ( )  (23)             )(1
 )(
 )(
  (t)
   )(   
)(
  (t)
 )(
 )(  
       2
 1
 2
 1
 1
 tr
 tr
 tl
 ?
 trC
 tl
 ?
 trC
 dt
 trd
 +
 ???+??=
 (24)                      
)( 
2   ))((sin10
 )( 
            2
 2
  T
 dt
 tdm
 tm
 dt
 tmd
 +????=
The same results obtained in Table2 for 15 hidden nodes, 
where WN shows better accuracy but not much difference than 
NN. The enhancement in training time of WN is clear where 
the differences reduced within 0.01- 0.1 seconds, meaning that 
WN with prior initializing tries to find optimality due to 
number of waveleons.    
TABLE II.  TRAINING TIME AND NORMALIZED MSE FOR 15 HIDDEN 
NODES OF WN AND NN BASED OS-ELM 
Magnetic Levitation CSTR Robot Arm
 NN WN NN WN NN WN
 Training Time 
(second) 
0.5148 0.5616 1.1076 0.9516 1.0920 1.0764 
NMSE 0.0746 0.0601 0.0825 0.0353 0.0074 0.0069 
In Table3 for 25 hidden nodes, WN shows better NMSE in 
robot arm and CSTR systems with less learning time 
comparing to NN in all three systems. This means that WN 
reached almost optimal waveleon numbers with robot arm and 
CSTR systems. However, NN showed better identification 
accuracy then WN in magnetic levitation system, and this 
could be the reason of over fitting tricky of WN with larger 
number of waveleons than the optimal number.   
TABLE III.  TRAINING TIME AND NORMALIZED MSE FOR 25 HIDDEN 
NODES OF WN AND NN BASED OS-ELM 
Magnetic Levitation CSTR Robot Arm
 NN WN NN WN NN WN
 Training Time 
(second) 
0.9204 0.5772 1.5288 0.9984 1.5132 1.1700 
NMSE 0.0136 0.0601 0.0390 0.0353 0.0106 0.0069 
From the above results, it’s clear that WN is less sensitive to 
the network size than NN and the reason is a good initialization 
procedure for wavelet parameters, while the initialization of 
NN done randomly. Moreover, the wavelets decomposition 
propriety helped WN to approximate output of the plants 
precisely giving better identification of the system dynamics. 
On the other hand, the time consuming for training varied with 
number of hidden nodes, and WN reached optimal 
performance in best fit and training time with specific number 
of waveleons no matter what size it is. In another word, WN is 
able to spend less training time than NN if the number of 
waveleons is chosen correctly.  
 
V. CONCLUSION 
Wavelet Network based on fixed structure serial-parallel 
NARX model with online sequential extreme learning 
machine has been applied to nonlinear dynamic systems as 
system identification scheme. The OSELM algorithm is 
adopted as training algorithm for wavelet network. 
Simulations have been carried out with proposed model on 
three nonlinear dynamic systems. The updating weights was 
done online sequentially, sample-by-sample and, for all the 
simulated dynamic systems.  The results show fast learning 
ability with good system identification accuracy for WN-
 OSELM preserving better stability and convergence over NN-
 OSELM in most tests.  The overall results show better 
acceptable training time, and could be applied in real time 
applications if real input and output observations are available. 
However, further investigation is required in order to 
implement the algorithm experimentally, since selecting the 
number of hidden nodes (waveleons) acts as a big rule in WN-
 OSELM training time, because it may lead to fatal estimations 
and much more time consuming. 
 
REFERENCES 
[1] A. Sureshbabu and J. Farrell, "Wavelet-based identification for 
nonlinearcontrol," IEEE Trans Autom Control, vol. 44, p. 412–7., 1999. 
[2] S. A. Billings and H.-L. Wei, "A New class of wavelet networks for 
nonlinear system identification," IEEE Trans neural networks, vol. 16, 
pp. 862-874, 2005.  
[3] A. Rubaai and R. Kotaru, "Online identification and control of a DC 
motor using learning adaptation of neural networks," IEEE Indusrial 
Applications, vol. 3, no. 36, pp. 935-942, 2000.  
[4] P. L. Guoping, K. Visakan and B. Steve, "On-line identification of 
nonlinear systems using Volterra polynomia lbasis function neural 
networks," Neural Networks , no. 11, p. 1645–1657, 1998.  
[5] Y. Tan, D. Xuanju, L. Feng and S. Chun-Yi, "Dynamic wavelet neural 
network for nonlinear dynamic system identification," in International 
IEEE Conference on Control Applications, Anchorage, AK, 2000.  
[6] G.-B. Huang, Q.-Y. Zhu and C.-K. Siew, "Extreme learning 
machine:Theorey and applications," Neurocomput, p. 489–501 2006.  
[7] G.-B. Huang, D. H. Wang and Y. Lan, "Extreme learning machines: a 
survey," Int. J. Mach. Learn. & Cyber., vol. 2, p. 107–122, 2011.  
[8] J. Cao, Z. Lin and G. Huang, "Composite function wavelet neural 
networks with differential evolution and extreme learning machine," 
Neural Process Lett, vol. 33, pp. 251-265, 2011.  
[9] N.-Y. Liang, G.-B. Huang, P. Saratchandran and N. Sundararajan, "A 
fast and accurate online sequential learning algorithm for feedforward 
networks," IEEE Transactions on Neural Network, vol. 17, pp. 1411-
 1423, 2006.  
[10] L. Ljung, System Identification - Theory for the User, 2nd ed., Upper 
Saddle River, N.J.: Prentice-Hall, 1998.  
[11] P. Seda and B. Yasar, "Wavelet networks for nonlinear system 
modeling," Neural Comput & Applic , vol. 16, p. 433–441, 2007.  
[12] Oussar and Dryafus, "Initializing by selection Wavelet Network using 
Training," Neurocomputing, vol. 34, p. 131–143, 2000.  
[13] B. Zhou, A. Shi, F. Cai and Y. Zhang, "Wavelet neural networks for 
nonlinear time series analysis," Lecture Notes in Computer Science,
 Springer, vol. 3174, p. 430–435, 2004.  
[14] H. L. Wei, S. A. Billings and M. A. Balikhin, "Wavelet based non-
 parametric NARX models for nonlinear input-output system 
identification," International Journal of Systems Science, vol. 37, no. 15, 
pp. 1089 - 1096, 2006.  
[15] W. W. Hager, "Updating the inverse of a matrix," SIAM Review , p. 
221–239, 1989.  
 

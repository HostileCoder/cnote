Two Stage Genetic Approach for Bio-chemical
 Named Entity Recognition
 Asif Ekbal
 Department of Computer Science and Engineering
 Indian Institute of Technology Patna
 India
 Email: asif@iitp.ac.in
 Sriparna Saha
 Department of Computer Science and Engineering
 Indian Institute of Technology Patna
 India
 Email: sriparna@iitp.ac.in
 Abstract—Determining different mentions of chemical names
 from texts has a wide-spread application in real life. Chemical
 names are complex in nature and there exist several repre-
 sentations and nomenclatures (like SMILES, InChI, IUPAC)
 which create a big challenge to their automatic identification
 and classification. In this paper we present a feature selection
 approach for appropriate feature subset selection from a well-
 known supervised machine learning approach namely conditional
 random field based classifier (CRF). Several features are identi-
 fied and extracted without using any domain specific knowledge
 and/or resources for determining mentions of IUPAC and IUPAC-
 like names from scientific text using some supervised classification
 technique. The appropriate set of features for a particular
 supervised classification technique is extracted from this huge
 collection of features using some single objective genetic algorithm
 based feature selection technique. Experiments are carried out
 on the benchmark patent dataset. Evaluation shows encouraging
 performance with the overall F-measure values of 70.01% by
 single objective optimization based approach on patent 2008 test
 data set.
 Keywords: Mention Detection and Classification from Bio-
 chemical Domain, Feature Selection, Genetic Algorithm, Con-
 ditional Random Field.
 I. INTRODUCTION
 In recent years, information extraction is one of the hottest
 field. It has drawn huge attention to the practitioners and
 researchers. There is a huge collection of online information
 which are unorganized. In every day, a large number of data
 documents are added to it. So it is crucial to organize, find and
 extract relevant information from such a huge amount of data
 in our day-to-day life. In particular, in medical domain more
 than 2000 documents are added daily in the form of publi-
 cations, research papers, patents, journals etc. and we cannot
 classify all those documents manually [1][2]. Ddocument cat-
 egorization, ranking and retrieving entity-related information
 constitute some important steps in research and work life of
 general users and the researchers who can utilize these large
 number of textual documents like Medline [3][4]. The develop-
 ment of systems for automatic identification of named entities
 relevant to a particular domain and their possible storage to
 different databases have helped many emerging application
 areas like information retrieval, information extraction, and
 information aggregation. In order to automate these tasks,
 some techniques have been devised since the last one decade
 especially for determining mentions of chemical names as
 well as to extract relations among them (i.e. protein–protein
 interaction).
 Chemical compounds like small signal molecules or other
 biological active chemical substances are the core entity classes
 in biological publications and patents. Many representations
 and nomenclatures exist for chemical names. Examples in-
 clude SMILES, InChI and IUPAC. The first two allow a
 direct structure search whereas IUPAC like names are most
 frequently occuring in biochemical texts. Medline [3][4] like
 huge text corpora databases can be beneficial if we can
 classify and rank all those documents. This will aid them in
 determining the entity related information in a fast and easy
 manner. The documents can be classified and ranked if we
 can extract, retrieve and gather the information which can
 be improved after identifying the entities of interest (IUPAC-
 like terms) with high recall rate and map those entities to
 corresponding entries in the database. Hence the process of
 identifying the entities of interest in the provided texts needs
 to be automated[5].
 Features of training and test data sets are very much impor-
 tant for good performance by some classification techniques.
 Feature selection [6], [7] is also having the name of variable
 selection, attribute selection or variable subset selection. It is
 the technique used in machine leraning to automatically select
 the relevant subset of features for developing the classification
 models. Feature selection helps to improve the performance
 of a classifier by selecting the appropriate sets of features.
 Here the problem of appropriate feature selection is formulated
 as an optimization problem and thereafter a single objective
 based technique is developed for automatically determining
 the appropriate set of features from some supervised machine
 learning approaches. For single objective optimization genetic
 algorithm [8] is used as the underlying optimization technique.
 Here we have used Conditional Random Field (CRF) [9] based
 classifier to find mentions of IUPAC and IUPAC-like names
 from scientific texts. GA is applied with CRF based classi-
 fier to determine appropriate set of features from individual
 classifiers. Finally GA results a set of solutions on the best
 population. The solutions on the best populations are then
 combined using a GA based classifier ensemble technique [10].
 The proposed systems are evaluated on the benchmark patent
 dataset. Single objective based approach attains the overall F-
 measure values of 70.01% for patent test data set of 2008.
 Comparative results show that the automatic feature selection
 technique is better than the systems with all features. The two
 stage approach (feature selection first followed by classifier
 ensembling) is much effective than only the feature selection
 based approach.
 709978-1-4673-6217-7/13/$31.00 c©2013 IEEE
II. PROBLEM FORMULATION
 In general, feature and parameter selection problem is
 formulated under the single objective optimization. It is stated
 as follows: Given a set of features ? and a classification
 quality measure D, determine the feature subset F ? such that:
 D(F ?) = max
 F??
 D(F )
 In case of named entity recognition from chemical domain,
 the classification quality measure is F-measure. We have to
 select such feature combination for which corresponding F-
 measure value is optimized.
 In general the search space for this type of problem is
 2d, where d is the total number of possible features. Thus,
 exhaustive search strategies can not be applied in this case.
 III. PROPOSED APPROACH
 In this section we describe our proposed approach for
 chemical mention detection and classification in texts. The task
 is formulated with respect to the machine learning perspective
 where the overall task focuses on entities such as IUPAC and
 MODIFIER mentions. Chemical entities in general are named
 following different nomenclatures which are also combined
 by the authors of biomedical texts. Rather than concentrating
 only on the correct IUPAC terms, the definition is broadened
 by including any chemical substance mentioned in a IUPAC-
 like manner. Additionally it also includes IUPAC names in
 which a part is abbreviated, fragmented and group names. The
 proposed approach is based on a popular and robust machine
 learning technique namely Conditional Random Field (CRF).
 A. Features for Chemical Name Identification
 The success of any machine learning algorithm is based
 on the feature sets. We identify and implement a very rich
 set of features that exploit morphological, lexical, statistical
 and contextual information [5]. Most of these features are
 automatically extracted from the training data without using
 any domain dependent resources and/or tools. The below
 description of features is taken from [5].
 Context words: These are the words occurring within the
 context window wi+3i?3 = wi?3 . . . wi+3, w
 i+2
 i?2 = wi?2 . . . wi+2
 and wi+1i?1 = wi?1 . . . wi+1, where wi is the current word. We
 include this feature because contextual tokens carry effective
 information for identification of chemical names.
 Word prefix and suffix. These are the word prefix and
 suffix character sequences of length up to n. The sequences
 are stripped from the leftmost (prefix) and rightmost (suffix)
 positions of the words. We set the feature values to ‘undefined’
 if either the length of wi is less than or equal to n ? 1, wi
 is a punctuation symbol or if it contains any special symbol
 or digit. We set the feature values to ‘undefined’ if either the
 length of wi is less than or equal to n? 1, wi is a punctuation
 symbol or if it contains any special symbol or digit. We
 experiment with n=3 (i.e., 6 features) and 4 (i.e., 8 features)
 both.
 Word length. We define a binary-valued feature that fires if
 the length of wi is greater than a pre-defined threshold. Here,
 the threshold value is set to 5. This feature captures the fact
 that short words are likely not to be the chemical names.
 Infrequent word. A list is compiled from the training data
 by considering the words that appear less frequently than a
 predetermined threshold, i.e. 10 in our current experiment.
 Now, a feature is defined that fires if wi occurs in the
 compiled list. This is based on the observation that more
 frequently occurring words are rarely the chemical names.
 Unknown token feature: This is a binary valued feature
 that checks whether the current token was seen or not in
 the training data. In the training phase, this feature is set
 randomly.
 Word normalization: We define two different types of
 features for word normalization. The first type of feature
 attempts to reduce a word to its stem or root form. This helps
 to handle the words containing plural forms, verb inflections,
 hyphen, and alphanumeric letters. The second type of feature
 indicates how a target word is orthographically constructed.
 Word shapes refer to the mapping of each word to their
 equivalence classes. Here each capitalized character of the
 word is replaced by ‘A’, small characters are replaced by ‘a’
 and all consecutive digits are replaced by ‘0’. For example,
 ‘IL’ is normalized to ‘AA’, ‘IL-2’ is normalized to ‘AA-0’
 and ‘IL-88’ is also normalized to ‘AA-0’.
 Word class feature: Certain kind of chemical names, which
 belong to the same class, are similar to each other. The word
 class feature is defined as follows: For a given token, capital
 letters, small letters, numbers and non-English characters are
 converted to ”A”, ”a”, ”O” and ”-”, respectively. Thereafter,
 the consecutive same characters are squeezed into one
 character. This feature will group similar names into the same
 chemical class.
 Orthographic features: We define a number of orthographic
 features depending upon the contents of the wordforms.
 Several binary features are defined which use capitalization
 and digit information. These features are: initial capital, all
 capital, capital in inner, initial capital then mix, only digit,
 digit with special character, initial digit then alphabetic, digit
 in inner. The presence of some special characters like (‘,’,‘-
 ’,‘.’,‘)’,‘(’ etc.) is very much helpful to detect chemical names.
 For example, many biochemical names have ‘-’ (hyphen) in
 their construction. Some of these special characters are also
 important to detect boundaries of chemicals. We also use the
 features that check the presence of ATGC sequence and stop
 words.
 IV. GENETIC APPROACH FOR AUTOMATIC FEATURE
 SELECTION
 In this paper a single objective GA is now used for solving
 the feature selection problem for any classifier.
 A. Chromosome Representation and Population Initialization
 If the total number of features is F , then the length of the
 chromosome is F . As an example, the encoding of a particular
 chromosome is represented in Figure 1. Here, F = 12 (i.e.,
 total 12 different features are available). The chromosome rep-
 resents the use of 7 features for constructing a classifier (first,
 third, fourth, seventh, tenth, eleventh and twelfth features). The
 entries of each chromosome are randomly initialized to either
 710 2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)
Fig. 1. Chromosome representation for SOO based feature selection
 0 or 1. Here, if the ith position of a chromosome is 0 then it
 represents that ith feature does not participate in constructing
 the classifier. Else, if it is 1 then the ith feature participates in
 constructing the classifier.
 If the population size is P then all the P number of
 chromosomes of this population are initialized in the above
 way.
 B. Fitness Computation
 We execute the following steps for fitness computation.
 1) Suppose, there are N number of features present in a
 particular chromosome (i.e., there are total N number
 of 1’s in that chromosome).
 2) Construct the classifier with only these N features
 encoded in the chromosome.
 3) The training data is divided into 3 parts. The classifier
 is trained using 2/3 parts of the training data with
 the set of features encoded in the corresponding
 chromosome, and evaluated with the remaining 1/3
 part.
 4) The overall recall, precision and F-measure values of
 this classifier for the 1/3 training data are calculated.
 5) Steps 2-4 are repeated 3 times to perform 3-fold cross
 validation. Then, the overall average recall, precision
 and F-measure values of the classifier are determined
 from this cross validation test.
 In case of SOO, the objective function corresponding to a
 particular chromosome is f = F-measureavg . The objective
 is maximized using the search capability of genetic algorithm
 (GA). The objective is to maximize this objective function
 using the search capability of GA.
 C. Other Operators
 For single objective GA, normal single point crossover [11]
 is used. Here, mutation operator is applied to each feature entry
 of the chromosome where the entry is randomly replaced by
 either 0 or 1. Roulette wheel selection is used to implement
 the proportional selection strategy.
 D. Combining the Solutions of the Final Population
 Both the approaches are executed for some maximum
 number of generations. In case of SOO, we obtain a set of
 solutions on the best population. The final solution is the one
 with the best F-measure value. But some of the solutions on
 the best population are best with respect to recall values and
 some are best with respect to precision values. Thus instead of
 selecting a single solution we have used a classifier ensemble
 technique [10] to combine the solutions on the best population.
 TABLE I. STATISTICS OF THE DATASETS
 Set #abstracts #sentences #tokens #IUPAC
 names
 Training 463 3,700 161,591 3,712
 Test set 26 152 4,309 411
 (Patent)
 V. DATA SETS
 Chemical names in texts can be present in various forms,
 one standardized nomenclature comes from the International
 Union of Pure and Applied Chemistry (IUPAC) and forms a
 systematic way of naming organic chemical compounds that
 can be mapped to their structures. The below description of
 dataset is taken from [5].
 Chemical entities in general are named following differ-
 ent nomenclatures which are also combined by the authors
 of biomedical texts. Rather than concentrating only on the
 correct IUPAC terms, the definition is broadened by including
 any chemical substance mentioned in a IUPAC-like manner.
 Additionally it also includes IUPAC names in which a part
 is abbreviated, fragmented and group names. We used the
 datasets available at 1. The training set was collected from
 the Medline abstracts. The test data set is a collection of
 patent documents. The test dataset was annotated with seven
 classes, namely TRIVIAL (single word terms like aspirin,
 estragon, testosterone, Acetylsalicylate etc.), IUPAC (multi-
 word systematic names such as 1-hexoxy-4-methyl-hexane,
 1,4-dihydronaphthoquinones etc.), PART (partial chemical
 names such as 8-( methylthio)-and . . . , 17beta- etc.), MODI-
 FIER, ABBREVIATION (abbreviations of names but not those
 that appear as part of IUPAC such as TPA, AMPA etc.),
 SUM (sum formulas like CH 3SNa, KOH etc.) and FAM-
 ILY (chemical families: disaccaride, pyrimidine, hydrazides;
 but not pharmacological/functional families:anti-inflammatory
 drug, chelator etc.). However, it is to be noted that training
 dataset has only the instances of IUPAC, PART and MODI-
 FIER classes. The test dataset contains the instances of all the
 seven classes. Statistics of the datasets are presented in Table
 I.
 A. Experimental Results
 Here a base classifier is used: conditional Random Fields
 (CRFs) [9]. We have used the C++ based CRF++ package
 2
 , a simple, customizable, and open source implementation
 of CRF for segmenting or labeling sequential data. We set
 the following parameter values for genetic algorithm: pop-
 ulation size=100, number of generations=50, probability of
 mutation=0.2 and probability of crossover=0.9.
 Here first the proposed SOO based feature selection tech-
 nique is applied for selecting relevant features from CRF based
 classifier. In case of SOO we have reported the best solution of
 the final population according to the F-measure value as well
 as we have combined the solutions on the final population
 by a genetic algorithm based classifier ensemble technique
 proposed in [10].
 1http://www.scai.fraunhofer.de/chem-corpora.html
 2http://crfpp.sourceforge.net
 2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI) 711
In this work, ?, i.e. the set of features contains the
 following:
 (i). Various context word window within the previous three and
 next three words (ii). Prefixes of length up to three (3 features)
 or four (4 features) characters (iii). Suffixes of length up to
 three (3 features) or four (4 features) characters, (iv). Length
 of the word (v). Infrequent word (vi). Unknown token feature
 (vii) word normalization feature (viii) word class feature (ix)
 several orthographic features and (x) dynamic NE information.
 Initially, we construct following four baseline classifiers
 based on CRF frameworks using various random subsets of
 the above mentioned feature set. Here, C[?i,+j] denotes the
 context spanning from the previous ith word to the next jth
 word with the current token at position 0; P rei and Sufi
 denote the prefixes and suffixes of character sequences up to
 i of the current word, respectively.
 1) Baseline 1: CRF classifier trained using feature com-
 binations: C[?3,+3], P re4, Suf4, and the features
 (iv)-(x) descried in Section III-A.
 2) Baseline 2: CRF classifier trained using feature com-
 binations: C[?2,+2], P re4, Suf4, and the features
 (iv)-(x) descried in Section III-A.
 3) Baseline 3: All the classifiers of the best population
 generated after application of the proposed technique
 on CRF based classifier are combined using majority
 voting.
 4) Baseline 4: All the classifiers of the best population
 generated after application of the proposed technique
 on CRF based classifier are combined using weighted
 voting.
 Thereafter, we apply our proposed SOO based feature
 selection techniques (Section IV) to solve the problem of
 NER from chemical domain for a different classifier, CRF.
 In the first step a CRF is trained using training data corpus
 from Medline and evaluation is done on an independent set of
 test data corpus. The recall, precision and F-measure values
 obtained by the proposed SOO based technique are also
 shown in Tables II for chemical test corpus prepared in 2008.
 After application of the proposed SOO based approach on an
 annotated chemical test corpus prepared in 2008 we were able
 to obtain a F1 measure of 67.69% (recall = 62.12 and precision
 = 74.36).
 After application of the SOO based feature selection
 technique for CRF based classifiers we obtain a set of best
 solutions on the final population. Based on the features of these
 solutions we have generated a set of CRF based classifiers.
 Some of them are good with respect to recall and some
 are good with respect to precision. These solutions represent
 different feature combinations. By using these features we
 can again generate several CRF based classifiers. These CRF
 based classifiers are combined using a newly developed SOO
 based classifier ensemble technique [10]. Overall evaluation
 results along with the baseline models are reported in Table
 II for chemical test corpus prepared in 2008. Evaluation of
 our proposed feature selection algorithm shows the state-
 of-the-art performance for a test corpus. Final SOO based
 ensemble yields overall recall, precision and F-measure values
 of 63.45%, 78.08%, and 70.01%, respectively, for test corpus
 of 2008. Results show that SOO based feature selection is
 better than automatic feature selection techniques. Table II
 shows that proposed approach is better than Baseline 1 and
 Baseline 2 for all the test data sets. Results on all the test
 corpus also prove that the use of SOO based ensembling to
 combine the solutions on the final population obtained after
 application of SOO based feature selection technique is more
 effective than the use of any other standard techniques like
 majority voting (Baseline 3) or weighted voting (Baseline 4).
 VI. CONCLUSION
 In this paper we have proposed a single objective based
 bio-chemical named entity recognition system. The possible
 performance is highly dependent on the quality and rep-
 resentativeness of the features that were mostly identified
 without using any domain dependent resources. In this work,
 a classifier, conditional random field (CRF), is used as the
 underlying classification technique. Then the problem of ap-
 propriate feature selection for any classifier is posed as an
 optimization problem. Thereafter a single objective based
 approach is developed for automatic feature selection from
 an underlying supervised classifier namely conditional random
 field (CRF). The proposed SOO based approach is based on
 genetic algorithm (GA). The proposed approach can determine
 the appropriate feature subsets which not only reduce the
 problem of overfitting but also optimizes the classification
 quality. We applied this technique for appropriate feature
 selection from a base classifier namely CRF. The proposed
 SOO based approach generates a set of solutions on the final
 best population. Solutions on the final best population are
 combined using a SOO based classifier ensemble technique.
 The proposed systems are evaluated to solve the problem of
 NER from a chemical test corpus, namely 2008 test corpus. In
 future, we would like to add some domain dependent features.
 Future works include use of some other well known classifiers
 like decision tree and memory based learner.
 REFERENCES
 [1] R. Klinger, C. Kolarik, J. Fluck, M. Hofman-Apitius, and
 C. M.Friedrich, “Detection of IUPAC and IUPAC-like chemical
 names,” Bioinformatics, vol. 24, no. 13, pp. 268–276, 2008.
 [2] S. Anstein, G. Kremer, U. Reyle“Identifying and classifying terms in
 the life sciences: the case of chemical terminology.” Fifth Language
 Resources and Evaluation Conference. ELRA-ELDA, Genoa, Italy, pp.
 1095–1098, 2006.
 [3] NCBI., “Pubchem data.” Online. Available at
 ftp://ftp.ncbi.nlm.nih.gov/pubchem/Compound/CURRENT-Full/XML/,
 2007.
 [4] H. Al-Mubaid and H. A. Nguyen, “Using MEDLINE as standard
 corpus for measuring semantic similarity in the biomedical domain,”
 in Proceedings of the Sixth IEEE Symposium on BionInformatics
 and BioEngineering, ser. BIBE ’06. Washington, DC, USA:
 IEEE Computer Society, 2006, pp. 315–318. [Online]. Available:
 http://dl.acm.org/citation.cfm?id=1169220.1169412
 [5] A. Ekbal, S. Saha, and K. Ravi, “Mention detection and classification in
 bio-chemical domain using conditional random field,” in Emerging Ap-
 plications of Information Technology (EAIT), 2012 Third International
 Conference on, 2012, pp. 335–338.
 [6] H. Liu and L. Yu, “Toward integrating feature selection algorithms for
 classification and clustering,” IEEE Trans. on Knowl. and Data Eng.,
 vol. 17, no. 4, pp. 491–502, 2005.
 [7] H. Liu and H. Motoda, Feature Selection for Knowledge Discovery and
 Data Mining. Norwell, MA, USA: Kluwer Academic Publishers, 1998.
 [8] D. E. Goldberg, Genetic Algorithms in Search, Optimization and
 Machine Learning. New York: Addison-Wesley, 1989.
 712 2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)
TABLE II. OVERALL RESULTS FOR TEST CORPUS OF 2008
 Model recall (in %) precision (in %) F-measure (in %)
 SOO + CRF based approach 62.12 74.36 67.69
 SOO based ensembling 63.45 78.08 70.01
 Baseline 1 61.41 73.59 66.95
 Baseline 2 61.55 73.70 67.08
 Baseline 3 63.90 76.37 69.58
 Baseline 4 63.99 76.58 69.72
 [9] J. D. Lafferty, A. McCallum, and F. C. N. Pereira, “Conditional Random
 Fields: Probabilistic Models for Segmenting and Labeling Sequence
 Data,” in ICML, 2001, pp. 282–289.
 [10] A. Ekbal and S. Saha, “Weighted vote-based classifier ensemble for
 named entity recognition: A genetic algorithm-based approach,” ACM
 Trans. Asian Lang. Inf. Process., vol. 10, no. 2, p. 9, 2011.
 [11] J. H. Holland, Adaptation in Natural and Artificial Systems. AnnArbor:
 The University of Michigan Press, 1975.
 2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI) 713

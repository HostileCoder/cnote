The 2013 International Conference on High Performance Computing and Simulation (HPCS 2013) 
July 1 Ð July 5, 2013, Helsinki, Finland 
 
 
HPCS 2013 KEYNOTES 
 
 
TUESDAY KEYNOTE 
 
 
Big Process for Big Data 
 
 
Ian Foster 
Argonne National Laboratory & The University of Chicago 
Illinois, USA 
 
 
ABSTRACT 
Large and diverse data result in challenging data management problems that researchers and facilities are often ill-
 equipped to handle. I propose a new approach to these problems based on the outsourcing of research data management 
tasks to software-as-a-service providers. I argue that this approach can both achieve significant economies of scale and 
accelerate discovery by allowing researchers to focus on research rather than mundane information technology tasks. I 
present early results with the approach in the context of Globus Online. 
 
 
SPEAKER BIOGRAPHY AND PHOTO 
Ian Foster is Director of the Computation Institute, a joint institute of the University of Chicago and Argonne 
National Laboratory. He is also the Arthur Holly Compton Distinguished Service Professor of Computer 
Science, and an Argonne Distinguished Fellow. Ian's research interests are in distributed, parallel, and data-
 intensive computing, and the application of these methods in domains such as biomedicine and economics. He 
has also a long record of contributions in open platforms, software, and standards for distributed systems and 
information sharing. He has published six books and over 300 articles and technical reports in these areas. His 
work has been recognized by numerous awards, including the Lovelace Medal, R&D Magazine's Innovator of 
the Year, and DSc Honoris Causa from the University of Canterbury. Ian Foster is a fellow of the American 
Association for the Advancement of Science, the British Computer Society, and the Association for Computing 
Machinery.   
 
The 2013 International Conference on High Performance Computing and Simulation (HPCS 2013) 
July 1 Ð July 5, 2013, Helsinki, Finland 
 
 
WEDNESDAY KEYNOTE I 
 
 
ÒKiller-Mobiles: The Way Towards Energy Efficient  
High Performance Computers?Ó   
 
 
Mateo Valero 
Barcelona Supercomputing Center 
Spain 
 
 
ABSTRACT 
It is widely recognized that Exascale systems will be constrained by power. The Mont-Blanc project aims to build an 
alternative approach towards Exascale based on aggregating parts from the embedded and mobile market, which offer a 
better FLOPS/Watt ratio and a lower unit cost, at the expense of lower peak performance per chip. HPC systems built 
from these parts will require a higher number of processors, or resort to extensive use of compute accelerators. Using a 
higher number of chips increases the available memory bandwidth, alleviating the bandwidth wall, but increases the 
pressure on the interconnection network. 
 
The use of a high number of processors and accelerators, and the increased pressure on the interconnect require extensive code 
optimizations to achieve strong scaling, point to point synchronizations, and overlap data transfer with computation. The role 
of the OmpSs Parallel Programming Model is paramount, as the key enabling technology that hides the complexity from the 
programmer, and transparently performs all the required optimizations. 
 
In this talk, we will review the design philosophies of several vendors, including HPC compute accelerators, and ARM-
 based mobile application processors in terms of peak performance, memory bandwidth, and energy efficiency; and we 
will review how the OmpSs programming models exploits the benefits of the Mont-Blanc approach while overcoming the 
drawbacks. 
 
 
SPEAKER BIOGRAPHY AND PHOTO 
Mateo Valero, http://personals.ac.upc.edu/mateo/, is a professor in the Computer Architecture Department at 
UPC, in Barcelona. His research interests focuses on high performance architectures. He has published 
approximately 600 papers, has served in the organization of more than 300 International Conferences and he 
has given more than 400 invited talks. He is the director of the Barcelona Supercomputing Centre, the 
National Centre of Supercomputing in Spain. 
 
Dr. Valero has been honoured with several awards. Among them, the Eckert-Mauchly Award, Harry Goode 
Award, ACM Distinguished service,  the ÒKing Jaime IÓ in research and two National Awards on Informatics 
and on Engineering. He has been named Honorary Doctor by the University of Chalmers, by the University 
of Belgrade, by the Universities of Las Palmas de Gran Canaria, Zaragoza and Complutense de Madrid in Spain and by the University 
of Veracruz in Mexico.  "Hall of the Fame" member of the IST European Program (selected as one of the 25 most influents European 
researchers in IT during the period 1983-2008. Lyon, November 2008)   
 
In December 1994, Professor Valero became a founding member of the Royal Spanish Academy of Engineering. In 2005 he was 
elected Correspondant Academic of the Spanish Royal Academy of Science, in 2006  member of the Royal Spanish Academy of 
Doctors, in 2008 member of the Academia Europaea and in 2012 Correspondant Academic of the Mexican Academy of Sciences. He 
is a Fellow of the IEEE, Fellow of the ACM and an Intel Distinguished Research Fellow.  
The 2013 International Conference on High Performance Computing and Simulation (HPCS 2013) 
July 1 Ð July 5, 2013, Helsinki, Finland 
 
 
WEDNESDAY KEYNOTE II 
 
 
The UberCloud HPC Experiment Ð  
Paving the Way to HPC as a Service 
 
 
Wolfgang Gentzsch 
Executive HPC Consultant, Chairman of the ISC Cloud'13,  
Chairman of the UberCloud HPC Experiment 
Germany 
 
 
ABSTRACT 
There are several million of small and medium-size manufacturers around the world, most of them using workstations for 
their daily design and development work. However, there is often the need for more computing. 
 
Buying an expensive compute cluster is usually not an option, especially for small and medium enterprises, and renting 
computing power from the Cloud still comes with severe roadblocks, such as the complexity of the applications and their 
implementation itself, intellectual property and sensitive data, expensive data transfers, conservative software licensing, 
performance bottlenecks from virtualization, user-specific system requirements, and missing standards and lack of 
interoperability among different clouds. 
 
On the other hand, the benefits of using remote computing resources are extremely attractive: no lengthy procurement and 
acquisition cycles; shifting some budget from capex to the more flexible opex; gaining business flexibility by getting 
additional resources on demand, at your finger tip; and scaling resource usage automatically up and down according to 
your actual needs. 
 
The UberCloud Experiment has been designed to reduce many of the barriers mentioned above. By participating and 
moving the engineering application onto a remote computing resource, end-users can expect a long list of real benefits, 
such as: UberCloud is vendor neutral; no hunting for resources in a crowded Cloud market; professional match-making of 
end-users with suitable service providers; free, on-demand access to hardware, software, and expertise during the 
experiment; carefully tuned end-to-end, step-by-step process to accessing remote resources; learning from the best 
practices of other participants; no-obligation, risk free proof-of-concept: no money involved, no sensitive data transferred, 
no software license concerns, and the option to stay anonymous. 
 
With these benefits, the experiment is leading the way to increasing business agility, competitiveness, and innovation, and 
participants are not getting left behind in the emerging world of Cloud Computing. Last but not least, all participants are 
encouraged to make use of the interactive UberCloud Exhibit, a directory of professional cloud services to the wider CAE, 
Life Sciences, and Big Data communities. 
 
This presentation will focus on all the aforementioned topics in further detail and provide some real use cases from small 
and medium enterprises in digital manufacturing. 
 
SPEAKER BIOGRAPHY AND PHOTO 
Wolfgang Gentzsch is consultant for HPC, Grid and Cloud; Co-founder of the UberCloud Experiment; 
Advisor to the EU funded project EUDAT; and the Chairman of the ISC Cloud Conferences. Previously, he 
was an Advisor to the EU project DEISA, directed the German D-Grid Initiative, and was a member of the 
Board of Directors of the Open Grid Forum, and of the US President's Council of Advisors for Science and 
Technology, PCAST.  
 
Before, Wolfgang was a professor of computer science and mathematics at several universities in the US and 
Germany, and held leading positions at the North Carolina Grid and Data Center in Durham, Sun 
Microsystems in California, the DLR German Aerospace Center in Gottingen, and the Max-Planck-Institute for Plasmaphysics in 
Munich. In the 90s, he founded HPC software companies Genias and Gridware, the latter developing what is now Grid Engine.  
 
 
The 2013 International Conference on High Performance Computing and Simulation (HPCS 2013) 
July 1 Ð July 5, 2013, Helsinki, Finland 
 
 
THURSDAY KEYNOTE 
 
 
High Performance Fault Tolerance /  
Resilience at Extreme Scale 
 
 
Franck Cappello 
Argonne National Laboratory 
INRIA-Illinois Joint Laboratory on PetaScale Computing 
fci@lri.fr, cappello@illinois.edu  
 
 
ABSTRACT 
In 2008-2009, many projections about extreme scale systems raised critical concerns about the applicability of existing 
fault tolerance approaches at Exascale. Five years later huge progresses have been made in many aspects of fault tolerance 
and resilience for HPC applications. Can we consider the problem as solved? Certainly not but we have serious reasons to 
be much more confident on the feasibility of Exascale systems and in particular on advanced fault tolerance techniques for 
them. 
 
In this Keynote, we will first expose the main reasons that made the community thought that Exascale executions could 
not complete successfully. We will then review the progresses in several key areas of fault tolerance: Advanced 
checkpointing, fault tolerance protocols, failure prediction. We will show that despite huge improvements, there are still 
many attractive research problems. Whatever is the outcome of this research, progresses have already went so far that 
fault tolerance/resilience techniques for HPC in 2020 will be dramatically different compared to techniques commonly 
used in 2010! 
 
SPEAKER BIOGRAPHY AND PHOTO 
Franck Cappello is Program Manager and Senior Computer Scientist at Argonne National Laboraoty. Since 
2009, he is co-director with Marc Snir of the INRIA-Illinois Joint-Laboratory on PetaScale Computing where 
he is also leading the Resilience/Fault Tolerance effort. He is leading the roadmaping effort on 
Resilience/Fault Tolerance for EESI2 (European Exascale Software Initiative) and led similar effort for IESP 
(International Exascale Software Project) and EESI1. He is the main PI of the G8 ECS (Enabling Climate 
Simulation at Exascale) project gathering researchers from USA, France, Germany, Japan, Canada and Spain 
with the objective of identifying scalability, performance and resilience solutions for running the CESM 
climate model at extreme scale.  
 
 

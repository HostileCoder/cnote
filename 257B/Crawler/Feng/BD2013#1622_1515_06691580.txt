A Selective Checkpointing Mechanism for Query
 Plans in a Parallel Database System
 Ting Chen
 The University of Tokyo
 Email: chenting@eidos.ic.i.u-tokyo.ac.jp
 Kenjiro Taura
 The University of Tokyo
 Email: tau@eidos.ic.i.u-tokyo.ac.jp
 Abstract—Most existing parallel database systems achieve fault
 tolerance by aborting unfinished queries upon a failure and
 restart the entire from the beginning. This is inefficient for long
 running queries of OLAP workloads. To solve this problem,
 this paper presents a selective checkpointing mechanism which
 materializes the outputs of some necessary operators, enabling
 to resume queries from middle of the execution upon failures.
 Each query is represented by a DAG of relational operators
 in which data are typically pipelined between operators. The
 goal of the mechanism is to find a set of operators whose
 outputs are worth being checkpointed to minimize the expected
 runtime of the whole query. It firstly provides a cost model to
 estimate the expected runtime of a whole query plan under a
 given failure probability for each operator. Then a divide-and-
 conquer algorithm is proposed to find a close-to-optimal solution
 to the problem. The algorithm divides the query plan into sub-
 plans with smaller search spaces. For a given query plan with
 n operators, the algorithm runs in O(n) time. The mechanism
 is implemented in a shared-nothing parallel database system
 called ParaLite which provides a coordination layer to glue
 many SQLite instances together, and parallelizes SQL queries
 across them. The experimental results indicate that different
 fault-tolerant strategies affect the overall runtimes of queries.
 Our selective checkpointing mechanism can choose reasonable
 operators to be checkpointed and outperforms other fault-
 tolerant strategies. In addition, the divide-and-conquer algorithm
 taken by our mechanism has a smaller overhead than brute-force
 approach while keeping a similar effectiveness.
 I. INTRODUCTION
 Parallel database systems [1] are high performance com-
 puting platforms to provide a high-level programming envi-
 ronment and widely used in analytical data warehouse appli-
 cations (OLAP). As data that need to be analyzed continue
 to grow, the size of computing resources grows accordingly.
 Thus, the probability of a failure during query processing
 is increasing rapidly. Most existing database systems achieve
 fault tolerance by aborting unfinished queries upon a failure
 and restart the entire query processing. This approach is
 reasonable for queries with OLTP workload as almost all
 transactions must be completed within a small amount of
 time. However, for long running queries of OLAP workload,
 it is costly to restart the query from the beginning as lots
 of work are lost. Therefore, we should distinguish the fault
 tolerance of read-only queries in analytical workloads from
 that of traditional transactional workloads.
 While the basic mechanim for fault tolerance in parallel
 database systems is a transaction, MapReduce [2] and its open-
 source implementation Hadoop [3] support fine-grained fault
 tolerance mechanisms. In Hadoop, when one node fails (or
 is slow), its jobs are rescheduled on another node, allowing
 the entire task to complete without restarting. The goal of
 Hadoop is to make sure that only the jobs running on the
 very failed node need to be re-executed. To accomplish this
 goal, Hadoop stores the output of every map task to the
 durable storage before the data are available for the reduce
 tasks. On the reducer side, each reducer fetches partitions
 from every map task and writes them to local disks before
 the operation is performed. While this strategy is safe-first,
 it is not always efficient for small to medium-sized jobs as
 writing all intermediate data to durable storage before making
 progress may not be better than re-executing the tasks.
 Similar to workflows [4], a query is described in a database
 system through a directed graph structure, where nodes corre-
 spond to the activities (relational operator in database such
 as join, aggregate and sort) and edges correspond to the
 data dependencies between nodes. To achieve fault tolerance,
 workflow systems often use the combination of two tech-
 niques: checkpointing and rollback recovery [5]. There are two
 common checkpointing schemes in workflow systems based
 on the granularity level: task-level checkpointing which saves
 intermediate states of running tasks, so that in case of failure
 a task can be re-started from a previously saved intermediate
 state, and workflow-level checkpointing which captures the
 state of the workflow as a whole. The main difference between
 the execution of a workflow and a database query is that
 most of workflow systems store and transfer all intermediate
 data in/via files while database systems generally pipeline
 intermediate data to obtain better performance. Therefore,
 fault tolerance issue in workflow systems is more about the
 adaptation of the checkpoint frequency for reducing overall
 execution performance.
 To make an efficient intra-query fault tolerance for long-
 running queries, we propose a selective checkpointing mech-
 anism which for a given query plan aims to automatically
 find necessary operators whose outputs are worth being check-
 pointed to minimize the expected runtime of the whole query
 in a failure-prone environment. The mechanism provides a cost
 model to simulate the execution of the query plan and takes a
 divide-and-conquer approach to solve the problem. Finally, the
 mechanism is implemented in ParaLite [6], a shared-nothing
 parallel database system which provides a coordination layer
 2013 IEEE International Conference on Big Data
 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 237
to connect single-node database systems (SQLite [7]) and
 parallelize SQL queries across them.
 In summary, the major contributions of our work are as
 follows:
 • We provide a cost model to estimate the expected runtime
 of a whole query plan under a given failure probability of
 each operator. The model takes fine-grained parallelism
 within each operator into consideration. It splits each
 operator into many small logical tasks which is loosely
 coupled with processors.
 • We propose a divide-and-conquer algorithm to solve the
 problem. The algorithm divides the query plan into sub-
 plans with smaller search spaces. For a given query plan
 with n operators, the algorithm runs in O(n) time.
 • We develop the mechanism in a shared-nothing parallel
 database system, ParaLite, and verify it with several
 queries from TPC-H benchmark[8]. The experimental
 results firstly show that different fault-tolerant strategies
 affect the overall runtimes of queries. Then our selective
 checkpointing can choose reasonable operators to be
 checkpointed and outperforms other fault-tolerant strate-
 gies. In addition, the divide-and-conquer algorithm taken
 by our mechanism has a smaller overhead than brute-
 force approach while keeping a similar effectiveness.
 The rest of the paper is organized as follows: Section II
 presents related work. Section III gives a brief introduction
 of ParaLite. In Section IV, we introduce the problem setting
 and the detail of cost model for a query plan, and give the
 heuristic algorithm for the problem. Then Section V evaluates
 the mechanism and presents the experimental results. Finally,
 we draw conclusions in Section VI.
 II. RELATED WORK
 Most commercial parallel database systems [9] [10] [11]
 provide fault-tolerance through replication [12] [13] [14]. As
 they cannot handle the intra-query fault tolerance, if a failure
 occurs during a long running query, the entire query must be
 restarted from the beginning. To efficiently recover a long-
 running query from the middle of the execution, systems
 achieving intra-query fault tolerance have been proposed.
 FTOpt [15] provides an intra-query fault tolerant framework
 which enables the mixing and matching of different fault-
 tolerant techniques in a single pipelined query plan. However,
 FTOpt focuses on non-blocking query plans, where data are
 pipelined from one operator to the next, producing results
 incrementally. In this case, they assume that aggregation
 operators, if any, appear only at the top of a plan. Besides,
 FTOpt uses a brute-force algorithm to enumerate through
 the search space to get an optimal combination of fault-
 tolerant strategies. For a given query plan with n operators,
 the algorithm runs in ?(3n) time. Another work, Osprey [16],
 provides the ability to detect and recover from failures (or slow
 nodes) in long-running queries. It divides queries into sub-
 queries to be executed by PostgreSQL [17], and re-schedules
 a sub-query which failed or progresses too slowly to a different
 node based on data replication. Similar to MapReduce, Osprey
 adopts the MapReduce-style load balancing strategy of greedy
 assignment of work to solve the skew problem. However,
 Osprey is designed for data-warehouse applications in which
 tables are arranged in a star schema and it cannot support many
 other queries such as non-star joins and nested queries. On the
 other hand, ParaLite is designed for more general queries.
 In contrast to parallel database systems, MapReduce model
 [2] [3] [18] [19] [20] provides fine-grained fault tolerance
 by storing all intermediate results to a durable storage before
 making a further progress. As a result, map jobs and reduce
 jobs running on a failed worker are rescheduled on another
 worker, allowing that task to complete without restarting.
 However, this naive strategy is not always efficient especially
 for short to middle running jobs. To improve the performance,
 recent work [21] introduced the ability to partly pipeline data
 in Hadoop.
 III. OVERVIEW OF PARALITE
 ParaLite [6] is a workflow-oriented shared-nothing parallel
 database system based on a popular single-node database
 SQLite [7]. The basic idea of ParaLite is to provide a co-
 ordination layer to glue many SQLite instances together, and
 parallelize an SQL query across them. The architecture of our
 system is shown in Figure 1. It uses a classic master/worker
 pattern to organize resources. ParaLite is designed to be a
 serverless and zero-configuration system, so no process is
 running before a query is executed. ParaLite has multiple
 clients which present an SQL interface to users and allows
 a group of queries to be submitted at the same time.
 The basic flow to process a query in ParaLite is as follows:
 A single (or a group of) query is issued through multiple
 clients. One authorized client starts the master process which
 then parses the query, translates the query into the logical
 plan (DAG) which is the key structure to connect each logical
 component. Each node in the logical plan is either a relational
 operator such as join and aggregate or a sub-query which in-
 tegrates one or more operators. Edges represent data transfers
 among operators. Based on the logical plan, the master spawns
 corresponding operators on worker nodes. Then the master
 schedules each operator to worker nodes by the runtime. Each
 worker process is responsible for receiving data from children
 operators, processing them and distributing the output to the
 next operator. The top (root) operator returns the final results
 to clients. All data are sent directly between worker nodes and
 thus the master is not a bottleneck for any data transfers.
 As the intended applications for ParaLite are workflows
 typically built out of various independently developed executa-
 bles, ParaLite extends SQL to support seamless integration of
 arbitrary executable called User-Defined Executable (UDX)
 which can be written in any language. Long-running exe-
 cutables in workflow makes intra-query fault tolerance more
 important. UDX is very flexible because a user does not need
 to develop a program respecting to rigid formatting rules such
 as ?key, value? input/output format or write code with respect
 to strict specifications of databases.
 238
 !"#$% &'($
 )!#! *'++,-./!#.'- 01.2$"3 4*5 5.6$"7
 )8
 9'%:$% &'($ 9'%:$% &'($ 9'%:$% &'($
 )!#! *!#!;'<
 &$#='%: 4'6'2'<
 >-?'%+!#.'- *$-#$%
 @,$%A >($-#.B/!#.'-
 *2.$-#
 @,$%A C$",2#
 *2.$-#*2.$-#
 @,$%A 4%!-"?'%+$%
 D'E  !-!<$%
 D'E F<$-# D'E F<$-# D'E F<$-#
 )8)8
 D'EGHI$/,#'% D'EGHI$/,#'% D'EGHI$/,#'%
 Fig. 1. Architecture of ParaLite
 The syntax of UDX is similar to that of the traditional User-
 Defined Functions, with the main difference being that the
 function can be given as a command line.
 select col1, F(col2) as new_col2
 from T
 where <predicate1, predicate2, ...>
 with F = "command_line"
 A UDX can work on and produce arbitrary columns. To
 avoid registration to system before the query is executed,
 ParaLite allows users to define the UDX within the query
 using WITH clause. It starts from a command line followed
 by data format options, such as, input, input row delimiter,
 output and out row delimiter.
 IV. SELECTIVE CHECKPOINTING MECHANISM
 A. Query Model
 A query is expressed by a DAG (query plan) of relational
 operators each of which forwards data tuples to the next. For
 example, the query plan of Query 3 of TPC-H benchmark
 [8] is shown in Figure 2. As ParaLite stores data in SQLite
 database, it uses SQL query to access data. To take advantage
 of database technologies of SQLite, e.g. indexing and query
 optimization, we push as much operations as possible into
 the query to the underlying SQLite. So the leaf nodes of the
 plan are sub-query operators which read relations using cor-
 responding predicates and produce a row-and-column subset
 of the relational table. Operators J3 and J4 join two relations.
 The output from J3 is then aggregated and sorted. Specially,
 some operators can be integrated into a single query if no
 repartitioning operations are necessary for them. For example,
 if relations Customer and Orders are both hash-partitioned
 across data nodes by the join attribute, J4, S6 and S7 can
 be integrated into a single query.
 Each operator is either a pipeline operator, which can
 process each tuple independently without the knowledge of
 all tuples, or a blocking operator which must receive all tuples
 before emitting the result, e.g, an aggregation operator and a
 sorting operator. For hash joins, we need to have all tuples
 from at least one table to build the hash table for it. Then we
  !"# $%&$#
 '("#'$)*+#
 ,-"#,)./#012# 3#
 ,4"#,)./#052# 3#
 67"#6*89:*&$;#053 6<"#6*89:*&$;#0 3
 6="#6*89:*&$;#013#
 select 
   L.orderkey,
    sum(L.extendedprice * (1 - L.discount)) as revenue,
    O.orderdate, O.shippriority
 from
    Customer C, Orders O, LineItem L
 where
    C.mktsegment = ’AUTOMOBILE’
    and C.custkey = O.custkey
    and L.orderkey = O.orderkey
    and O.orderdate < ’1995-10-11’
    and L.shipdate > ’1995-10-11’
 group by
    L.orderkey, O.orderdate, O.shippriority
 order by
    revenue DECS, O.orderdate
 Fig. 2. A Logical Plan for TPC-H Query 3
 could in principle emit outputs as tuples from the other table
 are coming. To simplify the implementation, we consider a
 join operator as a blocking operator in our work.
 Each operator is split into multiple logical tasks and as-
 signed to a set of processors. The number of tasks is de-
 termined by the number of partitions for the input tuples of
 the operator and is usually much larger than the number of
 assigned processors. If an operator’s successor is a pipeline
 operator, it forwards the output tuples of each task to the
 its successor as soon as the task is finished. The target
 processor is chosen based on its processing capacity in terms
 of estimated runtime. For an operator whose successor is a
 blocking operator, it holds all output data on memory until
 it reaches a threshold, at which point it writes them into an
 intermediate file. Tasks of a blocking operator is scheduled
 to processors using a greedy algorithm to balance the load
 across all processors. Once a processor becomes idle, a task
 is allocated to the processor.
 Next, let’s consider the checkpointing strategy used in query
 plans. If all data are pipelined from one operator to the next,
 once a job fails, all operators must be re-executed because
 all intermediate data would be lost. To avoid re-execution
 from the beginning, a common strategy is checkpointing and
 rollback recovery approach which periodically records the
 state of operators to a durable storage. To ensure the right
 recovery from a failure, an operator is required to save
 sufficient information to replay its state such as join hash
 tables and partial aggregation results. Once failures occur, the
 operator restarts from the last state (checkpoint).
 In our work, we take a simpler checkpointing method
 which does not save internal states of an operator but only
 save the output of an operator in a durable storage. This
 checkpointing method works because the execution of each
 operator can be divided into many independent tasks and only
 recovering failed tasks can reduce the work to be lost. The
 output tuples for each operator are split or hash partitioned
 into n parts according to its successor operator. Accordingly,
 the processing of the successor is split into n logical tasks each
 of which works on the corresponding part of data. The tasks
 are then scheduled to a specific number of processors (which
 is usually much smaller than the number of tasks) performing
 the calculation such as join and aggregation. For long-running
 operator i, a checkpointing should be made for i, that is, the
 output tuples of each task are materialized to the disk. Once
 239
a failure occurs, saying a process for i fails, only the failed
 task (which is running exactly when the failure occurs) need
 to be re-executed. As a result, the recovery overhead of the
 failed operator is very limited as each task usually runs in a
 small time.
 With checkpointing strategy, the places to insert check-
 pointing heavily affect the overall execution performance. On
 one hand, not making a necessary checkpoint may lead to a
 loss of important computation, degrading the overall execution
 performance. A checkpointed operator speeds up the recovery
 for its successor operator as reproducing the output tuples
 is simply re-reading materialized data. For example, for a
 query plan A?B?C?D with only A checkpointed, if D fails,
 operator B, C and D have to be re-executed while A does not.
 So only checkpointing A leads to too much lost work when
 a failure occurs. On the other hand, making an unnecessary
 checkpoint leads to an increase in the checkpointing overhead.
 We also take the above query plan as an example. If the
 operator C is light on CPU but emits lots of tuples, it is
 unlikely to be worth being checkpointed. An extreme case
 is that the checkpointing time is wasted if no failure occurs.
 B. Problem Setting
 Assume that the logical plan of a query is G =< V, E >,
 where V is a set of operators V = v1, v2, ..., vn and E is
 the set of edges (or dependencies) between operators E =
 (vi, vj)|vi, vj ? V . When the query is executed without any
 failure, the expected runtime of the query Ttotal is calculated
 as a function of the operator and edge sets:
 Ttotal = F (V, E) (1)
 In a failure-prone environment, we assume that each operator
 is executed with a probability of failure P = p1, p2, ..., pn. If
 an operator vi is failed, all of its children are required to be re-
 executed to replay the input for vi if they are not checkpointed.
 Otherwise, the checkpointed operators can simply replay its
 output by reading them from a durable storage. The problem
 is that what operators should be checkpointed. Not making a
 necessary checkpoint may lead to a loss of important com-
 putation, affecting the overall execution performance while
 making an unnecessary checkpoint leads to an increase in the
 checkpointing overhead as well. Let’s denote another variable
 CK = ck1, ck2, ..., ckn) to mark if a checkpoint is necessary
 or not for each operator. In the case of failure, the expected
 runtime of the query is calculated by the following equation:
 Ttotal = F (V, E, CK, P ) (2)
 Therefore, the problem is equivalent to get the a set of
 checkpoints to minimize the expected runtime:
 CKopt = argmin
 CK
 F(V, E, CK, P) (3)
 C. Cost Model
 In this section, we explain how we model the cost of an
 execution plan of a query and propose the problem for the
 selective checkpointing.
 Processing Cost: For operator i, the processing cost TPi
 is the delay introduced by the operator. TPi is the sum of
 execution time for its input tuples and the communication time
 for its output tuples as shown in Equation 4.
 TPi = TEXEi + TSENDi (4)
 TEXEi : the cost for executing input tuples of operator i;
 TSENDi : the cost of sending output data to the successor for
 operator i.
 To estimate TPi , we assume the following two functions are
 known for each operator i:
 fi(Nin tuplei): this function provides the number of output
 tuples produced for a given number of input tuples Nin tuplei
 of operator i:
 Nout tuplei = fi(Nin tuplei) (5)
 gi(Nin tuplei): this function provides the time to produce all
 output tuples for a given number of input tuples Nin tuplei of
 operator i:
 TEXEi = gi(Nin tuplei) (6)
 In practice, such infomation could come from profiling and
 statistics, or could be supplied by user. Specifically, the
 function fi is simply given with respect to the selectivity for
 each operator while gi is more complicated estimated. The
 function gi models the processing for each tuple, e.g. for an
 operator only with a sorting algorithm, the gi(n) is of the form
 n? log(n). We ignore other fixed, auxiliary cost for operators
 such as initiating and terminating the operation. If the output
 tuples of the operator i is divided into Ntask tasks with hash
 function h(key) = i 1 ≤ i ≤ Ntask, the partitioning cost is
 added to TEXEi .
 The cost for sending Nout tuplei tuples depends on the
 number of the successor operator. For the blocking successor
 operator, such as group operator, the output tuples are required
 to be re-partitioned on the group key and transferred to all
 successor processors. For other operators, the output tuples
 are transferred to only one or a few processors.
 Checkpointing Cost: We set a checkpoint value CKi for
 operator i:
 CKi =
 {
 0 if a checkpoint for i is not necessary
 1 otherwise
 For operator i, the checkpointing cost TCi represents the cost
 to write the output tuples of i to disk:
 TCi = CKi ? TI/O ?Nout tuplei (7)
 TI/O: the time taken to write/read a tuple from/to disk.
 Recovery Cost: For operator i, the recovery cost TRi
 depends on the checkpointing values of both i and its pre-
 decessors. The recovery time of an operator i contains two
 parts as equation 8 shows: T ?Ri which is the cost for getting
 the input tuples again and T ??Ri which is the cost for the re-
 execution of the operator i itself.
 For the former, assuming that the operator i fails, its
 input tuples are required to be reproduced, that is, all of its
 240
predecessor operators need to reproduce their output tuples.
 Thus, the recovery time for the operator i is determined by
 the predecessor who takes longer time to reproduce its output
 tuples as equation 9 shows. For each of its predecessors j,
 the cost for reproducing the output tuples of j is reading data
 from disk if j is checkpointed (CKj = 1). Otherwise, if j is
 not checkpointed (CKj = 0), j must be re-executed.
 TRi = T ?Ri + T
 ??
 Ri (8)
 T ?Ri = maxj?PREDi
 ((TPj + T ?Rj )? (1? CKj) + TCj ) (9)
 For the second part of the recovery cost, T ??Ri , if the
 successor of operator i is a blocking operator, i keeps the
 output tuples of received tasks until all tasks are finished.
 As a result, all tasks are re-executed if this operator is not
 checkpointed. If the operator is checkpointed, only the failed
 task (which is running at the time the operator fails) needs
 to be re-executed. If the successor of operator i is a pipeline
 operator, the output of a task is sent to its successor quickly.
 So still only the failed task needs to be re-executed.
 Objective Function: Assuming that infinite resources are
 used, the expected runtime of an operator i is the sum of
 the processing cost of the operator TPi (Eq 4), the expected
 runtime of its children, the recovery cost TRi (Eq 8) and the
 checkpoint cost TCi (Eq 7).
 Ti = TPi + maxj?PREDi
 (Tj) + pi ? TRi + TCi (10)
 P REDi: the set of predecessor operators of i
 pi: the probability of a failure for operator i.
 Obviously, the expected runtime of a query plan Ttotal is
 estimated by the following equation according to equation 2
 (where operator 0 represents the root of the query plan):
 Ttotal = F (V, E, CK, P ) = T0
 = TP0 + maxj?PRED0
 (Tj) + p0 ? TR0 + TC0 (11)
 The objective is to find the optimal checkpointing value for
 each operator to minimize the expected runtime of a given
 plan.
 CKopt = argmin
 CK
 Ttotal (12)
 D. Heuristic Algorithm
 The straightforward approach to the problem is to generate
 all solutions in the full search space and get an optimal one.
 However, if the size of the query plan (saying that the number
 of operator is n) is large, this approach is not practical as the
 search space SG is 2n. So we propose a heuristic algorithm
 (using divide-and-conquer approach) to reduce the search
 space. Although the algorithm cannot produce the global
 optimal solution, its efficiency is verified by the experiments.
 x y
 CKx = ?
 y
 x1
 x2
 CKx2 = ?
 CKx1= ?
 (a) simple plan (b) complex plan
 Fig. 3. Example of Simple and Complex plan
 1) Reduction of Checkpointing Candidates: First of all, we
 use a simple inequality to filter the operators which should
 not be checkpointed. Let’s firstly consider a simple sub-plan
 in which each operator has at most one upstream operator as
 Figure 3 shows.
 Assume that a failure occurs at the operator y, the runtime of
 y depends on whether operator x is checkpointed (CKx = 1)
 or not (CKx = 0). When x is not checkpointed, the recovery
 starts from the beginning and the runtime of y should be:
 Ttotaly{CKx=0} = 2? Ttotalx + T ?Ry (13)
 With the checkpoint, the recovery of y starts from read
 tuples from disk and the runtime of y with consideration of a
 checkpoint is:
 Ttotaly{CKx=1} = Ttotalx + 2? tx + T ?Ry (14)
 tx is the I/O cost for reading/writing the output of x from/to the
 disk. For n tuples, the I/O cost is estimated as tx = TI/O?n.
 To get benefit from making a checkpoint with a failure, the
 runtime of y with a checkpoint should be smaller than that
 without a checkpoint:
 Ttotaly{CKx=1} < Ttotaly{CKx=0} ? 2? tx < Ttotalx
 (15)
 Inequality 15 intuitively shows that the process of a query
 plan can gain from a checkpoint of operator i only if the
 checkpointing cost (writing and reading the output) of i is
 smaller than its execution time. This observation generates the
 opportunities to reduce whole search space by setting CKi =
 0 if i does not satisfy the inequality.
 Next, let’s consider a more complex sub-plan as the right
 side of Figure 3 shows. Operator y has two predecessors
 x1 and x2 which are executed in parallel. We take the
 following algorithm to decide the checkpoint values for x1
 and x2. Sibling Checkpointing Algorithm firstly decides the
 checkpointing value for the operator with larger execution time
 (denoted by i) according to the single Checkpointing function.
 The checkpointing value of the operator with smaller execution
 time (denoted by j) is decided by i’s checkpointing value. If
 i is not worth being checkpointed, the algorithm thinks it is
 better for not checkpointing j because the recovery time for
 their parent is determined by the execution time of i. When i
 is checkpointed, j is also checkpointed if the overall cost of
 the processing and checkpointing for j is smaller than that of
 241
i and otherwise the checkpointing value of j is set through
 the single Checkpointing function.
 Algorithm 1 Sibling Checkpointing
 Require: two sibling operators: x1 and x2
 procedure SINGLE CHECKPOINTING(i)
 if inequality15(i) = T rue then return 1
 else return 0
 end if
 end procedure
 procedure SIBLING CHECKPOINTING(x1, x2)
 small ? min execution time(x1, x2)
 big ? max execution time(x1, x2)
 CKsmall ? 0
 CKbig ? single Checkpointing(big)
 if CKbig = 1 then
 if TPbig + TCbig > TPsmall + TCsmall then
 CKsmall ? 1
 else
 CKsmall ? single Checkpointing(small)
 end if
 end if
 end procedure
 2) Divide-and-Conquer Approach: Divide-and-conquer ap-
 proach is a common strategy that comes next to the brute-force
 to reduce the search space. It works as follows: query plan G
 is divided into sub-plans, denoted by G(i), each with a smaller
 search space SiG such that the globally close-to-optimal choice
 in SG can be found by composing the optimal choices found
 for each SiG. For each sub-plan, we use a brute-force search
 to find an optimal decision. We assume that the checkpointing
 decisions for operators within a sub-plan affect each other but
 not affect the decisions for operators in other sub-plans. In
 other words, the goal is to break the large plan space SG into
 independent subspaces SiG such that SG =
 ?
 S(i)G . Within
 each G(i), we take a brute-force approach to get an optimal
 solution among all possibilities.
 While the plan can be arbitrarily divided into sub-plans,
 we generate the sub-plans based on a key insight: how check-
 pointing decisions affect each other. In theory, a decision for a
 specific operator can influence the choice of any other operator.
 But from our experience, the checkpointing decision for an
 operator mainly affects the decision for its successors. For
 example, a typical case is that when an operator A produces
 large data and its successor B produces small data with light
 computations, then it is better to checkpoint B rather than A.
 Based on this observation, we make each sub-plan a small
 component of the original plan, as described below.
 Partitioning into Sub-plans: Each sub-plan consists of one
 or several sibling operators and their unique successor. Sibling
 operators with the same successor are executed concurrently
 and the checkpointing decisions for them are influenced by
 each other as shown in algorithm Sibling Checkpointing.
 We take a query plan for TPC-H Query 3 as an example
 (Figure 4). As the algorithm traverses the plan in topological
  !  "
 #$  %
 #&
 '(
 )*
  !  "
 #$  %
 #&
 '(
 )*
 '+*,
 '+(,
  !  "
 #$  %
 #&
 '(
 )*
 '+&,
 Fig. 4. An example for the sub-plan generation
 order, the first sub-plan G(1) consists of operator J4 and its
 two predecessors S6 and S7. The next sub-plan G(2) comes
 with J4, its sibling S5 and its successor J3. When an operator
 has no siblings, the sub-plan only contains two operators such
 as G(3).
 Search in a Sub-plan: For each sub-plan G(i), the algo-
 rithm is to make the optimal checkpointing decisions for all
 operators in the sub-plan to minimizes the expected runtime
 of the successor of the sub-plan using a brute-force approach.
 The number of operators within any individual sub-plan is
 typically small. With a query plan presented as a binary tree
 with n operators, the number of operators in each sub-plan is
 at most 3 and the number of sub-plans is between n+12 ? 1
 (for full binary tree in which every operator other than the
 leaves has two children) and n?1 (for degenerate binary tree
 in which every operator only has one child) where n > 1. So
 the search space is varying from (n+12 ?1)?23 to (n?1)?23
 which is much smaller than the original 2n.
 Algorithm: Overall, the selective checkpointing algorithm
 is described as follows: (1) For a given query plan G, the
 algorithm firstly finds the candidate operators to be check-
 pointed based on the Sibling Checkpointing Algorithm de-
 scribed above; (2) Then it generates the first sub-plan; (3) It
 enumerates all solutions within the sub-plan to find the optimal
 solution for the sub-plan; (4) It generates the next sub-plan and
 repeats the process until the entire plan is visited.
 V. EVALUATION
 We conducted all experiments with a variety of queries
 in a 16-node cluster. Each node uses 2.40 GHz Intel Xeon
 processor with 8 cores running 64-bit Debian 6.0 with 24GB
 RAM. The experiments are performed to verify: (1) Different
 fault-tolerant strategies heavily affect the overall runtimes
 of queries; (2) Our selective checkpointing mechanism can
 choose reasonable operators to be checkpointed; (3) The
 mechanism outperforms other fault-tolerant strategies; (4) The
 divide-and-conquer algorithm has a smaller overhead than
 brute-force approach while keeping a similar effectiveness;
 (5) Our mechanism can achieve similar slowdown with Hive
 (Hadoop) upon a failure; (6) The system (ParaLite) scales well
 with the mechanism.
 242
150
 86
 15270600 600  84 0.002
 150 150 150
 600 270 270 6 1
 270
 600
 86
 150
 3
 15
 110150 22 66 0.0006
 180
 600
 110
 150
 S1 S1
 S1
 S6
 S5
 (a) Query 1
 (b) Query 2
 (c) Query 3
 (d) Query 4
 (e) Query 5
 Fig. 5. Query plans of several TPC-H queries: all numbers are in millions
 A. Effects of Fault-tolerant Strategy
 We firstly show that different fault-tolerant strategies affect
 the overall performance of a query plan. We perform typical
 SQL tasks (selection (S), join (J) and aggregation (G)) through
 queries 1 and 2 from Figure 5 and a special task with User-
 defined Executable (U) by query 3. The number of tuples of
 each relations are shown in the query plans and each tuple is
 about 0.2KB.
 Recall that our cost model requires two functions for
 operator i: fi provides the number of output tuples for a given
 number of input tuples and gi gives the processing time for
 a given number of input tuples (shown in equation 5 and 6).
 Function fi is determined by the selectivity of the operator. We
 define gi based on our measurements. For example, for a sub-
 query, the execution time (seconds) is estimated as 3? 10?6
 times the number of input tuples. We estimate the failure
 probability of each operator by pi = TiTtotal ? Z where Ti
 is the estimated runtime of operator i, Ttotal is the runtime
 of the query and Z is the expected number of failures for the
 query which can be specified by administrators according to
 their observations.
 Figure 6 through 8 show the actual and predicted runtime
 for Queries 1 through 3. X-axises are the fault-tolerant strate-
 gies for operators. For example, in Figure 7, NN means No
 checkpoint for S1 and G0 while NC indicates No checkpoint
 for S1 and a checkpoint for G0. Note that the order of the
 operators is from left to right in the query plans. For Query
 1, to show the results clearly, we assume that no checkpoint
 for S2. We inject a failure at the middle of the execution.
 • Different fault-tolerant strategies heavily affect the overall
 runtimes of queries. The differences between the overall
 execution time with the best and worst strategy are high:
 For normal execution without a failure, the differences
 are 30% for Query 1, 15% for Query 2, and 61% for
 Query 3. For the execution recovered from a failure,
 the differences are 14% for Query 1, 30% for Query
 2, and 56% for Query 3. Each of queries 1 through
 3 achieves the best performance with a different fault-
 tolerant strategy in both failure-free (normal) and failure-
 prone environments. For all queries, checkpointing noth-
 ing (NN) is the best option if no failure occurs. However,
 once a failure occurs, only Query 3 achieves the best
 performance with this strategy while Query 1 requires
 the strategy NC and Query 2 requires CC.
 • Our mechanism can identify the best strategy for all
 queries. The predicted execution time can not exactly
 match but is close to the actual time. Most of the differ-
 ence come from the simple model for the data transfer and
 the assumption that data are balanced among processes,
 which are not satisfied in real cases. The model was
 still able to estimate the relative order between execution
 times of different strategies. Our mechanism simulates
 the execution in a failure-prone environment and predicts
 the runtime of a query plan with the assumption that each
 operator has a fixed probability of failure, so it considers
 the strategy with the smallest predicted execution time
 with failures as the best one. Therefore, for Query 1,
 the option of only checkpointing the output of the join
 operator (NC) is chosen although the execution time with
 option NC in normal situation is slightly larger than
 that with the option of checkpointing nothing (NN). For
 Query 2, the strategy of checkpointing everything (CC)
 is considered as the best strategy by our mechanism. For
 Query 3, it decides to checkpointing nothing.
 • Finally, restarting a query, the strategy used in most
 existing database systems, produces the largest slowdown
 upon a failure among all strategies. Strategies other than
 RESTART reduce recovery times with minimal impact on
 the execution time without failures. For Query 1 through
 3, RESTART is 14%, 30% and 39% worse than the best
 strategy respectively.
   0
   20
   40
   60
   80
   100
   120
   140
   160
   180
 NN NC(S) CN CC RESTART
 Elapse Time(s
 )
 Query 1
  Real(Normal)
  Predict(Normal)
  Real(w/ failure)
  Predict(w/ failure)
 Fig. 6. Query 1 (JOIN)
 B. Effectiveness of Selective Checkpointing
 In this section, we apply several fault-tolerant strategies to
 two TPC-H queries, Query 4 and 5 from Figure 5.
 We compare the overhead for choosing checkpointing op-
 erators and the places of checkpointing with both Divide-
 243
  0
   50
   100
   150
   200
 NN CN NC CC(S) RESTART
 Elapse Time(s
 )
 Query 2
  Real(Normal)
  Predict(Normal)
  Real(w/ failure)
  Predict(w/ failure)
 Fig. 7. Query 2 (GROUP)
   0
   20
   40
   60
   80
   100
   120
   140
 NN(S) CN NC CC RESTART
 Elapse Time(s
 )
 Query 3
  Real(normal)
  Predict(normal)
  Real(w/ failure)
  Predict(w/ failure)
 Fig. 8. Query 3 (UDX)
 and-Conquer (DaC) and Brute-Force (BF) algorithms. From
 Table I, we can see that although the overhead with both
 algorithms is very small but DaC is several times faster than
 BF. The differences exponentially increase with the number of
 operators. For the tested queries, the places for checkpointing
 are the same for both algorithms.
 We inject a failure at 80% point of the execution for Query
 4 and at 50% point of the execution for Query 5. Figure
 9 and Figure 10 show the actual and predicted execution
 time with/without a failure. In the figures, CKNONE means
 that no operators are checkpointed; CKALL indicates all
 intermediate data are materialized; SELECTIVE shows that
 operators chosen by our selective checkpointing mechanism
 are checkpointed.
 Firstly, SELECTIVE outperforms other strategies for both
 queries. It adds the minimal time upon a failure to the
 execution time of the NONE strategy in failure-free situations.
 For Query 4, SELECTIVE adds 3% overhead to the execution
 time of NONE (the best strategy) when no failures occur, while
 it is 58% faster than NONE when a failure is injected. For
 Query 5, SELECTIVE is 6% slower than NONE in failure-free
 execution but is 17% better than NONE with a failure. While
 TABLE I
 DIVIDE-AND-CONQUER COMPARED WITH BRUTE-FORCE
 Overhead(second) Checkpoints
 Query4 Query5 Query4 Query5
 Divide-and-Conquer 0.011 0.0095 S1, J2 G0,S2,J3,S4,S5
 Brute-force 0.15 0.092 S1, J2 G0,S2,J3,S4,S5
   0
   50
   100
   150
   200
   250
   300
 CKNONE SELECTIVE CKALL RESTART
 Elapse Time(s
 )
 Query 4
  Real(Normal)
  Predict(Normal)
  Real(w/ failure)
  Predict(w/ failure)
 Fig. 9. Query 4 (SJJJ)
   0
   50
   100
   150
   200
 CKNONE SELECTIVE CKALL RESTART
 Elapse Time(s
 )
 Query 5
  Real(Normal)
  Predict(Normal)
  Real(w/ failure)
  Predict(w/ failure)
 Fig. 10. Query 5 (SJJG)
 SELECTIVE and CKALL both produce several times smaller
 recovery time than NONE and RESTART, SELECTIVE out-
 performs CKALL in the overall execution time from 6% to
 44%. Specially, although RESTART is at most 5% better than
 SELECTIVE when no failures occur, it is 25% and 60% slower
 than SELECTIVE upon a failure for both queries respectively.
 Secondly, SELECTIVE chooses reasonable operators to
 be checkpointed. For Query 4, as the previous operator J4
 produces a large number of output tuples, it is better to
 checkpoint its predecessor J2 if it takes a small time and
 produces a small number of output tuples. As we expected,
 SELECTIVE checkpoints the operator J2. Meanwhile, based
 on the Sibling Checkpointing Algorithm described in section
 4.3, operator S1 should be checkpointed too. For Query 5, all
 operators are checkpointed except J1. This is also reasonable
 because re-producing the output of J1 is not time-consuming
 since its two input source are checkpointed.
 C. Comparison of Slowdown
 We conducted a set of experiments to compare the slow-
 down of Query 5 with ParaLite and Hive[18]. Hive is a data
 warehouse system built on top of Hadoop[3]. It translates
 the HiveQL (SQL-like language) into MapReduce jobs which
 are executed by Hadoop. Hadoop provides fine-grained fault
 tolerance because it stores all intermediate data into a durable
 storage (HDFS). Once a node fails, all failed tasks on that
 node rather than the whole job are re-scheduled to another
 node to be executed. ParaLite uses our selective checkpointing
 mechanism to materialize the output of a set operators and
 restart all related tasks once a failure occurs.
 244
  0
   50
   100
   150
   200
   250
   300
 ParaLite Hive ParaLite Hive ParaLite Hive
 Elapse Time(second
 )
 20% 50% 80%
  Recovery 
 Normal
 Fig. 11. The slowdown of Query 5 with ParaLite and Hive
 The results are shown in Figure 11. The X-axis is the
 percentage of the normal completion time of Query 5 when a
 failure is injected. Firstly, it is not surprised that ParaLite is
 about twice faster than Hive. One reason for the superiority
 is that not all the operators are checkpointed in ParaLite.
 Moreover, Hive uses sort-join while ParaLite uses hash-join.
 In addition, the start-up overhead for Hadoop cannot be
 ignored. Hive uses 3 MapReduce jobs to express the query
 and each takes 10 15 seconds before all map tasks are
 started. Secondly, the slowdown for both system are similar.
 However, the slowdonw of Hadoop is limited to the interval of
 heartbeat message send to the JobTracker(master) from each
 TaskTracker(worker). After a fix time (we set it 30 seconds)
 from receiving the last heartbeat, if the master does not receive
 a new heartbeat message, it sets the status of the worker failed
 and re-schedules the failed tasks. So for a light-weight job
 whose execution time is not much larger than the interval, the
 slowdown becomes larger because it needs to wait for the task
 re-execution. On the other hand, ParaLite detects the failure
 immediately after a process fails and then starts the recovery.
 D. Scalability
 To evaluate the scalability of ParaLite, we perform TPC-
 H Query 3 on 10, 20 and 30 nodes with the scale factor 100,
 200 and 300 respectively. As Figure 12 shows, ParaLite scales
 well with the increase of nodes. The selective checkpointing
 mechanism adapts to clusters with different number of nodes.
   0
   50
   100
   150
   200
   250
   300
   350
   400
 10 20 30
 Execution Time(s
 )
 # of nodes
 Fig. 12. The scalability of TPC-H Query 2
 VI. CONCLUSION
 To efficiently support intra-query fault tolerance for long-
 running queries, we propose a selective checkpointing mecha-
 nism which automatically chooses necessary operators whose
 output data are worth being checkpointed to minimize the
 expected runtime of the query in a failure-prone environ-
 ment. The mechanism provides a cost model to simulate the
 execution of the query plan and takes a divide-and-conquer
 approach to solve the problem. The mechanism is implemented
 in a share-nothing parallel database ParaLite and evaluated
 with several TPC-H queries. The experimental results firstly
 show that different fault-tolerant strategies affect the overall
 runtimes of queries. Our selective checkpointing can choose
 reasonable operators to be checkpointed and outperforms other
 fault-tolerant strategies. In addition, the divide-and-conquer
 algorithm taken by our mechanism has a smaller overhead than
 brute-force approach while keeping a similar effectiveness.
 REFERENCES
 [1] D. DeWitt and J. Gray, “Parallel Database Systems: the Future of High-
 performance Database Systems,” Commun, vol. 35, no. 6, pp. 85–98,
 1992.
 [2] J. Dean and S. Ghemawat, “MapReduce: Simplified Data Processing on
 Large Clusters,” in OSDI’04, 2004, pp. 137–150.
 [3] T. White, Hadoop: The Definitive Guide. O’Reilly Media, Inc., 2009.
 [4] E. Deelman, D. Gannon, M. S. Shields, and I. Taylor, “Workflows and
 E-science: An Overview of Workflow System Features and Capabilities,”
 Future Generation Comp. Syst., vol. 25, no. 5, pp. 528–540, 2009.
 [5] K. Plankensteiner, R. Prodan, and T. Fahringer, “Fault-tolerant Be-
 haviour in State-of-the-art Grid Workflow Management Systems,” in
 CoreGRID Technical Report, TR-0091, 2007.
 [6] T. Chen and K. Taura, “ParaLite: Supporting Collective Queries in
 Database System to Parallelize User-Defined Executable,” in 12th
 IEEE/ACM International Symposium on Cluster, Cloud and Grid Com-
 puting, 2012, pp. 474–481.
 [7] SQLite. Http://www.sqlite.org/.
 [8] TPC-H. Http://www.tpc.org/tpch.
 [9] Oracle Database. Http://www.oracle.com/.
 [10] Vertica. Http://www.vertica.com/.
 [11] Greenplum Database. Http://www.greenplum.com/.
 [12] L. Golubchik and R. R. Muntz, “Fault Tolerance Issues in Data
 Declustering for Parallel Database Systems,” Bulletin of the Technical
 Committee on Data Engineering, vol. 14, 1994.
 [13] D. Bitton and J. Gray, “Disk Shadowing,” in Proc. of VLDB, 1988, pp.
 331–338.
 [14] G. P. Copeland and T. W. Keller, “A Comparison Of High-Availability
 Media Recovery Techniques,” in SIGMOD Conference, 1989, pp. 98–
 109.
 [15] P. Upadhyaya, Y. Kwon, and M. Balazinska, “A Latency and Fault-
 tolerance Optimizer for Online Parallel Query Plans,” in SIGMOD, 2011,
 pp. 241–252.
 [16] C. Yang, C. Yen, C. Tan, and S. Madden, “Osprey: Implement-
 ing MapReduce-style Fault Tolerance in a Shared-nothing Distributed
 Database,” in ICDE, 2010, pp. 657–668.
 [17] PostgreSQL. Http://www.postgresql.org/.
 [18] A. Thusoo, J. S. Sarma et al., “Hive-A Warehousing Solution Over a
 Map-Reduce Framework,” in Proceedings of VLDB Endow, 2009, pp.
 1626–1629.
 [19] A. Abouzeid, K. Bajda-Pawlikowski et al., “HadoopDB: an Architec-
 tural Hybrid of MapReduce and DBMS Technologies for Analytical
 Workloads,” Proceedings of VLDB, vol. 2, no. 1, pp. 922–933, 2009.
 [20] C. Olston, B. Reed, U. Srivastava, R. Kumar, and A. Tomkins, “Pig
 Latin: A Not-So-Foreign Language for Data Processing,” in Interna-
 tional Conference on Management of Data - SIGMOD, 2008, pp. 1099–
 1110.
 [21] T. Condie et al., “MapReduce Online,” in Proc. of the 7th NSDI Symp.,
 2010.
 245

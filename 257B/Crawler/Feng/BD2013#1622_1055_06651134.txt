Energy Efficient Workflow Job Scheduling for Green Cloud
 Fei Cao (Fourth year Ph.D. student)
 Department of Computer Science
 Southern Illinois University, Carbondale
 Carbondale, IL, 62901, USA
 Email: vicky@siu.edu
 Michelle M. Zhu (Advisor)
 Department of Computer Science
 Southern Illinois University, Carbondale
 Carbondale, IL, 62901, USA
 Email: mzhu@cs.siu.edu
 Abstract—The elastic resource provision, noninterfering re-
 source sharing and flexible customized configuration provided
 by the Cloud infrastructure has shed light on efficient execution
 of many scientific applications modeled as Directed Acyclic
 Graph (DAG) structured workflows. However, the energy cost
 on running the increasingly deployed Cloud data centers
 around the globe together with the amount of  emissions
 have skyrocketed. In order to maintain sustainable Cloud
 computing facing with ever-increasing problem complexity and
 big data size, we propose an energy-efficient scientific workflow
 scheduling algorithm to minimize energy consumption and
  emission while satisfying certain Quality of Service (QoS).
 Our multiple-step resource provision and allocation algorith-
 m applies Dynamic Voltage and Frequency Scaling (DVFS)
 technology to reduce energy consumption within acceptable
 performance bounds, and minimize the Virtual Machine (VM)
 overhead for further reduced energy consumption and higher
 resource utilization rate. The candidacy of multiple data cen-
 ters from the energy and performance efficiency perspectives is
 also evaluated. The simulation results show that our algorithm
 is able to achieve an average up to 30% of energy savings and
 increase the resource utilization rate for about 25% leading to
 higher profit and less  emissions.
 Keywords-Energy-efficient; Green Cloud Computing; Work-
 flow Scheduling; VM Allocation
 I. INTRODUCTION
 The flexible utility-oriented pay-as-you-go Cloud comput-
 ing model has demonstrated tremendous potential for both
 commercial and scientific users to access and deploy their
 applications anytime from anywhere at reasonable prices
 depending on their QoS specifications. The computing power
 of the Cloud environment is supplied by a collection of
 data centers that are typically installed with hundreds to
 thousands of servers which are built on virtualized compute
 and storage technologies. Meanwhile equally massive cool-
 ing systems are required to keep the servers within normal
 operating temperatures. However, these infrastructures con-
 sume tremendous amounts of energy. The cost of servers
 running and cooling systems has increased by 400% over
 the past decade, and such cost is expected to continue to
 rise [1]. Following the current usage and efficiency trends,
 energy consumption by data centers could nearly double
 in another five years to more than 100 billion kWh [2].
 Besides the energy cost, considerable amount of  emis-
 sions produced by these data centers significantly contribute
 to the growing environmental issue of Global Warming.
 Gartner estimated that the Information and Communication
 Technologies (ICT) industry generates about 2% of the
 total global  emissions in 2007 [3]. Reducing energy
 consumption for modern data centers has been recognized
 as an ever increasingly important technique for operation
 cost, environment footprint and system reliability.
 Therefore, we want to study the problem of supporting
 scientific applications modeled as DAG-structured work-
 flows over the Cloud. The dependency and parallelism
 embedded in a workflow requires that the tasks be dispatched
 to a group of distributed VMs in order to maximize the
 execution efficiency. Also the Cloud resource availability
 map is time-dependent due to on-demand and reservation
 instances. The startup, idle and shutdown time of VMs
 compose the overhead of resource utilization. The overhead
 could result in higher energy cost, and resource under-
 provisioning which will inevitably hurt the application per-
 formance [4]. Therefore, reducing the overhead to maximize
 resource utilization is always the Cloud provider’s desire.
 Motivated by the above-mentioned challenges and promis-
 es, our paper proposes a resource provisioning and allocation
 scheme to strategically schedule the DAG-structured work-
 flow onto the underlying time-dependent Cloud infrastruc-
 ture. The key contributions of this paper are: (1) A realistic
 energy model based on various factors such as energy
 consumption, energy cost, and  emission rate. (2) A
 novel DVFS based strategy to find the optimal frequency
 to run each task of a scientific workflow without affecting
 the performance. (3) VMs allocation strategy to achieve
 task consolidation and maximized resource utilization which
 eventually lead to enhanced system performance, reduced
 energy cost and  emission.
 The rest of the paper is organized as follows. Section II
 discusses the related works. Section III presents our green
 Cloud system architecture, and defines the energy model as
 well as the workflow scheduling model. Section IV presents
 the problem formulation and the algorithm design. Section V
 explains the experimental setup followed by the analysis
 of results. Conclusion and future work can be found in
 Section VI.
 2013 IEEE 27th International Symposium on Parallel & Distributed Processing Workshops and PhD Forum
 978-0-7695-4979-8/13 $26.00 © 2013 IEEE
 DOI 10.1109/IPDPSW.2013.19
 2218
II. RELATED WORKS
 There are many research works addressing energy-
 efficient computation for either cluster servers or virtual-
 ized servers [5], [6], [7]. Technologies such as Dynamic
 Voltage and Frequency Scheduling (DVFS) and Dynamic
 Power Management (DPM) [8] were extensively studied
 and widely deployed for energy savings. Some limited
 research works [9], [10] studied on the energy-efficiency
 issue from an economic cost perspective. However, most of
 previous works focused on energy reduction within a single
 server instead of across multiple data centers whereas Cloud
 providers typically have multiple geographically located data
 centers. Furthermore, most of these previous works focused
 on reducing energy usage, not on the profit boost while
 reducing the carbon emissions. Recently, a few research
 work began to take the environment sustainability issue
 such as  emission into consideration. Garg et al. [11]
 proposed energy-efficient scheduling policies on determining
 the mapping order of application/data center pairs along
 with DVFS strategy to either minimize the  emission or
 maximize Cloud provide’s profit. However, their work did
 not consider complex workflow structure which were com-
 monly used by the scientific community, and virtualization
 mechanism was not considered.
 There are several commonly used task scheduling policies
 in Cloud computing management systems such as Euca-
 lyptus [12], and Pegasus Workflow Manage System [13].
 However, energy efficiency issue was not considered in these
 scheduling algorithms.
 Our work differs from the aforementioned efforts in
 that we consider all of the following aspects: (i) scientific
 workflow applications as opposed to individual tasks are
 considered under a time-dependent Cloud environment; (ii)
 a bi-objective scheduling problem is formulated to meet
 both the QoS and energy reduction requirements. (iii) DVFS
 based technology is applied to each task in the workflow to
 lower the CPU frequency without affecting the execution
 deadline; (iv) multiple data centers are available for work-
 flow scheduling.
 III. ANALYTICAL MODELS
 A. Green Cloud System Architecture
 There are basically five main entities involved in our
 system architecture:
 1. Users: Submit their service requests (i.e., workflow ap-
 plications, QoS requirements, etc.) anytime from anywhere
 to the Cloud.
 2. Green Service Selector: Acts as an interface between
 the users and the Cloud infrastructure to enable energy/-
 efficient Cloud services.
 3. Local Scheduler: Performs the actual scheduling of
 workflows. Provides the Green Service Selector with Co-
 efficient Of Performance factor (COP) as the efficiency of
 the cooling system [14],  emission rate, CPU power-
 frequency relationship, and electricity cost.
 4. Virtual Machines (VMs): Multiple independent VMs
 can be created and deployed on a single physical server.
 5. Physical Machines (servers): Each data center consists
 of multiple physical machines with limited OS.
 B. Green Cloud Energy Model
 Table I
 PARAMETER OF A DATA CENTER 
 Parameters Definitions
   CPU frequency range of CPU 
  Constant power consumption
  Proportionality constant
 	 Coefficient Of Performance factor
 
 Electricity cost ($/kWh)
 
 	

   emission rate (kg/kWh) Executing price ($/hour)
 We assume that a particular Cloud provider has  data
 centers located at different places around the world. The
 related parameters of a data center       are given
 in Tab. I. According to previous models [6], [11], executing
 a workflow  at a data center  results the following:
 (1) Energy consumption of CPU  executing task  :
 	  
       		 (1)
 where   is the frequency of CPU  executing task  ,
 and   is the execution time of task  running on
 CPU  at maximum frequency.
 (2) Total Energy consumption
 	    
 
 
 
 	 (2)
 (3) Energy cost
   	   (3)
 (4)  emission of data center 
 		  	   (4)
 C. Workflow Scheduling Model
 We construct the workflow scheduling model in Fig. 1 to
 facilitate the scheduling problem. A workflow is constructed
 as a Directed Acyclic Graph (DAG)    	 where
  is the number of tasks and 	 is the number of de-
 pendency edges. We consider a general Cloud environment
 (i.e., a data center)    	 where  is the number
 of servers and 	 is the number of network links.
 The resource cost of a workflow includes the task execu-
 tion time, and overhead for VM startup, idle and shutdown
 time. The total resource cost for server  with allocated
 computing capacity 	
 during time interval   is
 calculated by Eq. 5 and the overhead cost is calculated by
 Eq. 6. Utilization Rate (UR) stated in Eq. 7 indicates the
 effective resource utilization of a workflow.
 2219
.
 .
 .
 m3
 m2
 m1
 ms
 mt
 ml
 mn
 
 …..
 
 …..
 Data Center Dependencyedge
 Layer1 Layer2 Layerl-1 Layerl
 Schedule
 Figure 1. Workflow Scheduling Model.
   	
         (5)
   	
       (6)
   
 
 
 
 
 
 
 (7)
 IV. PROBLEM FORMULATION AND ALGORITHM DESIGN
 A. Problem Formulation
 The scheduling problem is defined as follows:
 Definition 1: Assume that a Cloud provider has  data
 centers at different geographical locations. Each data center
 has different electricity cost,  emission rate and other
 energy related parameters. Cloud users submit jobs (i.e., a
 single task, pipeline, or DAG-structured workflow) to the
 Cloud with specified deadlines. For a particular workflow
  submitted by a user, our objective is to schedule the
 workflow to a chosen data center such that the energy cost
 and  emission can be minimized for higher profit and
 less environmental footprint while still satisfying deadline
 requirement.
 B. Algorithm Design
 A five-step workflow scheduling algorithm, namely
 Energy-Aware Resource Efficient workflow Scheduling un-
 der Deadline constraint (EARES-D) is proposed. The com-
 plexity of this algorithm is 	 	 	.
 Step 1 Earliest Completion Time Estimation Phase:
 For all  data centers, calculate the estimated earliest com-
 pletion time (	 ) for a workflow by assuming that all
 CPUs in each data center run at their maximum frequencies
 without considering actual VM allocation.
 Step 2 DVFS Phase: Use DVFS technology to reduce
 energy consumption by scaling down the CPU frequency
 under the deadline constraint and the optimal frequency for
 executing each task is also determined.
 Step 3 Data Center Selection Phase: For each available
 data center that satisfies the deadline requirement, choose
 the best data center based on workflow completion time,
 energy cost and  emission (	).
 Step 4 Forward Workflow Scheduling Phase: A layer-
 based forward task scheduling is performed to the selected
 data center, and tasks are scheduled from the first layer to the
 last layer. Earliest completion time 	 is calculated. Each
 task is scheduled on a specific VM running at the desired
 frequency as calculated by Step 2. Go back to Step 3 to pick
 another data center if 	 exceeds the deadline constraint.
 Step 3-4 is repeated until we find a data center that satisfies
 the deadline constraint or reject the user request.
 Step 5 Backward Workflow Scheduling Phase: Based
 on the temporary mapping result from Step 4, the resource
 utilization rate (UR) is further improved by reusing VM
 and shrinking the idle time between tasks. In this step,
 a backward scheduling strategy is conducted to adjust the
 task mapping from the last layer to the first layer while the
 minimum 	 and deadline is still guaranteed.
 V. EXPERIMENTAL RESULTS
 A. Experimental Setup
 We use open source Java-based CloudSim toolkit [15]
 to model our green Cloud infrastructure and evaluate our
 scheduling algorithm. Many Java classes have been adapted
 to accommodate our workflow application structure as well
 as the time-dependent Cloud resources.
 1) Data Center Configuration: We model 8 different
 data centers with different configurations (with data center
 ID from 1 to 8). CPU power factors of different data
 centers are derived from Wang and Lu’s work [6], [11]. As
 current commercial CPUs only support discrete frequency
 levels, we consider discrete CPU frequencies in the range
 of [
 ]. We assume that the COP value of each
 data center is 1.5. VM startup time is 100 seconds, and the
 shutdown time is 8 seconds based on [4].
 2) Workflow Configuration: We experiment with 10 dif-
 ferent sizes of workflows (with workflow ID from 1 to 10)
 represented by a two-tuple ( 	): (10, 21), (20, 35), (40,
 88), (60, 119), (80, 158), (100, 212), (200, 398), (300, 601),
 (400, 784), (500, 1024).
 B. Analysis of Results
 Performance metrics of energy consumption, energy cost,
 and  emission are used to examine the effect of the three
 phases. For each metric, we compare the following phases of
 our algorithm: earliest completion time estimation (	 ),
 DVFS ( ), forward workflow scheduling ( ) and
 backward workflow scheduling (! ).
 On the left side of Fig. 2, the energy consumption
 decreases by up to 40% from 	 to   due to
 2220
1 2 3 4 5 6 7 8
 0
 50
 100
 150
 200
 250
 300
 Energy Consumption VS Data Center (Workflow ID = 8)
 Data Center ID
 En
 er
 gy
  C
 on
 su
 m
 pt
 io
 n (
 kW
 h)
  
 
eECT
 DVFS
 FWS
 BWS
 1 2 3 4 5 6 7 8 9 10
 0
 20
 40
 60
 80
 100
 120
 140
 160
 Energy Consumption VS Workflow Size (Data Center ID = 5)
 Workflow ID
 En
 er
 gy
  C
 on
 su
 m
 pt
 io
 n (
 kW
 h)
  
 
eECT
 DVFS
 FWS
 BWS
 Figure 2. Effect of DVFS and scheduling policy on energy consumption.
 DVFS technology, up to 12% from   to   due
 to VM allocation policy with VM reuse, and up to 13%
 from   to !  due to backward VM reuse strategy
 across different data centers. On the right side of Fig. 2, the
 energy consumption decreases by up to 11% from 	 to
  , up to 11% from   to  , and up to 12%
 from   to !  across different workflows. Similarly,
 the energy cost and  emission also drop by each step.
 From   to ! , the resource utilization rate increases
 about 20% to 25% for different workflows and data centers.
 VI. CONCLUSIONS
 In this paper, we propose a multi-step heuristic workflow
 scheduling algorithm, namely EARES-D to address the var-
 ious objectives including guaranteed QoS, energy efficiency
 and 	 emission by a group of available data centers.
 Thorough experiments are performed to demonstrate the
 efficiency of EARES-D . Energy consumptions, energy cost
 and  emissions are decreased, and resource utilization
 rate is improved. Our future plan is to run our experiments
 on a local private Cloud, called Saluki Cloud established
 and managed by Eucalyptus with a few Beowulf clusters. We
 also would like to extend our model to consider possible VM
 migration, hibernation of some selected idle servers without
 affecting the response delay to future jobs.
 REFERENCES
 [1] D. Filani, S. G. J. He, M. Rajappa, A. Kumar, P. Shah,
 and R. Nagappan, “Dynamic data center power managemen-
 t: Trends, issues, and solutions,” Intel Technology Journal,
 vol. 12, no. 1, pp. 59–68, 2008.
 [2] U.S. Environmental Protection Agency ENERGY STAR
 Program, “Report to congress on server and data cen-
 ter energy efficiency public law 109-431. http://hightech.lbl
 .gov/documents/data centers/epa-datacenters.pdf,” 2007.
 [3] “Gartner Newsroom, Gartner estimates ICT industry accounts
 for 2 percent of global CO2 emissions. http://www.gartner
 .com/it/page.jsp?id=503867,” April 2007.
 [4] M. Mao and M. Humphrey, “A performance study on the VM
 startup time in the cloud,” in 5th International Conference
 on Cloud Computing (Cloud 2012), Honolulu, Hawaii, USA,
 June 2012, pp. 423–430.
 [5] K. Kim, R. Buyya, and J. Kim, “Power aware scheduling of
 bag-of-tasks applications with deadline constraints on DVS-
 enabled clusters,” in the Seventh IEEE International Sympo-
 sium on Cluster Computing and the Grid, Rio de Janeiro,
 Brazil, 2007, pp. 541–548.
 [6] L. Wang and Y. Lu, “Efficient power management of het-
 erogeneous soft real-time clusters,” in the 2008 Real-Time
 Systems Symposium, Barcelona, Spain, 2008, pp. 323–332.
 [7] G. Laszewski, L. Wang, A. Younge, and X. He, “Power-aware
 scheduling of virtual machines in DVFS-enabled clusters,” in
 IEEE International Conference on Cluster Computing and
 Workshops (CLUSTER ’09), 2009, pp. 1–10.
 [8] T. Horvath, T. Abdelzaher, K. Skadron, and X. Liu, “Dynamic
 voltage scaling in multitier web servers with end-to-end delay
 control,” IEEE Transactions on Computers, vol. 56, no. 4, pp.
 444–458, 2007.
 [9] J. Chase, D. Anderson, P. Thakar, A. Vahdat, and R. Doyle,
 “Managing energy and server resources in hosting centers,”
 IGOPS Oper. Syst. Rev., vol. 35, no. 5, pp. 103–116, 2001.
 [10] J. Burge, P. Ranganathan, and J. L. Wiener, “Cost-
 aware scheduling for heterogeneous enterprise machines
 (CASH’EM),” Technical Report HPL-2007-63, HP Labs, Palo
 Alto, Tech. Rep., 2007.
 [11] S. Garg, C. Yeo, A. Anandasivam, and R. Buyya, “Energy-
 efficient scheduling of HPC applications in cloud computing
 environments,” CoRR abs/0909.1146, 2009.
 [12] D. Nurmi, R. Wolski, C. Grzegorczyk, G. Obertelli, S. So-
 man, L. Youseff, and D. Zagorodnov, “The Eucalyptus open-
 source cloud-computing system,” in 9th IEEE International
 Symposium on Cluster Computing and the Grid (CCGrid ’09),
 2009, pp. 124–131.
 [13] E. Deelman, G. Singh, M. Su, J. Blythe, Y. Gil, C. Kesselman,
 G. Mehta, K. Vahi, G. Berriman, J. Good, A. Laity, J. Jacob,
 and D. Katz, “Pegasus: A framework for mapping complex
 scientific workflows onto distributed systems,” Sci Program.,
 vol. 13, pp. 219–237, 2005.
 [14] J. Moore, J. Chase, P. Ranganathan, and R. Sharma, “Making
 scheduling “cool”: Temperature-aware workload placement in
 data centers,” in the 2005 Annual Conference on USENIX
 Annual Technical Conference, Anaheim, CA, 2005, pp. 61–
 74.
 [15] R. N. Calheiros, R. Ranjan, A. Beloglazov, C. A. F. D.
 Rose, and R. Buyya, “Cloudsim: A toolkit for modeling and
 simulation of cloud computing environments and evaluation
 of resource provisioning algorithms,” Software: Practice and
 Experience, vol. 41, no. 1, pp. 23–50, 2011.
 2221

A Reliable Infrastructure Based On COTS Technology 
For Affordable Space Application 
David Ngo 
Sanders, a Lockheed Martin Company 
Nashua, NH 03061-0868 
david.c.npo~lmco.com 
PO BOX 868, NCA1-6247 
603-885-9555 
Abstract- The benefit of using COTS technology for space 
onboard computing applications is compelling, as most new 
NASA applications demand the highest performance but 
with severely constrained budgets. COTS products are 
designed, however, to address the mass market and do not 
account for the unique requirements of the space 
applications. Fortunately, in a coincidence of requirements, 
commercial technology products for desktop computing, 
imaging, and wireless applications also demand the highest 
quality and reliability and with lowest power consumption. 
An encouraging trend from recent radiation test results 
conducted on mainstream COTS computing devices 
manufactured using leading foundry processes (e.g. 
microprocessors and memories) which show total dose 
performance with latch-up immunity that are deemed 
suitable for many space applications. However, these same 
tests also show that nearly all COTS devices exhibit some 
degree of single event upset (SEU) induced by cosmic 
particles that would interfere with correct system 
performance and potentially corrupt data. 
This paper discusses a combined hardware and software 
approach to provide SEU mitigation techniques to achieve 
reliable, scaleable high performance computing systems 
using COTS products in spaceborne environments. A First 
Generation Testbed has been developed by Sanders under 
the Remote Exploration and Experimentation (ME) project 
to validate this concept by the Jet Propulsion Laboratory. 
1. 
2. 
3. 
4. 
5. 
6. 
Space 
TABLE OF CONTENTS 
INTRODUCTION 
ARCHITEC“W?&PROACH 
ARCHITECTURE FAULT TOLERANCE 
TESTBED PROGRESS 
CONCLUSION 
SOFTWARE STRUCTURE AND TOOLS 
1. INTRODUCTION 
exploration is moving from the relatively simple, 
ground-based data processing and human-based real time 
Michael Harris 
Sanders, a Lockheed Martin Company 
Nashua, NH 03061-0868 
~nichael.harris~~!lnico.com 
PO BOX 868, NCA1-6247 
603-885-905 1 
control paradigm of its early years to a more sophisticated 
paradigm characterized by onboard science-data processing 
and autonomous goal-directed operation. However, the 
major limitation in achieving these next-generation missions 
is the lack of computational throughput and communication 
bandwidth onboard remote spacecraft and in earth-orbiting 
satellites. Thus, a key requirement of this paradigm shift will 
be the provision of an onboard “supercomputer” to take on 
these higher-order, computationally intensive tasks, with 
power/performance efficiency that is near the latest 
generation of the commercial state-of-the-art to achieve 
affordable size, power, and weight for flight system 
realization. This space-based supercomputer will, at least 
initially, not be required to manage the spacecrafl itself, and 
is currently being viewed as an adjunct computational 
element to the traditional, highly fault tolerant and radiation- 
hardened Spacecraft Control Computer (SCC). 
The radiation-hardened computer systems traditionally used 
in spacecraft, while highly tolerant or immune to radiation- 
induced damage and transient upset, are several generations 
behind the COTS technology curve. This gap is expected to 
widen, as the commercial computing industry is two orders 
of magnitude larger than the entire space and defense 
electronics industry. While great strides are being made in 
earth-based, commercial-off-the-shelf (COTS) computing 
and supercomputing technologies, there have been 
significant economic challenges that have stalled timely 
transition of even simple general purpose computing 
technologies to space. To affordably realize the paradigm 
shift as stated earlier, it is paramount that contemporary 
COTS components are utilized in future supercomputer 
space applications. Two of the most diflicult aspects of 
computing in space are the susceptibility of hardware to 
space-radiation-induced upset, and the limited power levels 
available on spacecraft. Radiation-induced transient fault 
rates in COTS computers can be 1-3 upsets per processor 
per hour or higher, in even the relatively benign 
environments of Deep Space (DS) or Low Earth Orbit 
(LEO). Typical power allocations to the computing 
subsystems of a spacecraft are lOOW or less. Although the 
use of the latest generation of COTS products will benefit 
the powerlperformance challenges of space systems, the 
susceptibility of COTS components to space-radiation- 
0-7803-6599-2/01/$10.00 Q 2001 IEEE 
5-2435 
induced upset is a significant challenge to attain system 
reliability required to achieve mission success. 
The Remote Exploration and Experimentation (REE) 
Project seeks to leverage the considerable investment of the 
commercial, ground-based computing industry to bring 
supercomputing technologies into space using COTS 
components effectively in the space environment. Further, it 
seeks to accomplish this at a cost that is commensurate with 
a COTS solution in a time frame that is not significantly 
behind the COTS technology curve and thus enables space- 
based computing systems to closely follow that curve. The 
availability of onboard supercomputing capability will 
enable a new way of doing science in space at significantly 
reduced overall cost. This is consistent with the vision of 
the REE Project: To bring commercial supercomputing 
technology into space in a form that meets the demanding 
environment requirements to enable a new class of science 
investigation and discovery. 
Computing Testbeds Initiative 
The Computing Testbeds Initiative is an element of the REiE 
Project Goals and Objectives to explore and develop a 
process for translating commercial high performance 
scaleable parallel computing architectures into low-power 
spacebome implementations. This architecture must rely, to 
the maximum extent practical, on COTS technologies and 
must minimize or eliminate the use of radiation-hardened 
components. The process must be consistent with the rapid 
(1 8 months or less) transfer of new earth-based technologies 
to NASA space missions. Translated architectures must 
satisfy a number of additional criteria, including no single 
point of failure and graceful performance degradation in the 
event of hardware failure. 
The Computing Testbeds Initiative will develop a series of 
hardware prototypes, leading to the demonstration of a 
capability of at least 300 MOPS2/Watt. This represents an 
increase of two orders of magnitude over the power 
performance of the flight computer onboard the Mars 
Pathfinder spacecraft which landed on Mars in July 1997. 
At the present time, a first generation testbed has been 
developed by Sanders under contract to JPL, to demonstrate 
that significant power performance (30 MOPS/Watt) can be 
achieved in a scaleable embedded architecture using 
commercial technology. The architecture implemented in 
the REE First Generation Testbed wil satisfy the criteria of 
having no single point of failure and graceful degradation 
MOPS: Millions of Operations Per Second. These may be 
a mixture of 32-bit integer and floating point arithmetic or 
logical operations. Although MPS (millions of instructions 
per second) is a more traditional measure of processor 
capabiliv, it does not quantify the actual amount of work 
accomplished on processors which have complex instruction 
sets. In many cases, however, MOPS and M P S  will be 
interchangeable. 
due to permanent faults. The testbed will be used to conduct 
Software-Implemented Fault Tolerance (SIFT) experiments 
and to develop the system software needed to achieve the 
reliability goals. Follow on activities will include the design 
and fabrication of a hardware prototype that will match the 
mass and form factor of a future flight model and will 
demonstirate scalability (50 nodes), reliability (0.99 over 10 
years), and a power performance of at least 300 
MOPS/Watt. 
2. ARCHITECTURE APPROACH 
The First Generation Testbed system developed by Sanders, 
implements a state-of-the-art instantiation of an REE system 
fully caplable of being targeted for space flight application. 
The architecture is based on COTS technology and industry 
standards adapted for NASA space missions requiring low 
power, scaleable performance, and fault tolerance that are 
critical En meeting the REE project goals. The Testbed will 
be based1 on the Two-Level Architecture (Figure 1) and the 
Myrinet network fabric developed from previous DARPA 
research The architecture consists of distributed nodes that 
are interconnected by a fault-resilient system network that 
supports graceful degradation and has no single point of 
failure. Within the node, industry standard hardware and 
software interfaces are employed to isolate the application 
processolr environment from the complex system control and 
communications environment. This separation enables a 
stable system control and communications infrastructure that 
will endure over time, providing future systems with an 
established reliable platform that is independent of the fast 
changing COTS processor technology. The direct benefit is 
rapid insertion (18 months or less) of latest COTS 
technology to NASA missions, enabling at least an order of 
magnitude improvement in performance/power over 
traditional space radiation hardened computer solutions. The 
separation of the application from the control and 
communications layers also effectively isolates direct fault 
effects introduced in the application environment from the 
Application Processing Element (PE) 
Rapid fechiiolczgy insertion cyciees - Latest commodity processing dew- (RISC, ASIC, etc.) 
* COTS software tools - Heterogeneous technology 
Node Conlroller 
Enables Use of COTS far "~~st-l~Time"ln~aegraiiDn 
* Open architecture. technology neutral interfaces - Fault tolerant and autonomws nodecontrol 
* High reliability control and communications - ASIC implementation enables COTS PE insertion 
Scaleable Network Fabric . Based COTS standsrd~ 
* ScalaMe, high band\nndth for supercomputer application - Redundant topology fw reliaMe system owrahcm 
Figure 1. Two Level Multicomputer 
system e:nvironment. 
Node Htxrdware Architecture 
5-2436 
The first generation node architecture consists of a 
Processing Element (PE), Node Controller (NC), and 
interface to the system network (Figure 2), thus fully 
reflecting the two-level architectural approach. Details of 
the node architecture components are provided in the 
sections that follow. 
Figure 2: First Generation Node Architecture 
Node Controller 
Within the node architecture, the Node Controller is the 
“first-level” computer, providing consistent control and 
communication functions locally within a node and between 
nodes within a system. A generic programmable controller 
FromKo 
FromKo FromKo 
Maintenance High Speed 
Network Network Fabric 
Figure 3: Node Controller Architecture 
and the Network Transport Engine (NTE) form the core of 
the NC. The programmable controller provides the local 
control and status functions to a node. It also provides 
system wide startup and runtime operation support such as 
health status and fault reconfiguration. The programmable 
controller and the NTE combine to provide network 
communications between nodes. The Node Controller also 
contains the interfaces to the Processing Element (PE) and 
to the system network fabric. The NTE is implemented in 
hardware to significantly improve the performance:power 
ratio associated with network communications. In the First 
Generation Testbed, the NTE is implemented in a Xilinx 
Virtex field programmable gate array (FPGA). The NTE- 
based Node Controller also provides efficient fault-tolerant 
support for reliable control and communications for all 
supported protocols. The programmable flexibility of the 
Node Controller also allows support of future standard 
network protocols for maximum application flexibility. 
The NTE is connected to the Processing Element within the 
node through the industry standard PCI. The PCI 
implementation is 32 bits wide at 33 MHz and is fully 
compliant to the PCI Local Bus Specification, Revision 2.1. 
The NTE is also connected to the Myrinet network fabric 
through an external Myricom FI device for full compatibility 
with other nodes on the Myrinet (e.g., workstations with 
Myrinet interfaces). A redundant Myrinet FI interface is 
provided for fault-tolerance. For the First Generation 
Testbed, the Node Controller is packaged in a daughter card 
assembly compatible with the industry PCI Mezzanine Card 
(PMC) standard. The Node Controller assembly can be 
mated with other COTS processor boards that have standard 
PMC support to form a complete node. This packaging 
approach allows easy integration of new PE technology with 
the Node Controller to enable the testbed to continuously 
insert the latest COTS technology for performance:power 
improvements. 
+ + 
Figure 4: PowerPC 750 Block Diagram 
Processing Element 
The Processing Element is the “second level” computer 
within the node architecture and is the primary application 
processing resource within a node. For the First Generation 
Testbed, the PowerPC@ 750 was selected as the leading 
generation of general purpose COTS microprocessors based 
5-2437 
~ 
on its powerlperformance characteristics and its strong 
family heritage. The PowerPC 750 is a low-power, 32-bit 
implementation of the PowerPC Reduced Instruction Set 
Computer (RISC) architecture with a strong legacy of 
software support. Based on a superscalar architecture 
(Figure 4), it is capable of issuing three instructions per 
clock cycle into six independent execution units: two integer 
units; floating-point unit; branch processing unit; loadstore 
unit; and system register unit. Cache supports include 
separate L1 instruction and data caches at 32 KB each, and 
backside L2 cache support of up to 1 MB [l]. The latest 
version of the PowerPC 750 product is based on a 0.22- 
micron, six-layer metal CMOS process, 2V internal core 
voltage, with a typical power dissipation of 5.5 Watts at the 
maximum clock rate of 466 MHz. 
The First Generation Testbed PE will be a direct application 
of a prevailing commercial computer board product based 
on the latest PowerPC technology. The Cetia VMPC5a 
series was selected to represent the latest instantiation of 
leading edge COTS technology in 1999. The dual parallel 
PowerPC 750 configuration of the VMPC5a was also 
chosen to maximize performance:power at the node level. 
Each PowerPC runs at the nominal clock rate of 366 MHz 
and is supported by a dedicated 1 MB of backside L2 cache, 
allowing both PowerPC microprocessors to operate in 
parallel sharing a common Node Controller and network 
interface to reduce overheads. Each PowerPC 750 can 
deliver 17.2 SPECint95 and 11.1 SPECfp95 in performance. 
A bridge chip onboard the Cetia provides the interface 
between the two PowerPC, DRAM memory, <and PCI 
interface that connects the PE to the Node Controller (Figure 
2). There is 128 MB of DRAM memory on the PE provided 
for each testbed node with expansion support for up to 256 
MB. The DRAM memory is supported by error correction 
code (ECC) that corrects on single-bit errors and detects on 
double-bit errors. Based on preliminary characterization 
results of -1.98 operations attained per clock cycle for each 
PowerPC 750 operating at 366 MHz, the node was estimated 
to achieve - 45 MOPS/Watt based on an estimated typical- 
max power consumption of 32 Watts accounting only the 
relevant PE functions onboard the Cetia Since the Cetia 
module contains other functions (VME I/F, second PCI 
bridge, Ethernet I/F, SCSI I/F, etc.), the First Generation 
Testbed is expected to achieve -32 MOPS/Watt based on 
the published power rating of the Cetia VMPC5a-Dual 
module at 45 Watts. The node architecture supporting a 
modular PE design will allow continuous insertion of fbture 
generations of COTS processor products to achieve frequent 
technology refresh cycles of less then 18 months (e.g., 
PowerPC G4 insertion). 
Reconfigurable Network 
Multiple nodes are interconnected by a reconfigurable high- 
speed system network based on the Myrinet fabric 
(ANSINITA 26-1998) [2]. Myrinet is a COTS packet 
switch network supporting scaleable system configuration of 
up to 64,000 nodes using the currently available 4-, 8-, or 
16-port switches. A Myrinet interface consists of a pair of 
byte-wide links for concurrent input and output transfers (a 
link per direction), achieving a bi-section peak bandwidth of 
1.28 Gbit/s per link for each direction. Myrinet employs 
low-voltage signaling technology to reduce power. Figure 5 
shows the flexible network connectivity offered by Myrinet. 
In the First Generation Testbed, each node is configured 
with a separate primary and secondary (redundant) M m e t  
fabric interfaces. These interfaces are interconnected with 
other nodes by a unified Myrinet system network through 
different links and switches. Using 8-port Myrinet switches, 
the system network implementation provides at least two 
independent routes to each node’s primary and redundant 
interfaces to enhance bandwidtMatency performance and 
system fault tolerance. The switch-based network and LAN- 
like characteristics provided by Myrinet enable a scaleable, 
flexible, high performance network that is also highly 
resilient to faults. The redundant interface is normally 
powered off to conserve power. 
LAN ReYxlneS 
p V O r * S b t i ~ S .  
PCS. mass 
storage. etc.) 
Figure 5: Myrinet Network Fabric 
:$. SOFTWARE STRUClVRE AND TOOLS 
Sofhyme Architecture 
The software architecture was conceived to provide an 
environment to easily port advanced science applications 
that are typically developed on general-purpose ground- 
Software Architecture 
I 
N-TranrpoltSsMcCll 
Figure 6: S o h e  Architecture 
5-2438 
based computers, to the embedded platform as represented 
by the testbed. As shown in Figure 6, the software 
architecture extensively leveraged fiom COTS software to 
allow science applications to be moved to the testbed with 
little or no redesign, while supporting enhanced services that 
are important to embedded space system applications. 
COTS software products employed include operating 
system, development tools, and libraries to fully leverage 
commercial investments. The software architecture takes 
advantage of the two-level architecture with communication 
services provided by the Node Controller to maximize 
system performance:power. Enhanced software services 
include a reliable communications layer to enhance system 
fault tolerance; system services for fault injection, power 
control and monitoring; system configuration; status and 
error logging. Together, these software services provide a 
robust environment to support reliable system operations. 
Application Programming Model 
The Programming Model (Figure 7) is based on a 
distributed system concept employing explicit message 
passing for interprocessor communications. Each node is a 
complete computer function with an independent operating 
system per processor, supporting multiple processes per 
processor at both user level and supervisory level. 
Supervisory level processes share a common address space 
while user process have separate, protected address spaces. 
Processes can control other processes. Typical applications 
may have'one or more processes that communicate among 
themselves. 
............................................................................... ...... r i * *  
Figure 7: Programming Model 
The application programming environment employs industry 
standard APIs to ensure seamless code portability between 
workstation, testbed, and future generation flight hardware. 
Industry standard portable application programming 
interfaces (API) supported include MPI 1.2, Socket, and 
NFS. The MPI 1.2 standard provides a standard parallel 
programming API favored by the high performance, real- 
time user community. Using MPI, parallel applications 
developed under widely supported workstation platforms 
can be directly ported to the Testbed to preserve application 
s o h e  investments. The Testbed implementation of the 
MPI 1.2 is designed for performance and portability by fully 
exploiting the NTE features in the Node Controller, 
providing packet acceleration to maximize 
performance:power, while providing standard support for C, 
C++, and Fortran bindings. General-purpose support for 
TCP/IP is also provided for standard Socket and NFS 
services to further maximize application programming 
5-2 
flexibility, portability, and third party product integration 
(e.g. COTS software tools). 
COTS Operating System and Software Tools 
The software architecture is independent of the specific 
operating system that runs on the PE (e.g., PowerPC 750). 
However, the First Generation Testbed will be running Lynx 
as the COTS-embedded OS in its implementation. This 
selection was based on the existing Lynx support for 
separate user and supervisory processes, process protection 
ftom other processes, and memory protection features that, 
together, provide the Testbed with a robust OS platform to 
support SIFT development. Lynx supports industry standard 
APIs that include standard Unix file YO, threads (Posix 
1003.lc), sockets (Posix 1003.1g), NFS and RPC, process 
and memory management (Posix 1003.1), Posix 1003.lb 
real-time extensions, and third party numerical processing 
library. 
The Testbed is supported with a comprehensive board 
support package (BSP) for the PowerPC 750 processing 
element that includes the Lynx OS, standard development 
environment (C, C++, Fortran), and Totalview parallel 
source level debugger. At the node level, the BSP software 
also includes support for node power-up initialization, boot- 
up, built-in test (BIT), and FLASH memory loader 
functions. At the system level, the Testbed will be 
supported by an autonomous system startup capability even 
in the presence of permanent faults. 
I "  
Figure 8 Testbed Fault Injection GUI 
The Testbed is supported by a fault injector to support 
system fault analysis as an alternative to traditional 
approaches requiring exhaustive failure effects and modes 
analysis. The faut injector approach is s o w t r i c  as it is 
constrained by the use of COTS components. The f d t  injector 
emulates transient and semi-permanent faults that are induced by 
single event upsets (SEWS). The faut injector will be used by JPL 
to experimentally measure system fault behavior, to v* system 
design assumptions, and as a debugging tool for SIFT systems. 
439 
Figure 9: Fault Tolerant System Architecture 
4. ARCHITECTURE FAULT TOLERANC13 
Essential to the long term survival of unmanned space on 
board processing systems is the absence of single ]points of 
failure and the presence of a high degree of fault tolerance. 
The architecture incorporates an inherent fault tolerance 
hardware/software infkastructure within the limitation of 
using COTS components, to provide embedded systems with 
no single points of failure and with graceful degradation in 
the presence of permanent faults. 
Referring to Figure 9, the system architecture is comprised 
of parallel modular nodes, each with its own independent 
operating system and redundant interfaces. The n’odes are 
interconnected by a reconfigurable network that provides 
multiple redundant paths frondto each node. This enables 
an n of m fault redundancy approach (as opposed to block 
redundancy), such that suf€icient spare nodes are added 
based on an application’s end-of-life reliability 
requirements. Graceful performance degradation is achieved 
through the ability to allocate any tasks to any node, 
allowing embedded system applications the flexibility to re- 
allocate tasks and revise data distribution destinations to 
recover Erom permanent node or network failures. 
Robust fault containment mechanisms inherently exist 
throughout the architecture based on the highly imodular 
hardware/sohare functionality. These include: 
independent node operating systems, separate application 
and communications environments (as provided by the two- 
level architecture), redundant network fabric interfaces on 
each node (either of which can be powered down), point-to- 
point network links and modular network switches, and the 
ability to power down faulty nodes and faulty inetwork 
switches for fault isolation. 
The node controller also provides reliable communications 
capability that is transparent to the applications. These 
include message level CRC, message validation and 
authentication, and automatic re-try and re-route of 
messages due to failure. The distributed node controllers at 
each node combined with the redundant network topology 
also provide embedded systems with automatic 
configuration of healthy nodes and network paths during 
system startup to automatically remove failed nodes, and 
failed network paths or switches from the system startup 
configuration. The architecture also supports a redundant 
maintenance bus to allow a direct, “back-door” control and 
status path by a spacecraft’s system controller, offering a 
low-level control path to remove power from faulty nodes 
and switches. 
5.  TESTBED PROGRESS 
The First Generation Testbed configuration is shown in 
Figure 10. The Testbed consists of 20 homogeneous nodes 
and a reconfigurable network housed in standard VME 
chassis. The Myrinet network interconnects the nodes and 
extemal Sun workstations to complete the Testbed facility. 
The nodes and the Myrinet network operate as a complete 
embedded system. The Sun workstations provide a host 
facility on the ground that include: 
1. Extemal program storage for system startup support and 
for emulation of embedded mass-storage to applications 
m h g  on the Testbed. 
Fault injection control and storage facility 
and interfaces 
2. 
3. Emulation of Space Craft Computer (SCC) functions 
4. System reset 
20 19 18 17 16 15 14 13 12 I 1  
Figure 10: 20-Node Testbed Configuration 
Each node in the TB configuration is supported by a primary 
and a redundant network fabric interface that are connected 
to the unified system network through altemate Myrinet 
switches. As shown in Figure 2, the primary fabric interface 
is connected to an eight-port switch residing on the module 
for each node, allowing a maximum of 7 switch ports to 
connect with other nodes in the system. The redundant 
fabric inttxface is connected to the system network through a 
different physical switch residing on a different module. In 
the Testbled implementation each node is provided with five 
separate routes to each of the fabric interface to reach other 
nodes. The redundant topology enabled by the Myrinet 
switch-based network provide a scaleable, flexible, high 
performance network that is also highly resilient in the 
presence of faults. 
In tests conducted in May 2000 on a 4 node version of the 
testbed (Fig 11) running a prototype application, the REE 
First Generation Testbed achieved 34 MOPSWatt, 
5-2440 
providing a starting reliable architecture framework to 
incorporate latest COTS technology to attain 
performance:power within one generation of the commercial 
state-of-the-art. When combined with future SIFT 
applications, reliable high performance onboard space 
computing solutions utilizing modem COTS components 
can be realized cost effectively. 
The full 20-node testbed is under completion by Sanders and 
will be delivered to JPL in 2000. The Testbed environment 
is supported by a baseline set of software including the node 
level operating system, communication and network control 
software, system boot up software, and a system fault 
injector for use by JPL in developing global system 
executive, SIFT middleware, ABFT, and applications 
software. 
800 MByteslsec 
Bandwidth 
Four Dual PowerPC Nodes 
~~~~~~a~ Network Performi 
Figure 11 : First Generation Testbed 
6. CONCLUSION 
.Insertion of supercomputing facilities into NASA space 
exploration missions has been shown to be critical to the 
Continuation of the space program. NASA’s REE project has 
been initiated to address this need through the use of COTS 
hard- and software components. A first instantiation of an 
REE system, in the form of the First Generation REE Testbed, has 
been developed and demonstmted powedperfiice that is 
Consistent with leading COTS technolw. The Testbed also 
implemented a hardularelsoftware infrasbructure to provide basic 
h d t  toleraut support within the limitation of using COTS 
components. The Testbed will be used by NASA and space 
scientists to develop and validate the REE project and system 
development concepts and applications. The knowledge gained 
through the development of the Testbed and its subsequent use as 
a test and experimentalion vehicle will provide the basis for the 
design of the Flight Prototype System, which is expected to 
achieve at least 300 MOPSAVatt m a spaceqdfied system ready 
for fielding in the 2004 time frame. 
ACKNOWLEDGMENTS 
This work is performed at Sanders, A Lockheed Martin 
Company, under a contract with the National Aeronautics 
and Space Administration, Jet Propulsion Laboratory, 
California Institute of Technology. This project is part of 
NASA’s High Performance Computing and 
Communications Program, and is funded through the NASA 
Ofice of Space Sciences. 
REFERENCES 
[l] PowerPC 750 RISC Microprocessor Technical Summary, 
MPC750/D, 8/97, IBM, Motorola 
[2] Myrinet-on-VME Protocol Specification Draft Standard, 
VITA 26-19%, Draft 1.1 
PowerPC@ is a registered trademark of International 
Business Machines Corporation 
David Ngo is a member of the 
Technical Stafi Space 
Electronics, Sanders, A 
Lockheed Martin Company. He 
has more then 20 years 
experience developing 
advanced processors for the 
military and space sector. He is 
the chief architect of the ISAC 
and REE space processing systems. Previously he has been 
the principal investigator on several advanced computer 
architecture programs for both DARPA and the US Air 
Force. He has a BSEE from City College of New York and 
conducted graduate studies in Computer Science at the 
Polytechnic Institute efNew York. 
Mike Harris is currently the 
Engineering Director of Space 
Electronics, Sanders, A 
Lockheed Martin Company. He 
has more then 20 years 
experience developing advanced 
processors for the military and 
space sector. Previously at 
Sanders he was the Director of 
the Digital Processing Center of Technology. He has a BA 
in Psychology/Chemis@from Clark University, 

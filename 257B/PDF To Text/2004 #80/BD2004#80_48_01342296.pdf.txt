Proceedings of the 5Ih World Congress on Intelligent Control 
and Automation, June 15-19. 2004, Hangzhou. P.R. China 
Ellipse Detection Based on Improved-GEVD 
Technique* 
Zhong-Gen Yang, Gui-Xiang Jiang, Lei Ren 
Department of Electronic Engineering 
Shanghai Maritime University 
Shanghai, 2001 35,China 
zgyang@cen.shmtu.edu.cn 
Abstract ~ The standard Generalized ETen Value 
Decomposition (GEVD) is a popular ellipse detection technique 
whose statistical analysis is given to prove its disadvantages of 
very big est\mation bias and MSE. It is also proved that the 
effective measurement to improve the performance of ellipse 
detection is whitening the data noise and regularizing data 
observation. This theoretic analysis has strongly supported the 
Hartley's regularization method. Then, an improved-GEVD 
algorithm was developed. The theoretical analysis and computer 
simulation experiments have demonstrated that the proposed 
technique has the advantages that it is  intrinsically able to whiten 
the data noise and to regularize the data observation so as to 
output a non-hiased estimation of ellipse parameter with very 
small MSE. Furthermore, the computation complex &largely 
simplified. 
Index Terms - Robot vision, Ellipse detection, GEVD, 
Regularization. 
I. INTRODVCT~OY 
Ellipse extraction plays an important role in image 
recognition and robot vision, because ellipse is the most 
simple and most popular curve geometrical prime. A typical 
application example is the man-made target recognition and 
tracking in the underwater environment completed by an 
Autonomous Underwater Vehicle ['I. The key step in ellipse 
extraction is ellipse detection, i.e. ellipse fitting. In the long 
time, it interested the academic field [*, 3,4. 5 ,  
The traditional method is the standard Generalized Eigen 
Value Decomposition (GEVD) technique in which the optimal 
estimation of ellipse parameter vector is the eigen vector 
corresponding to the smallest generalized eigen value of the 
matrix pencil composed o f  data ACF.matrix and parameter 
constraint matrix. But, it has been shown that the practical 
performance of the standard GEVD (SGEVD) is poor, 
especially when it is used to fit a digital ellipse. Har t l e~ [~ l  has 
intuitively considered that the main reason is the too big 
condition number of  the data ACF matrix. Based on this 
viewpoint, he has suggested a regularization measurement. 
Although the performance is greatly improved, the theoretical 
proof isn't given out. The statistical analysis of GEVD ellipse 
fitting algorithm has also not been seen yet. 
'1. 
By means of the statistical performance analysis, we has 
proved in theory that the poor performance of SGEVD is 
resulted from the color noise contained in the data vector and 
the big condition number of the data ACF matrix, and has 
pointed out that the two basic measurements to improve the 
performance are to whiten the data noise and to regularize the 
data ACF matrix. As the regularization transformation 
suggested by Hartley has strong regularization ability and can 
whiten data vector in some degree, so the GEVD algorithm 
fusing Hartley's regularization technique (abbreviated to 
HGEVD) has a good performance. Thus, our analysis has 
strongly supported HGEVD in theory and in practice. In the 
same time, we also has pointed out that aRer Hartley's 
regularization transformation the data noise is still color so 
that HGEVD can just give a biased estimation of the ellipse 
parameter vector. Then we develop an improved GEVD 
algorithm that intrinsically has the ability to whiten the data 
noise and to regularize the data ACF matrix on the condition 
of not canying any whitening transformation or regularization 
transformation. In addition, the dimension number of the 
optimization procedure is reduced from 6 to 2.  So, IGEVD 
can give estimation slightly super over or the same as 
HGEVD, on the condition that the computation complex and 
realization cost have been 'Beally reduced. The experiments 
have demonstrated our analysis. 
11. GEVD-BASED ELLIPSE FITTING 
It is well known that a conic can be described by the 
following algebraic equation 
where y 2  x I p  is the data vector , 
e =  loo a, a2 a, a, a5y is the parameter vector , 
superscript T denotes transposition. Ellipse is a conic 1 constrained by 
D'O=O ( 1 )  
orc'e = 1 ( 2 )  
'This work is suppaned by project OIG02 of Science and Technology Development Fund Program for Universities in Shanghai 
4181 
0-7803-8273-0/04/$20.00 02004 IEEE 
Ellipse fitting is a procedure that t i ts  ellipse equation to 
thegiven2Dpointset bj =[xi y i y I i = l , 2 ,  ..., N )  bymeans 
of a least square optimization so as to minimize the cost 
function 
J = e r R ~ e - / Z ( O T c , o - i )  ( 3 )  
is the ACF matrix of data observation, superscript ( s ) 
denotes sample average operation. 
It is easy to prove that the minimization of ( 3 )  leads us 
to the well-known standard GEVD procedure, in which the 
optimal estimation 6 of the parameter vector 0 is the 
generalized eigen-vector corresponding to the minimal 
generalized eigen-value A,,, = 6'Re)i of the matrix pencil 
(It:), C ' ) ,  where 
RK'0 = A,,nC'O ( 5 )  
111. Statistical Analysis of GEVD algorithm 
Their mathematical expectations are 
Thus the data noise is a non-independent color noise with 
nonzero mean and with a non-diagonal ACF matrix. 
E. Stafisfical performance analysis 
The estimation error of the parameter vector 0 is 
68 =0-0,  where 0, is the tmth-value of 0 . It is can be 
A .  Statisfical model 
Suppose that the noisy observation of a 2D pointp,, is 
n = n  + n  ( 6 )  rl r i l  --p 
proved that R , 0 ,  = O  and R, '0 ,  =R,,'C'9, = O .  So, by 
means of matrix disturbing theory, it is can be proved that 
where np, is the additive observation noise, which is an i.i.d. 
Gaussian white noise with zero mean and is independent on 
the truth-value p,, , so that 68 = -WR,+AR',"O, (12)  
( 7 )  
where E( ) denotes mathematical expectation , 
, I is the unit matrix. So, the corresponding 
noisy data vector can be described by 
where D,, is the truth value of the data vector and the additive 
noise vector is 
D, = %  +"D, ( 8 )  
r + n, bzr 1 
(9)  
with the mean 
pn0 =E(nD,)=cri[l 0 I 0 0 Or. 
By means of the above equations, we have 
where R,,'"' is the truth value of RgI and 
R"' D -  R,,(*) + AR'," (10) 
where 
w =  I+R,,(AR;I-/Z,,,,~C,) /,' ( 1 3 )  
z I + R ~ , ( A R ~ - / Z ~ , ~ C , )  
(14)  
l l ~ ~ ~ l l ~ l l ~ ~ l l ~  +Lb+ll*llARL4 ( 1 5 )  
 
is the weight matrix. Therefore, the estimation bias is 
Its strength is 
E(dB)=-WR,+RnnO, 
= W ~ (  R.)(I + p & ) / s ~ ~  
where 
W =  w ~ R ~ , + ~ ] A R , ,  -L..c[) ( 1 6 )  
= I  /P - K(R,>JIAR. - ~~"C~~J~ l lR t , l I l  
= ~ ~ D , + ~ . p t D , ~ ~  = (kx -L,n)/(Asm,n -Am,") 
Here the condition number of matrix R, is defined as 
K(R.) = K(RDC) (17)  
where ,Imx , A,,,Ln and A,,,, are the maximal, minimal, and 
second minimal eigen values of matrix R, , respectively. The 
S N R  of the data vector is defined as 
s N R = ~ ~ R D t ~ ~ ~ ~ ~ R n o  1) ( I S )  
The relative correlation coefficient between data and noise is 
defined as 
4182 
( 1 9 )  
After mathematical derivation, it can be proved that the 
indeterminacy of the parameter estimation is 
where xde is the variance matrix of the parameter estimation 
error. It can also be seen that the mean of the ellipse fitting 
error is 
ua llle,ll = m / l p , i l S  2 w *  K( R , ) J m I f i  (20) 
and the indeterminacy of the ellipse fitting error is 
ui /Ik,ll i I I R n O ( I J m / N  (22)  
0 21RmD1&&6/N 
If the data vector set is de-correlated so that = 0 ,  
in turn that p = 0, then the strength of the estimation bias, the 
indeterminacy of the parameter estimation and the 
indeterminacy of the ellipse fitting error can be greatly 
reduced. Furthermore, when ARK) = omn * I  replaced to 
( 1 2 ) ,  
dB = - u ~ ~ ~ R ~ , + O ,  = 0 
Therefore, if the additive noise of the data vector set is pre- 
whitened, the parameter vector estimation can be unbiased. 
From the above analysis, the following conclusions can 
be made. 
( 2 3 )  
The standard GEVD (SGEVD) technique of ellipse 
fitting cannot give us an unbiased estimation of the 
parameter vector. Both of bias strength and estimation 
indeterminacy are directly proportional to the condition 
number K(R, )  of data ACF matrix. The higher the 
data's SNR is and the larger the N is, the smaller the 
estimation indeterminacy is. 
The lower the noise power is and the larger the N is, the 
smaller the error strength and its indeterminacy of the 
ellipse fitting are. 
Pre-whitening processing of the data vector can not only 
efficiently eliminate the estimation bias of the parameter 
vector to give us an unbiased estimation, but also greatly 
reduce the fitting error strength, the parameter estimation 
indeterminacy and the fitting error indeterminacy. 
The regularization to greatly reduce the condition 
number K(R. )  of data ACF matrix or the denoising 
filtering to increase the data's SNR can efficiently reduce 
both of the fitting error and its indeterminacy. 
The increase of the point number N helps the decrease of 
the estimation indeterminacy as well as the fitting error 
indeterminacy to enhance the estimation precision and 
the fitting precision. 
The effect of the constraint on the optimization 
procedure is embodied in the weight matrix w in 
other words, in the weight factor W .  Because w z I and 
both of m, andA2,,, are infinitesimal, therefore w is 
slight bigger than I .  So, the fitting precision will be little 
reduced. 
111. HARTLEY'S REGULARIZATION GEVD ALGORITHM 
A great deal of practice has proved that when the 
SGEVD algorithm is employed to fit a digital ellipse, its 
performance is very poor. Hartley, when analyzing its reason, 
has understood that the big difference in the scatter of values 
between the homogeneous coordinates of 2 D  digital points 
must lead the condition number of the data ACF matrix 
extremely big so that the estimation bias and the estimation 
indeterminacy are both insufferably big. He has suggested that 
the pre-whitening processing of 2D point set can be utilized to 
efficiently reduce the condition number to a rather small level. 
This is a rather simple and effect regularization procedure. 
Specifically, he used the linear regularization transformation 
of the 2 D  Doint set 
so that the regularized 2D point G j  has zero mean and unit 
variance R-  =E- =LT. L~ = I  . So, the regularization 
transformation matrix must be satisfied to 
From the mapping relationship from the point set to the data 
vector set D , we have the regularization transformation for 
the data vector set 
P P  
.q L-IL-' (25)  
+. 
D = L , D  ( 2 6 )  
where the regularization transformation matrix for the data 
vector can be analytically determined from L and p. 
It is can be shown that, after regularization 
transformation, the condition number K&) of the ACF 
matrix k6 ofthe data vector is in the order of @I) so that the 
estimation bias strength and indeterminacy can be greatly 
reduced. It is should be noticed that, when 6 is the 
generalized eigen vector corresponding to the minimal 
generalized eigen value ,Tm =i'ii,e of the m4trix pencil 
(&, c') ,  that is, when 
. .  
(27)  
the corresponding optima1 estimation of the parameter vector 
is 6 = I,'&. 
Actually, the optimal estimation of the ellipse parameter 
vector does not be the generalized eigen of the matrix pencil 
(i;, c')  at all, because 
I 
= A,,,L,-'C'~ = , Y m , n ~ D - ' ~ D - r ~ ' 6  # A C ~  
which has explained the reason why the Hartley's regularized 
4183 
GEVD (HGEVD) can have the effect ultimately different 
from the SGEVD. 
It must be pointed out that, because Hartley's whitening 
is made for the 2D point set rather than for the noise 
component of the data vector, its parameter vector estimation 
is still biased. In 'addition, its computation complex is a little 
higher. 
IV. IMPROVED GEVD (IGEVD) ELLIPSE FITTING 
In section 2, it is proved that the method to give an 
unbiased estimation is to whiten the noise component of data 
vector. But, the prerequisite that requires its mean and 
variance must be known a priori is very difficult to he 
satisfied in practice. 
In order to overcome this difficulty, we firstly centralize 
the data vector. i.e. ( 1 ) so that 
(28) 
where 
and 0, = [al 
centralization operation. It is also concluded that 
where the above bar sign represents averaging operation. 
Based on (28), the fitting error for the centralized data is 
.,I' , in which the superscript " " denotes 
a, = -(filrel +D,~o, )  (29) 
(30) 
= e , k , , e ,  +ZO,TR,,O: +e;R,,e, 
where Rg=(kG,s,r),N . Thus, we have the minimized 
criterion function 
which is equivalent to the following cost function 
Matrix c,  must be chosen to make the constraint 
e,Tc,ez = 1 equivalent to the constraint e,'ce, = 1 , which 
means that we must chose 
'.I 
J]  =e:~,,e, +2elrqze, +e;I1,e, -&al -I) 
J, =e,% ,e, +B:R,@, +~,'RA -4e;ce2 -1) . 
c, = R , ~ ' R , , - ' C R , , - ' R ~ ~  (31) 
According to J ,  , it is easy to h o w  that the optimal 
estimation must be satisfied to 
R;,O, = ,lm,i.C202 (32) I e, = - R , , - ' R , ~ ~ >  
where R;, .. =R,:  - R , , ~ R , , - ' R , ,  is certainly not negative definite, 
Amin is the minimal generalized eigen value of R& and 
constraint matrix C, , 6 ,  is the corresponding vector. This 
procedure is just the suggested IGEVD algorithm. 
V. Statistical performance analysis of IGEVD 
It is easy to prove that on the condition that the noise 
contained in 2D point set b,} is supposed to be suit for the 
assumption of (7), the noise component of the centralized data 
vector has a zero mean vector and a block-diagonal variance 
matrix, that is 
where on? is the variance matrix of the noise np of the 2D 
point set ( p i )  . So, we can complete the pre-whitening 
operation on the "observation noise" of the centralized data 
vector by means of the pre-whitening transformation to the 
noise component contained in 6, , that is, let Gt =wt , 
where w, is a scaling factor. This results in mapping 
relationship 6, = w,R,w,' V i j  E {I 1,12,22), where W, = w21 , 
between the ACF matrixes of before- and after- 
transformation. Thus, $11 = ( w , ) ~  R;, is valid. It means that, 
after the noise pre-whitening process, IGEVD algorithm can 
still give an optimal solution of e2 which is the same as the 
original. Furthermore, after transformation, the optimal 
solution of 8, is 5, =-k1,-'6,& , thus the corresponding 
optimal solution of 0, is certainly equal to 
[G,=w$*  
- .  
- - -  e ,  =W,'e, =-W,'R,,-'R,,B, = - R , , - ' R , 2 6 2 .  
Therefore, without noise pre-whitening process and by 
directly using the computation of (32), the IGEVD can output 
the same optimal solution as the one given by the IGEVD 
based on the noise pre-whitening process. As the latter can 
output an unbiased estimation, the former can also output an 
unbiased estimation of the parameter vector. It is also shown 
that the condition number of R& is in the order of 0(1), so 
IGEVD is intrinsically able to regularize data. 
In general, the suggested IGEVD algorithm naturally has 
the ability to pre-whiten the data noise as well as to regularize 
the condition number of the data's ACF matrix on condition 
that no any pre-whitening or regularizing procedure is 
performed. Because it has the advantages given by both of 
noise pre-whitening and data regularization, it can output an 
unbiased estimation with very small error variance. In 
addition, the dimension number of parameter estimation is 
decreased from 6 to 2. So, IGEVD also has the advantages of 
fast computation and simplified realization. 
VI. COMPUTER SIMULATION EXPERIMENTS 
In order to analyze the practical performance of the 
developed algorithm by experiment, we analyze and compare 
the performances of SGEVD, HGEVD and IGEVD, by a 
great deal of computer simulations for the ellipse fitting, The 
experiment results presented here are for the typical ellipse 
example whose center is (Cx, C v ) =  (IO, 5), whose long and 
4184 
short half axis length pair is (a, b ) = ( 2 5 ,  12) and whose 
incline angle is a = 30” . 
The statistical performances of SGEVD, HGEVD and 
IGEVD for different noise strength in whole ellipse case or a 
half of ellipse case are shown in Table 1. Typical results fitted 
from the points belong to a half of the ellipse degraded by the 
additive noise with variance d=0.5 for SGEVD, HGEVD and 
lGEVD are displayed in Figure 1 in which all the reported 
numbers are the averaged relative error amplitudes . 
The experiments in all cases have demonstrated that both 
of IGEVD and HGEVD have much better perfo-rmances than 
the performances of SGEVD and that the performance of 
JGEVD is slightly better than or almost the same as 
HGEVD’s. The running speed of IGEVD is much faster than 
HGEVD speed, which has the same speed-order as SGEVD. 
From these facts, the advantages of the proposed IGEVD 
algorithm are obviously revealed. 
TABLE I. AVERAGE RELATIVE ERRORS 
Whole ellipse cases Half ellipse cases 
- , 
I c. I 0.0605 I 0.1344 I 0.3505 I 0.5536 I 0.5271 I 2.0161 I 
E 
.~ 
a 0.0307 0.0767 0.2055 0.4891 0.5043 0.0570 
V b 
a 
C, 
C,, 
I 
0.0051 0.0045 0.0533 0.5301 0.5483 0.6166 
0.1132 0.2258 0.4103 0.5005 0.4747 0.5548 
O.OOOI 0.0003 0.0003 0.0098 o.0070 0.0315 
o.onoi o.ooni o.oooi o.0006 O.OOOI 0.0064 
Fig 1. Typical fitting result 
REFERENCES 
[I]. Z:G. Yang, J:L. Qiao. C.-B. Zhao, X.-M. Luan, “A real-time 
underwater 3D vision system guiding A W  in underwater missions,” 
IEEEACCV’9S, Vol. 3 of3, pp. 111-509-11-513, Dec. 1995. 
[ZI. Z.-G. Yang, L L .  Qiao, C.-B. Zhao, X.-M. Luan, S.-H. Sun, “Robustly, 
exactly and real-timely extracting lines and conics from a gray-level 
image,”lEEElCNNSP’95, Vol. 11, pp. 11-1410-11-1413, Dec. 1995. 
[3]. Y. C. Cheng, S. C. Lee, “A new method for quadratic curve dctection 
using K-RANSAC with acceleration techniques,” Panem Recognilion 
1995. Vol. 28, No. 5,pp. 663-682. May 1995. 
[4]. C. F. Olson, “Constrained Hough transform for curve detection,” 
Compurer Yisiun & Image Underwondig 1999, Vol. 73, No. 3, pp. 329- 
345. 
[ 5 ] .  0. Chutampa, L. Guo, “A modified Hough vdnsform far line detection 
and its performance,” Parrern Recognilion, 1999, Vol. 32, No. 2, pp. 
181-192,Feb. 1999. 
[6]. N. Bennea, R. Burridge, N .  Sailo, “A melhod to detect and characterize 
ellipses using the Hough bansform.” IEEE Trons. Potrern Anolysir atid 
Mochinelntelligence 1999, Vol. 21, No. 7, pp. 652-657. July 1999. 
[7]. A. Fitrgibban, M. Pilu. R. B. Fisher, “Direct least square fining of 
ellipses.” IEEE Trans. Pollem Anolysis and Machine Inrelligence 1999, 
Vol. 21,No. 5, pp. 476480. May 1999. 
[SI. R. 1. Hartley, “In defense of eight-point algorithm,” IEEE Trms. Pollern 
Ano1,wis ond Machine Inlelligence 1997, Vol. 19, No. 6: pp. 580-593, 
lune 1997. 
E 
4185 
a 0.0040 0.0076 0.0150 0.0669 0.0861 0.0509 
b 0.0038 n.0060 0.0127 0.0876 0.1464 0.2186 
a n.0014 0.0034 0.0071 0.0239 0.0442 0.0285 

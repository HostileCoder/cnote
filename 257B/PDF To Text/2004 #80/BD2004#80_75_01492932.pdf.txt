Using Distributed Memory Dynamically in a Cluster of PVM-Based Linux 
Workstations 
OP Gupta', Kiran Gupta' and Karanjeet Kalilon2 
PunjnO Agricultzn-al University, Lridhiana, INDIA. 
Gwvu Nanuk Dev University, Amritsar, INDIA.. 
opgenizis@yahoo.com, kirangl23@lycos.com, karanvkuhlon@yahoo. corn 
I 
2 
Abstract 
Pauaiiel Virtual Machine (PVW has been idenlified as 
an efective s-vstem for heterogeneou~ computing. In this 
paper wr investigate the performance of a 
hetet-ogmeom disrt-ibuteu' system based on PVh[ where 
the cornpuling nodes arc connected each orher through 
sever-a1 levels qf cummicnication hierarchy and have 
d @ m t  sprc{fificatiuns. Mutrix Compulutiuri is tulien us 
sample case ,fur. ckeckirzg cite COSI crnd pe<fimwiice qf 
(he heleroyeneuris PC oiusier. Exper.imeni s h o w  thut 
Daiu size nerds to be us big as possible and 
cammunicatioii cost increases wilh increuse in nuniber 
of hosts for /;xed size problem. Heierngsneorcs PC 
cltisfw which is cost efeciive und psrfom"ce 
orieiited, can used in proviifing quality education and 
veseairh for  Meother forecastirig , image processing 
unci iiruriy Firom areus ofsrieritl$c corrrputiprg. 
Kewords: P VM, MPP, Matrix Conzp~itatin~i. Cliisfi.v 
compl/iing. 
1. Introduction 
In the quality education and research era, the need 
for faster computers is driven by the demands of both 
data-intensive applications in commcrce and 
computational intensive applications in science and 
Engineering. Components of high performance parallel 
computers are evolving from proprietary parts e.g. 
CPUs disks and memories, into commodity parts. This 
is because technologies such commodity parts have 
matured enough to be used for high end compute 
systems. Another important trend changing the face of 
computing i s  an cnonnous increase in the capabilities of 
the networks that connects computers. These trends 
make it feasible to develop applications that use 
physically distributed resources as if they were part of 
the some computer. Computing on networked 
computers (also called Distributed computing' i s  not 
just a subfield of parallel computing, but it is deeply 
concerned witk problems such as reliability securely 
and heterogeneity. 
Parallel and distributed processing has merits in 
pcrformance and cost. Here the two widely employed 
paradigms arc Massively Parallel Processors (MPP) 
and Heterogeneous Computing (HC) [ I ,  21. Even 
though MPP run much faster than HC, i t  costs more 
while the tatter costs far less. There exists a clear 
tradeoff between thc two computing paradigms with 
respect to the performance and cost. Under this 
condition, Parallel Virtual Machine (PVM) 171 was 
devclopcd for allowing efficient hcterogencous 
computing. PVM divides the problem into subtasks 
and assign each one to be executed on the processor 
architccture that is best suited for that subtask. 
PVM i s  based on message-passing model. 
Messages are passed between tasks over the 
connecting networks. The paradigm requires the user 
to partition the data among processor and specify 
interaction among processor explicitly. Of coursc, on 
a cluster of PCI WS connected via some high latency 
LAN, message-passing is the only rcasonablc to 
program them. 
Previous works on PVM [3, 71 have already 
identified its effectiveness. It has been used for 
solving various problems including material design, 
climate prediction, groundwater modeling, and so 
forth. I t  has also been found to be useful for solving 
large scale compute intensive problems. In these 
problems, matrix operations such as matrix 
multiplications and inverted indexing etc. are usually 
required to be solved. Using PVM for implementing 
these operations not only saves time, but also saves 
cost. 
The rest of the paper is organized as follows. 
Section 2 introduces the pilot system used. Section 3 
reveals about the parallel algorithm and data 
partitioning scheme. The experiment results are 
presented in Section 4 far various parameters. 
Section 5 provides a conchsion and the direction for 
the future research. 
0-7803-8680-9/04/$20.00 WO04 IEEE. R\IMIC 2004 
2. PC Cluster Pilot System 
111 earlier studies mostly a network o f  workstations 
was used as the main platform. Here the constituent 
nodes usually have the same (or similar) architectures 
and consequently same perfonnances. They are also 
located closely with direct Ethernet connection, and the 
perfonnanccs are measured while the system is used for 
only the problems being solved. The performance of 
such system will be greatly different from that of the 
systcin with the heterogeneous environment. Here the 
nodes are connected through several levels of 
communication nchvork hierarchy. and have quite 
different specifications. Thc loads of them also 
network environment is based on the conventional 
serial algorithm as given below. 
Procedure MAT-MULT (A, B, C) 
Begin 
For I = 0 to n-1 do 
Fnr j  =@ ton-1 do 
Begin 
C [ij]=O 
For k=@ to n-1 do 
C[i,j]=C[i,j]+AEi, 
kl * B I  i,k1 
End for 
End MAT-MULT 
Figure. 1 PC Cluster 
dynamicaliy change. 
For the cxperirncnt, the PVM system is 
implemented in four computer systems [Figure. 11 
whose performances are widely varying. TCPiIP is used 
as a communication protocol. We also study the 
communication performance of PVM by measuring the 
traiismission time of various size data. 
It reveals that the larger the data, the more regular 
and efiicient the communication time is. 
3. Parallel Matrix Computation 
Matrix multiplication [6] i s  a building block in 
many matrix operations and for solving graph theory 
problems. Due to its fundamental importance in 
sciences and engineering, Personal 
computcriworkstation Clusters are promising 
candidates for friture high performance computers, 
because. of their good scalability and cost performance 
ratio. Thus investigating the scalability and 
performance of PCiWorkstations based on PVM for 
parallel matrix multiplication is meaninghi. 
The' parallel matrix multiplication algorithm that is 
taken for the implementation in the heterogeneous 
3.1 Parallel Implementation 
The concept that is used in parallel matrix 
computations [ I ,  21 is of block matrix operation. The 
matrix-multiply algorithm to calculate C = AB, 
where C, A, and B are all square matrices, also uses 
block matrix operations, where m x m tasks will be 
used to calculate the solution. Each task will calculate 
a sub block of the resulting matrix C. The block size 
and the value of m is given as a command line 
argument to the program. The matrices A aiid B are 
stored as blocks distributed aver the m' tasks The 
details of the algorithms at a high level is as follows: 
Given: 
i) A grid of m'x m tasks. 
ii) Each task (t where O c = i j  < m) initially contains 
blocks C ij, A ij, and B ij. 
Step I .  The tasks on the diagonal (t ;, where i = j)  
send their block A ,j to all the other tasks in row i .  
Step 2. After the transmission of A ii, all tasks 
calculate A ii x B i j  and add the result into C ij. 
Step 3. The column blocks of B are rotated. That is, t 
ij sends its block ofB to T (i - 1) j. (Task to, sends its 
B block to t (m- I )  j.) The tasks now return to the 
first step; 
Step 4. A is multicast to a11 other tasks in row i, 
and the algorithm continues. After m iterations the C 
matrix contains A x B, and the B matrix has been 
rotated back into place. 
4. Experiments and Observations 
In this section, we investigate the performance of 
a heterogeneous computing system based on PVM 
using the problem discussed in the earlier section and 
then study the communication speed of PVM systcm 
since it i s  a very important factor in deciding the 
overall performance. 
513 
4.1 Network View 
VLSIl,VLSI2,VLS13,VLSI4 
in reverse hosts cchoes the messagc back to tnaster 
Table 1 and 
0.618 I 11 
Table 1 VCSI1-VLSIZ 
The Network view displayed the present virtual 
machine configuration and the activity of the hosts. 
Computer icons illuininated in different colors to 
indicate their status in executing PVM tasks. From the 
network view [Fig11 we can see how well the virtual 
machine is being utilized by PVM application. If all the 
hosts are green most of the time, then machine 
utilization is good. 
4.2 Call Trace View 
I t  providcs a textual record of the last PVM call 
made in each task. The list of tasks is the same as in the 
Space -Time view. This view is useful as a call level 
debugger to identify where a PVM program's execution 
hangs 
4.3 Space - Time View 
The Space -Time view displayed the activities of 
individual PVM tasks that are running on the virtual 
machine. Listed on thc left-hand side of the view are 
the executable names o f  the tasks, preceded by the host 
they are running on. 
4.4 Communication Speed 
Communication speed is checked using VLSIl 
as the master and other hosts as slaves (VLS12, VLSI3, 
and VLSI4). As soon as the task identification (tid) 
from all the hosts is reached at master i.e. console i.e. 
conncction is established among the master and slaves. 
Master packs a message and sends to all other hosts and 
1 Size I Send time I Pack Time 1 
I I 
1 o3 1 0.28 1 0.391 
Table 2 show the time taken to it and to other 
slave hosts using varying mcssage sizes (lo3, lo4, 
105, 106). 
Table 2 VLSll -SELF 
Note from the tables that cominunication time vary a 
1 Send time 1 Pack Time 1 1 Size 
1 10' 0.122 I 0.660 I 
0.184 0.7 IO 
0.133 0.9 10 
0.147 0.998 
lot when the size of data is relative small. It became 
stable as the size of data increases. This shows that, 
for efficient communication, the data size needs to be 
big as possible. 
4.5 Computational Performance 
Computation pcrfomance is checked while 
problem size kept constant, while hosts are increased 
step by step. Time taken by the system to solve the 
problem is recorded in the table 3. 
Table 3 Computational Speed 
VLSll 
I I 0.534 I 1.29 VLSI I ,VLS12,VLS13 
5 I4 
These results are i n  conformity with our expectations 
since the inter-process communication overhead and 
synchronization overhead tends to increasingly 
dominate the overall execution rime with increasing no. 
of processors values for a given value of data size. 
Speed 
1 4  
1 2  
1 
08 
06 
0.4 
0.2 
0 
1 
1 2 3 4 
No of Hosts 
Figure 4: Speedometer 
5. Conclusions 
We have studied the performance of* heterogeneous 
distributed computing based on PVM on Linux cluster. 
PVM can cfficiently partition a job and distribute them 
to scveral hosts so that they can run i n  parallel. 
Experimental results showed khat 
To be communication c.hanne1 efficicnt, data 
size should as Iarge as possible. 
Number of hosts in the Linux cluster will 
depend on the size of the problem. 
Data partitioning should be done according to 
the number of hosts. 
Future research and study is open for the 
controlling the master process so that it should not be 
terminated abruptly when any one of hosts hangs or 
fails. 
References 
[ 11 Kumar, Vipin, Grama, Ananth, Ciupta, Anshul, Karypis, 
George. Introduction 10 Parallel Computbzg. Design nnd 
Amzlysis ofAlgoriihms. Addison Wesley, 1994. 
[2] M. Soch, J. Trdlicda, P.Turidik, PVM Cornpzitutionai 
Geonteiy, and Parallel Computing Course, Lecture 
Notes in Computer Science, Vol. 1156, Springer- Verlag, 
f 996. 
131 J. Tourifio, R.  Doallo, A PVM-Bused Library@ Spartle 
Matrix Facfori-utions. Lecture Notes in Computet 
Science, Vol. 1497, Springer-Verlag, pp.304-3 1 I ,  
1998. 
(41 Peter Carlin. Disfribuled L i w w  Algebra OH Nefwovks ,U{ 
Worksmriom. Thesis. California Institue of. 
Tcchnology, 1994. 
[5] Paul Gray, Alan Kmntz, Soeren Olesen, Vaidy 
Sunderam, Advances in fielerogeneom Network 
Conrputing, Lccturc Notcs in Comptitcr Scicncc, 
Vol.1497, Springer-Verlag, 1998. 
[e] ' Stewart G. W., Introduction to Matrix Computations, 
Academic Press Inc., SanDicgo California 1973. 
171 A.Geist, A. Beguclin, and V. Sunderam, PVM Parallel 
Virtual Machine ~ A User's Guide and Tutorial for 
Nctworkcd ParallcI Computing, MTT Prcss, 
Cambridge, MA, 1994. 
515 

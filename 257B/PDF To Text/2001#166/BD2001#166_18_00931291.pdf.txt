A Benchmark Evaluation of Network Intrusion Detection 
Sys terns1 
Terrence Champion 
Skaion Corporation 
5 1 Middlesex Street, Suite 114 
North Chelmsford, MA 0 1863 
tgc@sliaion.com 
(978) 251-3963 
Mary L. Denz 
Air Force Research Laboratory 
525 Brooks Road 
Rome, NY 13441-4505 
deiizm@rI. af.m il 
(315) 330-2030 
Abstract-This paper outlines a benchmarking method for 
quantifying the detection sensitivity of network intrusion 
detection systems (IDSs) on an important class of denial-of- 
service (DOS) attacks using the real-time testbed developed 
at AFRL. This effort involved the development of tunable 
attack insertion tools and reference s o h a r e  to measure the 
state of the victim. 
Two systems were evaluated, a representative signature- 
based commercial system and a DARPA research system 
that relies on statistically based techniques. 
The experiments tried to answer these questions: Are there 
regions of operation where the attack tool can degrade 
performance while escaping detection? Is there any added 
detection power using the research detection system over 
standard commercial practice? The results can be 
summarized as follows: the reference victim's performance 
can be degraded without detection; and the research system 
had a broader detection region than the commercial system. 
TABLE OF CONTENTS 
1. INTRODUCTION 
2. TESTOBJECTIVES 
3. 
4. TEST SETUPh'ROCEDURE 
5. TESTRESULTS 
6. CONCLUSIONS 
7. FURTHERRESEARCH 
DESCRIPTION OF SYSTEMS UNDER TEST 
1. INTRODUCTION 
An important class of network attacks that have emerged as 
a major threat over the past couple of years are denial-of- 
service (DOS) attacks that involve the use of multiple hosts. 
Such types of attacks are often called distributed 
coordinated attacks (DCAs). One reason they are potentially 
dangerous is because they can harness the power of many 
idle machines around the Internet. This kind of attack poses 
another problem for primitive intrusion detection systems 
(IDSs) since it comes from multiple sources the source and 
goal of the attack are obscured. Additionally, for a lot of 
attacks the critical factor on whether the attack succeeds or 
not is the aggregate attack rate, and the use of multiple 
machines allows the attacker to use per-host attack rates that 
fall below the threshold of most ID systems. 
In an attempt to do a more complete characterization of the 
capability of network-based ID systems to detect such 
attacks, we developed an attack injection tool which 
simulates a distributed coordinated attack. It can be varied 
both in terms of the attack rate and the number of 
participating attacking hosts. We also developed a reference 
tool that allows us to measure the degree to which an attack 
degrades performance on a reference victim. By fixing the 
degree the victim is degraded and varying the two attack 
parameters we can characterize the capability of a system to 
detect such attacks. 
We evaluated two systems using this method, one 
commercial and one research system funded by the Defense 
Advanced Research Projects Agency (DARPA). The 
commercial system is a representative network detector 
using signature-based detection, while the research system 
0-7803-6599-2/01/$10.00 Q 2001 IEEE 
This work was supported by Air Force Research Laboratory and DARpk 
6-2705 
relies on statistical based techniques. They each protected an 
FTP server on a Sun UltraSparc-1 running Solaris 2.5. 
2. TEST OBJECTIVES 
We wish to determine whether a research system provides a 
detection capability which can enhance the performance of 
the ACTDs managed by AFRL. Naturally, such a system 
would be considered a useful addition if it provided better 
overall performance, but would also be considered just as 
useful if it covered a portion of the attack space which 
commercial-off-the-shelf (COTS) and government-off-the- 
shelf systems previously left uncovered. 
Trying to measure the performance of intrusion detection 
systems is generally difficult, as illustrated by these issues: 
when a system declares an attack, has it detected the attack 
or misclassified an artifact of the attack? In either case, does 
it give enough information to be useful? In a similar vein, if 
a system does alarm on such an artifact, is that a false alarm, 
or do we give credit to the system for detecting the actual 
attack? With the increasing interest on automated response 
the quality of the detection is becoming increasingly 
important; therefore, while our primary objective is to 
quantify the detection boundaries of the systems under test, 
we include a discussion in the results which is somewhat 
subjective as to the quality of each system’s reports. To that 
end, given that a report was generated as a result of an 
attack, we can also ask the following questions: 
Were the proper host@) identified? 
Would the information be sufficient to allow an 
administrator to diagnose the real nature of the 
problem? 
If the intrusion detection system was coupled with a 
response component, would the ID system provide 
enough information to halt or slow down the 
attack? 
3. DESCRIPTION OF SYSTEMS UNDER TEST 
Ebayes 
The system we selected for evaluation is the network 
intrusion detection system Ebayes, developed by A1 Valdes 
and Keith Skinner at Stanford Research Institute (SRI) 
International [l]. We selected this system to evaluate 
because Ebayes has a sophisticated detection scheme and 
some correlation capabilities which make it an ideal system 
to test against this type of attack. 
Most intrusion detection systems try to detect attacks by 
looking for specific patterns characteristic of a particular 
attack and alarming when that pattern is recognized. These 
systems usually have a very limited capability to maintain 
state, to act on partial information about an attack, or to 
correlate events, and therefore they miss attacks that are 
simple variations on signatures they have defined. In 
addition many commercial systems alarm on hosts which try 
to connect to after the host has been brought down because 
the signature has been modeled on the after-effects of an 
attack and non-offending host generates the same kind of 
signature when it tries to connect to a host that’s offline. 
Rather than using simple signatures, Ebayes hypothesizes 
attack classes, and these hypotheses are updated as data 
comes in, until a winning hypothesis is chosen. The claim is 
that such a probabilistic approach has both the capability of 
using the knowledge of known attacks, like signature 
detectors, while having the ability to generalize about those 
attack classes. 
We used an attack that triggered Ebayes to label them 
ptable, dict, other-bad, and portsweep. The class ptabie 
refers to the “process table” attack described below. The 
class dict is the dictionary attack, where someone tries to 
brute force guess a user and password. A portsweep is a 
scan of a host by connecting to several ports to try to find 
out what ports are active. The last class, other-bad, seems 
to be a more general class that allows classification of 
malicious activity for which a signature is not known. 
Commercial Baseline 
The system we chose as the commercial baseline is ftom a 
leading vendor of COTS products. It is a signature-based 
network detector. Although it doesn’t have a specific 
signature for our attack (most vendors don’t), it has many 
network signatures for attacks that have many similar 
characteristics to our attack tool. 
4. TEST SETUP AND PROCEDURES 
The test setup consists of the following components: 1) 
Traffic generator, 2) Attack injection program, 3) Victim, 4) 
Reference monitor, and 5 )  the ID systems under test. 
TrafJic Generator 
The design of the test calls for the tests to be run both in the 
presence and absence of normal network trafk. To generate 
normal trflic we used the trflic generation system used in 
the 1999 AFlU evaluation 121. Using this system we were 
able to vary the traffic load in a controllable way that 
simulated the statistics and characteristics of normal network 
traffic. 
Victim Machine 
The victim machine for this test was an anonymous FTP 
server running on a Sun UltraSparc-1 using a Solaris 2.5 
operating system. While this is not a very powerful sewer, 
there is in reality only one attacking machine, so the scale of 
the attacking resources corresponds reasonably with the 
6-2706 
server resources. A server or server cluster on the Internet 
could face hundreds of attacking machines. 
Attack Injection Programs 
Since the objective of this test is to characterize the 
performance of network intrusion detection systems, the 
attack injection program had the following requirements: 
That it manifest attacks in some way upon the 
network; 
That the level of attack be scalable in a predictable 
way; and 
That the amount of degradation in the victim’s 
performance correlates well with the level of 
attack. 
The third requirement is important since it allows an 
evaluator to guage the consequences of missing an attack. 
While it is important to detect an attack after it brings a 
server down, we are interested in an IDS’S capability to 
detect an attack that has less than catastrophic consequences. 
The program we developed is based on a fairly well-known 
attack, called “process table” in the Lincoln Laboratory and 
AFRL 1998 evaluations. Our version of the attack works by 
assuming the unused IF’ addresses of the subnet on which the 
attacking host resides.. The attacker needs administrative 
control of the machine he’s on, since he’ll be writing raw 
packets out of the network interface. The attacking programs 
writes multiple syns and listens for the returning syn-ack. It 
then returns an ack, completing the connection and thereby 
taking up resources on the victim. This procedure uses very 
little of the capability of the attacking machine, so new 
connections can be opened up at a very high rate. 
If the attacker wants to avoid detection, he might want to set 
a low attack rate; however, there is a limit to how low the 
attack can be set. If the attack rate is too low the victim will 
start to release earlier connections before the attack has 
reached the critical point, and the attack will have no effect. 
Since it is the aggregate rate of multiple attacking machines 
that is important, the attacker can run the attack at low 
individual attack rates by assuming the identity of multiple 
machines. An attacker can assume the identity of as many 
unused IP addresses as are on the attacking machine’s 
subnet. Our program models that behavior. 
Although this attack is very effective against our victim 
server, the attack gets very noisy after the attack has 
succeeded, since it continues to fire packets onto the 
network. To counteract this effect we added a governor into 
the program that slows starting on new connections as SYN- 
ACKs cease to be returned. This causes the attack to let up 
after a certain point, but then the attack kicks back in as the 
server starts to return to normal. Every attack cycles through 
periods where it brings the host down, backs off and then 
starts up again as the server returns to normal. 
Reference Program 
The Reference software has very simple structure consisting 
of a program that creates a flag file, opens an FTP 
connection to the victim, performs a simple command (bye), 
exits the FTP connection, and finally deletes the flag file. A 
shell script loops continuously, launching the above 
program, counting the number of flag files (corresponding to 
the number of still open connections) and writing the 
number, along with the time, to a log file. After each loop 
the program sleeps for two seconds. Every two seconds, 
then, the shell script counts the number of hung connections 
between the reference program and the victim FTP server. 
Since the reference program interacted with the victim it had 
some impact on the experiment. In order to minimize this 
interaction the number of hung sessions was capped at 20, 
after which no new connections were attempted. Even with 
no attacks going on the reference program caused several 
connections waiting to close on the victim (and thus used up 
some resources on the victim); however, while running 
baseline tests of the reference program we found that it 
never in and of itself caused an open connection to be 
registered as “hung” due to server degradation on the victim. 
The fact that the reference program is opening up 
connections at a constant rate means that there will be 
several unanswered syns when the server is degraded, 
causing both systems on several runs to identify the 
reference host as the attacker. In fact, often this was the only 
report the commercial system made because the attacking 
program would back off after the victim was unavailable. 
Figure 1 shows the plot of three attacks as measured by the 
benchmarking tool. The attacks have aggregate attack rates 
of 4,2,  and 1.66 attacks per second. All three attacks decay 
back to 0 at about the 250 tick (representing 500 seconds 
into the attack). The higher rate attack disables the server for 
about 300 seconds, the midrange attack about 250 seconds, 
and the low level attack tops out at about 16 hung 
connections and is only active for about 100 seconds. 
One measure we’ve defined for the attack we call the 
virulence, which is simply the area under curve of one of 
these graphs. Figure 2 shows the average virulence for each 
condition for the tests that were run. Lines radiating from the 
rigin are lines of equal attack. Notice that the virulence 
drops off as line sweeps from the “Number of Hosts” axis to 
the “Seconds between attacks axis”. 
Evaluation Method 
We evaluated the two systems under test under 20 
conditions: where there were 10, 15, 30, 40, and 60 
attacking hosts, each with aggregate rates of 4, 3.33, 2, and 
6-2707 
1.66 attacks per second. In each condition we measured the 
percentage of attacking hosts detected. 
One might suppose that, since computers are machines, 
running a test once or twice under any given condition 
would be sufficient to evaluate the performance of an IDS 
for every such occurrence of that same condition. In reality 
evaluating an IDS under realistic conditions produces results 
that are highly variable due to factors such as the states of 
the attacking machine. In an effort to account for that 
variability we repeated each condition six times. 
We analyzed the variance on the resulting data sets to 
determine significant differences between the sets. By 
grouping together equivalent data sets, we attempted to map 
out the attack space into regions of like detection. 
5. TEST RESULTS 
Table 1 summarizes the results of the commercial system 
and the research system without added background noise. 
Notice that both systems had difficulty detecting the attack 
when the number of hosts were high; in general, however, 
Ebayes had much better detection over the range of the 
attack space. Since there did not seem to be much variation 
for the commercial system, we did not do extensive analysis 
on it. 
Commercial 
All the reports coming out of the commercial system were 
syn-flood declarations. The signature for declaring 
syn-flood seemed to be unanswered SYNs coming in at a 
rate that was above a threshold parameter. This seems to 
follow fiom the data for the following reason: while the 
attack was in progress but the server was still handling 
requests, no reports were made, but when the server was 
finally taken offline, the detector triggered on unanswered 
SYNs. If this is in fact the signature, then it is an incomplete 
definition and thus is prone to false alarms. Indeed, because 
of this signature the commercial system alarmed a high 
number of times against the reference monitor, whose 
regular attempts to make connections mimicked the 
syn-flood signature when the server was down. 
There seemed to be no regular pattern within the alarms as 
to which hosts would be flagged as attackers and which 
wouldn’t. In all but the most virulent attacks no more than 
one host was identified, and for that reason the detector 
would not have been useful in stopping the attack. 
Ebayes 
Table 2 gives the results of the analysis for the conditions 
run in the absence of background noise. Since there were no 
detections for any configuration containing 60 hosts, those 
conditions are not included in the analysis. There are several 
configurable parameters in Ebayes, and we are currently 
investigating if tuning these parameters can increase its 
detection range. 
Some results that are apparent from this table seemed 
counterintuitive. While we expected the detection rate to go 
down as the number of attacking hosts increased (because 
the lower frequency attacks can be used to obtain the same 
aggregate rate), we did not expect that attacks with more 
hosts that used the same frequency of attack per instance 
would exhibit lower detection rates. In addition, Ebayes 
consistently did better at detecting the conditions with 
aggregate attack rates of 3.33 attacks per second than 4 
attacks per second. We have not yet determined the reasons 
behind these phenomena. 
In Figure 3 we have divided the Ebayes data into three 
equivalence classes of interest, corresponding roughly to 
high, medium, and low regions of Ebayes detector 
performance. The best detection occurs near the origin, with 
good detection extending out in a finger along the attack line 
representing an attack rate of 3.33 attacks per second. Low 
attack detection rates occur around the outer part of that 
finger. Notice that some of the conditions fall into two 
classes because of the overlapping confidence intervals. 
In the areas where the detector’s performance was high, the 
primary attack class declared was ptable, which the 
underlying attack of the DCA is patterned after. Where the 
detector’s performance was moderate the more general 
attack classification, other-bad was declared. In the region 
where the detector performance was poor the classification 
dict was declared. The consistent declarations of other-bad 
in some regions seems to be evidence that Ebayes has some 
detection capability when the characteristics of the attack do 
not fit the original signature or hypothesis. 
Ebayes in general did better than the commercial system in 
reporting the extent of the attack. It reported 50% or more of 
the hosts in four of the attacks listed, and 25% or greater in 
ten of the attacks. 
6. CONCLUSIONS 
Our preliminaxy results provide some answers to the three 
fundamental questions we asked in designing the 
experiment. 
Are there regions of operation where an attack can degrade 
performance while escaping detection? 
For both systems there were regions in which the attack 
could be executed, degrading the server and even taking it 
off line for long periods of time, and still not be detected. 
Ebayes identified a significant fraction of the attacks for 
which the individual attack rate was below fifteen seconds, 
but missed all the attacks above fifteen seconds. Ebayes has 
several configurable options, and it is possible that there are 
settings that will extend the coverage. We plan to do future 
6-2708 
experiments to find the widest possible coverage against this 
attack. 
The commercial system’s reporting was a little more 
random, but in general it seemed to do best in the more 
aggressive attacks where the server had been taken offline. 
In all cases it reported the attack after the attack had 
achieved its goal. This system also has some settable 
options, although its configuration capability is limited. We 
will also run some experiments to try to maximize its 
coverage. 
Is there any added detection power using the research 
detection system over standard commercial practice? 
There does seem to be a significant region for which the 
research system detects the attack and the commercial 
system doesn’t. 
When an attack was identified by a system we asked these 
additional questions. 
Were the proper host@) identified? 
In all detections the commercial system just identified one or 
two hosts, often mislabeling the reference host as an 
attacker. Ebayes did much better in general. Although the 
fraction of hosts it identified varied across the attack space, 
it rarely misidentified the reference program as an attacker. 
Would the information be suficient to allow an 
administrator to diagnose the real nature of the problem? 
Ebayes certainly provided more diagnostic information than 
the commercial system. In a majority of cases the multiple 
reports fiom Ebayes could alert an administrator that a 
distributed coordinated attack was underway. 
If the intrusion detection system was coupled with a 
response component, would the ID system provide enough 
information to halt or slow down the attack? 
In our opinion neither of these systems would have 
prevented an attack. Ebayes in many cases could have 
blocked enough of the attack to protect the server fiom 
being completely disabled. 
7. FURTHER RESEARCH 
Currently we are planning to extend this work in three 
directions: 
In addition, we plan to explore the degree to which the 
variability in detector performance generalizes to other 
detection systems. This can become a critical issue in the 
evaluation of these types of systems. We feel that no one to 
date has adequately accounted for the non-deterministic 
performance of intrusion detection systems. 
References 
[l] Champion, Terrence, and Durst, Steve. Air Force 
Intrusion Detection Environment 
http://www.raid-symposium.org/raid99, Section 5.  
[2] Valdes, Alfonso, and Skinner, Keith. Adaptive Model- 
Based Monitoring for Cyber Attack Detection, 
http://www.sdl.sri.com/emerald/adaptbnpaper/adaptbn. html. 
Authors 
Terrence Champion is a co-founder of Skaion Corporation. 
He has a B.S. in mathematics fiom Louisiana State 
University, a B.S.E.E. from the University of New Mexico, 
and an M.S.E.E. fiom the University of Massachusetts, 
Lowell. 
Mary Denz received a B.S. in Computer Science and an 
M.S. in Computer Informational Science from Syracuse 
University. She is a program manager within the Information 
Directorate at Air Force Research Laboratory (AFRL,/IF). 
1. Effect of background traffic ; 
2. Effect of tuning Ebayes on detection coverage; and 
3. Effect of using additional attack types. 
6-2709 
15 
.e 
Y 
10 
.* 2 
i3 c 
4-, 
0 
5 
0 
t 
f 
0 90 100 150 200 300 350 
Seconds x 2 
Figure 1. Number of hung connections for three different rates. 
Mapping of Attack Virulence 
onto Attack Space 
2500 30 1 I I 8 200Q 
.- 3 1500 
1000 
500 
0 
0 
F a = l . 6 6 a t t a c k g y q  2 0 
Fa=2 attadrdsec 4 
‘“OQ. 
F,=Aggregate Attack Rate 
Figure 2. Average virulence of attack at test data points. 
6-2710 
25 
20 
0 
I I I I I I 
HighDetechonRate 
- - - - Moderate DetechonRaie 
Low DetectimRate ............ . 
0 5 1 Q  15 28 25 3 8  35 48  45 50 
Number of hosts 
Figure 3. Regions of equivalent detection performance for Ebayes. 
6-2711 
Table 1. Commercial and research system performance with no background noise. 
Number of attacking 
hosts 
10 
1 0  
10  
1 0  
1 5  
Seconds Mean fraction Lower 95% Upper 95% confidence 
between attacks detected confidence bound bound 
2 . 5  0 . 7 0 0  0 .5466 0 .8534 
3 . 3 3  0 . 8 8 3  0 . 7 2 9 9  1 .037 
5 . 0  0 .517 0 .3633 0 . 6 7 0 1  
6 . 0  0 .367 0 .2133 0 . 5 2 0 1  
3 . 7 5  0 . 4 2 2  0 .2688 0 .5756 
15  
15  
1 5  
Table 2. Ebayes results without background traffic 
4.995 0 . 7 0 0  0 . 5 4 6 6  0 .8534 
7 . 5  0 .567 0 .4133 0 . 7 2 0 1  
9 . 0  0 . 1 4 4  0 . 0  0.2978 
6-2712 
